<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>11 Timestamp Ordering Concurrency Control</title>
      <link href="/2023/08/20/Courses/15445/11-Timestamp-Ordering-Concurrency-Control/"/>
      <url>/2023/08/20/Courses/15445/11-Timestamp-Ordering-Concurrency-Control/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="T-O-Protocols"><a href="#T-O-Protocols" class="headerlink" title="T/O Protocols"></a>T/O Protocols</h1><h2 id="What-is-the-difference-between-2PL-and-T-O"><a href="#What-is-the-difference-between-2PL-and-T-O" class="headerlink" title="What is the difference between 2PL and T/O?"></a>What is the difference between 2PL and T/O?</h2><ol><li>2PL determine serializability order of conflicting operations at runtime while transactions execute while T/O determine serializability order of transactions before they execute. <ul><li>If $TS(T_i) &lt; TS(T_j)$, then the DBMS must ensure that the execution schedule is equivalent to a serial schedule where $T_i$ appears before $T_j$. </li><li>Different schemes assign timestamps at different times during the transaction. </li><li>Timestamps can be implemented by different strategies: system/wall clock, logical counter, or hybrid. </li></ul></li><li>2PL is in a manner of pessimistic way. It assumes that conflicts between transactions are very common. <ul><li>Hence it uses locks to prevent conflicts. </li></ul></li><li>Timestamp  ordering (T/O) is a more optimistic way. It assumes that conflicts between transactions are rare. <ul><li>Hence it allows each transaction to execute all operations they want and validates their legitimate after each transaction committing and before applying anything to main database. </li></ul></li></ol><h2 id="Basic-T-O-protocol"><a href="#Basic-T-O-protocol" class="headerlink" title="Basic T/O protocol"></a>Basic T/O protocol</h2><h3 id="What-is-the-main-idea-of-basic-T-O-protocol"><a href="#What-is-the-main-idea-of-basic-T-O-protocol" class="headerlink" title="What is the main idea of basic T/O protocol?"></a>What is the main idea of basic T/O protocol?</h3><ol><li>Txns read and write objects without locks.</li><li>The timestamp of each transaction is assigned at the <code>BEGIN</code> command. </li><li>Every object $X$ is tagged with timestamp of the last transaction that successfully did read/write<ul><li>$W-TS(X)$: Write timestamp on $X$</li><li>$R-TS(X)$: Read timestamp on $X$</li></ul></li><li>Check timestamps for every operation. If transaction tries to access an object “from the future”, it aborts and restarts. </li></ol><h3 id="How-does-basic-T-O-check-each-operation"><a href="#How-does-basic-T-O-check-each-operation" class="headerlink" title="How does basic T/O check each operation?"></a>How does basic T/O check each operation?</h3><ol><li>When $T_i$ wants to read $X$:<ul><li>If $TS(T_i)&lt;W-TS(X)$, abort $T_i$ and restart it with a new TS to prevent it from starvation. <ul><li>This condition means that this $T_i$ is trying to read something from the future. </li></ul></li><li>Else, allow $T_i$ to read $X$, and update $R-TS(X)$ to $max(R-TS(X), TS(T_i))$. </li><li>A local copy of $X$ is made to ensure repeatable reads for $T_i$. </li></ul></li><li>When $T_i$ wants to write $X$:<ul><li>If $TS(T_i)&lt;R-TS(X)$ or $TS(T_i)&lt;W-TS(X)$, abort and restart $T_i$. <ul><li>The first condition means that another transaction from the future cannot see the write from $T_i$ in the past. </li><li>The second condition means that another transaction from the future already wrote this object and $T_i$ cannot overwrite it in the past. </li></ul></li><li>Else, allow $T_i$ to write $X$ and update $W-TS(X)$. </li><li>Also, a local copy is made. </li></ul></li></ol><h3 id="Can-we-optimize-the-write-rule-to-decrease-possibility-of-abort"><a href="#Can-we-optimize-the-write-rule-to-decrease-possibility-of-abort" class="headerlink" title="Can we optimize the write rule to decrease possibility of abort?"></a>Can we optimize the write rule to decrease possibility of abort?</h3><ol><li>Thomas write rule: If $TS(T_i) &lt; W-TS(X)$, ignore the write to allow the transaction to continue executing without aborting. <ul><li>The thought is that we can see this violation as an immediate write from a future transaction right after the successful write from $T_i$. </li><li>The effects are similar, i.e. no one sees what does $T_i$ write. </li></ul></li><li>If $TS(T_i)&lt;R-TS(X)$, we still need to abort $T_i$. </li></ol><h3 id="What-is-the-issues-of-basic-T-O"><a href="#What-is-the-issues-of-basic-T-O" class="headerlink" title="What is the issues of basic T/O?"></a>What is the issues of basic T/O?</h3><ol><li>High overhead from copying data to transaction’s workspace and from updating timestamps. Every read requires the transaction to write to the database. </li><li>Long running transactions can get starved. The likelihood that a transaction will read something from a newer transaction increases. </li><li>If you assume that conflicts between transactions are rare and that most transactions are short-lived, then forcing transactions to acquire locks or update timestamps adds unnecessary overhead. </li></ol><h2 id="Optimistic-concurrency-control"><a href="#Optimistic-concurrency-control" class="headerlink" title="Optimistic concurrency control"></a>Optimistic concurrency control</h2><h3 id="What-is-the-main-idea-of-OCC"><a href="#What-is-the-main-idea-of-OCC" class="headerlink" title="What is the main idea of OCC?"></a>What is the main idea of OCC?</h3><ol><li>OCC assumes that the number of conflicts is low. Especially when: <ul><li>All transactions are read-only (ideal).</li><li>Txns access disjoint subsets of data.</li><li>The database is large and the workload is not skewed. </li></ul></li><li>The DBMS creates a private workspace for each transaction. <ul><li>Any object read is copied into workspace. Modifications are applied to workspace. </li><li>When a transaction commits, the DBMS compares workspace write set to see whether it conflicts with other transactions. </li><li>If there are no conflicts, the write set is installed into the “global” database. </li></ul></li><li>OCC has three phases:<ul><li>Read Phase: Track the read/write sets of transactions and store their writes in a private workspace, i.e. execution of transaction content. </li><li>Validation Phase: When a transaction commits, check whether it conflicts with other transactions. </li><li>Write Phase: If validation succeeds, apply private changes to database. Otherwise abort and restart the transaction. <ul><li>Serial Commits: Use a global latch to limit a single transaction to be in the Validation/Write phases at a time. </li><li>Parallel Commits: Use fine-grained write latches to support parallel Validation/Write phases. Txns acquire latches in primary key order to avoid deadlocks.</li></ul></li></ul></li></ol><h3 id="What-will-happen-in-validation-phase"><a href="#What-will-happen-in-validation-phase" class="headerlink" title="What will happen in validation phase?"></a>What will happen in validation phase?</h3><ol><li>When transaction $T_i$ invokes <code>COMMIT</code>, the DBMS checks if it conflicts with other transactions. <ul><li>The DBMS needs to guarantee only serializable schedules are permitted.</li><li>Checks other transactions for RW and WW conflicts and ensure that conflicts are in one direction (e.g., older→younger). </li></ul></li><li>There are two approaches to valid: <ul><li>In backward validation, check whether the committing transaction intersects its read/write sets with those of any transactions that have already committed.<br><img src="/imgs/15445/TO/backward.png" width="50%"></li><li>In forward validation, check whether the committing transaction intersects its read/write sets with any active transactions that have not yet committed.<br><img src="/imgs/15445/TO/forward.png" width="50%"></li></ul></li><li>Each transaction’s timestamp is assigned at the beginning of the validation phase. Check the timestamp ordering of the committing transaction with all other concerned transactions. When $TS(T_i)&lt;TS(T_j)$, there are only three cases: <ul><li>If $T_i$ completes all three phases before $T_j$ begins its execution. This just means that there is serial ordering. </li><li>If $T_i$ completes before $T_j$ starts its Write phase, then we require that $T_i$ does not write to any object read by $T_j$, i.e. $WriteSet(T_i)\cap ReadSet(T_j)=\empty$. <ul><li>At this condition, we can conclude that $T_j$ cannot see anythin written by $T_i$. Therefore, if $T_j$ read anything written by $T_i$, it is a violation. </li></ul></li><li>If $T_i$ completes its Read phase before $T_j$​ completes its Read phase, then we require that $T_i$ does not write to any object that is either read or written by $T_j$. <ul><li>$WriteSet(T_i) \cap ReadSet(T_j) = \empty$ and $WriteSet(T_i) \cap WriteSet(T_j) = \empty$. </li><li>Anything wrote by $T_i$ should be seen by $T_j$ and should not conflict with what $T_j$ intends to write. </li><li>OCC wants more than just serializable order. Similar with Thomas write rule, if we allow the write sets have something in common it still would be serializable, yet in conflict. </li></ul></li></ul></li></ol><h3 id="What-are-the-issues-of-OCC"><a href="#What-are-the-issues-of-OCC" class="headerlink" title="What are the issues of OCC?"></a>What are the issues of OCC?</h3><ol><li>High overhead for copying data locally. </li><li>Validation/Write phase bottlenecks.</li><li>Aborts are more wasteful than in 2PL because they only occur after a transaction has already executed.</li></ol><h1 id="The-phantom-problem"><a href="#The-phantom-problem" class="headerlink" title="The phantom problem"></a>The phantom problem</h1><h2 id="What-is-the-phantom-problem"><a href="#What-is-the-phantom-problem" class="headerlink" title="What is the phantom problem?"></a>What is the phantom problem?</h2><ol><li>In the above transaction management protocols, we assume that the total number  of tuples in a table is fixed, i.e. transactions will not execute insertion or deletion. </li><li>Insertion or deletions result in different results for the same range scan queries, e.g. count, maximum. <ul><li>The reason is that transactions can only lock on existing records and not one under way. </li></ul></li></ol><h2 id="How-can-we-solve-the-phantom-problem"><a href="#How-can-we-solve-the-phantom-problem" class="headerlink" title="How can we solve the phantom problem?"></a>How can we solve the phantom problem?</h2><ol><li>The first approach is to re-execute scans. <ul><li>The DBMS tracks the <code>WHERE</code> clause for all queries that the transaction executes. Retain the scan set for every range query in a transaction. </li><li>Upon commit, re-execute just the scan portion of each query and check whether it generates the same result. </li><li>This could double the execute time for all queries, which may be unacceptable. </li></ul></li><li>The second approach is by predicate locking. <ul><li>Shared lock on the predicate in a <code>WHERE</code> clause of a <code>SELECT</code> query. </li><li>Exclusive lock on the predicate in a <code>WHERE</code> clause of any <code>UPDATE</code>, <code>INSERT</code>, or <code>DELETE</code> query. </li><li>Prevent any query changing the result of locked predicate from executing. </li></ul></li><li>The third approach is by index locking. <ul><li>Key-value locks only cover a single existing key-value in an index, while gap locks cover those virtual keys for non-existent values. </li><li>Key-range locks takes multiple key-value locks and gap locks to lock on a range. </li><li>Hierarchical locking allows for a transaction to hold wider key-range locks with different locking modes to reduce the number of visits to lock manager. </li></ul></li><li>If there is no suitable index, then the transaction must obtain: <ul><li>A lock on every page in the table to prevent a record’s attributes from being changed to fit the predicates. </li><li>The lock for the table itself to prevent records fit the predicates from being added or deleted. </li></ul></li></ol><h2 id="What-are-isolation-levels"><a href="#What-are-isolation-levels" class="headerlink" title="What are isolation levels?"></a>What are isolation levels?</h2><ol><li>We may want to use a weaker level of consistency to improve scalability. </li><li>Provides for greater concurrency at the cost of exposing transactions to uncommitted changes: dirty reads, unrepeatable reads and phantom reads. </li><li>The four isolation levels are as shown below:<br><img src="/imgs/15445/TO/isolation.png" width="50%"></li><li>Each isolation level requires different locks to implement:<ul><li><code>SERIALIZABLE</code>: Obtain all locks first; plus index locks, plus strict 2PL.</li><li><code>REPEATABLE READS</code>: Same as above, but no index locks.</li><li><code>READ COMMITTED</code>: Same as above, but S locks are released immediately.</li><li><code>READ UNCOMMITTED</code>: Same as above but allows dirty reads (no S locks). </li></ul></li><li><code>SNAPSHOT ISOLATION</code> is another isolation supported by Oracle. <ul><li>It guarantees that all reads made in a transaction see a consistent snapshot of the database that existed at the time the transaction started.</li><li>A transaction will commit only if its writes do not conflict with any concurrent updates made since that snapshot.</li><li>Susceptible to write skew anomaly.</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Concurrency Control </tag>
            
            <tag> Transaction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>10 Two-Phase Locking</title>
      <link href="/2023/08/04/Courses/15445/10-Two-Phase-Locking/"/>
      <url>/2023/08/04/Courses/15445/10-Two-Phase-Locking/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Two-phase-Locking"><a href="#Two-phase-Locking" class="headerlink" title="Two-phase Locking"></a>Two-phase Locking</h1><h2 id="How-can-we-guarantee-serializable-without-knowing-the-entire-schedule-ahead-of-time"><a href="#How-can-we-guarantee-serializable-without-knowing-the-entire-schedule-ahead-of-time" class="headerlink" title="How can we guarantee serializable without knowing the entire schedule ahead of time?"></a>How can we guarantee serializable without knowing the entire schedule ahead of time?</h2><ol><li><p>We can use locks to protect database objects. </p><ul><li><p>When a transaction wants to access some objects, it needs to acquire locks of that objects from a centralized lock manager. </p></li><li><p>Locks are issued by applications and handled in lock manager while latches are issued and acquired locally. Hence acquiring locks are more expensive than acquiring latches even if locks are free. </p></li></ul></li><li><p>There are <code>S-LOCK</code> and <code>X-LOCK</code>. </p><ul><li><p><code>S-LOCKs</code> are shared locks for reads while <code>X-LOCKs</code> are exclusive locks for writes. </p></li><li><p>Their compatibility matrix is as followed: </p><p><img src="/imgs/15445/2pl/sx_comp_matrix.png" width="50%"></p></li></ul></li><li><p>Lock manager keeps track of what transaction hold what locks and what transactions are waiting to acquire any locks. </p><ul><li>When transactions request or upgrade locks, lock manager grants or blocks requests. When transactions release or downgrade locks, lock manager updates its internal lock-table. </li><li>Lock manager is responsible for detecting deadlock and choosing some transactions to kill. </li></ul></li></ol><h2 id="What-is-the-problem-of-releasing-locks-and-acquiring-it-later-again"><a href="#What-is-the-problem-of-releasing-locks-and-acquiring-it-later-again" class="headerlink" title="What is the problem of releasing locks and acquiring it later again?"></a>What is the problem of releasing locks and acquiring it later again?</h2><ol><li>This may cause in-consistent reads for a transaction when another transaction modified the object during lock is available. </li><li>This problem can be solved by two-phase locking. <ul><li>The first phase is growing: <ul><li>Each transaction requests or upgrades the locks that it needs from the DBMS’s lock manager. The lock manager grants/denies lock requests.</li></ul></li><li>The second phase is shrinking: <ul><li>The transaction is allowed to only release or downgrade locks that it previously acquired. It cannot acquire new locks. </li></ul></li></ul></li><li>Two-phase locking on its own is sufficient to guarantee conflict serializability because it generates schedules whose precedence graph is acyclic.</li></ol><h2 id="What-is-the-problem-of-two-phase-locking"><a href="#What-is-the-problem-of-two-phase-locking" class="headerlink" title="What is the problem of two-phase locking?"></a>What is the problem of two-phase locking?</h2><ol><li>It is subject to cascading aborts caused by dirty reads. <ul><li>When a transaction modified an object, and released the lock before it is aborted, the modified object is exposed to other transactions. </li><li>When the modifier is aborted, all other transactions that have used the modified object needs to abort. </li></ul></li><li>This can be solved by strong strict two-phase locking (rigorous two-phase locking). <ul><li>The transaction is only allowed to release locks after it has ended, i.e., committed or aborted. </li></ul></li><li>A schedule is strict if a value written by a transaction is not read or overwritten by other transactions until that transaction finishes. <ul><li>Its advantages are that it does not incurcascading aborts, and aborted transactions can be undone by just restoring original values of modified tuples. </li><li>However, it allows only conflict serializable schedules, but it is often stronger than needed for some apps. Most DBMSs prefer correctness before performance. </li></ul></li></ol><h1 id="Deadlock"><a href="#Deadlock" class="headerlink" title="Deadlock"></a>Deadlock</h1><h2 id="How-to-detect-and-resolve-deadlocks"><a href="#How-to-detect-and-resolve-deadlocks" class="headerlink" title="How to detect and resolve deadlocks?"></a>How to detect and resolve deadlocks?</h2><ol><li>The two-phase locking may lead to deadlocks. </li><li>The DBMS creates a waits-for graph to keep track of what locks each transaction is waiting to acquire<ul><li>Nodes are transactions. Edge from $T_i$ to $T_j$ if $T_i$ is waiting for $T_j$ to release a lock. </li><li>The system periodically checks for cycles in waits- for graph and then decides how to break it. </li></ul></li><li>When the DBMS detects a deadlock, it will select a “victim” transaction to rollback to break the cycle. <ul><li>The victim transaction will either restart or abort (more common) depending on how it was invoked. </li><li>There is a trade-off between the frequency of checking for deadlocks and how long transactions wait before deadlocks are broken. </li></ul></li><li>Selecting the proper victim depends on a lot of different variables<ul><li>By age (lowest timestamp)</li><li>By progress (least/most queries executed)</li><li>By the number of items already locked</li><li>By the number of transactions that we have to rollback with it</li><li>We also should consider the # of times a transaction has been restarted in the past to prevent starvation. </li></ul></li><li>After selecting a victim transaction to abort, the DBMS can also decide on how far to rollback the transaction’s changes. <ul><li>The first approach is to rollback entire transaction and tell the application it was aborted. </li><li>The second approach is rolling back a portion of a transaction to break deadlock and then attempts to re-execute the undone queries. </li></ul></li></ol><h2 id="How-to-prevent-deadlocks"><a href="#How-to-prevent-deadlocks" class="headerlink" title="How to prevent deadlocks?"></a>How to prevent deadlocks?</h2><ol><li>When a transaction tries to acquire a lock that is held by another transaction, the DBMS kills one of them to prevent a deadlock. </li><li>Assign priorities based on timestamps, e.g older timestamp means higher priority. </li><li>The first kind of rule is Wait-Die (“Old Waits for Young”)<ul><li>If requesting transaction has higher priority than holding transaction, then requesting transaction waits for holding transaction. Otherwise requesting transaction aborts. </li></ul></li><li>The second kind of rule is Wound-Wait (“Young Waits for Old”)<ul><li>If requesting transaction has higher priority than holding transaction, then holding transaction aborts and releases lock. Otherwise requesting transaction waits. </li></ul></li><li>In this case, only one “type” of direction allowed when waiting for a lock. </li><li>When a transaction restarts, its new priority is still its original timestamp to prevent it from getting starved for resources like an old man at a corrupt senior center. </li></ol><h1 id="Lock-granularity"><a href="#Lock-granularity" class="headerlink" title="Lock granularity"></a>Lock granularity</h1><h2 id="What-are-the-database-objects"><a href="#What-are-the-database-objects" class="headerlink" title="What are the database objects?"></a>What are the database objects?</h2><ol><li>It can be attributes, tuples, pages, tables depending on the lock granularity. </li><li>The trade-off is between parallelism versus overhead of requesting and lock manager processing. </li><li>In a hierachical lock scheme, the objects from top-layer to lower-layer are database, table, page, tuple, attribute. </li></ol><h2 id="How-to-support-multiple-granularities"><a href="#How-to-support-multiple-granularities" class="headerlink" title="How to support multiple granularities?"></a>How to support multiple granularities?</h2><ol><li><p>With only <code>S-LOCK</code> and <code>X-LOCK</code>, we have to check the locks of all children when we try to lock a higher-level node. </p></li><li><p>An intention lock allows a higher-level node to be locked in shared or exclusive mode without having to check all descendent nodes. </p></li><li><p>If a node is locked in an intention mode, then some transaction is doing explicit locking at a lower level in the tree. </p><ul><li><p>Intention-Shared (<code>IS</code>) indicates explicit locking at lower level with shared locks. </p></li><li><p>Intention-Exclusive (<code>IX</code>) indicates explicit locking at lower level with exclusive locks. </p></li><li><p>Shared+Intention-Exclusive (<code>SIX</code>) indicates that the subtree rooted by that node is locked explicitly in shared mode and explicit locking is being done at a lower level with exclusive-mode locks. </p></li><li><p>Their compatibility matrix is as followed: </p><p><img src="/imgs/15445/2pl/intention.png" width="50%"></p></li></ul></li><li><p>Each transaction obtains appropriate lock at highest level of the database hierarchy. </p><ul><li>To get <code>S</code> or <code>IS</code> lock on a node, the transaction must hold at least <code>IS</code> on parent node. </li><li>To get <code>X</code>, <code>IX</code>, or <code>SIX</code> on a node, must hold at least <code>IX</code> on parent node. </li></ul></li><li><p>Multiple lock granularities is shown in the <code>S</code>, <code>X</code>, <code>SIX</code> locks on higher-level objects. </p><ul><li>Intention-Shared (<code>IS</code>): Intent to get <code>S</code> lock(s) at finer granularity. </li><li>Intention-Exclusive (<code>IX</code>): Intent to get <code>X</code> lock(s) at finer granularity. </li><li>Shared+Intention-Exclusive (<code>SIX</code>): Like <code>S</code> and <code>IX</code> at the same time. </li></ul></li></ol><h2 id="How-to-use-locks"><a href="#How-to-use-locks" class="headerlink" title="How to use locks?"></a>How to use locks?</h2><ol><li>Applications typically don’t acquire a transaction’s locks manually (i.e., explicit SQL commands). <ul><li>Sometimes you need to provide the DBMS with hints to help it to improve concurrency. </li><li>Explicit locks are also useful when doing major changes to the database. </li></ul></li><li>Lock escalation: The DBMS can automatically switch to coarser- grained locks when a transaction acquires too many low-level locks. This reduces the number of requests that the lock manager must process. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Concurrency Control </tag>
            
            <tag> Transaction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Project #3: Query Execution</title>
      <link href="/2023/08/02/OpenSource/BusTub/Project-3-Query-Execution/"/>
      <url>/2023/08/02/OpenSource/BusTub/Project-3-Query-Execution/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Executors"><a href="#Executors" class="headerlink" title="Executors"></a>Executors</h1><h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><ol><li>The planner for each operation stores the necessary information to calculate the correct output. </li><li>The executor class for each operation stores the corresponding plan and other information for it to determine what tuple will be returned. <ul><li>All executors need to override the <code>Init()</code> and <code>Next(Tuple *tuple, RID *rid)</code>. </li><li><code>Init()</code> is used to initialize the information to determine what tuple will be returned. It is separated from the constructor because its parent executor may need to fetch the tuples of the current executor several times. </li></ul></li><li>The <code>ExecutorContext</code> in each executor stores the metadata of the database system, including catalog, buffer pool manager. <ul><li>Catalog maintains the tables and indexes in the current database. </li><li>We can register a new table or index through catalog or acquire metadata of a table (<code>TableInfo</code>) or index (<code>IndexInfo</code>) from catalog. </li></ul></li><li>The <code>TableInfo</code> stores the name, OID, schema of the table, and a pointer of <code>TableHeap</code>. <ul><li>We can manipulate a table through its <code>TableHeap</code>. </li><li>The <code>TableHeap</code> provides methods to insert or get tuples and update or get tuple metadata. <ul><li>To delete a tuple, we just need to mark the <code>is_deleted_</code> flag in its metadata as <code>true</code>. </li><li>To update a tuple, we need to delete it first and insert the new updated tuple into the table again. </li><li>To iterate through the table, <code>TableHeap</code> also provided <code>MakeIterator()</code>. </li></ul></li></ul></li><li>The <code>IndexInfo</code> stores the name and OID of the index, the schema for the index key, the name of the table and a pointer of <code>Index</code>. <ul><li>We can manipulate an index through its <code>Index</code>. </li><li>The <code>Index</code> provides methods to insert or delete an entry from the index, and get metadata of the index. <ul><li>The metadata includes the names of the index and table, mapping relation between key schema and tuple schema, and the schema of the indexed key. </li><li>The <code>Index</code> is the abstract class of all kinds of index implementations. To use a specific known index implementation, we can <code>dynamic_cast</code> it. </li></ul></li></ul></li><li>This system support <code>ArithmeticExpression</code>, <code>ColumnValueExpression</code>, <code>ComparisonExpression</code>, <code>ConstantValueExpression</code>, <code>LogicExpression</code>, <code>StringExpression</code>. <ul><li>The <code>AbstractExpression</code> provides an <code>Evaluate</code> method to calculate desired <code>Value</code> according to provided one <code>Tuple</code> and <code>Schema</code>. <ul><li>The <code>ArithmeticExpression</code> only supports <code>PLUS</code> and <code>MINUS</code> operation of two <code>Value</code>s. </li><li>The <code>ColumnValueExpression</code> returns the <code>Value</code> of the designated column. </li><li>The <code>ComparisonExpression</code> supports the comparison result of two <code>Values</code>s of <code>equal, not equal, less, less or equal, greater, greater or equal</code>. </li><li>The <code>ConstantValueExpression</code> returns a single constant <code>Value</code>. </li><li>The <code>LogicExpression</code> supports the <code>AND/OR</code> of two <code>Value</code>s. </li><li>The <code>StringExpression</code> converts a string  to lower-case or upper-case. </li></ul></li><li>The <code>AbstractExpression</code> also provides an <code>EvaluateJoin</code> method to calculate the join condition of two <code>Tuple</code>s. They support the same functions as <code>Evaluation</code> except that they consider two tuples. </li><li><code>ArithmeticExpression</code>, <code>ComparisonExpression</code>, <code>LogicExpression</code> and <code>LogicExpression</code> may have child-expressions. During evaluation, they will perform child expressions first recursively. </li></ul></li></ol><h2 id="Scan"><a href="#Scan" class="headerlink" title="Scan"></a>Scan</h2><ol><li>In sequential scan, we only need to know which table to scan. <ul><li>The object ID and name of the table is stored in the planner. </li><li>In the <code>Init()</code>, the metadata of the table (<code>TableInfo</code>) is fetched from catalog and the corresponding iterator of the table is created. </li><li>In the <code>Next(Tuple *tuple, RID *rid)</code>, as long as the iterator is not at the end, we need to find the first tuple with <code>is_deleted_</code> being <code>false</code> and matching with <code>filter_predicate_</code>. </li></ul></li><li>In the index scan, we need to know which index to scan and which table to fetch the tuple. <ul><li>The OID of the index is stored in the planner while the name of the table is stored in the <code>IndexInfo</code>. </li><li>To fetch the iterator of the index, we need to cast the <code>Index</code> into the specific implementation. </li><li>Each iterator points to a paire of the key and the RecordID. We can fetch the tuple with the <code>GetTuple</code> of the <code>TableHeap</code>. </li><li>As long as the iterator is not at the end, we need to find the first tuple with <code>is_deleted_</code> being <code>false</code>. </li></ul></li></ol><h2 id="Modification"><a href="#Modification" class="headerlink" title="Modification"></a>Modification</h2><ol><li>In the modification executors, we will modify the table and all the associated indexes. <ul><li>The table OIDs are stored in corresponding planner, while we can fetch all indexes of that table with the name of the table in <code>TableInfo</code>. </li><li>The indexed keys can be acquired with the <code>KeyFromTuple</code> method of the <code>Tuple</code>. The stored values are RecordIDs. </li></ul></li><li>All the modification executors only return one row representing the number of modified tuples. Hence in one <code>Next</code> call, the executor should handle all the modification. <ul><li>When no modification is performed, we should also return one row of $0$. </li><li>To distinguish between no modification and modification already finished in last call, we should have a flag representing whether or not we have already modified the table. </li></ul></li><li>In the insert executor and delete executor, the tuples to insert or delete are acquired from child executor. </li><li>In the update planner, there is a target expression for each output column. <ul><li>After acquired the original tuple from the child executor, we can evaluate each output column with target expressions. </li><li>The expressions can be any kind of expressions here. </li><li>When updating indexes, it needs to first delete the old indexes first before insert updated indexes. </li></ul></li></ol><h2 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h2><ol><li>The aggregations are implemented with a hash table. <ul><li>The hash table hashes the aggregate keys to the aggregate result. </li><li>When insert a tuple into the hash table, it updates the aggregate result in it according to the aggregate function and the aggregate values from the tuple. </li><li>To use <code>std::hash</code>, the key is <code>AggregateKey</code> containing a vector of <code>Value</code> describing the group-by keys while the value is <code>AggregateValue</code> containing another vector of <code>Value</code> describing aggregate values. <ul><li>The <code>AggregateKey</code> needs to override <code>==</code> operator and provide <code>()</code> operator in <code>struct std::hash&lt;bustub::AggregateKey&gt;</code> to calculate the hash value of <code>AggregateKey</code>. </li></ul></li></ul></li><li>Aggregations are pipeline breakers. Hence the build phase could be performed in the <code>Init()</code> where all tuples from child executor are read and inserted into the hash table to calculate the outputs. <ul><li>If no tuple is inserted, the aggregation executor still need to return the default values for each aggregate function. </li><li>The aggregation planner has two vectors of <code>AbstractExpressionRef</code> to fetch the aggregate keys and aggregate values from tuples received from child executor for aggregate computation. They are all <code>ColumnExpression</code>. </li></ul></li><li>In the <code>Next(Tuple *tuple, RID *rid)</code>, we just need to iterate over the hash table and output a tuple combining the aggregate keys and aggregate results. </li></ol><h2 id="NestedLoopJoin"><a href="#NestedLoopJoin" class="headerlink" title="NestedLoopJoin"></a>NestedLoopJoin</h2><ol><li><p>The executor needs to iterate over left child executor and right child executor. </p><ul><li>When the right child executor has emitted all tuples, the join executor will try to fetch a new tuple from the left child executor and re-initialize the right child executor for the following comparison. </li><li>Since we do not always fetch from left child executor when the <code>Next</code> is called, we need to record the current left tuple when it is fetched. </li><li>To distinguish between the status of no more left tuples and have not fetched any left tuples yet, we can fetch the first left tuple in <code>Init()</code>. </li><li>To mark the status of no more left tuples, i.e. no more tuples to emit, we need a flag to record the status of left tuples, which is the returned value of the <code>Next</code> of left child executor. </li></ul></li><li><p>For left join, if an outer tuple does not match with any inner tuple, we still need to emit the concatenation of that outer tuple with all-null inner tuple. </p></li><li><p>The predicate expression of the executor can be either a <code>LogicExpression</code> or <code>ComparisonExpression</code>. </p></li></ol><h2 id="HashJoin"><a href="#HashJoin" class="headerlink" title="HashJoin"></a>HashJoin</h2><ol><li>Similar with aggregations, we need a hash table for the outer table. We define <code>JoinKey</code> and <code>JoinBucket</code> to hash. <ul><li>We store all tuples with the same values in the attributes of join condition. </li><li>To support left join, we also need to record whether one tuple is used. <ul><li>When a tuple is matched with some inner tuple, all tuples in the same bucket must also matched. Hence we only need to record the usage of each <code>JoinBucket</code>. </li><li>For left join, when there is no more right tuples, the executor still need to iterate through the hash table to see if any bucket is unused. </li></ul></li><li>Similar with aggregation, <code>HashJoin</code> need to build the hash table based on the outer table in <code>Init()</code>. </li></ul></li><li>Different with <code>NestedLoopJoin</code>, the plan of <code>HashJoin</code> only need to know how to fetch columns from each tuples to determine whether those tuples are matched. <ul><li>The expressions of the <code>HashJoin</code> planner are only <code>ColumnValueExpression</code>. </li></ul></li></ol><h2 id="Sort-amp-Top-N"><a href="#Sort-amp-Top-N" class="headerlink" title="Sort &amp; Top-N"></a>Sort &amp; Top-N</h2><ol><li>The compare function needs specific expressions to fetch designated columns from each tuple. <ul><li>Hence we cannot only implement a non-static member function or a function with expressions being one of the parameters. </li><li>We need to implement a structure with override <code>()</code> operator. The expressions are passed to the object in initialization. </li><li>For <code>priority_queue</code>, two nodes will be swapped when the comparison function returns true. </li></ul></li><li>In the <code>Init()</code>, we need to fetch all tuples from child executor and sort them. <ul><li>For Top-N executor, the heap size should not be larger than $N$. The heap after the <code>Init()</code> is the counter-order of the output order, hence we still need a stack to support output in <code>Next</code>. </li></ul></li></ol><h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><ol><li>When trying to optimize a plan, the root plan is passed to the optimizer and the optimizer will perform a post-root-order DFS of the plan tree. </li><li>Each optimizer tries to recognize the pattern they need to handle and produce a new plan when it is matched. </li><li>To optimize limit followed by sort into top-N, we just need to check whether the child of a limit plan is a sort. </li></ol><h2 id="Optimize-nested-loop-join"><a href="#Optimize-nested-loop-join" class="headerlink" title="Optimize nested loop join"></a>Optimize nested loop join</h2><ol><li>To optimize nested loop join with hash join, the optimizer need to find a nested loop join and separate all the conditions from <code>LogicExpression</code> into <code>ComparisonExpression</code>. <ul><li>If all the <code>ComparisonExpression</code> are <code>ComparisonType::Equal</code>, we can create a new plan of <code>HashJoin</code> with <code>left_key_expressions</code> and <code>right_key_expressions</code> extracted from those <code>ComparisonExpression</code>. </li></ul></li><li>We can push down predicates of nested loop join to reduce the complexity of join. <ul><li>If a predicate is the conjunctions of <code>ComparisonExpression</code>s, we can decompose it into basic <code>ComparisonExpression</code> to examine them one by one to determine whether they can be pushed down. </li><li>If the two sides of a <code>ComparisonExpression</code> are from two different tables, it cannot be pushed down. </li><li>To push down a <code>ComparisonExpression</code>, we need to ajust the expressions of the two sides. <ul><li></li></ul></li></ul></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>09 Concurrency Control</title>
      <link href="/2023/07/16/Courses/15445/09-Concurrency-Control/"/>
      <url>/2023/07/16/Courses/15445/09-Concurrency-Control/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Concurency-control-amp-recovery"><a href="#Concurency-control-amp-recovery" class="headerlink" title="Concurency control &amp; recovery"></a>Concurency control &amp; recovery</h1><h2 id="What-do-we-want-from-concurrency-control-amp-recovery"><a href="#What-do-we-want-from-concurrency-control-amp-recovery" class="headerlink" title="What do we want from concurrency control &amp; recovery?"></a>What do we want from concurrency control &amp; recovery?</h2><ol><li>The concurrency control is responsible for lost update problem. <ul><li>How can we avoid race conditions when updating records at the same time? </li><li>This involves the buffer pool manager layer, access methods layer and operator execution layer. </li></ul></li><li>Recovery is responsible for durability problems. <ul><li>How can we ensure the correct state in case of a power failure? </li><li>This involves the disk manager layer and buffer pool manager layer. </li></ul></li></ol><h2 id="How-does-applications-issue-changes-to-a-DBMS"><a href="#How-does-applications-issue-changes-to-a-DBMS" class="headerlink" title="How does applications issue changes to a DBMS?"></a>How does applications issue changes to a DBMS?</h2><ol><li>A transaction is the execution of a sequence of one or more operations (e.g., SQL queries) on a database to perform some higher-level function. <ul><li>Transaction is the basic unit of change in a DBMS. Partial transactions are not allowed. </li><li>A new transaction starts with the <code>BEGIN</code> command. </li><li>The transaction stops with either <code>COMMIT</code> or <code>ABORT</code>:<ul><li>If commit, the DBMS either saves all the transaction’s changes <strong>or aborts</strong> it.</li><li>If abort, all changes are undone so that it’s like as if the transaction never executed at all. </li><li>Abort can be either self-inflicted or caused by the DBMS.</li></ul></li></ul></li><li>In a strawman system, each transaction is executed one-by-one as  they arrive at the DBMS. <ul><li>Before a transaction starts, copy the entire database to a new file and make all changes to that file.  <ul><li>If the txn completes successfully, overwrite the original file with the new one. </li><li>If the txn fails, just remove the dirty copy. </li></ul></li><li>The problem of this system is that there is no concurrency and copying the entire database can be expensive if the it is large. </li></ul></li><li>Besides correctness and fairness, we also want the DBMS to allow concurrent execution of independent transactions to provide better utilization / throughput and increase response times to users. </li></ol><h2 id="What-does-the-DBMS-want-to-prevent-when-supporting-concurrency"><a href="#What-does-the-DBMS-want-to-prevent-when-supporting-concurrency" class="headerlink" title="What does the DBMS want to prevent when supporting concurrency?"></a>What does the DBMS want to prevent when supporting concurrency?</h2><ol><li>Arbitrary interleaving of operations can lead to temporary inconsistency and permanent inconsistency. <ul><li>Temporary inconsistency is unavoidable and it is fine as long as no other transactions can see it. </li><li>Permanent inconsistency is unacceptable. </li></ul></li><li>The DBMS is only concerned about what data is read/written from/to the database. Changes to the “outside world”, e.g. sending an email, are beyond the scope of the DBMS. </li></ol><h1 id="Correctness"><a href="#Correctness" class="headerlink" title="Correctness"></a>Correctness</h1><h2 id="What-is-the-correctness-criteria"><a href="#What-is-the-correctness-criteria" class="headerlink" title="What is the correctness criteria?"></a>What is the correctness criteria?</h2><ol><li><strong>Atomicity</strong>: All actions in transaction happen, or none happen, i.e. “all or nothing”. </li><li><strong>Consistency</strong>: If each transaction is consistent and the DB starts consistent, then it ends up consistent. </li><li><strong>Isolation</strong>: Execution of one transaction is isolated from that of other transactions. </li><li><strong>Durability</strong>: If a txn commits, its effects persist. </li></ol><h2 id="How-to-ensure-atomicity"><a href="#How-to-ensure-atomicity" class="headerlink" title="How to ensure atomicity?"></a>How to ensure atomicity?</h2><ol><li>The first approach is logging (Write Ahead Log / WAL). <ul><li>DBMS logs all actions so that it can undo the actions of aborted transactions. </li><li>Maintain undo records both in memory and on disk. </li><li>When the DBMS come back from a crash, it need to undo partial transactions according to the undo records. </li></ul></li><li>Another approach is shadow paging. <ul><li>DBMS makes copies of pages and txns make changes to those copies. </li><li>Only when the txn commits is the page made visible to others by modifying the pointer at directory. </li><li>It does not need extra operations when come back from crash. </li></ul></li></ol><h2 id="What-is-consistency"><a href="#What-is-consistency" class="headerlink" title="What is consistency?"></a>What is consistency?</h2><ol><li>At a high level, consisitency means the “world” represented by the database is logically correct. All questions (i.e., queries) that the application asks about the data will return logically correct results. </li><li>There are two notions of consistency<ul><li>Database consistency means that the database accurately models the real world and follows integrity constraints. <ul><li>Transactions in the future see the effects of transactions committed in the past inside of the database. </li><li>The designer of DBMS should maintain this consistency. </li></ul></li><li>Transaction consistency means that if the database is consistent before the transaction starts, it will also be consistent after. <ul><li>The application programmer is responsible for this consistency. The DBMS does not know the semantics of correctness. </li></ul></li></ul></li></ol><h2 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h2><h3 id="What-do-we-want-from-isolation"><a href="#What-do-we-want-from-isolation" class="headerlink" title="What do we want from isolation?"></a>What do we want from isolation?</h3><ol><li>Users submit txns, and each txn executes as if it was running by itself, i.e. it is executed in a strawman system where no other transaction is executing at the same time. </li><li>Isolation provides an easier programming model to reason about. </li><li>But the DBMS achieves concurrency by interleaving the actions (reads/writes of DB objects) of txns. Hence we need to schedule to interleave txns but still make it appear as if they ran one-at-a-time. </li><li>There is no guarantee that $T_1$ will execute before $T_2$ or vice-versa, if both are submitted together. The net effect must be equivalent to these two transactions running serially in some order.</li></ol><h3 id="How-to-decide-whether-a-schedule-matches-isolation"><a href="#How-to-decide-whether-a-schedule-matches-isolation" class="headerlink" title="How to decide whether a schedule matches isolation?"></a>How to decide whether a schedule matches isolation?</h3><ol><li>If the schedule is equivalent to some serial execution, we can consider it correct. </li><li>For any database state, if the effect of executing the first schedule is identical to the effect of executing the second schedule, we say these two schedules are equivalent. </li><li>A schedule is serializable schedule if it is equivalent to some serial execution of the transactions. <ul><li>If each transaction preserves consistency, every serializable schedule preserves consistency. </li></ul></li><li>There are two different levels of serializability: conflict serializability (most commonly used) and view serializability. </li></ol><h3 id="What-conflicts-do-we-want-to-prevent"><a href="#What-conflicts-do-we-want-to-prevent" class="headerlink" title="What conflicts do we want to prevent?"></a>What conflicts do we want to prevent?</h3><ol><li>Two operations conflict if: They are by different transactions, and they are on the same object while one of them is a write. </li><li>A read-write conflict will cause unrepeatable read. <ul><li>Transaction gets different values when reading the same object multiple times. </li><li>The conflict is between the write from one transaction and a repeated following read from another transaction, i.e. this is actually $read_1-write-read_2$ conflict where the conflict is between $write-read_2$. </li><li>If there is only one read, it is not a read-write conflict. The first read will never be a read-write conflict. </li></ul></li></ol><ul><li>A write-read conflict will cause dirty read. <ul><li>One transaction reads data written by another transaction that has not committed yet. </li><li>The problem will happen when the read transaction is commited before the write transaction aborts. </li><li>If the write transaction successfully commits, there is no problem. But we cannot know that when we commit the read transaction first. </li></ul></li><li>A write-write conflict will cause lost update. <ul><li>One transaction overwrites uncommitted data from another uncommitted transaction. </li><li>This may cause the result becoming combination of two partial transactions. </li><li>There is no problem when every data written by $T_2$ is the last write to that data, i.e. every data written by $T_2$ is not overwritten by $T_1$. The problem happens when $T_1$ overwrites some data of $T_2$ while $T_2$ overwrites some data of $T_1$. </li></ul></li></ul><h3 id="How-do-we-determine-whether-a-schedule-is-conflict-serializable"><a href="#How-do-we-determine-whether-a-schedule-is-conflict-serializable" class="headerlink" title="How do we determine whether a schedule is conflict serializable?"></a>How do we determine whether a schedule is conflict serializable?</h3><ol><li>When there are only two schedules: <ul><li>Two schedules are conflict equivalent if and only if they involve the same actions of the same transactions and every pair of conflicting actions is ordered the same way. </li><li>Schedule S is conflict serializable if S is conflict equivalent to some serial schedule. </li><li>We can transform S into a serial schedule by swapping consecutive non-conflicting operations of different transactions. </li></ul></li><li>For more schedules, we can use the dependency graphs. <ul><li>Create one node per txn in the graph. </li><li>Create an edge from $T_i$ to $T_j$ if an operation $O_i$ of $T_i$ conflicts with an<br>operation $O_j$ of $T_j$ and $O_i$ appears earlier in the schedule than $O_j$. </li><li>A schedule is conflict serializable if and only if its dependency graph is acyclic. </li></ul></li></ol><h3 id="How-do-we-determine-whether-a-schedule-is-view-serializable"><a href="#How-do-we-determine-whether-a-schedule-is-view-serializable" class="headerlink" title="How do we determine whether a schedule is view serializable?"></a>How do we determine whether a schedule is view serializable?</h3><ol><li>Schedules $S_1$ and $S_2$ are view equivalent if:<ul><li>If $T_1$ reads initial value of A in $S_1$, then $T_1$ also reads initial value of A in $S_2$. </li><li>If $T_1$ reads value of A written by $T_2$ in $S_1$, then $T_1$ also reads value of A written by $T_2$ in $S_2$. </li><li>If $T_1$ writes final value of A in $S_1$, then $T_1$ also writes final value of A in $S_2$. </li></ul></li><li>In a word, each transaction is different schedules read the same values written by the same transaction and at the final end of all transactions, all data are written by the same transaction in the same value. </li><li>View Serializability allows for (slightly) more schedules than Conflict Serializability does. Neither definition allows all serializable schedules. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Concurrency Control </tag>
            
            <tag> Transaction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>08 Query Planning &amp; Optimization</title>
      <link href="/2023/07/15/Courses/15445/08-Query-Planning-Optimization/"/>
      <url>/2023/07/15/Courses/15445/08-Query-Planning-Optimization/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Query-planning"><a href="#Query-planning" class="headerlink" title="Query planning"></a>Query planning</h1><h2 id="What-are-the-logical-plans-and-physical-plans"><a href="#What-are-the-logical-plans-and-physical-plans" class="headerlink" title="What are the logical plans and physical plans?"></a>What are the logical plans and physical plans?</h2><ol><li>The optimizer generates a mapping of a logical algebra expression to the optimal equivalent physical algebra expression. </li><li>Physical operators define a specific execution strategy using an access path, i.e. a specific algorithm. <ul><li>They depend on the physical format of the data that they process, i.e., sorting, compression. </li></ul></li></ol><h2 id="What-is-the-process-flow-of-query-execution"><a href="#What-is-the-process-flow-of-query-execution" class="headerlink" title="What is the process flow of query execution?"></a>What is the process flow of query execution?</h2><ol><li>The application connected to the database system and sends a SQL query, which may be rewritten to a different format in SQL rewriter. </li><li>The SQL string is parsed into tokens that make up the syntax tree. </li><li>The binder converts named objects in the syntax tree to internal identifiers by consulting the system catalog. </li><li>The binder emits a logical plan which may be fed to a tree rewriter for additional schema info. </li><li>The logical plan is given to the optimizer which selects the most efficient procedure to execute the plan. </li></ol><p><img src="/imgs/15445/Optimizer/architecture.png" width="50%"></p><h2 id="How-does-optimizer-work"><a href="#How-does-optimizer-work" class="headerlink" title="How does optimizer work?"></a>How does optimizer work?</h2><ol><li>One way is heuristics / rules. <ul><li>Rewrite the query to remove stupid / inefficient things. </li><li>These techniques may need to examine catalog, but they do not need to examine data. </li></ul></li><li>Another way is the cost-based search. <ul><li>We need to use a model to estimate the cost of executing a plan. </li><li>Enumerate multiple equivalent plans for a query and pick the one with the lowest cost. </li></ul></li></ol><h1 id="Heuristics-optimization"><a href="#Heuristics-optimization" class="headerlink" title="Heuristics optimization"></a>Heuristics optimization</h1><h2 id="How-should-we-optimize-logical-plans"><a href="#How-should-we-optimize-logical-plans" class="headerlink" title="How should we optimize logical plans?"></a>How should we optimize logical plans?</h2><ol><li>Split conjunctive predicates. <ul><li>Decompose predicates into their simplest forms to make it easier for the optimizer to move them around. </li></ul></li><li>Predicate pushdown<ul><li>Move the predicate to the lowest applicable point in the plan. </li></ul></li><li>Replace cartesian products with joins<ul><li>Replace all Cartesian Products with inner joins using the join predicates. </li></ul></li><li>Projection pushdown<ul><li>This is to eliminate redundant attributes before pipeline breakers to reduce materialization cost and the data passed aroung. </li></ul></li></ol><h2 id="How-should-we-optimize-for-nested-sub-queries"><a href="#How-should-we-optimize-for-nested-sub-queries" class="headerlink" title="How should we optimize for nested sub-queries?"></a>How should we optimize for nested sub-queries?</h2><ol><li>Rewrite to de-correlate and/or flatten them<ul><li>E.g. an <code>EXISTS</code> sub-query in <code>WHERE</code> clause may be rewrited as an inner-join. </li></ul></li><li>Decompose nested query and store result to temporary table. <ul><li>For those sub-queries uncorrelated with outer query, the optimizer breaks up queries into blocks and then concentrates on one block at a time. </li><li>Sub-queries are written to a temporary table that are discarded after the query finishes. </li></ul></li></ol><h2 id="How-can-we-rewrite-expression"><a href="#How-can-we-rewrite-expression" class="headerlink" title="How can we rewrite expression?"></a>How can we rewrite expression?</h2><ol><li>This is implemented using if/then/else clauses or a pattern-matching rule engine. <ul><li>Search for expressions that match a pattern. When a match is found, rewrite the expression. Halt if there are no more rules that match. </li></ul></li><li>One approach is replacing impossible or unnecessary predicates by false. </li><li>Another approach is merging predicates, e.g. numeric ranging predicates. </li></ol><h1 id="Cost-based-search"><a href="#Cost-based-search" class="headerlink" title="Cost-based search"></a>Cost-based search</h1><h2 id="Cost-estimation"><a href="#Cost-estimation" class="headerlink" title="Cost estimation"></a>Cost estimation</h2><h3 id="What-cost-do-we-care"><a href="#What-cost-do-we-care" class="headerlink" title="What cost do we care?"></a>What cost do we care?</h3><ol><li>Physical Costs<ul><li>Predict CPU cycles, I/O, cache misses, RAM consumption, network messages. </li><li>This cost depends heavily on hardware. </li></ul></li><li>Logical Costs<ul><li>Estimate output size per operator. </li><li>This cost is independent of the operator algorithm since algorithms are physical. </li><li>It need estimations for operator result sizes. </li></ul></li><li>Algorithmic Costs<ul><li>Mainly the complexity of the operator algorithm implementation. </li></ul></li><li>We may use a combination of multiple costs that are weighted by magic constant factors. <ul><li>Some assumptions is that processing a tuple in memory is $400\times$ faster than reading a tuple from disk, and sequential I/O is $4\times$ faster than random I/O. </li><li>Most commonly used cost is the combination of the physical costs and logical costs. </li></ul></li></ol><h3 id="How-do-DBMS-estimate-the-costs"><a href="#How-do-DBMS-estimate-the-costs" class="headerlink" title="How do DBMS estimate the costs?"></a>How do DBMS estimate the costs?</h3><ol><li>The DBMS stores internal statistics about tables, attributes, and indexes in its internal catalog. <ul><li>Different systems update them at different times. </li></ul></li><li>Then DBMS derives the <strong>selection cardinality</strong> (<strong>selectivity</strong>) of a predicate which is the fraction of tuples that qualify. </li><li>We can make some assumptions to estimate selectivity<ul><li>Uniform data: The distribution of values (except for the heavy hitters) is the same. May maintain a heavy hitter list that stores most common values and assume that the occurrence of the rest data is the same. </li><li>Independent predicates: The predicates on attributes are independent, i.e. the conjuction of predicates can result in multiplication or addition of probabilities. </li><li>Inclusion principle: The domain of join keys overlap such that each key in the inner relation will also exist in the outer table.</li><li>These assumptions may not be true. </li></ul></li></ol><h3 id="What-statistics-does-the-DBMS-maintain"><a href="#What-statistics-does-the-DBMS-maintain" class="headerlink" title="What statistics does the DBMS maintain?"></a>What statistics does the DBMS maintain?</h3><ol><li>Histograms: <ul><li>The naive and most accurate way is to maintain an occurrence count per value in a column. </li><li>Equi-width histograms maintain counts for a group of values. All buckets have the same width, i.e. the same number of values. </li><li>Equi-depth histograms vary the width of buckets so that the total number of occurrences for each bucket is roughly the same. </li><li>Equi-width or equi-depth histograms use the total count of a bucket dividing by the number of values in that bucket as the count of each values. </li></ul></li><li>Sketches: <ul><li>Probabilistic data structure that gives an approximate count for a given value. </li><li>Cost-model can replace histograms with sketches to improve its selectivity estimate accuracy. </li></ul></li><li>Sampling: <ul><li>DBMS maintains a small subset of each table that it then uses to evaluate expressions to compute selectivity. </li><li>The selectivity is estimated by running the same query on the sample table. </li><li>Sample table is updated when the underlying tables changes significantly. </li></ul></li></ol><h2 id="Query-optimization"><a href="#Query-optimization" class="headerlink" title="Query optimization"></a>Query optimization</h2><h3 id="How-do-we-perform-cost-based-optimization"><a href="#How-do-we-perform-cost-based-optimization" class="headerlink" title="How do we perform cost-based optimization?"></a>How do we perform cost-based optimization?</h3><ol><li>After performing rule-based rewriting, the DBMS will enumerate different plans for the query and estimate their costs. <ul><li>It chooses the best plan it has seen for the query after exhausting all plans or some timeout. </li><li>The time spent on search should be significantly smaller than the time of executing query. DBMS can set a time threshold to end search. </li></ul></li><li>DBMS mainly enumerates the access methods (sequential scan, binary search / clustered indexes, index scan) and evaluation ordering. </li><li>Query planning for OLTP queries is easy because they are <strong>sargable</strong> (Search Argument Able). <ul><li>It is usually just picking the best index. </li><li>Joins are almost always on foreign key relationships with a small cardinality. </li></ul></li><li>For multi-relation query planning, there are two choices. <ul><li>Bottom-up optimization: Start with nothing and then build up the plan to get to the outcome that you want. </li><li>Top-down optimization: Start with the outcome that you want, and then work down the tree to find the optimal plan that gets you to that goal. </li></ul></li></ol><h3 id="How-does-bottom-up-optimization-work"><a href="#How-does-bottom-up-optimization-work" class="headerlink" title="How does bottom-up optimization work?"></a>How does bottom-up optimization work?</h3><ol><li>Break query up into blocks and generate the logical operators for each block. For each logical operator, generate a set of physical operators that implement it. </li><li>The whole diagram can be layered by relations or temporary relations, i.e. results of logical operators. </li><li>We can visualize the whole optimization diagram as a tree with different layers. <ul><li>The top layer is the output of the query and the bottom layer is all the relations. </li><li>The middle layers are the enumerations of different ordering. Each layer only performs one more operator than its last layer. </li><li>Hence each pair of layers is connected with an undetermined physical operator. </li></ul></li><li>From bottom layer up, we enumarate the possible physical algorithm of each logical operator. <ul><li>Then estimate the cost of all possible physical algorithms. </li><li>Leave only the more efficient physical algorithm for each logical operator after comare with only the possible physical algorithms of the same logical operator. </li></ul></li><li>When reaches the top layer, we can determine the most efficient path of all possible paths. </li><li>Then iteratively construct a “left-deep” join tree that minimizes the estimated amount of work to execute the plan. <ul><li>Generate a left-deep tree is to take advantages of pipeline. </li></ul></li></ol><h3 id="How-does-top-down-optimization-work"><a href="#How-does-top-down-optimization-work" class="headerlink" title="How does top-down optimization work?"></a>How does top-down optimization work?</h3><ol><li>Start with a logical plan of what we want the query to be. </li><li>Perform a branch-and-bound search to traverse the plan tree by converting logical operators into physical operators. <ul><li>When traversing from logical operator to logical operators, it is enumarating different ordering. </li><li>When traversing from logical operator to physical operators, it is enumarating different physical algorithm. </li><li>The layers are similar with bottom-up optimization. </li><li>When we meet a logical operator, we need to estimate the cost of its all possible physical algorithms. <ul><li>So for each physical algorithm, we need to go deeper until the bottom to calculate the estimation. </li><li>For the sub-logical-operators in the physical algorithm, we will enumarate its optimal execution in the lower levels. </li></ul></li><li>During the search, we can cut-off a branch if its cost is already more expensive then another branch we have already seen. </li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Executor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>07 Query Execution</title>
      <link href="/2023/07/12/Courses/15445/07-Query-Execution/"/>
      <url>/2023/07/12/Courses/15445/07-Query-Execution/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Sequential-Execution"><a href="#Sequential-Execution" class="headerlink" title="Sequential Execution"></a>Sequential Execution</h1><h2 id="Processing-model"><a href="#Processing-model" class="headerlink" title="Processing model"></a>Processing model</h2><h3 id="What-does-processing-model-do"><a href="#What-does-processing-model-do" class="headerlink" title="What does processing model do?"></a>What does processing model do?</h3><ol><li>Processing model defines how the system executes a query plan, i.e. how to traverse the query plan tree. </li><li>There are two processing directions: <ul><li>Top-to-Bottom: Start with the root and “pull” data up from its children. Tuples are always passed with function calls. </li><li>Bottom-to-Top: Start with leaf nodes and push data to their parents. Allows for tighter control of caches/registers in pipelines. </li></ul></li><li>There are three commonly used model: iterator model (volcano/pipeline model), materialization model and vectorized/batch model. </li></ol><h3 id="How-does-iterator-model-execute"><a href="#How-does-iterator-model-execute" class="headerlink" title="How does iterator model execute?"></a>How does iterator model execute?</h3><ol><li><p>In iterator model, operators are executed top-to-down. </p></li><li><p>Each query plan operator implements a <code>Next()</code> function. On each invocation, the operator returns either a single tuple or a <code>null</code> marker if there are no more tuples. </p><ul><li><p>The operator implements a loop that calls <code>Next()</code> on its children to retrieve their tuples and then process them. </p></li><li><p><code>Next()</code> is first called on the root operator and the nodes on the tree will call the <code>Next()</code> on their children recursively. </p></li></ul></li><li><p>This model allows for uple pipelining. Although some operators must block until their children emit all their tuples. </p><ul><li>Joins must wait until all tuples in the leaf child are processed and built the hash table to further process tuples from right child. Hence, the tuples from leaft child need to block the pipeline while the tuples from right child can enable pipeline. </li><li>Subqueries and ordering (<code>Order By</code>) clauses also need to block pipeline. These are called pipeline breaker. </li></ul></li><li><p>Output control works easily with this approach. We only need to add constraints on the root operator. </p></li></ol><h3 id="How-does-materialization-model-execute"><a href="#How-does-materialization-model-execute" class="headerlink" title="How does materialization model execute?"></a>How does materialization model execute?</h3><ol><li><p>In materialization model, operators are called bottom-to-up. </p></li><li><p>Each query plan operator implements a <code>Output()</code> function. </p><ul><li><p>On each invocation, the operator processes its input all at once and then emits its output all at once. </p></li><li><p>The operator materializes its output as a single result. </p></li><li>The operators can send either a materialized row or a single column. </li></ul></li><li><p>The DBMS can push down hints (e.g., <code>LIMIT</code>) to avoid scanning too many tuples. </p></li><li><p>The output can be either whole tuples (NSM) or subsets of columns (DSM). </p></li><li><p>This model is better for OLTP workloads because queries only access a small number of tuples at a time which means lower execution and coordination overhead and fewer function calls. </p><ul><li>It is not good for OLAP queries with large intermediate results. </li></ul></li></ol><h3 id="How-does-vectorization-model-execute"><a href="#How-does-vectorization-model-execute" class="headerlink" title="How does vectorization model execute?"></a>How does vectorization model execute?</h3><ol><li>The problem of iterator model is that it can only process one tuple at a time when we can take multiple tuples and vectorize them to process in parallel (SIMD). </li><li>Vectorization model is similar with iterator model except that each operator emits a batch of tuples instead of a single tuple. </li><li>This is ideal for OLAP queries because it greatly reduces the number of invocations per operator. <ul><li>It allows for operators to more easily use vectorized (SIMD) instructions to process batches of tuples. </li></ul></li></ol><h2 id="Access-methods"><a href="#Access-methods" class="headerlink" title="Access methods"></a>Access methods</h2><h3 id="How-can-we-optimize-sequential-scan-with-data-skipping"><a href="#How-can-we-optimize-sequential-scan-with-data-skipping" class="headerlink" title="How can we optimize sequential scan with data skipping?"></a>How can we optimize sequential scan with data skipping?</h3><ol><li><p>The first approach is approximate queries. </p><ul><li><p>This method is lossy, which means that it may return incorrect results, but it is OK. </p></li><li><p>Execute queries on a sampled subset of the entire table to produce approximate results. </p></li></ul></li><li><p>The second approach is zone maps. </p><ul><li>This method is lossless. </li><li>Pre-computed aggregates for the attribute values in a page. DBMS checks the zone map first to decide whether it wants to access the page. </li><li>The trade-off is between page size and filter efficacy. </li></ul></li></ol><h3 id="What-is-multi-index-scan"><a href="#What-is-multi-index-scan" class="headerlink" title="What is multi-index scan?"></a>What is multi-index scan?</h3><ol><li>If there are multiple indexes that the DBMS can use for a query, one method for DBMS to execute is try to filter tuples with index that has least number of tuples matches and filter other indexes based on the filtered tuples of previous indexes. <ul><li>This is ideal in the case that some indexes has little tuples that matches. Filtering those indexes first can significantly reduce the number of tuples to process in following indexes. </li></ul></li><li>If all indexes has a lot of matching tuples, we can use another method:<ul><li>Compute sets of Record IDs using each matching index. Combine these sets based on the query’s predicates (union or intersect). Retrieve the records and apply any remaining predicates. </li><li>In this way, we can reduce a log of I/O and memory space by only fetching Record IDs in the first phase instead of fetching entire tuple. </li></ul></li></ol><h2 id="Modification-queries"><a href="#Modification-queries" class="headerlink" title="Modification queries"></a>Modification queries</h2><h3 id="How-should-we-execute-update-and-delete-queries"><a href="#How-should-we-execute-update-and-delete-queries" class="headerlink" title="How should we execute update and delete queries?"></a>How should we execute update and delete queries?</h3><ol><li>Operators that modify the database (<code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) are responsible for modifying the target table and its indexes. <ul><li>The output of these operators can either be Record Ids or tuple data (i.e., <code>RETURNING</code>). </li></ul></li><li>For update or delete, child operators pass Record IDs for target tuples. <ul><li>The operator should remove the corresponding item from index. </li><li>Then the operator can modify tuple or remove tuple. </li><li>Update should re-insert modified tuple into index again. </li></ul></li><li>Updates have a possible Halloween problem: <ul><li>When we re-inserted the modified tuple, its new place may be ahead of cursor, i.e. we will pass the new tuple again in the future. <ul><li>E.g. when the index is sorted according to an attribute, and the modification increases that attribute. </li></ul></li><li>An update operation changes the physical location of a tuple, which causes a scan operator to visit the tuple multiple times. It can occur on clustered tables or index scans. </li><li>The solution is to keep track of previously seen tuples (e.g. through Record IDs). </li></ul></li></ol><h3 id="How-should-we-execute-insert-queries"><a href="#How-should-we-execute-insert-queries" class="headerlink" title="How should we execute insert queries?"></a>How should we execute insert queries?</h3><ol><li>The first choice is to materialize tuples inside of the operator. <ul><li>The insert operator needs to implement its own method of how to materialize tuples. </li></ul></li><li>The second choice is that operator inserts any tuple passed in from child operators. <ul><li>The insert operator only need to accept tuples from its children instead of implementing itself. </li></ul></li></ol><h2 id="How-to-evaluate-expressions"><a href="#How-to-evaluate-expressions" class="headerlink" title="How to evaluate expressions?"></a>How to evaluate expressions?</h2><ol><li>The DBMS represents a <code>WHERE</code> clause as an expression tree. </li><li>The nodes in the tree represent different expression types: Comparisons (<code>=, &lt;, &gt;, !=</code>), conjunction (<code>AND</code>), disjunction (<code>OR</code>), arithmetic operators (<code>+, -, *, /, %</code>), constant values, tuple attribute references. </li><li>When evaluate the expression, DFS through the expression tree from the root. <ul><li>The performance is poor due to that the DBMS traverses the tree and for each node that it visits it must figure out what the operator needs to do. </li></ul></li><li>A better approach is to just evaluate the expression directly. <ul><li>Compile a function of the express (e.g. JIT compilation). In evaluation, DBMS just execute the compiled function instead of traversing through the tree. </li></ul></li></ol><h1 id="Parallel-execution"><a href="#Parallel-execution" class="headerlink" title="Parallel execution"></a>Parallel execution</h1><h2 id="Parallel-and-distributed"><a href="#Parallel-and-distributed" class="headerlink" title="Parallel and distributed"></a>Parallel and distributed</h2><h3 id="Why-care-about-parallel-execution"><a href="#Why-care-about-parallel-execution" class="headerlink" title="Why care about parallel execution?"></a>Why care about parallel execution?</h3><ol><li>It increased performance for potentially the same hardware resources, i.e. to gain higher throughput and lower latency. </li><li>It increased the responsiveness of the system. </li><li>It potentially lower total cost of ownership (TCO). <ul><li>Fewer machines means less parts / physical footprint / energy consumption. </li></ul></li></ol><h3 id="What-are-the-similarities-and-differences-between-parallel-and-distributed-DBMS"><a href="#What-are-the-similarities-and-differences-between-parallel-and-distributed-DBMS" class="headerlink" title="What are the similarities and differences between parallel and distributed DBMS?"></a>What are the similarities and differences between parallel and distributed DBMS?</h3><ol><li>Similarities:<ul><li>Database is spread out across multiple resources to improve different aspects of the DBMS. </li><li>They both need to make database  to appear as a single logical database instance to the application, regardless of physical organization. </li><li>SQL query for a single-resource DBMS should generate same result on a parallel or distributed DBMS. </li></ul></li><li>Differences:<ul><li>For parallel DBMSs, resources are physically close to each other. Hence resources communicate over high-speed interconnect. And communication is assumed to be cheap and reliable. </li><li>For distributed DBMSs, resources can be far from each other. Resources communicate using slow(er) interconnect. Therefore, communication cost and problems cannot be ignored. And communication is considered unreliable. </li></ul></li></ol><h2 id="Process-model"><a href="#Process-model" class="headerlink" title="Process model"></a>Process model</h2><h3 id="What-does-process-model-of-parallel-DBMSs-need-to-do"><a href="#What-does-process-model-of-parallel-DBMSs-need-to-do" class="headerlink" title="What does process model of parallel DBMSs need to do?"></a>What does process model of parallel DBMSs need to do?</h3><ol><li>It defines how the system is architected to support concurrent requests from a multi-user application. </li><li>A worker is the DBMS component that is responsible for executing tasks on behalf of the client and returning the results. </li><li>There are three approaches: process per DBMS worker, thread per DBMS worker and embedded DBMS</li></ol><h3 id="What-is-process-per-worker-model"><a href="#What-is-process-per-worker-model" class="headerlink" title="What is process per worker model?"></a>What is process per worker model?</h3><ol><li>Each worker is a separate OS process. Hence, this model relies on OS scheduler entirely. </li><li>When an application connect with DBMS, it connect with a dispatcher process. The dispatcher picks on the processes for the application. Then the application communicate with the process directly. </li><li>The processes can use shared-memory for global data structures. </li><li>The advantage of this model is that a process crash does not take down entire system. </li></ol><h3 id="What-is-thread-per-worker-model"><a href="#What-is-thread-per-worker-model" class="headerlink" title="What is thread per worker model?"></a>What is thread per worker model?</h3><ol><li>In this model, the whole DBMS is a single process with multiple worker threads. </li><li>DBMS (mostly) manages its own scheduling by controlling what each threads is doing. <ul><li>This also means less overhead per context switch and that DBMS does not have to manage shared memory. </li></ul></li><li>There may or may not have a dispatcher thread in the front. <ul><li>Applications may connect to dispatcher, and dispatcher immediately forward request to another thread while application does not know about it. </li><li>Or applications can use the same scheme as process per worker model. </li></ul></li><li>In this model, thread crash may kill the entire system. </li></ol><h3 id="What-is-considered-when-DBMS-scheduling-threads"><a href="#What-is-considered-when-DBMS-scheduling-threads" class="headerlink" title="What is considered when DBMS scheduling threads?"></a>What is considered when DBMS scheduling threads?</h3><p>For each query plan, the DBMS decides where, when, and how to execute it. </p><ol><li>How many tasks should it use?</li><li>How many CPU cores should it use?</li><li>What CPU core should the tasks execute on? </li><li>Where should a task store its output?</li></ol><h3 id="What-is-embedded-DBMS-model"><a href="#What-is-embedded-DBMS-model" class="headerlink" title="What is embedded DBMS model?"></a>What is embedded DBMS model?</h3><ol><li>In aforementioned systems and most common systems, applications are in separate machines. They are connected through TCP or socket. Even if the applications crashed, DBMS still remains running. </li><li>In embedded DBMS, DBMS runs inside of the same address space as the application. Application is (mostly) responsible for threads and scheduling. </li><li>The application may support outside connections. </li></ol><h2 id="Query-level-parallelism"><a href="#Query-level-parallelism" class="headerlink" title="Query-level parallelism"></a>Query-level parallelism</h2><h3 id="What-are-the-query-level-parallelisms"><a href="#What-are-the-query-level-parallelisms" class="headerlink" title="What are the query-level parallelisms?"></a>What are the query-level parallelisms?</h3><ol><li>Inter-Query: Execute multiple disparate queries simultaneously. <ul><li>This parallelism increases throughput and reduces latency. It improves overall performance by allowing multiple queries to execute simultaneously. </li><li>If queries are read-only, then this requires almost no explicit coordination between queries. Buffer pool can handle most of the sharing if necessary. </li><li>If multiple queries are updating the database at the same time, then this is hard to do correctly. </li></ul></li><li>Intra-Query: Execute the operations of a single query in parallel. <ul><li>This parallelism decreases latency for long-running queries, especially for OLAP queries. It improves the performance of a single query by executing its operators in parallel. </li><li>Organize operators in terms of a producer/consumer paradigm. </li></ul></li></ol><h3 id="How-can-we-achieve-intra-query-parallelism"><a href="#How-can-we-achieve-intra-query-parallelism" class="headerlink" title="How can we achieve intra-query parallelism?"></a>How can we achieve intra-query parallelism?</h3><ol><li>The first approach is using intra-operator (horizontal) parallelism. <ul><li>Decompose operators into independent fragments that perform the same function on different subsets of data. </li><li>In the generated query plan, those decomposed operators are copied for each thread. </li><li>The DBMS inserts an exchange operator into the query plan to coalesce/split results from multiple children/parent operators. The exchange operators are similar with barriers stating that data cannot be sent up to parent until received all results. </li><li>There are three kinds of exchange operators:<ul><li>Gather: Combine the results from multiple workers into a single output stream. </li><li>Distribute: Split a single input stream into multiple output streams. </li><li>Repartition: Shuffle multiple input streams across multiple output streams. </li></ul></li></ul></li><li>The second approach is using inter-operator (vertical / pipeline) parallelism. <ul><li>Operations are overlapped in order to pipeline data from one stage to the next without materialization. </li><li>Each operator is a worker. Workers execute operators from different segments of a query plan at the same time. </li></ul></li><li>We can also combine these two approaches, which is call bushy parallelism. </li></ol><h2 id="I-O-paralleism"><a href="#I-O-paralleism" class="headerlink" title="I/O paralleism"></a>I/O paralleism</h2><h3 id="What-is-the-problem-of-query-level-parallelism"><a href="#What-is-the-problem-of-query-level-parallelism" class="headerlink" title="What is the problem of query-level parallelism?"></a>What is the problem of query-level parallelism?</h3><ol><li>Using additional processes/threads to execute queries in parallel won’t help if the disk is always the main bottleneck. </li><li>It can sometimes make the DBMS’s performance worse if worker is accessing different segments of the disk at the same time. </li></ol><h3 id="How-can-we-parallel-I-Os-with-multi-disk"><a href="#How-can-we-parallel-I-Os-with-multi-disk" class="headerlink" title="How can we parallel I/Os with multi-disk?"></a>How can we parallel I/Os with multi-disk?</h3><ol><li><p>Split the DBMS across multiple storage devices to improve disk bandwidth latency. </p><ul><li>There are many options<ul><li>Multiple disks per database, one database per disk, one relation per disk, split relation across multiple disks. </li><li>The main trade-off is the number of disks and I/O parallelism. </li></ul></li></ul></li><li><p>Configure OS/hardware to store the DBMS’s files across multiple storage<br>devices, e.g. storage appliances, RAID configuration. </p><ul><li><p>This is transparent to the DBMS. </p></li><li><p>RAID 0 strips data into different disks. Each disk stores different data. </p></li><li>RAID 1 mirrors data in different disks. Each disk stores the same data. </li></ul></li></ol><h3 id="How-can-we-partition-database"><a href="#How-can-we-partition-database" class="headerlink" title="How can we partition database?"></a>How can we partition database?</h3><ol><li>Some DBMSs allow you to specify the disk location of each individual database. <ul><li>The buffer pool manager maps a page to a disk location. </li><li>This is also easy to do at the filesystem level if the DBMS stores each database in a separate directory. </li><li>The DBMS recovery log file might still be shared if transactions can update multiple databases. </li></ul></li><li>Logical splitting is to split single logical table into disjoint physical segments that are stored/managed separately. <ul><li>Partitioning should (ideally) be transparent to the application. </li><li>The application should only access logical tables and not have to worry about how things are physically stored. </li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Executor </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Project #2: B+Tree</title>
      <link href="/2023/07/07/OpenSource/BusTub/Project-2-B-Tree/"/>
      <url>/2023/07/07/OpenSource/BusTub/Project-2-B-Tree/</url>
      
        <content type="html"><![CDATA[<p>@<a href="c">toc</a></p><h1 id="B-Tree-Page"><a href="#B-Tree-Page" class="headerlink" title="B+Tree Page"></a>B+Tree Page</h1><ol><li><p>The <code>size_</code> in each page means the number of stored values not keys, i.e. in internal nodes, <code>size_</code> is $1$ larger than the number of keys. </p></li><li><p><code>KeyComparator</code> accept two keys to compare, which will return $0$ when they are equal, $-1$ for the first key “smaller” than the second key, $1$ for the second key being larger. </p></li><li><p>We need to design the sematic of the binary search that will be used in search, insert and delete a key inside a node. </p><ul><li>In the leaf node: <ul><li>For search and delete, we want this function to tell us the index of the key is it exists. </li><li>For insert, we want this function to indicate the index we need to place the key. </li></ul></li><li>In the internal node:<ul><li>For search, we only want it to inform us the child that might have the given key. </li><li>For insert, we want it to return the page ID to find a proper leaf page to store the key in forward search and give the index we need to place the split key when backward split is required. </li><li>For delete, we want it the same as for insert in the forward search and to provide the index to help merge two children when the backward merge is required. </li></ul></li><li>In the following implemetation of binary search, it will return the index of the key if it exists, or it will return the largest index with smaller key. <ul><li>Notably that if the given key is smaller than all keys in the node, it will return $-1$. </li><li>In internal node, we would expect the smallest possible result is $0$ since the first key is <code>NULL</code> which should be smaller than any other keys. </li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">BinarySearch</span><span class="params">(KeyType key, KeyComparator comparator)</span> <span class="type">const</span> -&gt; <span class="type">int</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> lo = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> hi = <span class="built_in">GetSize</span>();</span><br><span class="line">  <span class="keyword">while</span> (lo &lt; hi) &#123;</span><br><span class="line">    <span class="type">int</span> mid = (lo + hi) / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="number">0</span> &lt; mid &amp;&amp; <span class="built_in">comparator</span>(key, array_[mid].first) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      hi = mid;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      lo = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> lo - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><code>FindNextPageID</code> will provide the proper child for the caller to access to find a certain key. </p><ul><li>For leaf node, this will only be used in B+Tree search. If the binary search result is $-1$, we can conclude that that key does not exist and should tell caller that. </li><li>For internal node, this will be used in B+Tree search or the forward search in insert and delete. If the binary search result is $-1$, then we should return the first value to indicate the key is smaller than all keys. </li></ul></li><li><p><code>InsertKey</code> should </p></li></ol><h1 id="B-Tree-search"><a href="#B-Tree-search" class="headerlink" title="B+Tree search"></a>B+Tree search</h1><ol><li>The thought is simple: we just go down from the root until hit a leaf node. If the key is in the leaf node, return the value. Otherwise, the key does not exists. </li><li>For concurrency control, we can only release the latch of a page after acquired the latch of its child. <ul><li>This can be implemented with the move assignment operation of <code>PageGuard</code>. In the execution, the right expression will first acquire the latch of next page, then destroy the original page guard of the left variable. </li></ul></li></ol><h1 id="B-Tree-insert"><a href="#B-Tree-insert" class="headerlink" title="B+Tree insert"></a>B+Tree insert</h1><ol><li><p>For concurrency control, I implemented both the optimistic scheme and the pessimistic scheme. </p><ul><li><p>The program will first try with optimistic search with only write latch on leaf node. </p></li><li><p>If the leaf node may overflow, release all latches and start again trying to acquire write latch for all nodes. </p></li></ul></li><li><p>In the optimistic search, we can use the queue in <code>Context</code>. </p><ul><li>When we fetched a new page guard, we push it to the back of <code>ctx.read_set_</code> and pop the front element out of the queue. Every time we just need to access the last element of the queue to find the next page to read. </li><li>When we realised that we have reached the leaf node, we are holding a read latch for that page. Hence, we still need to release the read latch and re-acquire the write latch, i.e. we cannot release the latch of last internal page when we first acquire the leaf page. </li><li>The solution is to release the latch before read the “grand-child” of it. </li></ul></li><li><p>In the pessimistic search, we need to acquire write latch for all pages we want to access. </p><ul><li>We can check whether a node is safe after its write latch is acquired. If the node is safe, we can release all latches acquired before it, including the header page. </li></ul></li><li><p>In the insert function, there are three cases to handle: </p><ul><li>The first case is that this is an empty tree, i.e. the <code>root_page_id_</code> in header page is invalid. We just need to create a new page for the node and update header page. </li><li>The second case is that the leaf node won’t overflow where a simple insertion is enough. </li><li>The last case is that the leaf node might overflow. </li></ul></li><li><p>When the leaf node might overflow: </p><ul><li>After acquired the write latches, we need to check whether there is an overflow again in case that other thread already handled overflow causing unexpected split. </li><li><code>SolveLeafOverflow</code><ul><li>When a leaf reaches max size after insertion, it will immediately split. So this is used after the insertion. </li><li>It will create a new page to store the larger half the nodes and return the first key in the new page to insert to its parent node to indicate to this page. </li><li>Also, it need to take care of the sibling pointers between leaf nodes. The new page will point to what the original page points to. And the original page will point to the new page. </li></ul></li><li><code>SolveInternalOverflow</code><ul><li>Internal nodes won’t split immediately when it reaches max size. This means that internal nodes must split first before insertion, otherwise the address will overflow. </li><li>The process is similar with the leaf case, except that it will choose a proper page to insert the key after split. (Or it just does not need to split if it is a safe node). </li></ul></li></ul></li></ol><h1 id="B-Tree-delete"><a href="#B-Tree-delete" class="headerlink" title="B+Tree delete"></a>B+Tree delete</h1><ol><li>The concurrency control is similar with the insert operation, except that the condition of whether a node is safe is different. </li><li>Compare with insertion, <ul><li>if the tree is empty, there is nothing else to do; </li><li>if the leaf node won’t underflow, a simple deletion is enough; </li><li>if the leaf node might underflow, we need to handle it and possible following cascade underflow. </li></ul></li><li>When the leaf node might overflow:<ul><li>Similar with the insertion situation, we need to re-acquire write latches, and check whether the underflow is handled by another thread. </li><li>If the underflowed leaf is the root, i.e. the tree has only one node, we do not need to do anything further. </li><li><code>SolveLeafUnderflow</code><ul><li>There are three situations: left sibling can borrow a key, right sibling can borrow a key or neither siblings can borrow a key. </li><li>When we can borrow a key, the underflow is solved easily, no more cascading. We need to modify the key in parent node that separates the two involving node to the new first key of the right node. </li><li>When we need to merge with one of the siblings, we also need to handle the pointers between leaf nodes. If we move the data of the left node to the right node and delete the left node, it is hard for us to modify the pointer of the left of the left node. Instead, if we move delete the right node, we only need to modify the pointer of the left node to the original pointer of the right node. </li><li>If a leaf only has left sibling or right sibling to merge with, then we do not have a choice. </li></ul></li><li><code>SolveInternalUnderflow</code><ul><li>The process is similar with <code>SolveLeafUnderflow</code>, except how to borrow a key. </li><li>When a node is borrowing a key to its left sibling, it will borrow the first valid key and the fire value, i.e. <code>array_[1].first</code> and <code>array_[0].second</code>. </li><li>When a node is borrowing a key to its right sibling, it will borrow the last key-value pair. And the node accpeting those keys will use the key as its first valid key and the value as its first valid value. </li><li>We need to set the corresponding key in parent node to the borrowed key. </li></ul></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Source Code </category>
          
          <category> BusTub </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Data structure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>06 Operator Algorithms</title>
      <link href="/2023/06/28/Courses/15445/06-Operator-Algorithms/"/>
      <url>/2023/06/28/Courses/15445/06-Operator-Algorithms/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Execute-queries"><a href="#Execute-queries" class="headerlink" title="Execute queries"></a>Execute queries</h1><h2 id="How-are-queries-planned"><a href="#How-are-queries-planned" class="headerlink" title="How are queries planned?"></a>How are queries planned?</h2><ol><li>The operators are arranged in an abstract syntax tree. Data flows from the leaves of the tree up towards the root. </li><li>The leaf nodes are access methods, e.g scanning index and scanning table, feeding data up along its path to its parent node to do processing. </li><li>The output of the root node is the result of the query. </li><li>This tree only describes a logical plan, i.e. instead of telling what implementation to use, it is just the logical flow we want. SQL only declare logical plan while it is the database system’s job to figure out the optimal way to execute. </li></ol><h2 id="What-is-the-assumption-of-algorithms-in-database-system"><a href="#What-is-the-assumption-of-algorithms-in-database-system" class="headerlink" title="What is the assumption of algorithms in database system?"></a>What is the assumption of algorithms in database system?</h2><ol><li>Just like it cannot assume that a table fits entirely in memory, a disk-oriented DBMS cannot assume that query results fit in memory. </li><li>We will use the buffer pool to implement algorithms that need to spill to disk. </li><li>We are also going to prefer algorithms that maximize the amount of sequential I/O.</li></ol><h1 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h1><h2 id="What-implementation-should-we-use-to-execute-a-query-containing-an-ORDER-BY-with-a-LIMIT"><a href="#What-implementation-should-we-use-to-execute-a-query-containing-an-ORDER-BY-with-a-LIMIT" class="headerlink" title="What implementation should we use to execute a query containing an ORDER BY with a LIMIT?"></a>What implementation should we use to execute a query containing an ORDER BY with a LIMIT?</h2><ol><li>The DBMS only needs to scan the data once to find the top-N elements. </li><li>The ideal scenario for heapsort is when the top-N elements fit in memory, so that the DBMS only has to maintain an in-memory sorted priority queue while scanning the data. </li></ol><h2 id="What-if-the-data-is-too-large-to-fit-in-memory-for-any-clause-including-ORDER-BY-with-a-LIMIT"><a href="#What-if-the-data-is-too-large-to-fit-in-memory-for-any-clause-including-ORDER-BY-with-a-LIMIT" class="headerlink" title="What if the data is too large to fit in memory? (for any clause including ORDER BY with a LIMIT)"></a>What if the data is too large to fit in memory? (for any clause including ORDER BY with a LIMIT)</h2><ol><li>We do not want to use quick sort in this scenario since data spilling to disk will cause too many random access. </li><li>We can use external merge sort that splits data into separate runs, sorts them individually, and then combines them into longer sorted runs. </li><li>A run is a list of key/value pairs. <ul><li>Keys are the attribute(s) to compare to compute the sort order. </li><li>Values have two choices: it can either be the actual tuple data (i.e. early materialization) or be the Record IDs (i.e. late materialization)<ul><li>The advantage of early materialization is that it can be faster to produce result while the disadvantage is that it need to copy more data during the procedule. </li><li>The advantage of late materialization is that it can only fetch wanted data while it needs to find the actual data else where (probably involving another disk I/O). </li></ul></li></ul></li></ol><h2 id="How-to-perform-an-external-merge-sort"><a href="#How-to-perform-an-external-merge-sort" class="headerlink" title="How to perform an external merge sort?"></a>How to perform an external merge sort?</h2><ol><li>Data is broken up into $N$ pages. The DBMS has a finite number of $B$ buffer pool pages to hold input and output data.</li><li>In the first phase, sort chunks of data that fit in memory and then write back the sorted chunks to a file on disk. <ul><li>In the first pass, we can read all $B$ pages of the table into memory and sort pages into runs and write them back to disk. </li><li>Do not sort in buffer since we do not wish other threads seeing some partially sorted data which may cause errors. Copy the data to somewhere else and copy back to buffer after sorted. </li></ul></li><li>In the second phase, combine sorted runs into larger chunks. <ul><li>In the following passes, each pass use $B-1$ pages for input and $1$ page for output. </li><li>Recursively merge pairs of runs into runs $B-1$-times as long. </li></ul></li><li>In general, the first pass create $\lceil N/B\rceil$ sorted runs of size $B$ and the following passes are $(B-1)$-way merge. <ul><li>The number of passes is $1+\lceil\log_{B-1}\lceil N/B\rceil\rceil$. </li><li>The total I/O cost is $2N\cdot (number\ of\ passes)$. </li></ul></li><li>In a 2-way external merge sort, instead of sort the $B$ pages into a run in the first pass, each page is sorted into a run. <ul><li>So the number of passes is $1+\lceil\log_2N\rceil$. </li></ul></li></ol><h2 id="How-can-we-hide-the-disk-I-O"><a href="#How-can-we-hide-the-disk-I-O" class="headerlink" title="How can we hide the disk I/O?"></a>How can we hide the disk I/O?</h2><ol><li>A typical setup is using $3$ pages ($2$ for input pages and $1$ for output page). Even if we have more buffer space available ($B&gt;3$), it does not effectively utilize them if the worker must block on disk I/O. </li><li>We can prefetch the next run in the background and store it in a second buffer while the system is processing the current run. </li><li>This method reduces the wait time for I/O requests at each step by continuously utilizing the disk. </li></ol><h2 id="How-can-we-optimize-comparison"><a href="#How-can-we-optimize-comparison" class="headerlink" title="How can we optimize comparison?"></a>How can we optimize comparison?</h2><ol><li>The first approach is code specialization. <ul><li>Instead of providing a comparison function as a pointer to sorting algorithm, create a hardcoded version of sort that is specific to a key type. </li></ul></li><li>For string keys, the second approach is suffix truncation. <ul><li>First compare a binary prefix of keys instead of slower string comparison. </li><li>Fallback to slower version if prefixes are equal. </li></ul></li></ol><h2 id="How-can-use-use-B-Tree-for-sorting-since-it-is-sorted"><a href="#How-can-use-use-B-Tree-for-sorting-since-it-is-sorted" class="headerlink" title="How can use use B+Tree for sorting since it is sorted?"></a>How can use use B+Tree for sorting since it is sorted?</h2><ol><li>If the table that must be sorted already has a B+Tree index on the sort attribute(s), then we can use that to accelerate sorting. </li><li>Retrieve tuples in desired sort order by simply traversing the leaf pages of the tree. <ul><li>For clustered B+Tree, this is always better than external sorting because there is no computational cost, and all disk access is sequential. </li><li>For non-clustered B+Tree, this is almost always a bad idea. In general, one I/O per data record. </li></ul></li></ol><h1 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h1><h2 id="How-can-we-implement-aggregations"><a href="#How-can-we-implement-aggregations" class="headerlink" title="How can we implement aggregations?"></a>How can we implement aggregations?</h2><ol><li>The DBMS needs a way to quickly find tuples with the same distinguishing attributes for grouping. </li><li>For aggregations specified with a <code>ORDER BY</code> clause, we can directly use the aforementioned sorting algorithms. </li><li>For queries do not need the data to be ordered, e.g. forming groups in <code>GROUP BY</code> or removing duplicates in <code>DISTINCT</code>, hashing is a better alternative. Hashing can be computationally cheaper than sorting. <ul><li>For each record, check whether there is already an entry in the hash table. For <code>DISTINCT</code>, just discard duplicate. For <code>GROUP BY</code>, perform aggregate computation. </li></ul></li></ol><h2 id="How-to-do-hashing-when-spill-data-to-disk"><a href="#How-to-do-hashing-when-spill-data-to-disk" class="headerlink" title="How to do hashing when spill data to disk?"></a>How to do hashing when spill data to disk?</h2><ol><li>The first phase is partition. Divide tuples into buckets based on hash key. Write them out to disk when they get full. <ul><li>Use a hash function $h_1$ to split tuples into partitions on disk. </li><li>A partition is one or more pages that contain the set of keys with the same hash value. </li><li>Assume that we have $B$ buffers. We will use $B-1$ buffers for the partitions and $1$ buffer for the input data. </li></ul></li><li>The second phase is ReHash. Build in-memory hash table for each partition and compute the aggregation. <ul><li>For each partition on disk, read it into memory and build an in-memory hash table based on a second hash function $h_2$. </li><li>Then go through each bucket of this hash table to bring together matching tuples. </li><li>Each time we can use $B-1$ pages as input pages and $1$ page as output page. <ul><li>In each round, we can read in $B-1$ partitions. </li><li>After each round is finished, we can clear the hash table since the next $B-1$ partitions definitely won’t have the same keys as last round. </li></ul></li></ul></li><li>During the ReHash phase, store pairs of the form $(GroupKey→RunningVal)$. <ul><li>When we want to insert a new tuple into the hash table, if we find a matching $GroupKey$, just update the$RunningVal$ appropriately. Else insert a new $(GroupKey→RunningVal)$. </li><li>The running totals of different aggregation function is as followed:<ul><li>$AVG(col)\rightarrow (COUNT,SUM)$</li><li>$SUM(col)\rightarrow(SUM)$</li><li>$COUNT(col)\rightarrow(COUNT)$</li><li>$MIN(col)\rightarrow(MIN)$</li><li>$MAX(col)\rightarrow(MAX)$</li></ul></li></ul></li></ol><h1 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h1><h2 id="Why-do-we-need-to-join"><a href="#Why-do-we-need-to-join" class="headerlink" title="Why do we need to join?"></a>Why do we need to join?</h2><ol><li>Tables are normalized in a relational database to avoid unnecessary repetition of information. </li><li>Join operator is used to reconstruct the original tuples without any information loss. </li><li>Join is an important operator in both OLAP and OLTP systems. Especially for OLAP system, join could take up to $15\sim 50\%$ of time. </li></ol><h2 id="What-are-join-algorithms-doing"><a href="#What-are-join-algorithms-doing" class="headerlink" title="What are join algorithms doing?"></a>What are join algorithms doing?</h2><ol><li>Most important kind is binary joins using inner equijoin algorithms. <ul><li>Binary means that the operator takes two tables as input. </li><li>Inner means that it matches certain tuple of left table with another tuple in the right table. </li><li>Equijoin means that the condition of matching two tuples is the equivalence of some attributes. </li></ul></li><li>There are also other joins. <ul><li>Multi-way joins take more than two tables as input, which exists primarily in research literature. </li><li>Beside equijoin, there could also be anti-join, non-equijoin, etc. </li></ul></li><li>Compare with cross-product, join is more efficient and can be carefully optimized. </li></ol><h2 id="What-are-the-outputs-of-join-algorithms"><a href="#What-are-the-outputs-of-join-algorithms" class="headerlink" title="What are the outputs of join algorithms?"></a>What are the outputs of join algorithms?</h2><ol><li>In <code>R JOIN S</code>, for tuple $r \in R$ and tuple $s \in S$ that match on join attributes, concatenate $r$ and $s$ together into a new tuple. </li><li>The output contents can vary depends on processing model, storage model or data requirements in query. </li><li>Basically, there are two choises similar with sort. <ul><li>Early materialization<ul><li>Copy the values for the attributes in outer and inner tuples into a new output tuple. </li><li>Subsequent operators in the query plan never need to go back to the base tables to get more data.</li></ul></li><li>Late materialization<ul><li>Only copy the joins keys along with the Record IDs of the matching tuples. </li><li>This is ideal for column stores because the DBMS does not copy data that is not needed for the query. </li></ul></li></ul></li></ol><h2 id="How-can-we-measure-the-cost-of-join"><a href="#How-can-we-measure-the-cost-of-join" class="headerlink" title="How can we measure the cost of join?"></a>How can we measure the cost of join?</h2><ol><li>We can measure the cost of join by the number of I/Os to compute join. </li><li>Output costs are ignored since that depends on the data. </li><li>In the following analysis, we assume there are $m$ tuples stored in $M$ pages in table $R$, $n$ tuples stored in $N$ pages in table $S$. </li></ol><h2 id="Join-algorithms"><a href="#Join-algorithms" class="headerlink" title="Join algorithms"></a>Join algorithms</h2><h3 id="Nested-loop-join"><a href="#Nested-loop-join" class="headerlink" title="Nested loop join"></a>Nested loop join</h3><h4 id="What-is-the-most-naive-algorithm"><a href="#What-is-the-most-naive-algorithm" class="headerlink" title="What is the most naive algorithm?"></a>What is the most naive algorithm?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> S:</span><br><span class="line">    emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure><ol><li>For $R\bowtie S$, he left table $R$ in the outer loop is called outer table and $S$ is called the inner table. </li><li>For every tuple in $R$, it scans $S$ once. The cost is $M+(m\cdot N)$. </li><li>If we use the smaller table with less tuples as the outer table, we can have a better performance since the number of pages is significant smaller than the number of tuples. </li></ol><h4 id="How-can-we-better-use-the-data-already-read-from-disk"><a href="#How-can-we-better-use-the-data-already-read-from-disk" class="headerlink" title="How can we better use the data already read from disk?"></a>How can we better use the data already read from disk?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> block B_R <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> block B_S <span class="keyword">in</span> S:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> B_R:</span><br><span class="line">      <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> B_S:</span><br><span class="line">        emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure><ol><li>For every read block $B_R$ and $B_S$, we try to compute as much as possible, i.e. pair all tuples in $B_R$ with all tuples in $B_S$. </li><li>For every block in R, it scans S once. The cost is $M+(M\cdot N)$. </li><li>$M\cdot N$ won’t be affected by the order of tables. However, if we let the smaller table with less pages as the outer table, the first term can be smaller. </li></ol><h4 id="How-can-we-take-advantages-of-more-buffer-space"><a href="#How-can-we-take-advantages-of-more-buffer-space" class="headerlink" title="How can we take advantages of more buffer space?"></a>How can we take advantages of more buffer space?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> B-<span class="number">2</span> pages p_R <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> page p_S <span class="keyword">in</span> S:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> B_2 pages:</span><br><span class="line">      <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> p_S:</span><br><span class="line">        emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure><ol><li>Use $B-2$ buffers for scanning the outer table. Use one buffer for the inner table, one buffer for storing<br>output. </li><li>The cost is $M+(\lceil M/(B-2)\rceil\cdot N)$. </li><li>If the outer relation completely fits in memory, the cost is $M+N$. </li></ol><h4 id="Can-we-avoid-sequential-scans-by-using-an-index"><a href="#Can-we-avoid-sequential-scans-by-using-an-index" class="headerlink" title="Can we avoid sequential scans by using an index?"></a>Can we avoid sequential scans by using an index?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> Index(r_i = s_j):</span><br><span class="line">    emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure><p>Assume the cost of each index probe is some constant $C$ per tuple. The cost is $M+(m\cdot C)$</p><h3 id="Sort-merge-join"><a href="#Sort-merge-join" class="headerlink" title="Sort-merge join"></a>Sort-merge join</h3><h4 id="What-is-the-process-of-sort-merge-join"><a href="#What-is-the-process-of-sort-merge-join" class="headerlink" title="What is the process of sort-merge join?"></a>What is the process of sort-merge join?</h4><ol><li>The first phase is to sort both tables on the join key(s). </li><li>In the second phase, we step through the two sorted tables with cursors and emit matching tuples. </li><li>The sort cost of outer table is $2M\cdot(1+\lceil \log_{B-1}\lceil M/B\rceil\rceil)$ and the sort cost of inner table is $2N\cdot (1+\lceil\log_{B-1}\lceil N/B \rceil \rceil)$. </li><li>The merge cost is $M+N$. <ul><li>The worst case for the merging phase is when the join attribute of all the tuples in both relations contains the same value.</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sort R,S on join keys</span><br><span class="line">cursor_R points to R_sorted, cursor_S points to S_sorted</span><br><span class="line"><span class="keyword">while</span> cursor_R <span class="keyword">and</span> cursor_S:</span><br><span class="line">  <span class="keyword">if</span> cursor_R &gt; cursor_S:</span><br><span class="line">    increment cursor_S</span><br><span class="line">  <span class="keyword">if</span> cursor_R &lt; cursor_S:</span><br><span class="line">    increment cursor_R</span><br><span class="line">    possible backtrack cursor_S</span><br><span class="line">  <span class="keyword">elif</span> cursor_R <span class="keyword">and</span> surcor_s <span class="keyword">match</span>:</span><br><span class="line">    emit</span><br><span class="line">    increment cursor_s</span><br></pre></td></tr></table></figure><h4 id="How-does-the-two-cursors-move"><a href="#How-does-the-two-cursors-move" class="headerlink" title="How does the two cursors move?"></a>How does the two cursors move?</h4><ol><li>The cursor of outer table will only move forward. Specifically, it will only move forward when we can assure that we have already match the current tuple with all possible tuples, i.e. when we occurred a larger inner tuple. </li><li>The cursor of inner table may move both forward and backward. <ul><li>It moves forward when some later tuple in inned table may match with the current tuple in outer tuple, i.e. when inner tuple is smaller than outer tuple or when they matches (there are possible more matches in the following). </li><li>It moves backward when there might have some missing matches in the past, i.e. when the outer cursor moving forward, the key is the same as the last one, we need to backtrack to the earliest tuple that matches with the last outer tuple. </li></ul></li></ol><h4 id="When-is-sort-merge-join-useful"><a href="#When-is-sort-merge-join-useful" class="headerlink" title="When is sort-merge join useful?"></a>When is sort-merge join useful?</h4><ol><li>When one or both tables are already sorted on join key, we can save some sort cost. </li><li>When output must be sorted on join key, if the output of join is larger, we might use sort-merge join to produce sorted output directly. <ul><li>If the output has a small amout, hash join might still be the better way. </li></ul></li></ol><h3 id="Hash-join"><a href="#Hash-join" class="headerlink" title="Hash join"></a>Hash join</h3><h4 id="How-does-hash-join-work"><a href="#How-does-hash-join-work" class="headerlink" title="How does hash join work?"></a>How does hash join work?</h4><ol><li><p>The thought behind hash join is that: </p><ul><li>If tuple $r \in R$ and a tuple$s \in S$ satisfy the join condition, then they have the same value for the join attributes. </li><li>If that value is hashed to some partition $i$, the $R$ tuple must be in $r_i$ and the $S$ tuple in $s_i$. </li><li>Therefore, $R$ tuples in $r_i$ need only to be compared with $S$ tuples in $s_i$. </li></ul></li><li><p>In the first phase (build), we scan the outer relation and populate a hash table using the hash function $h_1$ on the join attributes. </p><p>In the second phase (probe), scan the inner relation and use $h_1$ on each tuple to jump to a location in the hash table and find a matching tuple. </p></li><li><p>The keys stored in the hash table is the attribute(s) that the query is joining the tables on. We always need the original key to verify that we have a correct match in case of hash collisions. </p><p>The values stored varies per implementation, which depends on what the operators above the join in the query plan expect as its input. </p></li><li><p>Assume that we have enough buffers, we need to read and write both tables with cost of $2(M+N)$ in partitioning phase, and read both tables with cost of $M+N$ in probing phase. </p><ul><li>We can see that there is no constraint on the size of inner table. </li></ul></li></ol><h4 id="How-big-of-a-table-can-we-hash-using-this-approach"><a href="#How-big-of-a-table-can-we-hash-using-this-approach" class="headerlink" title="How big of a table can we hash using this approach?"></a>How big of a table can we hash using this approach?</h4><ol><li>In the first phase of building hash table, we can use at most $B-1$ spill partitions leaving one page as input buffer. When one partition is full, we should write it out to disk and clear it. </li><li>total number of both outer and inner table of each partition should be no more than $B$ blocks big so that in the second phase, we can store all tuples in the same partition in memory. </li><li>The total page used is $B\cdot (B-1)$. A hash table of $N$ pages needs about $\sqrt{N}$ buffers if the hash distribution is even. </li><li>When including the fudge factor $f&gt;1$ when hash distribution is skewed, we need $B\cdot\sqrt{f\cdot N}$. </li></ol><h4 id="Can-we-optimized-the-search-for-tuples-that-does-not-have-any-match"><a href="#Can-we-optimized-the-search-for-tuples-that-does-not-have-any-match" class="headerlink" title="Can we optimized the search for tuples that does not have any match?"></a>Can we optimized the search for tuples that does not have any match?</h4><ol><li>We can create a Bloom filter during the build phase when the key is likely to not exist in the hash table. This method is called Bloom filter or sideways information passing. </li><li>The Bloom filter is a probabilistic data structure (bitmap) that answers set membership queries. <ul><li>False negatives will never occur while false positives can sometimes occur. </li><li>To insert a key into the filter, we use $k$ hash functions to set all $k$ bits to $1$. </li><li>During lookup a key, the key may exist if all $k$ bits hashed by the same $k$ hash function are all $1$. The key definitely does not exists if one of the $k$ bits is $0$. </li></ul></li></ol><h4 id="What-if-we-do-not-have-enough-memoty-to-fit-the-entire-hash-table"><a href="#What-if-we-do-not-have-enough-memoty-to-fit-the-entire-hash-table" class="headerlink" title="What if we do not have enough memoty to fit the entire hash table?"></a>What if we do not have enough memoty to fit the entire hash table?</h4><ol><li>We can use the recursive hash join (GRACE hash join). </li><li>Similar with aforementioned algorithm, hash both tables into same number of buckets with the same hash function. </li><li>Perform regular hash join on each pair of matching buckets in the same level between two tables. </li><li>If the buckets do not fit in memory, then use recursive partitioning to split the tables into chunks that will fit. <ul><li>Build another hash table for $bucket_{R,i}$ using hash function $h_2$ (with $h_2≠h_1$). </li><li>Then probe it for each tuple of the other table’s bucket at that level. </li></ul></li><li><strong>Hybrid hash join</strong>: If the keys are skewed, then the DBMS keeps the hot partition in-memory and immediately perform the comparison instead of spilling it to disk. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> External Algorithms </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Project #1: Buffer Pool</title>
      <link href="/2023/06/27/OpenSource/BusTub/Project-1-Buffer-Pool/"/>
      <url>/2023/06/27/OpenSource/BusTub/Project-1-Buffer-Pool/</url>
      
        <content type="html"><![CDATA[<p>@<a href="c">toc</a></p><h1 id="Task-1-LRU-K-Replacement-Policy"><a href="#Task-1-LRU-K-Replacement-Policy" class="headerlink" title="Task #1 - LRU-K Replacement Policy"></a>Task #1 - LRU-K Replacement Policy</h1><ol><li><p>Evict rule: </p><ul><li>When all evictable frames have more than $K$ access records, evict the one whose backward k-distance is maximum. </li><li>When some frames only have less than $K$ access records, evict the one with earliest first access record among those less than $K$ frames. </li></ul></li><li><p>Comparison: </p><ul><li>When each frame is created or stored with a new page, push an access record of $0$ to its history to represent $+\inf$. </li><li>Each comparison use the first two records in the list. If the first record (the earliest backward most k-distance) is the same, it must be $0$ causing comparing second record where represent true first access of that frame. </li><li>Remember to update second comparison timestamp when the first time find $0$ in first timestamp. </li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> k_timestamp = frame.second.<span class="built_in">GetKTimestamp</span>();</span><br><span class="line"><span class="type">size_t</span> sec_timestamp = frame.second.<span class="built_in">GetSecondTimestamp</span>();</span><br><span class="line"><span class="keyword">if</span> (k_timestamp &lt; cmp_k_timestamp) &#123;</span><br><span class="line">victim = frame.first;</span><br><span class="line">cmp_k_timestamp = k_timestamp;</span><br><span class="line"><span class="keyword">if</span> (k_timestamp == <span class="number">0</span>) &#123;</span><br><span class="line">cmp_sec_timestamp = sec_timestamp;</span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (k_timestamp == cmp_k_timestamp &amp;&amp; sec_timestamp &lt; cmp_sec_timestamp) &#123;</span><br><span class="line">victim = frame.first;</span><br><span class="line">cmp_sec_timestamp = sec_timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>After successfully choosing a victim, we need to clear its history (leaving the $0$ as sentinel), set it to non-evictable, and recude the current size of replacer. </p></li></ol><h1 id="Task-2-Buffer-Pool-Manager"><a href="#Task-2-Buffer-Pool-Manager" class="headerlink" title="Task #2 - Buffer Pool Manager"></a>Task #2 - Buffer Pool Manager</h1><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><ol><li>When the page that is asked to fetch is already in the buffer pool, we still need to do several things:<ul><li>Set it to non-evictable. </li><li>Record its access in replacer. </li><li>Increase its pin count. </li></ul></li><li><p>When the page is not in buffer pool, or creating a new page: </p><ul><li>First, we need to acquire a free frame. <ul><li>Erase it from page table when we are evicting one. </li><li>Write the content to disk when the old page is dirty. </li></ul></li><li>Then, similar to the other situation, we need to: <ul><li>Set it to non-evictable</li><li>Record its access in replacer</li><li>Put it in page table</li><li>Set its pin count to $1$</li><li>Set <code>is_dirty_</code> to <code>false</code></li><li>Set its page ID to frame. </li></ul></li><li>Even in <code>NewPage</code>, the <code>is_dirty_</code> is false due to it can directly “write” empty to file by increasing offset which is done when larger page ID is written. There is no need to actually write empty content. </li></ul></li><li><p>When setting <code>is_dirty_</code> in <code>Unpin</code>, if the page is already dirty, we should not set it to clean no matter what we are told to. </p></li></ol><h2 id="Leaderboard-optimization"><a href="#Leaderboard-optimization" class="headerlink" title="Leaderboard optimization"></a>Leaderboard optimization</h2><ol><li>Fine-grain lock<ul><li>A coarse-grain lock is easier to program while sacrificing performance. </li><li>In fine-grain lock, a <code>core_latch_</code> is used to protect the core data of buffer pool manager, i.e. <code>page_table_</code>, <code>free_list_</code>, <code>next_page_id_</code>). Each frame has its own latch to protect its data, i.e. <code>pin_count_</code>, <code>is_dirty_</code>, <code>page_id_</code>. </li><li>In each function thread, it will at most grab the <code>core_latch_</code> and one of frame latches. </li><li>To avoid deadlock, each procedule is designed that a thread will try to acquire a page latch only if it already has a core_latch, or it won’t want a core_latech later. </li><li>To obtain atomic when switching from <code>core_latch_</code> to a page latch, we need to acquire the page latch before release the <code>core_latch_</code>. <ul><li>If release <code>core_latch_</code> first, some other thread may change the frame we want to use (e.g. evict the frame, modify its metadata) before we can acquire the frame latch. </li></ul></li></ul></li><li>Delay write out: <ul><li>Disk I/Os consume a large amount of time. So we need to avoid holding a lock while communicate with disk. </li><li>When we need to write data into disk, instead of immediately call the <code>disk_manager_-&gt;WritePage</code>, we create a temporary buffer in memory, and copy data into the buffer. After all locks are released, we actually write those content in buffer into disk. </li><li>A similar thought is to delay read data until all locks are released. However, <code>ReadData</code> are call in <code>FetchPage</code> where  we can only release the frame latch after the whole frame is ready for others to visit. Still, we can <code>ReadData</code> after <code>core_latch_</code> is released since the <code>core_latch_</code> could be the bottleneck of the manager. </li><li>However, there is a problem: <ul><li>When a page is evicted, and only a short time later, that exact page is fetched again. Since all latches are release before write dirty data to disk, there is a chance that <code>FetchPage</code> read stale data from disk before the up-to-date data is written to disk. </li><li>Then my another thought is to maintain a list of page IDs in the writing buffer. If the page ID of <code>FetchPage</code> is in the list, it just copy data from the writing buffer and erase that page from writing buffer. </li><li>The problem here is that we need to guarantee the atomic between getting the writing content and writing them to disk. Otherwise, tricky concurrent operations may cause the written content to be wrong. </li><li>Hence we cannot unlock the latches until we are sure the content is written, which also makes fine-grained latches meaningless given that disk I/O is the key problem of performance. </li></ul></li></ul></li><li>Pre-fetch<ul><li>To better suit the scan case, when we accessing three consecutive page IDs in a row, the buffer pool manager is allowed to pre-fetch several following pages. </li><li>Different from fetching pages through <code>FetchPage</code>, pre-fetched frames need to set <code>pin_count_</code> to $0$ and evictable. </li><li>Pre-fetch should not stall <code>FetchPage</code> from returning, so it must be run in a separate thread. </li><li>The problem is that pre-fetch must be an independent background thread. It won’t know whether the buffer pool manager is destroyed, which will cause heap-use-after-free error. </li></ul></li></ol><h1 id="Task-3-Read-Write-Page-Guards"><a href="#Task-3-Read-Write-Page-Guards" class="headerlink" title="Task #3 - Read/Write Page Guards"></a>Task #3 - Read/Write Page Guards</h1><ol><li>Any <code>PageGuard</code> will automatically unpin itself when it is deconstructed. But they don’t need to pin themselves since they are pinned before <code>PageGuard</code> is created when fetching or creating page. </li><li>If a <code>Read/WritePageGuard</code> is acquired through <code>FetchPageRead</code> or <code>FetchPageWrite</code>, the latch is acquired inside these functions. However, if a <code>PageGuard</code> is acquired through it construction function, the latch won’t be acquired automatically. </li><li>No matter how <code>Read/WritePageGuard</code> is acquired, the latch will always be released automatically when the <code>PageGuard</code> is deconstructed by deconstructor or move operation. </li><li>In the move assignment, we need to first drop the original page, copy from right reference and finally deconstruct right reference. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Source Code </category>
          
          <category> BusTub </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>05 Index Concurrency</title>
      <link href="/2023/06/25/Courses/15445/05-Index-Concurrency/"/>
      <url>/2023/06/25/Courses/15445/05-Index-Concurrency/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Concurrency-control"><a href="#Concurrency-control" class="headerlink" title="Concurrency control"></a>Concurrency control</h1><h2 id="What-is-the-correctness-criteria-of-concurrency-control"><a href="#What-is-the-correctness-criteria-of-concurrency-control" class="headerlink" title="What is the correctness criteria of concurrency control?"></a>What is the correctness criteria of concurrency control?</h2><ol><li>Logical Correctness: <ul><li>Can a thread see the data that it is supposed to see? For example, correctly control data race. </li></ul></li><li>Physical Correctness: <ul><li>Is the internal representation of the object sound? For example, when fetched a ptr, it will points to a correct address, and it won’t be freed between fetching and accessing. </li></ul></li></ol><h2 id="What-is-more-specific-difference-between-locks-and-latches"><a href="#What-is-more-specific-difference-between-locks-and-latches" class="headerlink" title="What is more specific difference between locks and latches?"></a>What is more specific difference between locks and latches?</h2><ol><li>What are they used to separate? What are they protecting? How long will they be held?<ul><li>Locks are used to separate user transactions accessing the same tuples in database contents. Locks are held in the entire transaction. </li><li>Latches are used to separate threads accessing the same in-memory data structures. Latchs are held in the critical section. </li></ul></li><li>How many modes do they have?<ul><li>Locks have four modes: shared, excusive, update, intention. </li><li>Latches only have two modes: read and write. </li></ul></li><li>How do they solve deadlock?<ul><li>Locks use detection and resolution by waits-for, timeout or abort. </li><li>Latches can only avoid deadlock by code discipline. </li></ul></li><li>Where are they kept?<ul><li>Locks are kept in Lock Manager while latches are kept in protected data structure. </li></ul></li></ol><h2 id="What-are-the-two-latch-modes"><a href="#What-are-the-two-latch-modes" class="headerlink" title="What are the two latch modes?"></a>What are the two latch modes?</h2><ol><li><p>Read mode means that Multiple threads can read the same object at the same time. </p><ul><li>A thread can acquire the read latch if another thread has it in read mode. </li></ul></li><li><p>Write mode onely allows one thread to access the object. </p><ul><li>A thread cannot acquire a write latch if another thread has it in any mode. </li></ul></li><li>In the compatibility matrix, only two read mode access is allowed. </li></ol><h2 id="What-are-the-different-latch-implementations"><a href="#What-are-the-different-latch-implementations" class="headerlink" title="What are the different latch implementations?"></a>What are the different latch implementations?</h2><ol><li>The first is blocking OS Mutex. <ul><li>It is simple to use. </li><li>It takes about $25\ ns$ per lock/unlock invocation, which means that it is non-scalable. <ul><li><code>std::mutex</code> is slower than <code>pthread_mutex</code> and <code>futex</code> is faster than both of them. </li></ul></li><li><code>futex</code> stands for fast userspace mutex. <ul><li>It has a userspace spinlock and a heavy-weight OS latch. </li><li>Threads will first try to acquire the userspace lock. If success, then that is good. </li><li>But if failed, the thread will fall back to the OS latch. And OS takes control of the thread with when to schedule it and DBMS can do nothing with the thread. Also <code>syscall</code> is expensive. </li></ul></li></ul></li><li>The second approach is reader-writer latches. <ul><li>It allows for concurrent readers. Must manage read/write queues to avoid starvation. </li><li>It can be implemented on top of spinlock. </li><li>Still <code>std::shared_mutex</code> is slower than <code>pthread_rwlock</code>. </li></ul></li></ol><h1 id="Latching-scheme"><a href="#Latching-scheme" class="headerlink" title="Latching scheme"></a>Latching scheme</h1><h2 id="Hash-table-latching"><a href="#Hash-table-latching" class="headerlink" title="Hash table latching"></a>Hash table latching</h2><h3 id="Why-are-deadlocks-not-possible-in-hash-table"><a href="#Why-are-deadlocks-not-possible-in-hash-table" class="headerlink" title="Why are deadlocks not possible in hash table?"></a>Why are deadlocks not possible in hash table?</h3><ol><li>All threads move in the same direction and only access a single page/slot at a time. <ul><li>Hence there are no loop waiting in this scenario. </li></ul></li><li>To resize the table, take a global write latch on the entire table. </li></ol><h3 id="How-can-we-design-hash-table-latching"><a href="#How-can-we-design-hash-table-latching" class="headerlink" title="How can we design hash table latching?"></a>How can we design hash table latching?</h3><ol><li>The coarser-grain approach is to use page latches. <ul><li>Each page has its own reader-writer latch that protects its entire contents. </li></ul></li><li>The finer-grain approach is to use slot latches. <ul><li>Each slot has its own latch. </li><li>It can use a single-mode latch to reduce meta-data and computational overhead. </li></ul></li></ol><h2 id="B-Tree-latching"><a href="#B-Tree-latching" class="headerlink" title="B+Tree latching"></a>B+Tree latching</h2><h3 id="What-should-we-use-latching-to-prevent"><a href="#What-should-we-use-latching-to-prevent" class="headerlink" title="What should we use latching to prevent?"></a>What should we use latching to prevent?</h3><ol><li>Threads trying to modify the contents of a node at the same time. (This is logical correctness, i.e. data race)</li><li>One thread traversing the tree while another thread splits/merges nodes. <ul><li>This is physical correctness. Splitting/merging will causing free pointers, nodes or in-node entries. </li><li>This will also cause a problem with logical correctness, i.e. false negative. <ul><li>If a thread get the pointer of node which possess the key it want, then before it accesses the node, the key is borrowed by a sibling causing the thread thought the key does not exists. </li></ul></li></ul></li><li>When introducing sibling pointers, we may have a deadlock situation. </li></ol><h3 id="How-should-we-achieve-physical-correctness"><a href="#How-should-we-achieve-physical-correctness" class="headerlink" title="How should we achieve physical correctness?"></a>How should we achieve physical correctness?</h3><ol><li>The most naive method is to hold all locks until the entire process is done. But its performance is a disaster. <ul><li>We must release some latches when we are sure it is safe. </li><li>According to our goal, a node is safe when we know that it won’t be changed (split, merge or redistribute) when updated. </li><li>We can know a child is safe when its child is not full on insertion or more than half-full on deletion, i.e. any later opereations can be isolated on or below its level. </li></ul></li><li>For find operation, there won’t have any updates. Hence we can always unlatch parent when acquired a R latch on child. </li><li>For insert or delete operation, we need to obtaining W latches as needed. Once child is latched, check if it is safe. <ul><li>If the child is safe, we can release all latches on ancestors. The latches should be released from top to bottom to have a better performance. </li></ul></li></ol><h3 id="What-is-the-problem-of-aforementioned-strategy"><a href="#What-is-the-problem-of-aforementioned-strategy" class="headerlink" title="What is the problem of aforementioned strategy?"></a>What is the problem of aforementioned strategy?</h3><ol><li>Every insert/delete operation will take a write latch on the root, which makes the root a bottleneck with higher concurrency. </li><li>We can make the assumption that most modifications to a B+Tree will not require a split or merge. </li><li>Hence, instead of assuming that there will be a split/merge as aforementioned, optimistically traverse the tree using read latches. If the guess is wrong in the end, just repeat traversal with pessimistic algorithm. </li><li>For insert/delete operations, set latches as if for search, get to leaf, and set W latch on leaf. If leaf is not safe, release all latches, and restart thread using previous insert/delete protocol with write latches. </li></ol><h3 id="When-will-a-deadlock-occur"><a href="#When-will-a-deadlock-occur" class="headerlink" title="When will a deadlock occur?"></a>When will a deadlock occur?</h3><ol><li>With sibling pointers, we may move from one leaf node to another leaf node where deadlock could occur. </li><li>Latches cannot detect deadlock, so the only solution is to kill one thread. </li><li>The leaf node sibling latch acquisition protocol must support a “no-wait” mode. The DBMS’s data structures must cope with failed latch acquisitions. </li><li>Though some scenario is not a deadlock, the waiting thread cannot know what the other thread is doing, which means it can only kill itself to avoid deadlock. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Concurrency Control </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>04 Data Organization</title>
      <link href="/2023/06/24/Courses/15445/04-Data-Organization/"/>
      <url>/2023/06/24/Courses/15445/04-Data-Organization/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="What-does-the-database-access-methods-layer-need-to-do"><a href="#What-does-the-database-access-methods-layer-need-to-do" class="headerlink" title="What does the database access methods layer need to do?"></a>What does the database access methods layer need to do?</h1><ol><li>Data Organization<ul><li>How we layout data structure in memory/pages and what information to store to support efficient access. </li><li>This layer will in charge of the organization of all kinds of data inside database, e.g. internal meta-data, core data storage, temporary data structures, table indexes. </li></ul></li><li><p>Concurrency</p><ul><li>How to enable multiple threads to access the data structure at the same time without causing problems. </li></ul></li><li><p>There are two types of data structure: hash tables and trees. </p></li></ol><h1 id="Hash-table"><a href="#Hash-table" class="headerlink" title="Hash table"></a>Hash table</h1><h2 id="How-is-the-naive-static-hash-table"><a href="#How-is-the-naive-static-hash-table" class="headerlink" title="How is the naive static hash table?"></a>How is the naive static hash table?</h2><ol><li><p>A hash table implements an unordered associative array that maps keys to values. </p></li><li><p>For any input key, hash functions return an integer representation of that key. </p></li><li><p>The space complexity is $O(n)$, the average time complexity is $O(1)$ and the worst time complexity is $O(n)$. </p><ul><li>Databases need to are about constants. For time complexity, smaller constants can save much time in billions of operations. </li><li>The space will be several times larger than the number of keys. </li></ul></li><li><p>The naive static hash table assumes: </p><ul><li>The number of elements is known ahead of time and fixed. </li><li>Each key is unique. </li><li>We have the perfect hash function where two different keys must have different hash values. </li></ul></li><li><p>These assumptions is not always satisfied. </p><ul><li><p>To suit when the first assumption failed, we need dynamic hashing scheme. </p></li><li><p>To suit when the last two assumptions failed, we need to design more delicate hash function to reduce collision rate and hashing scheme to handle collisions after hashing. </p><ul><li><p>The trade-off of hash function is between being fast and collision rate. </p></li><li><p>The trade-off of hashing scheme is between allocating a larger hash table and additional instructions to get/put keys. </p></li></ul></li></ul></li></ol><h2 id="What-are-hash-functions"><a href="#What-are-hash-functions" class="headerlink" title="What are hash functions?"></a>What are hash functions?</h2><ol><li><p>We do not want to use a cryptographic hash function for DBMS hash tables, e.g., SHA-2. </p><ul><li>Because this hashing is only using internally, we will never worry about leaking keys. </li><li>We want something that is fast and has a low collision rate.</li></ul></li><li><p>The commonly used hash functions are: </p><ul><li>CRC-64: Used in networking for error detection. </li><li>MurmurHash: Designed as a fast, general-purpose hash function. </li><li>Google CityHash: Designed to be faster for short keys (&lt;64 bytes). </li><li>Facebook XXHash: From the creator of zstd compression, which is the state-of-the-art. </li><li>Google Farmhash: Newer version of CityHash with better collision rates. </li></ul></li><li><p>Their speed comparison is as followed:</p><p><img src="/imgs/15445/Organization/hash_func.png" width="75%"></p></li></ol><h2 id="Static-hashing-schemes"><a href="#Static-hashing-schemes" class="headerlink" title="Static hashing schemes"></a>Static hashing schemes</h2><h3 id="How-is-linear-probe-hashing"><a href="#How-is-linear-probe-hashing" class="headerlink" title="How is linear probe hashing?"></a>How is linear probe hashing?</h3><ol><li>As all the following schemes, a key-value paire is stored in hashing table. The key is to determin whether this is actually the key we want to find. </li><li>It resolve collisions by linearly searching for the next free slot in the table. </li><li>To determine whether an element is present, hash to a location in the index and scan for it. If find an empty slot before the key, then the element does not exists. </li><li>To delete an entry, there are two approaches: <ul><li><strong>Movement</strong>: Rehash rest of the keys until find the first empty slot. Nobody actually does this. </li><li><strong>Tombstone</strong>: Set a marker to indicate that the entry in the slot is logically deleted. The slot can be reused for new keys. This may still need periodic garbage collection. </li></ul></li></ol><h3 id="How-to-solve-the-non-unique-keys-problem"><a href="#How-to-solve-the-non-unique-keys-problem" class="headerlink" title="How to solve the non-unique keys problem?"></a>How to solve the non-unique keys problem?</h3><ol><li>The first choise is to store values in separate storage area for each key and the value of hash table points to the area. </li><li>The second choise is to store duplicate keys entries together in the hash table. <ul><li>Read would return the first key they found. </li><li>Deletes would remove all the keys or a specific key-value pair. </li></ul></li></ol><h3 id="How-is-the-robin-hood-hashing"><a href="#How-is-the-robin-hood-hashing" class="headerlink" title="How is the robin hood hashing?"></a>How is the robin hood hashing?</h3><ol><li>This is a variant of linear probe hashing that steals slots from “rich” keys and give them to “poor” keys. <ul><li>Each key tracks the number of positions they are from where its optimal position (original hash value) in the table. </li><li>The keys farther away from its optimal position is poorer while the keys closer is richer. </li><li>On insert, a key takes the slot of another key if the first key is “richer” than the second key. And the second key will keep searching linearly until it found another “richer” key. </li></ul></li><li>Stealing increases the number of writing operation compare with the linear probing scheme while may reduce the time of worst case. </li><li>This could have cascading/flooding problem where one insert causing multiple “stealing”. Also it does not consider the possibility of hot keys. </li></ol><h3 id="How-is-the-cuckoo-hashing"><a href="#How-is-the-cuckoo-hashing" class="headerlink" title="How is the cuckoo hashing?"></a>How is the cuckoo hashing?</h3><ol><li>Use multiple hash tables with different hash function seeds to ensure they won’t hash to the same value. <ul><li>On insert, check every table and pick anyone that has a free slot. </li><li>If no table has a free slot, choose one victim, evict it and then re-hash it find a new location. </li></ul></li><li>Look-ups and deletions are always $O(1)$ because only one location per hash table is checked. </li><li>There is a possibility of cascading. The worst case is an infinite cascading loop. <ul><li>We can use extra code to detect if replacing is going in a loop. If so, we need to double the hash table size with new hash functions and re-insert all keys. </li></ul></li></ol><h2 id="Dynamic-hashing-schemes"><a href="#Dynamic-hashing-schemes" class="headerlink" title="Dynamic hashing schemes"></a>Dynamic hashing schemes</h2><h3 id="How-is-chained-hashing"><a href="#How-is-chained-hashing" class="headerlink" title="How is chained hashing?"></a>How is chained hashing?</h3><ol><li>It maintains a linked list of buckets for each slot in the hash table. </li><li>Resolve collisions by placing all elements with the same hash key into the same bucket. <ul><li>To determine whether an element is present, hash to its bucket and scan for it. </li></ul></li><li>The problem is that the linked list can grow forever causing the time spent on search increases as the system runs. </li></ol><h3 id="How-is-extendible-hashing"><a href="#How-is-extendible-hashing" class="headerlink" title="How is extendible hashing?"></a>How is extendible hashing?</h3><ol><li>Multiple slot locations can point to the same bucket chain. </li><li>For the number of bits needs to examine, there is a global one and a local one. </li><li>It reshuffle bucket entries on split and increase the number of bits to examine when a list is full. <ul><li>If there is only one slot pointing to this list, i.e. the global examining bits is the same as the local one, <ul><li>The DBMS increases the global number causing the size of hashing table doubled. </li><li>Those lists with smaller local number will set the pointers of corresponding new slots (with only the last global examining bit different) still to themselves. </li><li>DBMS also increases the local examining bits of the fulled list. The keys in it will re-hash to their two new lists. </li></ul></li><li>If there are more than one slots pointing to this list, i.e. the global examning bits is larger the the local one, <ul><li>The DBMS only increases the local examining bits of the fulled list and do the re-hashing. </li></ul></li></ul></li></ol><h3 id="How-is-linear-hashing"><a href="#How-is-linear-hashing" class="headerlink" title="How is linear hashing?"></a>How is linear hashing?</h3><ol><li>It is similar to extensible hashing. But the hash table maintains a pointer that tracks the next bucket to split. </li><li>When any bucket overflows, split the bucket at the pointer location. <ul><li>When the split causing the number of slots in hash table doubled, it introduces a new hash function taking modulo with the new hash table size. </li></ul></li><li>It uses multiple hashes to find the right bucket for a given key. <ul><li>It always first use the old hash function with smaller modulus. </li><li>If this hash value is smaller than the split pointer, which means that this slot has already been splitted, DBMS need to use the new hash function to find the true hash value. </li><li>Otherwise, this slot is not splitted and the old hash value works fine. </li></ul></li><li>Splitting buckets based on the split pointer will eventually get to all overflowed buckets. <ul><li>When the pointer reaches the last slot, delete the first hash function and move back to beginning. </li></ul></li><li>Deleting may cause the size of hash table to shrink when it deleted the only entry in the second half of the hash table. <ul><li>DMBS shrinks the size of hash table and deletes the new hash function. </li></ul></li></ol><h1 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h1><h2 id="Table-indexes"><a href="#Table-indexes" class="headerlink" title="Table indexes"></a>Table indexes</h2><h3 id="What-does-table-indexs-do"><a href="#What-does-table-indexs-do" class="headerlink" title="What does table indexs do?"></a>What does table indexs do?</h3><ol><li>A table index is a replica of a subset of a table’s attributes that are organized and/or sorted for efficient access using those attributes. </li><li>It is used in queries to find tuples with attributes matches certain values. </li><li>The DBMS ensures that the contents of the table and the index are logically synchronized. </li><li>It is the DBMS’s job to figure out the best index(es) to use to execute each query. </li><li>There is a trade-off regarding the number of indexes to create per database, i.e. the lookup speed and synchronization overhead. <ul><li>We need extra storage overhead to store the data structure and maintenance overhead to keep synchronization. </li></ul></li></ol><h3 id="What-is-clustered-indexes"><a href="#What-is-clustered-indexes" class="headerlink" title="What is clustered indexes?"></a>What is clustered indexes?</h3><ol><li>The table is stored in the sort order specified by the primary key. <ul><li>It can be either heap- or index-organized storage. </li></ul></li><li>Some DBMSs always use a clustered index. If a table does not contain a primary key, the DBMS will<br>automatically make a hidden primary key. </li><li>Other DBMSs cannot use them at all. </li></ol><h2 id="Basic-B-Tree"><a href="#Basic-B-Tree" class="headerlink" title="Basic B+ Tree"></a>Basic B+ Tree</h2><h3 id="What-is-a-B-Tree"><a href="#What-is-a-B-Tree" class="headerlink" title="What is a B+ Tree?"></a>What is a B+ Tree?</h3><ol><li>A B+Tree is a self-balancing tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions always in $O(\log n)$. </li><li>A B+Tree is an M-way search tree. <ul><li>It is perfectly balanced, i.e. every leaf node is at the same depth in the tree. </li><li>Every node other than the root is at least half-full, i.e. $M/2-1 ≤ number\ of\ keys ≤ M-1$. </li><li>Every inner node with $k$ keys has $k+1$ non-null children. </li></ul></li></ol><h3 id="How-is-key-value-pairs-stored-in-each-node"><a href="#How-is-key-value-pairs-stored-in-each-node" class="headerlink" title="How is key/value pairs stored in each node?"></a>How is key/value pairs stored in each node?</h3><ol><li>Every B+Tree node is comprised of an array of key/value pairs. </li><li>The keys are derived from the attribute(s) that the index is based on. </li><li>The value stored in a inner node is a pointer to its corresponding children while the value stored in a leaf node is its specific value. </li><li>There are two storage methods for the key-value pair. <ul><li>One is to store them consecutively, i.e. each value is stored immediately after its key. </li><li>The other is to store them in the same place in two arrays, i.e. each value has the same index as its key. </li></ul></li><li>The arrays are (usually) kept in sorted key order.</li></ol><h3 id="What-is-stored-is-leaf-node"><a href="#What-is-stored-is-leaf-node" class="headerlink" title="What is stored is leaf node?"></a>What is stored is leaf node?</h3><ol><li><p>Each leaf node also has pointers points to its siblings. </p></li><li><p>There are two approaches to store the values: </p><ul><li>The first approach is to store them with record IDs which is a pointer to the page ID and offset of the tuple to which the index entry corresponds. </li><li>The second approach is to store them with tuple data, specifically primary keys. </li></ul></li><li><p>The pros of the second approach is that we do not need to access another address to fetch the contents when primary keys are all we want. </p><p>But seondary indexs must store the Record ID as their values. Hence if we want secondary indexs in second approach, we need to lookup another tables to find the Record ID with the same primary keys. </p></li></ol><h3 id="What-is-the-pros-and-cons-of-B-Tree-compare-with-B-Tree"><a href="#What-is-the-pros-and-cons-of-B-Tree-compare-with-B-Tree" class="headerlink" title="What is the pros and cons of B-Tree compare with B+Tree?"></a>What is the pros and cons of B-Tree compare with B+Tree?</h3><ol><li>The main difference is that  B-Tree stored keys and values in all nodes in the tree while B+Tree only stores values in leaf nodes. Inner nodes only guide the search process. </li><li>B-Tree is more space-efficient, since each key only appears once in the tree. </li><li>However, when we want to sequential access keys, B-Tree needs to jump between pages causing much more I/O. </li></ol><h3 id="How-does-B-Tree-insert-a-node"><a href="#How-does-B-Tree-insert-a-node" class="headerlink" title="How does B+Tree insert a node?"></a>How does B+Tree insert a node?</h3><ol><li>Find correct leaf node L. Insert data entry into L in sorted order. </li><li>If L has enough space, then it is done. </li><li>Otherwise, split L keys into L and a new node L2. Insert index entry pointing to L2 into parent of L. <ul><li>The parent of L may need rebalance after this insertion. </li></ul></li></ol><h3 id="How-does-B-Tree-delete-a-node"><a href="#How-does-B-Tree-delete-a-node" class="headerlink" title="How does B+Tree delete a node?"></a>How does B+Tree delete a node?</h3><ol><li>Start at root, find leaf L where entry belongs. Remove the entry. </li><li>If L is at least half-full, then it is done. </li><li>If L has only M/2-1 entries, <ul><li>First try to re-distribute, borrowing from sibling. </li><li>If re-distribution fails, merge L and sibling and delete entry (pointing to L or sibling) from parent of L. <ul><li>The parent of L may need rebalance . </li></ul></li></ul></li></ol><h2 id="B-Tree-usage-and-design"><a href="#B-Tree-usage-and-design" class="headerlink" title="B+Tree usage and design"></a>B+Tree usage and design</h2><h3 id="How-does-DBMS-use-B-Tree-in-selection-query"><a href="#How-does-DBMS-use-B-Tree-in-selection-query" class="headerlink" title="How does DBMS use B+Tree in selection query?"></a>How does DBMS use B+Tree in selection query?</h3><ol><li><p>When creating an index, a certain order of attributes are specified to sort the tuple. </p><ul><li><p>When $A_1, \dots,A_n$ are specified, the B+Tree will store corresponding $n$ values in as keys in each node. </p></li><li><p>When $a_1, \dots,a_n$ are stored in one node, all nodes in its left sub-tree have $A_1≤a_1,\dots,A_n≤a_n$ while all nodes in its right sub-tree only guarenteed with $A_1 &gt; a_1$, i.e. the B+Tree is maintained with $A_1$ as its primary sorting key. </p></li></ul></li><li><p>In a selection condition, we can easily select with certain condition on $A_1,\dots,A_k\ (k≤n)$. Some DBMS also support conditions specified only on $A_k, \dots,A_n$, which requires DBMS sequentially access all leaf nodes. </p></li><li><p>Compared with hash table, B+Tree can better support selection without a knowing attributes to lookup for. </p></li></ol><h3 id="How-to-handle-duplicate-keys"><a href="#How-to-handle-duplicate-keys" class="headerlink" title="How to handle duplicate keys?"></a>How to handle duplicate keys?</h3><ol><li>The first approach is to add the tuple’s unique Record ID as part of the key to ensure that all keys are unique. <ul><li>The DBMS can still use partial keys to find tuples.</li></ul></li><li>The second approach is to allow leaf nodes to spill into overflow nodes that contain the duplicate keys. <ul><li>Only duplicate keys of existing keys can overflow. </li><li>When split or merge nodes, overflow nodes also need to split or merge. </li><li>This is more complex to maintain and modify. </li></ul></li></ol><h3 id="How-to-solve-redundant-page-jump-when-sequential-access-leaf-nodes"><a href="#How-to-solve-redundant-page-jump-when-sequential-access-leaf-nodes" class="headerlink" title="How to solve redundant page jump when sequential access leaf nodes?"></a>How to solve redundant page jump when sequential access leaf nodes?</h3><ol><li>In a clustered B+Tree, nodes in the same page are in consecutive order. <ul><li>We can traverse to the left-most leaf page and then retrieve tuples from all leaf pages. </li><li>This will always be better than sorting data for each query. </li></ul></li><li>In a non-clustered B+Tree, the DBMS can first figure out all the tuples that it needs and then sort them based on their Page ID. </li></ol><h3 id="How-to-choose-node-size"><a href="#How-to-choose-node-size" class="headerlink" title="How to choose node size?"></a>How to choose node size?</h3><ol><li>The slower the storage device, the larger the optimal node size for a B+Tree. <ul><li>HDD takes $1\ MB$, SSD usually takes $10\ KB$, in-memory nodes have $512\ B$. </li></ul></li><li>Optimal sizes can vary depending on the workload. The trade off is between leaf node scans and root-to-leaf traversals. <ul><li>With larger size, we can have more sequential reads in leaf node scans, but we need to read more data in root-to-leaf traversals. </li></ul></li></ol><h3 id="How-to-choose-merge-threshold"><a href="#How-to-choose-merge-threshold" class="headerlink" title="How to choose merge threshold?"></a>How to choose merge threshold?</h3><ol><li>Some DBMSs do not always merge nodes when they are half full. </li><li>Delaying a merge operation may reduce the amount of reorganization. They assume that the missing part will be filled soon. </li><li>It may also be better to just let smaller nodes exist and then periodically rebuild entire tree. </li></ol><h3 id="How-to-handle-variable-length-keys"><a href="#How-to-handle-variable-length-keys" class="headerlink" title="How to handle variable length keys?"></a>How to handle variable length keys?</h3><ol><li>The first approach is to store the keys as pointers to the tuple’s attribute. </li><li>The second approach is to allow variable-length nodes. It requires careful memory management. </li><li>The third approach is always to pad the key to be max length of the key type. </li><li>The last approach is key map or indirection. It is similar to the in-node dictionary. </li></ol><h3 id="How-can-we-do-the-intra-node-search"><a href="#How-can-we-do-the-intra-node-search" class="headerlink" title="How can we do the intra-node search?"></a>How can we do the intra-node search?</h3><ol><li>The naive method is linear search. We can use SIMD to vectorize the process. </li><li>The second method is binary search given that keys in a node are already sorted. </li><li>The third method is interpolation search. <ul><li>This requires known distribution of keys. </li><li>It jumps to approximate location of desired key based on known distribution. </li></ul></li></ol><h3 id="How-can-we-optimize-space-usage"><a href="#How-can-we-optimize-space-usage" class="headerlink" title="How can we optimize space usage?"></a>How can we optimize space usage?</h3><ol><li>Prefix compression<ul><li>Sorted keys in the same leaf node are likely to have the same prefix. </li><li>Extract common prefix and store only unique suffix for each key. </li></ul></li><li>Deduplication<ul><li>Non-unique indexes can end up storing multiple copies of the same key in leaf nodes. </li><li>Store the key once and then maintain a list of tuples with that key. </li></ul></li><li>Suffix truncation<ul><li>The keys in the inner nodes are only used to “direct traffic”. We don’t need the entire key. </li><li>Store a minimum prefix that is needed to correctly route probes into the index. </li></ul></li></ol><h3 id="How-can-we-optimize-consumed-time"><a href="#How-can-we-optimize-consumed-time" class="headerlink" title="How can we optimize consumed time?"></a>How can we optimize consumed time?</h3><ol><li>Pointer swizzling<ul><li>Nodes use page ids to reference other nodes in the index. The DBMS must get the memory location from the page table during traversal. </li><li>If a page is pinned in the buffer pool, then we can store raw pointers instead of page IDs. This avoids address lookups from the page table. </li></ul></li><li>Bulk insert<ul><li>The fastest way to build a new B+Tree for an existing table is to first sort the keys and then build the index from the bottom up. </li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Data structure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03 Buffer Pool Manage</title>
      <link href="/2023/06/24/Courses/15445/03-Buffer-Pool-Manage/"/>
      <url>/2023/06/24/Courses/15445/03-Buffer-Pool-Manage/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Buffer-pool-manage"><a href="#Buffer-pool-manage" class="headerlink" title="Buffer pool manage"></a>Buffer pool manage</h1><h2 id="What-does-databse-storage-need-to-control"><a href="#What-does-databse-storage-need-to-control" class="headerlink" title="What does databse storage need to control?"></a>What does databse storage need to control?</h2><ol><li>Spatial Control: Where to write pages on disk. <ul><li>The goal is to keep pages that are used together often as physically close together as possible on disk. </li></ul></li><li>Temporal Control: When to read pages into memory, and when to write them to disk. <ul><li>The goal is to minimize the number of stalls from having to read data from disk. </li></ul></li></ol><h2 id="How-is-buffer-pool-organized"><a href="#How-is-buffer-pool-organized" class="headerlink" title="How is buffer pool organized?"></a>How is buffer pool organized?</h2><ol><li>Memory region organized as an array of fixed-size pages. An array entry is called a <strong>frame</strong>. <ul><li>Pages are in disk while frames are in memory. </li></ul></li><li>When the DBMS requests a page, an exact copy is placed into one of these frames. </li><li>The buffer pool manager need to maintain a <strong>page table</strong> to keep track of pages that are currently in memory. <ul><li>The page table is the mapping from page ids to a copy of the page in buffer pool frames. <ul><li>This is an in-memory data structure that does not need to be stored on disk. </li></ul></li><li>The page directory is the mapping from page ids to page locations in the database files. <ul><li>All changes must be recorded on disk to allow the DBMS to find on restart. </li></ul></li></ul></li><li>Some additional meta-data per page also need to be maintained: <ul><li><strong>Dirty flag</strong>: Mark whether a page has been modified. Used to know whether can safely evict this page. </li><li><strong>Pin/reference counter</strong>: Some query use it to prevent buffer pool manager from evicting this page. They unpin this page after no-longer use it. </li><li><strong>Latches</strong>: Can be used to pre-occupy an entry in page table, and release the latch after copied that page and updated the entry. </li><li>These meta-data can either be stored in page or page table. </li></ul></li></ol><h2 id="Clarification-What-is-the-locks-and-latches-referenced-in-database"><a href="#Clarification-What-is-the-locks-and-latches-referenced-in-database" class="headerlink" title="Clarification: What is the locks and latches referenced in database?"></a>Clarification: What is the locks and latches referenced in database?</h2><ol><li>Locks:<ul><li>It protects the database’s logical contents from other transactions. </li><li>It is held for transaction duration. Need to be able to rollback changes. </li><li>Locks can be taken on a tuple or table, but not on a page. </li></ul></li><li>Latches:<ul><li>It protects the critical sections of the DBMS’s internal data structure from other threads.</li><li>It is held for operation duration. Do not need to be able to rollback changes. </li><li>This is similar to the mutex provided by OS. </li></ul></li></ol><h2 id="How-can-we-optimize-performance-with-multiple-buffer-pools"><a href="#How-can-we-optimize-performance-with-multiple-buffer-pools" class="headerlink" title="How can we optimize performance with multiple buffer pools?"></a>How can we optimize performance with multiple buffer pools?</h2><ol><li>Advantages: <ul><li>Different pool can have different evict policy to improve locality. </li><li>Each time accessing meta-data of each buffer pool need to take a latch. Multiple buffer pool can reduce latch contention. </li></ul></li><li>The manager can use per-database buffer pool, per-page type buffer pool or per-index type buffer pool. </li><li>Manger decide which pool to store pages in two approaches: <ul><li>The first is to embed an object identifier in record ids and then maintain a mapping from objects to specific buffer pools. <ul><li>This can be used to specify certain pages must be stored in specific pool. </li></ul></li><li>The other is to hash the page id to select which buffer pool to access. </li></ul></li></ol><h2 id="How-can-we-optimize-performance-with-pre-fetching"><a href="#How-can-we-optimize-performance-with-pre-fetching" class="headerlink" title="How can we optimize performance with pre-fetching?"></a>How can we optimize performance with pre-fetching?</h2><ol><li>Pre-fetching can be easily used in two situations: sequential scans and index scans. </li><li>In sequential scans, when manager fetched the first few consecutive pages, it can infer that the next consecutive pages will be used soon and pre-fetch them into memory. </li><li>In index scans, the query may not access consecutive pages, but DBMS sees the B+ tree structure, and thus knows the where are the following indices. </li></ol><h2 id="How-can-we-optimize-performance-with-scan-sharing"><a href="#How-can-we-optimize-performance-with-scan-sharing" class="headerlink" title="How can we optimize performance with scan sharing?"></a>How can we optimize performance with scan sharing?</h2><ol><li>It allows multiple queries to attach to a single cursor that scans a table. <ul><li>The queries do not have to be the same. </li><li>They can also share intermediate results. </li></ul></li><li>If a query wants to scan a table and another query is already doing this, then the DBMS will attach the second query’s cursor to the existing cursor. </li><li>After the ealier query finished, the later query will return to read the pages the earlier one already read before the later one begins. </li><li>If the later query has a <code>LIMIT</code> clause with out <code>WHERE</code> or <code>ORDER BY</code> clause, given that the relation is unordered, it may do not need to read extra data if those scan sharing data is enough to satisfy the <code>LIMIT</code> clause. </li></ol><h2 id="How-can-we-optimize-performance-with-buffer-pool-bypass"><a href="#How-can-we-optimize-performance-with-buffer-pool-bypass" class="headerlink" title="How can we optimize performance with buffer pool bypass?"></a>How can we optimize performance with buffer pool bypass?</h2><ol><li>Sequential flooding: A query performs a sequential scan that reads every page. <ul><li>This pollutes the buffer pool with pages that are read once and then never again. </li></ul></li><li>Buffer pool bypass will not store fetched pages in the buffer pool. Insteach, the manager has a private pool where those pages will be stored temporarily and deleted once finished. </li><li>It works well if operator needs to read a large sequence of pages that are contiguous on disk. It can also be used for temporary data (sorting, joins). </li></ol><h2 id="Should-we-use-OS-page-cache"><a href="#Should-we-use-OS-page-cache" class="headerlink" title="Should we use OS page cache?"></a>Should we use OS page cache?</h2><ol><li>Most disk operations go through the OS API. Unless the DBMS tells it not to, the OS maintains its own filesystem cache. </li><li>Most DBMSs use direct I/O (<code>O_DIRECT</code>) to bypass the OS’s cache. </li><li>The OS page cache can cause redundant copies of pages. OS and DBMS have different eviction policies causing DBMS Loss of control over file I/O. </li></ol><h1 id="Buffer-replacement-policies"><a href="#Buffer-replacement-policies" class="headerlink" title="Buffer replacement policies"></a>Buffer replacement policies</h1><h2 id="What-is-LRU-Least-Recently-Used-policy-and-clock-policy"><a href="#What-is-LRU-Least-Recently-Used-policy-and-clock-policy" class="headerlink" title="What is LRU (Least-Recently Used) policy and clock policy?"></a>What is LRU (Least-Recently Used) policy and clock policy?</h2><ol><li>Manager maintain a single timestamp of when each page was last accessed. </li><li>When the DBMS needs to evict a page, select the one with the oldest timestamp. <ul><li>It can keep the pages in sorted order to reduce the search time on eviction. </li></ul></li><li>An approximation of LRU that does not need a separate timestamp per page is clock policy. <ul><li>Each page has a reference bit. When a page is accessed, set to $1$. </li><li>Organize the pages in a circular buffer with a “clock hand”: Upon sweeping, check if a page’s bit is set<br>to 1. If yes, set to zero. If no, then evict. </li></ul></li></ol><h2 id="What-is-the-problem-of-LRU-and-clock-how-to-alleviate"><a href="#What-is-the-problem-of-LRU-and-clock-how-to-alleviate" class="headerlink" title="What is the problem of LRU and clock, how to alleviate?"></a>What is the problem of LRU and clock, how to alleviate?</h2><ol><li>LRU and CLOCK replacement policies are susceptible to sequential flooding. And in some workloads the most recently used page is the most unneeded page. </li><li><strong>LRU-K</strong> policy can alleviate the problem. <ul><li>Track the history of last K references to each page as timestamps and compute the average interval between subsequent accesses. And evict the one that will be accessed latest according to the prediction. </li><li>In the implementation, the manager only need to maintain a queue of size K. Pop out the oldest timestamp when a new timestamp arrives. </li></ul></li><li>Another policy is <strong>localization</strong>: The DBMS chooses which pages to evict on a per transaction/query basis, e.g. it allocate private frames to the query. </li><li><strong>Priority hints</strong>: The DBMS knows about the context of each page during query execution. It can provide hints to the buffer pool on whether a page is important or not.</li></ol><h2 id="How-do-we-deal-with-evicted-pages"><a href="#How-do-we-deal-with-evicted-pages" class="headerlink" title="How do we deal with evicted pages?"></a>How do we deal with evicted pages?</h2><ol><li>Fast Path: If a page in the buffer pool is not dirty, then the DBMS can simply drop it. </li><li>Slow Path: If a page is dirty, then the DBMS must write back to disk to ensure that its changes are persisted. <ul><li>The DBMS can periodically walk through the page table and write dirty pages to disk. </li><li>When a dirty page is safely written, the DBMS can either evict the page or just unset the dirty flag. </li><li>Need to be careful that the system doesn’t write dirty pages before their log records are written. </li></ul></li><li>The Trade-off is between fast evictions versus dirty writing pages that will not be read again in the future. </li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02 Storage</title>
      <link href="/2023/06/24/Courses/15445/02-Storage/"/>
      <url>/2023/06/24/Courses/15445/02-Storage/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Disk-based-architecture"><a href="#Disk-based-architecture" class="headerlink" title="Disk-based architecture"></a>Disk-based architecture</h1><h2 id="What-are-the-storage-devices"><a href="#What-are-the-storage-devices" class="headerlink" title="What are the storage devices?"></a>What are the storage devices?</h2><ol><li><p>The first type is the volatile memory: CPU registers, CPU caches and DRAM</p><ul><li>They provide random access and they are byte-addressable. </li></ul></li><li><p>The second type is the non-volatile disks: SSD, HDD and network storage. </p><ul><li>They only provide sequential access.  <ul><li>Random access on non-volatile storage is almost always much slower than sequential access. </li><li>DBMS will want to maximize sequential access. </li></ul></li><li>They are block-addressable. </li></ul></li><li><p>The access times of each storage is as followed:</p><p><img src="/imgs/15445/Storage/access_times.png" width="50%"></p></li></ol><h2 id="What-is-disk-oriented-DMBS"><a href="#What-is-disk-oriented-DMBS" class="headerlink" title="What is disk-oriented DMBS?"></a>What is disk-oriented DMBS?</h2><ol><li>The DBMS assumes that the primary storage location of the database is on non-volatile disk. </li><li>DBMS manages a buffer pool in memory where directory and data pages are stored. </li><li>When execution engine asks DBMS for a certain page: <ul><li>If that page is not in memory already, DBMS will look up directory first (if also not in memory, load from disk) to find the disk position of that page. </li><li>Then DBMS will load that page from disk and return a pointer to the buffer pool to execution engine. </li></ul></li></ol><h2 id="Why-not-use-the-OS-memory-mapping-virtual-memory"><a href="#Why-not-use-the-OS-memory-mapping-virtual-memory" class="headerlink" title="Why not use the OS memory mapping (virtual memory)?"></a>Why not use the OS memory mapping (virtual memory)?</h2><ol><li>Transaction safety: OS can flush dirty pages at any time causing dirty data corrupt database. OS doesn’t know anything about transaction, hence it doesn’t care whether it is safe to write a page to disk. </li><li>IO stalls: When a page miss happens, the thread will be stalled and DBMS can do nothing about it. <ul><li>Allowing multiple threads to access the <code>mmap</code> files to hide page fault stalls is good enough for read-only access. But it is complicated when there are multiple writers. </li></ul></li><li>Error handling: Any access can cause a <code>SIGBUS</code> that the DBMS must handle. However, DBMS may isolate the error and handle it only in the storage layer. </li><li>Performance issues: Like OS data structure contention or TLB shootdowns. </li><li>In conclusion, DBMS always knows better than OS. Thus DBMS almost always wants to control things itself and can do a better job than the OS. <ul><li>Like how to flush dirty pages to disk in the correct order, provide specialized prefetching, better buffer replacement policy and thread/process scheduling. </li></ul></li></ol><h1 id="Page-oriented-architecture"><a href="#Page-oriented-architecture" class="headerlink" title="Page-oriented architecture"></a>Page-oriented architecture</h1><h2 id="File-storage"><a href="#File-storage" class="headerlink" title="File storage"></a>File storage</h2><h3 id="How-does-DBMS-store-files"><a href="#How-does-DBMS-store-files" class="headerlink" title="How does DBMS store files?"></a>How does DBMS store files?</h3><ol><li>The DBMS stores a database as one or more files on disk typically in a proprietary format. </li><li>The storage manager is responsible for maintaining a database’s files. <ul><li>It organizes the files as a collection of pages. </li><li>It also tracks data read/written to pages and the available space. </li><li>Some do their own scheduling for reads and writes to improve spatial and temporal locality of pages. </li></ul></li><li>A database page is a fixed-size block of data. <ul><li>Most systems do not mix page types, i.e. data in a page belong to the same table. </li><li>Some systems require a page to be self-contained, i.e. all metadata we need to interpret the page has to be contained in the page in itself. </li><li>Hardware pages and OS pages are usually $4\ KB$. But database pages may be $512\ B-16\ KB$ depending on DBMS and configuration. <ul><li>Larger page size can increase sequential IO, issue less system call and have a smaller page table. </li><li>Smaller page size only need to maintain less memory when only need small amout of data. </li></ul></li></ul></li></ol><h3 id="How-does-DBMS-manage-pages-in-files-on-disk"><a href="#How-does-DBMS-manage-pages-in-files-on-disk" class="headerlink" title="How does DBMS manage pages in files on disk?"></a>How does DBMS manage pages in files on disk?</h3><ol><li>There are different ways to manage: Heap File Organization, Tree File Organization, Sequential / Sorted File Organization (ISAM), or Hashing File Organization. </li><li>A heap file is an unordered collection of pages with tuples that are stored in random order. </li><li>The DBMS maintains directory in special pages that tracks the location of data pages in the database files. <ul><li>Must make sure that the directory pages are in sync with the data pages. </li><li>The directory also records meta-data about available space: the number of free slots per page and list of free / empty pages. </li></ul></li></ol><h2 id="Page-layout"><a href="#Page-layout" class="headerlink" title="Page layout"></a>Page layout</h2><h3 id="What-is-stored-in-each-page"><a href="#What-is-stored-in-each-page" class="headerlink" title="What is stored in each page?"></a>What is stored in each page?</h3><ol><li>Every page contains a header of meta-data about the page’s contents. <ul><li>Page Size</li><li>Checksum to check whether data is corrupted</li><li>DBMS version of creator of this page, to provide compatibility even when DBMS update changed the page layout. With this, when DBMS is updated, it can correct re-layout the data. </li><li>Transaction Visibility</li><li>Compression Information</li></ul></li><li>The data in a page can be organize in tuple-oriented or log-structured. </li></ol><h3 id="How-to-organize-tuple-oriented-data"><a href="#How-to-organize-tuple-oriented-data" class="headerlink" title="How to organize tuple-oriented data?"></a>How to organize tuple-oriented data?</h3><ol><li><p>A naive strategy is to store the number of tuples in the header, and just append a new tuple to the end. </p><ul><li>What if we delete a tuple? <ul><li>How do we know there are available space? </li><li>What do we do to the following tuples? Do we move them forward, or just leave them there?</li></ul></li><li>More serious problem is what happens if we have a variable-length attribute?<ul><li>How can we know the begin and end of a tuple?</li></ul></li></ul></li><li><p>In the slotted pages scheme, we store a <strong>slot array</strong> at the header. </p><ul><li><p>The slot array maps “slots” to the tuples’ starting position offsets. </p></li><li><p>The header keeps track of the number of used slots and the offset of the starting location of the</p><p>last slot used. </p></li><li><p>The space used to store tuples grows from tail to head, while the slot array grows from head to tail. </p></li><li><p>When a tuple is deleted, we need to invalidate its value in slot array where we can know available space. </p><ul><li>As for its following tuples, we can either leave them as be, or compact the space. </li><li>As modification goes in memory, and deleting is usually holding a lock, compact the space can be fast and there is no need to inform rest of the system. </li></ul></li><li><p><img src="/imgs/15445/Storage/slot-array.png" width="25%"></p></li></ul></li></ol><h3 id="How-do-we-find-the-tuple-we-need-in-a-page"><a href="#How-do-we-find-the-tuple-we-need-in-a-page" class="headerlink" title="How do we find the tuple we need in a page?"></a>How do we find the tuple we need in a page?</h3><ol><li>Each tuple is assigned a unique record identifier. </li><li>Most commonly used is <code>page_id + offset/slot</code>. </li></ol><h2 id="Tuple-layout"><a href="#Tuple-layout" class="headerlink" title="Tuple layout"></a>Tuple layout</h2><h3 id="What-is-stored-in-a-tuple"><a href="#What-is-stored-in-a-tuple" class="headerlink" title="What is stored in a tuple?"></a>What is stored in a tuple?</h3><ol><li>A tuple is essentially a sequence of bytes. It’s the job of the DBMS to interpret those bytes into attribute types and values. </li><li>Each tuple is prefixed with a header that contains meta-data about it, e.g visibility info for concurrency control, Bit Map for NULL values. <ul><li>We do not need to store meta-data about the schema. </li></ul></li><li>Attributes are typically stored in the order that you specify them when you create the table. </li></ol><h3 id="What-is-denormalized-data"><a href="#What-is-denormalized-data" class="headerlink" title="What is denormalized data?"></a>What is denormalized data?</h3><ol><li>DBMS can physically denormalize (pre join) related tuples and store them together in the same page. <ul><li>For two tables, one table has an attribute referenced to another table, DBMS can store the tuples and thier referenced tuples in the same slot. </li><li><img src="/imgs/15445/Storage/denorm_declare.png" width="25%"></li><li><img src="/imgs/15445/Storage/denorm_diagram.png" width="25%"></li></ul></li><li>This can potentially reduces the amount of I/O for common workload patterns but also make updates more expensive. </li></ol><h1 id="Log-structured-storage"><a href="#Log-structured-storage" class="headerlink" title="Log-structured storage"></a>Log-structured storage</h1><h2 id="What-is-stored-in-log-structured-storage"><a href="#What-is-stored-in-log-structured-storage" class="headerlink" title="What is stored in log-structured storage?"></a>What is stored in log-structured storage?</h2><ol><li>DBMS stores log records that contain changes to tuples (PUT, DELETE). <ul><li>Each log record must contain the tuple’s unique identifier. In this case, the identifier is not <code>page_id + offset/slot</code> metioned above since page doesn’t exists in this scheme. </li></ul></li><li>As the application makes changes to the database, the DBMS appends log records to the end of the file without checking previous log records. </li><li>When the page gets full, the DBMS writes it out disk and starts filling up the next page with records. <ul><li>All on-disk pages are immutable. </li><li>All disk writes are sequential. </li></ul></li></ol><h2 id="How-to-read-in-a-log-structured-storage"><a href="#How-to-read-in-a-log-structured-storage" class="headerlink" title="How to read in a log-structured storage?"></a>How to read in a log-structured storage?</h2><ol><li>To read a tuple with a given id, the DBMS finds the newest log record corresponding to that id. It needs to scan log from newest to oldest. <ul><li>To optimize the linear scan, DBMS can maintain an index that maps a tuple id to the newest log record. </li></ul></li><li>If log record is in-memory, just read it. If log record is on a disk page, retrieve it. </li></ol><h2 id="How-to-solve-that-the-log-will-become-larger-and-larger"><a href="#How-to-solve-that-the-log-will-become-larger-and-larger" class="headerlink" title="How to solve that the log will become larger and larger?"></a>How to solve that the log will become larger and larger?</h2><ol><li><p>The DBMS can periodically compact pages to reduce wasted space. </p><ul><li>It can take two continuous pages and scan from newest to oldest leaving one newest log for each tuple. </li></ul></li><li><p>The DBMS can sort the page based on id order to improve efficiency of future look-ups. </p><ul><li><p>This sorted table is called <strong>Sorted String Tables</strong>, <strong>SSTables</strong>. </p></li><li><p>After a page is compacted, the DBMS does need to maintain temporal ordering of records within the page since each tuple id is guaranteed to appear at most once in the page. </p></li></ul></li><li><p>There are two strategy to choose which pages to compact:</p><ul><li>The Universal compaction chooses any two continuous pages. </li><li>The level compaction chooses two continuous pages in the same level, and compacts them into next level. </li></ul></li><li><p>The downsides of compaction is write-amplification caused by duplicate writing to the newest record of a tuple, and also compacting itself is expensive. </p></li></ol><h1 id="Tuple-storage"><a href="#Tuple-storage" class="headerlink" title="Tuple storage"></a>Tuple storage</h1><h2 id="How-to-store-data-in-a-tuple"><a href="#How-to-store-data-in-a-tuple" class="headerlink" title="How to store data in a tuple?"></a>How to store data in a tuple?</h2><ol><li>A tuple is essentially a sequence of bytes. It’s the job of the DBMS to interpret those bytes into attribute types and values. </li><li>The DBMS’s catalogs contain the schema information about tables that the system uses to figure out the tuple’s layout. <ul><li>A DBMS stores meta-data about databases in its internal catalogs. <ul><li>Tables, columns, indexes, views</li><li>Users, permissions</li><li>Internal statistics</li></ul></li></ul></li></ol><h2 id="What-are-supported-data-types"><a href="#What-are-supported-data-types" class="headerlink" title="What are supported data types?"></a>What are supported data types?</h2><ol><li>The basic types are supported: <code>INTEGER</code>/<code>BIGINT</code>/<code>SMALLINT</code>/<code>TINYINT</code>, <code>FLOAT</code>/<code>REAL</code>, <code>VARCHAR</code>/<code>VARBINARY</code>/<code>TEXT</code>/<code>BLOB</code>, <code>TIME</code>/<code>DATE</code>/<code>TIMESTAMP</code>. </li><li>Since IEEE 754 floating points may be inaccurate, fixed-point decimals are also supported as <code>NUMERIC</code>/<code>DECIMAL</code>. But their execution is way slower than floating points. </li><li>Most DBMSs don’t allow a tuple to exceed the size of a single page. <ul><li>To store values that are larger than a page, the DBMS uses separate <strong>overflow</strong> storage pages. </li><li>Some systems allow you to store a really large value in an external file. And treat the data as a BLOB type. Then the DBMS cannot manipulate the contents of an external file. <ul><li>No durability protections. No transaaction protections. </li><li>They are outside of DBMS, hence we cannot update it through DBMS either. </li></ul></li></ul></li></ol><h1 id="Data-storage-models"><a href="#Data-storage-models" class="headerlink" title="Data storage models"></a>Data storage models</h1><h2 id="What-kind-of-database-workloads-are-there"><a href="#What-kind-of-database-workloads-are-there" class="headerlink" title="What kind of database workloads are there?"></a>What kind of database workloads are there?</h2><ol><li>On-Line Transaction Processing (OLTP): In this situation, commands are fast operations that only read/update a small amount of data each time. The data accessed in a query is related to a single entity in the database. </li><li>On-Line Analytical Processing (OLAP): Here, commands are complex queries that read a lot of data to compute aggregates, i.e. it will execute complex writes and complex reads. </li><li>Hybrid Transaction + Analytical Processing: OLTP + OLAP together on the same database instance. </li></ol><h2 id="How-does-DBMS-store-tuples"><a href="#How-does-DBMS-store-tuples" class="headerlink" title="How does DBMS store tuples?"></a>How does DBMS store tuples?</h2><ol><li><strong>N-ary storage model</strong> (<strong>row storage</strong>): The DBMS stores all attributes for a single tuple contiguously in a page. <ul><li>This model is ideal for OLTP workloads where queries tend to operate only on an individual entity and insert-heavy workloads. </li><li>The advantage is fast inserts, updates, and deletes. It is also good for queries that need the entire tuple. </li><li>However, it is not good for scanning large portions of the table and/or a subset of the attributes. </li></ul></li><li><strong>Decomposition storage model</strong> (<strong>DSM</strong>, <strong>Column storage</strong>): The DBMS stores the values of a single attribute for all tuples contiguously in a page. <ul><li>This model is ideal for OLAP workloads where read-only queries perform large scans over a subset of the table’s attributes. </li><li>DBMS can identify tuple in two choices:<ul><li>The first is using fixed-length offsets when each value is the same length for an attribute. It does not require each attributes to have the same length. This is the most used scheme. </li><li>The second is using embedded tuple ids. Each value is stored with its tuple id in a column. </li></ul></li><li>The advantages of DSM is that it reduces the amount wasted I/O because the DBMS only reads the data that it needs, and provides better query processing and data compression. </li><li>The disadvantage is that it is slow for point queries, inserts, updates, and deletes because of tuple splitting/stitching. </li></ul></li></ol><h2 id="Database-compression"><a href="#Database-compression" class="headerlink" title="Database compression"></a>Database compression</h2><h3 id="Why-do-we-need-database-compression-and-what-is-the-trade-off"><a href="#Why-do-we-need-database-compression-and-what-is-the-trade-off" class="headerlink" title="Why do we need database compression and what is the trade-off?"></a>Why do we need database compression and what is the trade-off?</h3><ol><li>I/O is the main bottleneck if the DBMS fetches data from disk during query execution. </li><li>The DBMS can compress pages to increase the utility of the data moved per I/O operation.</li><li>The key trade-off is between speed and compression ratio. <ul><li>To get a better compression ratio, it will take a higher CPU costs to both compress and decompress. But it can reduces DRAM requirements. </li><li>Some engines may work with compressed data, which can reduce CPU costs. </li></ul></li></ol><h3 id="Why-can-we-compress-data"><a href="#Why-can-we-compress-data" class="headerlink" title="Why can we compress data?"></a>Why can we compress data?</h3><ol><li>Data sets tend to have highly skewed distributions for attribute values. </li><li>Data sets tend to have high correlation between attributes of the same tuple. </li></ol><h3 id="What-is-the-goals-of-compression"><a href="#What-is-the-goals-of-compression" class="headerlink" title="What is the goals of compression?"></a>What is the goals of compression?</h3><ol><li>It must produce fixed-length values. The only exception is var-length data stored in separate pool. </li><li>Late materialization: Postpone decompression for as long as possible during query execution. </li><li>It must be a lossless scheme. <ul><li>When a DBMS uses compression, it is always lossless because people don’t like losing data. </li><li>Any kind of lossy compression must be performed at the application level. </li></ul></li></ol><h3 id="What-are-the-compression-granularities"><a href="#What-are-the-compression-granularities" class="headerlink" title="What are the compression granularities?"></a>What are the compression granularities?</h3><ol><li>Block-level: Compression is performed on a block of tuples for the same table. </li><li>Tuple-level: Compression is performed on the contents of the entire tuple (NSM-only). </li><li>Attribute-level: Compression is performed on a single attribute within one tuple (overflow). It can target multiple attributes for the same tuple. </li><li>Column-level: Compression is performed on multiple values for one or more attributes stored for multiple tuples (DSM-only). </li></ol><h3 id="What-is-the-naive-compression"><a href="#What-is-the-naive-compression" class="headerlink" title="What is the naive compression?"></a>What is the naive compression?</h3><ol><li>Naive means that data system does not understand the bits after compression. </li><li>We can compress data using a general-purpose algorithm. The scope of compression is only based on the data provided as input. </li><li>In disk, each page not only stores the compressed data, also stores the modification log of this page. </li><li>When DBMS read a page into the buffer pool<ul><li>It won’t decompress until queries actually need to return the whole tuple back. </li><li>Every update will only append an entry in the mod log. Hence no need to decompress data when only updating tuples. This only thing we need to know in updating is which page this tuple is. </li><li>When mod log is full, DBMS will decompress page and apply changes. </li></ul></li></ol><h3 id="How-can-we-do-better-with-the-high-level-meaning-or-semantics-of-the-data"><a href="#How-can-we-do-better-with-the-high-level-meaning-or-semantics-of-the-data" class="headerlink" title="How can we do better with the high-level meaning or semantics of the data?"></a>How can we do better with the high-level meaning or semantics of the data?</h3><ol><li>This is performed on column-level. </li><li>Run-length encoding:<ul><li>Compress runs of continuous same value in a single column into triplets: <code>(value, offset, length)</code>. <ul><li><code>Value</code> is the value of the attribute. </li><li><code>Offset</code> is the start position in the column segment of the continuous save value run. </li><li><code>Length</code> is the number of elements in the run. </li></ul></li><li>It requires the columns to be sorted intelligently to maximize compression opportunities. </li></ul></li><li>Bit-packing encoding:<ul><li>When values for an attribute are always less than the value’s declared largest size, store them as smaller data type. </li><li><strong>Mostly encoding</strong> uses a special marker to indicate when a value exceeds largest size and maintains a look-up table to store them. </li></ul></li><li>Bitmap encoding:<ul><li>Store a separate bitmap for <strong>each unique value</strong> for an attribute where an offset in the vector corresponds to a tuple. </li><li>When reading, DBMS need to look into the bits in the tuple to find which bit is <code>1</code> to know which value is stored in this attribute of this tuple. </li><li>We need to store the value for each bitmap. So the total space needed is the total length of possible values and the number of bits same as the number of tuples for each value. </li></ul></li><li>Delta encoding:<ul><li>Recording the difference between this tuple and its last tuple. </li><li>Store base value in-line or in a separate look-up table. </li><li>Combine with RLE to get even better compression ratios. </li></ul></li><li>Incremental encoding:<ul><li>Delta encoding is for numbers, while incremental encoding is for string. </li><li>It stores the length of common prefixes between this tuples and its last tuple and the extra suffixes of this tuple. </li><li>When there is no common prefix, the length is $0$. It performs better when we sorted tuples according to the strings. </li></ul></li><li>Dictionary compression (most widely used): <ul><li>Build a data structure that maps variable-length values to a smaller integer identifier. And replace those values with their corresponding identifier in the dictionary data structure. </li><li>The dictionary is required to support fast encoding , decoding and range queries. </li><li>Hash function can not be used due to key confliction and unable to provide deoding. </li><li>When the dictionary encode values in certain order (e.g. alphabetic order), the execution engine can be optimized to do some queries only access dictionary, especially for those <code>DISTINCT</code> queries. </li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Storage </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01 Relational Model</title>
      <link href="/2023/06/24/Courses/15445/01-Relational-Model/"/>
      <url>/2023/06/24/Courses/15445/01-Relational-Model/</url>
      
        <content type="html"><![CDATA[<p>@[toc]</p><h1 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h1><h2 id="Why-shouldn’t-we-just-store-database-in-a-flat-file-like-CSV-that-we-manage-ourselve-in-our-application-code"><a href="#Why-shouldn’t-we-just-store-database-in-a-flat-file-like-CSV-that-we-manage-ourselve-in-our-application-code" class="headerlink" title="Why shouldn’t we just store database in a flat file like CSV that we manage ourselve in our application code?"></a>Why shouldn’t we just store database in a flat file like CSV that we manage ourselve in our application code?</h2><ol><li>The first concern is data integrity:<ul><li>How to ensure that the data is make sense to the design schema? </li><li>How to prevent invalid data and malicious attack? </li><li>And the management of deleting some entries causing deleting entries in other databases is pain. </li></ul></li><li>Another problem is implementation:<ul><li>How to find a record?</li><li>How to share database between applications?</li><li>How to handle concurrent writes?</li></ul></li><li>Also concerned about durability:<ul><li>What if machine crashed? </li><li>How to replicate database?</li></ul></li></ol><h2 id="What-does-DBMS-and-data-models-do"><a href="#What-does-DBMS-and-data-models-do" class="headerlink" title="What does DBMS and data models do?"></a>What does DBMS and data models do?</h2><ol><li><p>DBMS supports the definition, creation, querying, update, and administration of databases in accordance with some data model. </p><ul><li><p>The physical storage is left up to the DBMS implementation. </p></li><li><p>Programmers access data through high-level language, DBMS figures out best execution strategy. </p></li></ul></li><li><p>A data model is a collection of concepts for describing the data in a database. </p><ul><li>A schema is a description of a particular collection of data using a given data model. </li></ul></li></ol><h2 id="What-kinds-of-data-models-are-there"><a href="#What-kinds-of-data-models-are-there" class="headerlink" title="What kinds of data models are there?"></a>What kinds of data models are there?</h2><ol><li>The most used is Relational model. </li><li>One class called NoSQL: <ul><li>Key/Value</li><li>Graph</li><li>Document / Object</li><li>Wide-Column / Column-family</li></ul></li><li>For machine learning, there are Array / Matrix / Vectors</li><li>The obsolete or legacy ones are:<ul><li>Hierarchical</li><li>Network</li><li>Multi-Value</li></ul></li></ol><h2 id="How-does-document-data-model-store-data"><a href="#How-does-document-data-model-store-data" class="headerlink" title="How does document data model store data?"></a>How does document data model store data?</h2><ol><li>It embeds data hierarchy into a single object like JSON, or XML, etc. </li><li>The problem is similar with the aforementioned store database in a flat file like CSV. </li></ol><h1 id="Relational-model"><a href="#Relational-model" class="headerlink" title="Relational model"></a>Relational model</h1><h2 id="How-are-data-stored"><a href="#How-are-data-stored" class="headerlink" title="How are data stored?"></a>How are data stored?</h2><ol><li>Data are stored in simple data structures called relations. </li><li>A relation is an <strong>unordered set</strong> that contain the relationship of attributes that represent entities. <ul><li>A n-ary relation is  a table with n columns. </li></ul></li><li>A tuple is a set of attribute values (domain) in the relation. <ul><li>NULL is a member of every domain, if allowed. </li></ul></li></ol><h2 id="What-is-primary-keys-and-foreign-keys"><a href="#What-is-primary-keys-and-foreign-keys" class="headerlink" title="What is primary keys and foreign keys?"></a>What is primary keys and foreign keys?</h2><ol><li>A relation’s primary key uniquely identifies a single tuple. <ul><li>In the definition of a relation, primary keys are usually marked with underline. </li></ul></li><li>A foreign key specifies that an attribute from one relation has to map to a tuple in another relation. <ul><li>Normally, foreign keys must be primary keys in another relation. </li></ul></li></ol><h2 id="What-is-supported-in-relational-algebra"><a href="#What-is-supported-in-relational-algebra" class="headerlink" title="What is supported in relational algebra?"></a>What is supported in relational algebra?</h2><ol><li><p>Select: $\sigma_{predicate}(R)$</p><ul><li><p>Choose a subset of the tuples from a relation that satisfies the selection predicate. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> R</span><br><span class="line"> <span class="keyword">WHERE</span> predicate;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Projection: $\Pi_{A_1,A_2,\dots,A_n}(R)$</p><ul><li><p>Generate a relation with tuples that contains only the specified attributes</p></li><li><p>It can be used to rearrange attributes’ ordering and manipulate the values. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> A1, A2, ..., An <span class="keyword">FROM</span> R;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Union: $R\cup S$</p><ul><li><p>Two relations must have same attributes. </p></li><li><p>The result may be duplicated if some tuples appeared in bother relations. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> R) <span class="keyword">UNION</span> <span class="keyword">ALL</span> (<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> S);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Intersection: $R\cap S$</p><ul><li><p>Two relations must have same attributes. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> R) <span class="keyword">INTERSECT</span> (<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> S);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Difference: $R-S$</p><ul><li><p>Two relations must have same attributes. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> R) <span class="keyword">EXCEPT</span> (<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> S);</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Product: $R\times S$</p><ul><li><p>Generate a relation that contains all possible combinations of tuple from the input relations, i.e. Cartesian product. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> R <span class="keyword">CROSS</span> <span class="keyword">JOIN</span> S;</span><br><span class="line"># <span class="keyword">Or</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> R, S;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Join: $R\bowtie S$</p><ul><li><p>The difference between join and intersection is that join can match attributes’ name automatically, while intersetion requires that two relations have the same attribute order. </p></li><li><p>It does product first, then compare attributes. </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> R <span class="keyword">NATURAL</span> <span class="keyword">JOIN</span> S;</span><br><span class="line"># <span class="keyword">Or</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> R <span class="keyword">JOIN</span> S <span class="keyword">USING</span> (A1, A2, ..., An);</span><br></pre></td></tr></table></figure></li><li><p>In the broad sense, join can have predicate $R\bowtie_{predicate}S$ which is the same as $\sigma_{predicate}(R\times S)$. </p></li></ul></li><li><p>It is obvious that different order of steps can have the same result with different amout of computation. </p><ul><li>$R\bowtie(\sigma_{predicate}(S))$ is much efficient than $\sigma_{predicate}(R\bowtie S)$. </li></ul></li></ol><h1 id="Morden-SQL"><a href="#Morden-SQL" class="headerlink" title="Morden SQL"></a>Morden SQL</h1><h2 id="What-is-provided-in-a-relational-language"><a href="#What-is-provided-in-a-relational-language" class="headerlink" title="What is provided in a relational language?"></a>What is provided in a relational language?</h2><ol><li>Data Manipulation Language (DML)</li><li>Data Definition Language (DDL)</li><li>Data Control Language (DCL)</li><li>Also view definition, integrity and referential constraints, transactions. </li></ol><h2 id="Aggregations-and-Group-By"><a href="#Aggregations-and-Group-By" class="headerlink" title="Aggregations and Group By"></a>Aggregations and Group By</h2><h3 id="What-are-aggregations"><a href="#What-are-aggregations" class="headerlink" title="What are aggregations?"></a>What are aggregations?</h3><ol><li>They are functions that return a single value from a bag of tuples. </li><li><code>AVG(col)</code>, <code>MIN(col)</code>, <code>MAX(col)</code>, <code>SUM(col)</code>, <code>COUNT(col)</code><ul><li>For <code>COUNT</code>, the passed argument actual does mater since it only returns the number of rows. </li></ul></li><li>Aggregate functions can almost only be used in the <code>SELECT</code> output list. </li><li><code>COUNT</code>, <code>SUM</code>, <code>AVG</code> support <code>DISTINCT</code>. <ul><li>In this case, the columns passed to <code>COUNT</code> matters. </li></ul></li><li>Output of other columns outside of an aggregate is undefined. <ul><li>Aggregations actually create a new relation with different number of tuples. </li><li>If we directly ask for other colmns, the number of tuples won’t match with output of aggregations</li><li>And we don’t know the relation between output of aggregations and values from other columns (some language may allow this but with chaos order). </li></ul></li><li>We also cannot filter output tuples with the result of aggregation column names. <ul><li>Since <code>SELECT</code> happens a last, when <code>WHERE</code> or <code>HAVING</code> is calculated, the aggregation columns haven’t been calculated yet. </li></ul></li></ol><h3 id="How-to-output-other-columns-with-aggregations"><a href="#How-to-output-other-columns-with-aggregations" class="headerlink" title="How to output other columns with aggregations?"></a>How to output other columns with aggregations?</h3><ol><li><code>GROUP BY</code> projects tuples into subsets and calculate aggregates against each subset. </li><li>Non-aggregated valuese in <code>SELECT</code> output clause must appear in <code>GROUP BY</code> clause. </li><li>With group-by, each aggregation output has the same values in those grouped columns. So <code>SELECT</code> knows their relation. </li><li><code>HAVING</code> like a <code>WHERE</code> clause for a <code>GROUP BY</code>. <ul><li>It can filter results based on aggregation computation. But it also shouldn’t use the columns names of aggregation in <code>SELECT</code>. </li><li>Instead, it should directly use aggregation function. Although the execution engine knows that these two are the same and don’t do the repeat calculation. </li></ul></li></ol><h2 id="What-string-operations-are-supported"><a href="#What-string-operations-are-supported" class="headerlink" title="What string operations are supported?"></a>What string operations are supported?</h2><ol><li><p>Predicate of string matching can be done with <code>=</code></p><ul><li><p>Some DBMSs are case sensitive while others are not. </p></li><li><p>We can also use <code>LIKE</code> to match with string-match operators. </p><ul><li><code>%</code> matches any substring (including empty strings)</li><li><code>_</code> match any one character. </li></ul></li></ul></li><li><p>Other string functions are provided:</p><ul><li><code>UPPER</code> and <code>LOWER</code></li><li><code>SUBSTRING(str, start_index, end_index)</code></li></ul></li><li><p>Different language has different ways to concatenate strings: <code>str1 || str2</code>, <code>str1 + str2</code> or <code>CONCAT(str1, str2)</code>. </p></li></ol><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><h3 id="Where-can-we-redirect-outputs"><a href="#Where-can-we-redirect-outputs" class="headerlink" title="Where can we redirect outputs?"></a>Where can we redirect outputs?</h3><ol><li>We can store query restuls in another table. <ul><li>That table must not already be defined. </li><li>It will have the same number of columns with the same types as the input. </li></ul></li><li>We can also insert tuples from query into another table. <ul><li>The inner select must generate the same columns as the target table. </li><li>DMBSs have diferent options/syntax on what to do with integrity violations. </li></ul></li></ol><h3 id="How-can-we-control-the-outputs"><a href="#How-can-we-control-the-outputs" class="headerlink" title="How can we control the outputs?"></a>How can we control the outputs?</h3><ol><li>We can order the output tuples by the values in one or more of their columns with <code>ORDER BY &lt;column*&gt; [ASC|DESC]</code>. </li><li>We can also limit the number of tuples returned in output with <code>LIMIT &lt;count&gt; [OFFSET &lt;count2&gt;]</code>. <ul><li>Although this limits the number of output, its execution may still need to compute the whole relation, e.g. to output the top-10 largest numbers still need to sort all numbers. </li></ul></li></ol><h2 id="What-if-we-need-temporary-relation-in-a-query"><a href="#What-if-we-need-temporary-relation-in-a-query" class="headerlink" title="What if we need temporary relation in a query?"></a>What if we need temporary relation in a query?</h2><ol><li><p>The first solution is nested queries. </p><ul><li><p>Inner queries can appear almost anywhere in query. </p></li><li><p>In <code>WHERE</code> clause, we can perform predicate between the tuples from current relation and result of subqueries. </p><ul><li><p><code>ALL</code> must satisfy expression for all rows in the subquery. </p></li><li><p><code>ANY</code> must satisfy expression for at least one row in the subquery. </p></li><li><p><code>IN</code> is equivalent to <code>=ANY()</code>. </p></li><li><p><code>EXISTS</code> returns true if the relation is not empty. </p></li><li><p><code>NOT</code></p></li></ul></li></ul></li><li><p>Another choice is common table expression using <code>WITH cteName AS (query)</code>. </p><ul><li>It also supports bind/alias output columns to names <code>WITH cteName (col1, ..., coln) AS (query)</code></li><li>In the next one query, we can reference this temporary relation with <code>cteName</code>. </li><li>We can enable recursive calculation using <code>WITH RECURSIVE</code>. </li></ul></li></ol><h2 id="How-does-window-functions-work"><a href="#How-does-window-functions-work" class="headerlink" title="How does window functions work?"></a>How does window functions work?</h2><ol><li>Window functions perform a sliding calculation across a set of tuples that are related. </li><li>Like an aggregation, they only appear in <code>SELECT</code> clause. But tuples are not grouped into a single output tuples. Instead, the number of rows of the output is the same as input. </li><li>In <code>SELECT</code> clause, window functions syntax is <code>FUNC-NAME(...) OVER(...)</code>. <ul><li>The <code>FUNC-NAME</code> can be aggregation functions or special functions (<code>ROW_NUMBER</code> and <code>RANK</code>)<ul><li><code>ROW_NUMBER</code> assigns the nuber of current row in certain order. </li><li><code>RANK</code> assigns the order position of the current row. </li><li>When two rows tie, <code>RANK</code> will assign the same number to them and skip the next number while <code>ROW_NUMBER</code> will assign different number. </li></ul></li><li>The <code>OVER</code> paramater controls how to slice up data. <ul><li>It controls how to group together tuples when computing the window function with <code>PARTITION BY</code>. </li><li>It also controls the order of computing with <code>ORDER BY ... [ASC|DESC]</code>. </li></ul></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Open Courses </category>
          
          <category> CMU 15-445/645 Database System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Database system </tag>
            
            <tag> Relational model </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
