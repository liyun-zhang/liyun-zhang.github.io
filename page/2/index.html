<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/page/2/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">37</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Aurora/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Aurora/" class="post-title-link" itemprop="url">Aurora</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:28:51" itemprop="dateCreated datePublished" datetime="2023-09-26T13:28:51+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:30:48" itemprop="dateModified" datetime="2023-10-04T16:30:48+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/aurora.pdf">Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#background-conceptions-of-aws">Background conceptions of AWS</a></li>
<li><a href="#durability-at-scale">Durability at scale</a>
<ul>
<li><a href="#how-does-aurora-tolerate-failures">How does Aurora tolerate failures?</a></li>
<li><a href="#how-does-aurora-shrink-the-window-of-vulnerability">How does Aurora shrink the window of vulnerability?</a></li>
<li><a href="#what-are-the-problems-of-io-volume-of-mirrored-mysql">What are the problems of I/O volume of mirrored MySQL?</a></li>
<li><a href="#how-does-aurora-reduce-network-traffic">How does Aurora reduce network traffic?</a></li>
<li><a href="#logs">Logs</a></li>
<li><a href="#how-does-aurora-decide-what-is-completed-and-what-is-durable">How does Aurora decide what is completed and what is durable?</a></li>
<li><a href="#how-does-the-database-and-storage-interact">How does the database and storage interact?</a></li>
<li><a href="#how-does-the-database-write-data">How does the database write data?</a></li>
<li><a href="#how-does-the-database-commit-transactions">How does the database commit transactions?</a></li>
<li><a href="#where-does-the-database-read">Where does the database read?</a></li>
<li><a href="#how-does-the-database-read">How does the database read?</a></li>
<li><a href="#how-does-the-database-replicate">How does the database replicate?</a></li>
<li><a href="#how-does-the-database-perform-undo-and-redo">How does the database perform undo and redo?</a></li>
<li><a href="#how-does-the-database-reestablish-runtime-state">How does the database reestablish runtime state?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Aurora is a relational database service for OLTP workloads.</li>
<li>Issue:
<ul>
<li>In modern distributed cloud services, resilience and scalability are increasingly achieved by decoupling compute from storage and by replicating storage across multiple nodes.</li>
<li>The central constraint in high throughput data processing has moved from compute and storage to the network.</li>
</ul>
</li>
<li>Contribution:
<ul>
<li>Build storage as an independent fault-tolerant and self-healing service across multiple data-centers.
<ul>
<li>Protect the database from performance variance and transient or permanent failures at either the networking or storage tiers.</li>
</ul>
</li>
<li>Only write redo log records to storage
<ul>
<li>Reduce network IOPS by an order of magnitude.</li>
<li>When bottleneck removed, the scalability of the system is greatly improved. And they can aggressively optimize numerous other points of contention.</li>
</ul>
</li>
<li>Move some of the most complex and critical functions (backup and redo recovery) from one-time expensive operations in the database engine to continuous asynchronous operations amortized across a large distributed fleet.
<ul>
<li>This yields near-instant crash recovery without checkpointing as well as inexpensive backups that do not interfere with foreground processing.</li>
</ul>
</li>
<li>Also, it achieves consensus on durable state across numerous storage nodes using an efficient asynchronous scheme, avoiding expensive and chatty recovery protocols.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="background-conceptions-of-aws"><a class="markdownIt-Anchor" href="#background-conceptions-of-aws"></a> Background conceptions of AWS</h2>
<h2 id="durability-at-scale"><a class="markdownIt-Anchor" href="#durability-at-scale"></a> Durability at scale</h2>
<h3 id="how-does-aurora-tolerate-failures"><a class="markdownIt-Anchor" href="#how-does-aurora-tolerate-failures"></a> How does Aurora tolerate failures?</h3>
<ol>
<li>It uses a quorum-based voting protocol. The oridinary quorum voting protocol is as followed:
<ul>
<li>If each of the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> copies of a replicated data item is assigned a vote, a read must obtain quorum of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">V_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> votes while a write must obtain quorum of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">V_w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> votes.</li>
<li>Each read must be aware of the most recent write, formulated as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>r</mi></msub><mo>+</mo><msub><mi>V</mi><mi>w</mi></msub><mo>&gt;</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">V_r + V_w &gt; V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>.</li>
<li>Each write must be aware of the most recent write to avoid conflicting writes, formulated as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>w</mi></msub><mo>&gt;</mo><mi>V</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">V_w &gt; V/2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mord">/</span><span class="mord">2</span></span></span></span>.</li>
</ul>
</li>
<li>The failure of each Availability Zone (AZ) is independent.
<ul>
<li>For a common setting <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">V=3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>r</mi></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">V_r=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>w</mi></msub><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">V_w=2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span>, we can set <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span> replicas in different AZs to be tolerant to large-scale events in addition to the smaller individual failures.</li>
<li>However, the failure of an AZ will break quorum for any of the replicas that concurrently have failures in another AZ.</li>
<li>While the individual failures of replicas in each of the AZs are uncorrelated, the failure of an AZ is a correlated failure of all disks and nodes in that AZ.</li>
</ul>
</li>
<li>Quorum protocol of Aurora:
<ul>
<li>Replicate each data item <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn></mrow><annotation encoding="application/x-tex">6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span></span></span></span> ways across <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span> AZs with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span> copies of each item in each AZ.</li>
<li>Use a quorum model with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn></mrow><annotation encoding="application/x-tex">6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span></span></span></span> votes (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">V = 6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span></span></span></span>), a write quorum of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi mathvariant="normal">/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">4/6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">4</span><span class="mord">/</span><span class="mord">6</span></span></span></span> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>w</mi></msub><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">V_w = 4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span></span></span></span>), and a read quorum of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mi mathvariant="normal">/</mi><mn>6</mn></mrow><annotation encoding="application/x-tex">3/6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">3</span><span class="mord">/</span><span class="mord">6</span></span></span></span> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>r</mi></msub><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">V_r = 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span>).</li>
<li>It can lose a single AZ and one additional node (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>Z</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">AZ+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>) without losing read availability, and lose any two nodes, including a single AZ failure and maintain write availability.</li>
</ul>
</li>
</ol>
<h3 id="how-does-aurora-shrink-the-window-of-vulnerability"><a class="markdownIt-Anchor" href="#how-does-aurora-shrink-the-window-of-vulnerability"></a> How does Aurora shrink the window of vulnerability?</h3>
<ol>
<li>To provide sufficient durability in this model, one must ensure the probability of a double fault on uncorrelated failures, represented by Mean Time to Failure (MTTF), is sufficiently low over the time it takes to repair one of these failures, Mean Time to Repair (MTTR).</li>
<li>It is difficult, past a point, to reduce the probability of MTTF on independent failures.</li>
<li>Focus on reducing MTTR
<ul>
<li>Partition the database volume into small fixed size segments, currently <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn></mrow><annotation encoding="application/x-tex">10</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">0</span></span></span></span>GB in size.
<ul>
<li>These are each replicated <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn></mrow><annotation encoding="application/x-tex">6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span></span></span></span> ways into Protection Groups (PGs) so that each PG consists of six  segments, organized across <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">3</span></span></span></span> AZs, with two segments in each AZ.</li>
<li>A storage volume is a concatenated set of PGs, physically implemented using a large fleet of storage nodes that are provisioned as virtual hosts with attached SSDs using Amazon Elastic Compute Cloud (EC2).</li>
<li>The PGs that constitute a volume are allocated as the volume grows.</li>
</ul>
</li>
<li>Segments are now unit of independent background noise failure and repair. The system monitor and automatically repair faults as part of service.</li>
</ul>
</li>
</ol>
<h3 id="what-are-the-problems-of-io-volume-of-mirrored-mysql"><a class="markdownIt-Anchor" href="#what-are-the-problems-of-io-volume-of-mirrored-mysql"></a> What are the problems of I/O volume of mirrored MySQL?</h3>
<img src="/imgs/Distributed/Aurora/MySQL.png" width="50%">
<ol>
<li>The engine needs to write various types of data:
<ul>
<li>The redo log, the binary (statement) log that is archived to Amazon Simple Storage Service (S3) in order to support point-in-time restores</li>
<li>The modified data pages, a second temporary write of the data page (double-write) to prevent torn pages, and finally the metadata (FRM) files.</li>
</ul>
</li>
<li>Steps 1, 3, and 5 are sequential and synchronous.
<ul>
<li>Latency is additive because many writes are sequential.</li>
<li>Even on asynchronous writes, one must wait for the slowest operation, leaving the system at the mercy of outliers.</li>
<li>From a distributed system perspective, this model can be viewed as having a 4/4 write quorum, and is vulnerable to failures and outlier performance.</li>
</ul>
</li>
<li>User operations that are a result of OLTP applications cause many different types of writes often representing the same information in multiple ways.</li>
</ol>
<h3 id="how-does-aurora-reduce-network-traffic"><a class="markdownIt-Anchor" href="#how-does-aurora-reduce-network-traffic"></a> How does Aurora reduce network traffic?</h3>
<ol>
<li>
<p>The only writes that cross the network are redo log records. The log applicator is pushed to the storage tier where it can be used to generate database pages in background or on demand.</p>
</li>
<li>
<p>Generating each page from the complete chain of its modifications from the beginning of time is prohibitively expensive.</p>
<ul>
<li>Continually materialize database pages in the background to avoid regenerating them from scratch on demand every time.</li>
<li>As far as the engine is concerned, the log is the database, and any pages that the storage system materializes are simply a cache of log applications.</li>
</ul>
</li>
<li>
<p>The primary only writes log records to the storage service and streams those log records as well as metadata updates to the replica instances.</p>
<img src="/imgs/Distributed/Aurora/NetworkIO.png" width=50%>
</li>
<li>
<p>The storage node involves the following steps:</p>
<ul>
<li>(1) receive log record and add to an in-memory queue</li>
<li>(2) persist record on disk and acknowledge</li>
<li>(3) organize records and identify gaps in the log since some batches may be lost</li>
<li>(4) gossip with peers to fill in gaps</li>
<li>(5) coalesce log records into new data pages</li>
<li>(6) periodically stage log and new pages to S3</li>
<li>(7) periodically garbage collect old versions</li>
<li>(8) periodically validate CRC codes on pages</li>
<li>Each of the steps above asynchronous, and only steps (1) and (2) are in the foreground path potentially impacting latency.</li>
</ul>
<img src="/imgs/Distributed/Aurora/StorageNode.png" width="50%">
</li>
</ol>
<h3 id="logs"><a class="markdownIt-Anchor" href="#logs"></a> Logs</h3>
<h3 id="how-does-aurora-decide-what-is-completed-and-what-is-durable"><a class="markdownIt-Anchor" href="#how-does-aurora-decide-what-is-completed-and-what-is-durable"></a> How does Aurora decide what is completed and what is durable?</h3>
<ol>
<li>At a high level, The system maintains points of consistency and durability, and continually advances these points as it receives acknowledgements for outstanding storage requests.</li>
<li>The logic for tracking partially completed transactions and undoing them is kept in the database engine, just as if it were writing to simple disks.<br />
Upon restart, before the database is allowed to access the storage volume, the storage service does its own recovery which is focused not on user-level transactions, but on making sure that the database sees a uniform view of storage despite its distributed nature.</li>
<li>Completeness:
<ul>
<li><em>Log Sequence Number (LSN)</em>: Each log record has an associated LSN that is a monotonically increasing value generated by the database.</li>
<li><em>Volume Complete LSN (VCL)</em>: The storage service determines the highest LSN for which it can guarantee availability of all prior log records.</li>
<li>During storage recovery, every log record with an LSN larger than the VCL must be truncated.</li>
</ul>
</li>
<li>Durability:
<ul>
<li><em>Consistency Point LSNs (CPL)</em>: The database can further constrain a subset of points that are allowable for truncation by tagging log records</li>
<li><em>Volume Durable LSN (VDL)</em>: the highest CPL that is smaller than or equal to VCL and truncate all log records with LSN greater than the VDL.</li>
<li>A CPL can be thought of as delineating some limited form of storage system transaction that must be accepted in order.</li>
</ul>
</li>
</ol>
<h3 id="how-does-the-database-and-storage-interact"><a class="markdownIt-Anchor" href="#how-does-the-database-and-storage-interact"></a> How does the database and storage interact?</h3>
<ol>
<li>Each database-level transaction is broken up into multiple mini-transactions (MTRs) that are ordered and must be performed atomically.</li>
<li>Each mini-transaction is composed of multiple contiguous log records (as many as needed).</li>
<li>The final log record in a mini-transaction is a CPL.</li>
<li>On recovery, the database talks to the storage service to establish the durable point of each PG and uses that to establish the VDL and then issue commands to truncate the log records above VDL.</li>
</ol>
<h3 id="how-does-the-database-write-data"><a class="markdownIt-Anchor" href="#how-does-the-database-write-data"></a> How does the database write data?</h3>
<ol>
<li>Database view:
<ul>
<li>As the database receives acknowledgements to establish the write quorum for each batch of log records, it advances the current VDL.</li>
<li>The database allocates a unique ordered LSN for each redo log record of each transaction that is no greater than the sum of the current VDL and the LSN Allocation Limit (LAL).</li>
<li>This limit ensures that the database does not get too far ahead of the storage system and introduces back-pressure that can throttle the incoming writes if the storage or network cannot keep up.</li>
</ul>
</li>
<li>PG view:
<ul>
<li>Each segment of each PG only sees a subset of log records in the volume that affect the pages residing on that segment.</li>
<li>Each log record contains a backlink that identifies the previous log record for that PG to track the point of completeness of the log records that have reached each segment.</li>
<li><em>Segment Complete LSN (SCL)</em>: Identifies the greatest LSN below which all log records of the PG have been received, established through backlinks.</li>
<li>The SCL is used by the storage nodes when they gossip with each other in order to find and exchange log records that they are missing.</li>
</ul>
</li>
</ol>
<h3 id="how-does-the-database-commit-transactions"><a class="markdownIt-Anchor" href="#how-does-the-database-commit-transactions"></a> How does the database commit transactions?</h3>
<ol>
<li>When a client commits a transaction, the thread handling the commit request sets the transaction aside by recording its “commit LSN” as part of a separate list of transactions waiting on commit and moves on to perform other work.</li>
<li>Completing a commit, if and only if, the latest VDL is greater than or equal to the transaction’s commit LSN.</li>
<li>As the VDL advances, the database identifies qualifying transactions that are waiting to be committed and uses a dedicated thread to send commit acknowledgements to waiting clients.</li>
</ol>
<h3 id="where-does-the-database-read"><a class="markdownIt-Anchor" href="#where-does-the-database-read"></a> Where does the database read?</h3>
<ol>
<li>Pages are served from the buffer cache and only result in a storage IO request if the page in question is not present in the cache.</li>
<li>While the Aurora database does not write out pages on cache eviction (or anywhere else), it enforces a similar guarantee: a page in the buffer cache must always be of the latest version.
<ul>
<li><em>Page LSN</em>: identify the log record associated with the latest change to the page</li>
<li>Evict a page from the cache only if its page LSN is greater than or equal to the VDL.</li>
<li>Hence, all changes in the page have been hardened in the log, and on a cache miss, it is sufficient to request a version of the page as of the current VDL to get its latest durable version.</li>
</ul>
</li>
</ol>
<h3 id="how-does-the-database-read"><a class="markdownIt-Anchor" href="#how-does-the-database-read"></a> How does the database read?</h3>
<ol>
<li>When reading a page from disk, the database establishes a read-point, representing the VDL at the time the request was issued.</li>
<li>The database can then select a storage node that is complete with respect to the read point, knowing that it will therefore receive an up to date version.</li>
<li>Protection Group Min Read Point LSN (PGMRPL): represents that all the log records of the PG below it are unnecessary.
<ul>
<li>If there are read replicas, the writer gossips with them to establish the per-PG Minimum Read Point LSN across all nodes.</li>
<li>A storage node segment is guaranteed that there will be no read page requests with a read-point that is lower than the PGMRPL.</li>
<li>Each storage node is aware of the PGMRPL from the database and can, therefore, advance the materialized pages on disk by coalescing the older log records and then safely garbage collecting them.</li>
</ul>
</li>
</ol>
<h3 id="how-does-the-database-replicate"><a class="markdownIt-Anchor" href="#how-does-the-database-replicate"></a> How does the database replicate?</h3>
<ol>
<li>A single writer and up to 15 read replicas can all mount a single shared storage volume.</li>
<li>To minimize lag, the log stream generated by the writer and sent to the storage nodes is also sent to all read replicas.</li>
<li>In the reader, the database consumes this log stream by considering each log record in turn.
<ul>
<li>If the log record refers to a page in the reader’s buffer cache, it uses the log applicator to apply the specified redo operation to the page in the cache.</li>
<li>Otherwise it simply discards the log record.</li>
<li>The replicas consume log records asynchronously from the perspective of the writer, which acknowledges user commits independent of the replica.</li>
<li>The only log records that will be applied are those whose LSN is less than or equal to the VDL.<br />
The log records that are part of a single mini-transaction are applied atomically in the replica’s cache to ensure that the replica sees a consistent view of all database objects.</li>
</ul>
</li>
</ol>
<h3 id="how-does-the-database-perform-undo-and-redo"><a class="markdownIt-Anchor" href="#how-does-the-database-perform-undo-and-redo"></a> How does the database perform undo and redo?</h3>
<ol>
<li>The same redo log applicator is used in the forward processing path as well as on recovery where it operates synchronously and in the foreground while the database is offline.</li>
<li>The redo log applicator is decoupled from the database and operates on storage nodes, in parallel, and all the time in the background. Once the database starts up it performs volume recovery in collaboration with the storage service.</li>
<li>Undo recovery can happen when the database is online after the system builds the list of these in-flight transactions from the undo segments.</li>
</ol>
<h3 id="how-does-the-database-reestablish-runtime-state"><a class="markdownIt-Anchor" href="#how-does-the-database-reestablish-runtime-state"></a> How does the database reestablish runtime state?</h3>
<ol>
<li>It contacts for each PG, a read quorum of segments which is sufficient to guarantee discovery of any data that could have reached a write quorum.</li>
<li>Once the database has established a read quorum for every PG it can recalculate the VDL above which data is truncated by generating a truncation range that annuls every log record after the new VDL, up to and including an end LSN which the database can prove is at least as high as the highest possible outstanding log record that could ever have been seen.</li>
<li>The database infers this upper bound because it allocates LSNs, and limits how far allocation can occur above VDL (the 10 million limit described earlier).</li>
<li>The truncation ranges are versioned with epoch numbers, and written durably to the storage service so that there is no confusion over the durability of truncations in case recovery is interrupted and restarted.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/CRAQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/CRAQ/" class="post-title-link" itemprop="url">CRAQ</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:20:23" itemprop="dateCreated datePublished" datetime="2023-09-26T13:20:23+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:30:18" itemprop="dateModified" datetime="2023-10-04T16:30:18+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/craq.pdf">Object Storage on CRAQ High-throughput chain replication for read-mostly workloads</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#related">Related</a>
<ul>
<li><a href="#what-is-object-based-storage">What is object-based storage?</a></li>
<li><a href="#what-is-strong-consistency-and-eventual-consistency">What is strong consistency and eventual consistency?</a></li>
<li><a href="#chain-replication">Chain replication</a>
<ul>
<li><a href="#what-is-chain-replication">What is chain replication?</a></li>
<li><a href="#what-is-the-problem-of-basic-chain-replication">What is the problem of basic chain replication?</a></li>
<li><a href="#how-does-cr-handle-client-requests">How does CR handle client requests?</a></li>
<li><a href="#why-cr-cannot-provide-read-from-intermediate-nodes">Why CR cannot provide read from intermediate nodes?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#craq-system-model">CRAQ System Model</a>
<ul>
<li><a href="#how-does-craq-handle-write-requests">How does CRAQ handle write requests?</a></li>
<li><a href="#how-does-craq-handle-read-requests-to-guarantee-strong-consistency">How does CRAQ handle read requests to guarantee strong consistency?</a></li>
<li><a href="#in-what-scenarios-does-craq-out-performs-basic-cr">In what scenarios does CRAQ out-performs basic CR?</a></li>
<li><a href="#how-does-craq-support-eventual-consistency">How does CRAQ support eventual consistency?</a></li>
<li><a href="#how-does-craq-recover-from-failure">How does CRAQ recover from failure?</a></li>
<li><a href="#how-does-craq-manage-configuration">How does CRAQ manage configuration?</a></li>
<li><a href="#how-does-craq-handle-transient-failure-eg-partition-failure">How does CRAQ handle transient failure (e.g. partition failure)?</a></li>
<li><a href="#how-should-craq-choose-nodes-within-a-datacenter">How should CRAQ choose nodes within a Datacenter?</a></li>
<li><a href="#how-does-craq-support-mini-transaction-of-single-key-operations">How does CRAQ support mini-transaction of single-key operations?</a></li>
<li><a href="#how-does-craq-lower-write-latency-with-multicast">How does CRAQ lower write latency with multicast?</a></li>
<li><a href="#how-does-craq-use-zookeeper-to-manage-configuration">How does CRAQ use ZooKeeper to manage configuration?</a></li>
<li><a href="#how-does-nodes-communicate-with-each-other">How does nodes communicate with each other?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#evaluation">Evaluation</a></li>
</ul>
</p>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Issue: many commercially-deployed systems sacrifice stronger consistency properties in the desire for greater availability and higher throughput.</li>
<li>Contribution:
<ul>
<li>This system is an improvement on Chain Replication, maintains strong consistency while greatly improving read throughput by enabling any chain node to handle read operations.
<ul>
<li>By distributing load across all object replicas, CRAQ scales linearly with chain size without increasing consistency coordination.</li>
</ul>
</li>
<li>CRAQ’s design naturally supports eventual-consistency among read operations for lower-latency reads during write contention and degradation to read-only behavior during transient partitions.
<ul>
<li>CRAQ allows applications to specify the maximum staleness acceptable for read operations.</li>
</ul>
</li>
<li>Leveraging these load-balancing properties, we describe a wide-area system design for building CRAQ chains across geographically-diverse clusters that preserves strong locality properties.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="related"><a class="markdownIt-Anchor" href="#related"></a> Related</h2>
<h3 id="what-is-object-based-storage"><a class="markdownIt-Anchor" href="#what-is-object-based-storage"></a> What is object-based storage?</h3>
<ol>
<li>Object-based storage: Supported by key-value databases, data is presented to applications as entire units.</li>
<li>Object stores support two basic primitives: read (or query) operations return the data block stored under an object name, and write (or update) operations change the state of a single object.</li>
<li>Object stores are better suited for flat namespaces, such as in key-value databases, as opposed to hierarchical directory structures where file systems are better.</li>
<li>Object stores simplify the process of supporting whole-object modifications.<br />
Typically only need to reason about the ordering of modifications to a specific object, as opposed to the entire storage system.<br />
Significantly cheaper to provide consistency guarantees per object instead of across all operations and/or objects.</li>
</ol>
<h3 id="what-is-strong-consistency-and-eventual-consistency"><a class="markdownIt-Anchor" href="#what-is-strong-consistency-and-eventual-consistency"></a> What is strong consistency and eventual consistency?</h3>
<ol>
<li>
<p>Strong consistency guarantees that all read and write operations to an object are executed in some sequential order, and that a read to an object always sees the latest written value.</p>
</li>
<li>
<p>Eventual consistency implies that</p>
<ul>
<li>
<p>Writes to an object are still applied in a sequential order on all nodes, but eventually-consistent reads to different nodes can return stale data for some period of inconsistency.</p>
</li>
<li>
<p>However, read operations will never return an older version than the latest committed write.</p>
</li>
<li>
<p>A client will also see monotonic read consistency if it maintains a session with a particular node.</p>
</li>
</ul>
</li>
<li>
<p>The Eventual consistency is still different from the guarantees from ZooKeeper or Raft. In ZooKeeper or Raft, clients only see monotonic read consistency even if they cross sessions with different servers.</p>
</li>
</ol>
<h3 id="chain-replication"><a class="markdownIt-Anchor" href="#chain-replication"></a> Chain replication</h3>
<h4 id="what-is-chain-replication"><a class="markdownIt-Anchor" href="#what-is-chain-replication"></a> What is chain replication?</h4>
<ol>
<li>It organizes all nodes storing an object in a chain, where the chain tail handles all read requests, and the chain head handles all write requests,  as shown below:<br />
<img src="/imgs/Distributed/CRAQ/basicCR.png" alt="" /></li>
<li>Writes propagate down the chain before the client is acknowledged, thus providing a simple ordering of all object operations, and hence strong consistency, at the tail.</li>
<li>The lack of any complex or multi-round protocols yields simplicity, good throughput, and easy recovery.</li>
</ol>
<h4 id="what-is-the-problem-of-basic-chain-replication"><a class="markdownIt-Anchor" href="#what-is-the-problem-of-basic-chain-replication"></a> What is the problem of basic chain replication?</h4>
<ol>
<li>All reads for an object must go to the same node, leading to potential hotspots.</li>
<li>Multiple chains can be constructed across a cluster of nodes for better load balancing via consistent hashing or a more centralized directory approach.</li>
<li>But these algorithms might still find load imbalances if particular objects are disproportionally popular.<br />
All reads to a chain may then be handled by a potentially-distant node, namely the chain’s tail.</li>
</ol>
<h4 id="how-does-cr-handle-client-requests"><a class="markdownIt-Anchor" href="#how-does-cr-handle-client-requests"></a> How does CR handle client requests?</h4>
<ol>
<li>The <em>head</em> of the chain handles all write operations from clients.
<ul>
<li>When a write operation is received by a node, it is propagated to the next node in the chain.</li>
<li>Once the write reaches the tail node, it has been applied to all replicas in the chain, and it is considered committed.</li>
<li>When the tail commits the write, a reply is sent to the client. The CR paper describes the tail sending a message directly back to the client.</li>
</ul>
</li>
<li>The tail node handles all read operations, so only values which are committed can be returned by a read.</li>
<li>The simple topology of CR makes write operations cheaper than in other protocols offering strong consistency.<br />
Multiple concurrent writes can be pipelined down the chain, with transmission costs equally spread over all nodes.</li>
</ol>
<h4 id="why-cr-cannot-provide-read-from-intermediate-nodes"><a class="markdownIt-Anchor" href="#why-cr-cannot-provide-read-from-intermediate-nodes"></a> Why CR cannot provide read from intermediate nodes?</h4>
<ol>
<li>The reading result can violate the strong consistency.</li>
<li>Concurrent reads to different nodes could see different writes as they are in the process of propagating down the chain.</li>
</ol>
<h2 id="craq-system-model"><a class="markdownIt-Anchor" href="#craq-system-model"></a> CRAQ System Model</h2>
<h3 id="how-does-craq-handle-write-requests"><a class="markdownIt-Anchor" href="#how-does-craq-handle-write-requests"></a> How does CRAQ handle write requests?</h3>
<ol>
<li>A node in CRAQ can store multiple versions of an object, each including a monotonically-increasing version number and an additional attribute whether the version is clean or dirty. All versions are initially marked as clean.</li>
<li>When a node receives a new version of an object via a write being propagated down the chain, the node appends this latest version to its list for the object.
<ul>
<li>If the node is not the tail, it marks the version as dirty, and propagates the write to its successor.</li>
<li>Otherwise, if the node is the tail, it marks the version as clean, at which time we call the object version (write) as committed.</li>
<li>The tail node can then notify all other nodes of the commit by sending an acknowledgement backwards through the chain.</li>
</ul>
</li>
<li>When an acknowledgement message for an object version arrives at a node, the node marks the object version as clean. The node can then delete all prior versions of the object.</li>
<li>If the node has exactly one version for an object, the object is implicitly in the clean state; otherwise, the object is dirty and the properlyordered version must be retrieved from the chain tail.</li>
</ol>
<h3 id="how-does-craq-handle-read-requests-to-guarantee-strong-consistency"><a class="markdownIt-Anchor" href="#how-does-craq-handle-read-requests-to-guarantee-strong-consistency"></a> How does CRAQ handle read requests to guarantee strong consistency?</h3>
<ol>
<li>When a node receives a read request for an object, if the latest known version of the requested object is clean, the node returns this value.</li>
<li>If the latest version number of the object requested is dirty, the node contacts the tail and asks for the tail’s last committed version number (a version query). The node then returns that version of the object.</li>
<li>The tail could commit a new version between when it replied to the version request and when the intermediate node sends a reply to the client.<br />
This does not violate strong consistency, as read operations are serialized with respect to the tail.</li>
</ol>
<p><img src="/imgs/Distributed/CRAQ/read.png" alt="" /></p>
<h3 id="in-what-scenarios-does-craq-out-performs-basic-cr"><a class="markdownIt-Anchor" href="#in-what-scenarios-does-craq-out-performs-basic-cr"></a> In what scenarios does CRAQ out-performs basic CR?</h3>
<ol>
<li>In read-mostly workloads, most of the read requests handled solely by the C − 1 non-tail nodes (as clean reads), and thus throughput in these scenarios scales linearly with chain size C.</li>
<li>In write-heavy workloads, most read requests to non-tail nodes as dirty. But these version queries are lighter-weight than full reads, allowing the tail to process them at a much higher rate before it becomes saturated.</li>
</ol>
<h3 id="how-does-craq-support-eventual-consistency"><a class="markdownIt-Anchor" href="#how-does-craq-support-eventual-consistency"></a> How does CRAQ support eventual consistency?</h3>
<ol>
<li>
<p>It allows read operations to a chain node to return the newest object version known to it.</p>
</li>
<li>
<p>It can also support eventual consistency with maximum-bounded inconsistency.</p>
<ul>
<li>The limit imposed can be based on time (relative to a node’s local clock) or on absolute version numbers.</li>
</ul>
</li>
<li>
<p>If the chain is still available, this inconsistency is actually in terms of the returned version being newer than the last committed one.</p>
<p>If the system is partitioned and the node cannot participate in writes, the version may be older than the current committed one.</p>
</li>
</ol>
<h3 id="how-does-craq-recover-from-failure"><a class="markdownIt-Anchor" href="#how-does-craq-recover-from-failure"></a> How does CRAQ recover from failure?</h3>
<ol>
<li>Each chain node needs to know its predecessor and successor, as well as the chain head and tail.</li>
<li>When a head fails, its immediate successor takes over as the new chain head; likewise, the tail’s predecessor takes over when the tail fails.</li>
<li>If intermediate node fails, drop it from chain, predecessor may need to re-send recent writes since write request pipeline may have been broken from the failure server.</li>
</ol>
<h3 id="how-does-craq-manage-configuration"><a class="markdownIt-Anchor" href="#how-does-craq-manage-configuration"></a> How does CRAQ manage configuration?</h3>
<ol>
<li>An object’s identifier consists of both a chain identifier and a key identifier.</li>
<li>Applications can specify their requirements in multiple ways:
<ul>
<li>Implicit Datacenters &amp; Global Chain Size <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>n</mi><mi>u</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>s</mi><mo separator="true">,</mo><mi>c</mi><mi>h</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{num\_datacenters, chain\_size\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">{</span><span class="mord mathnormal">n</span><span class="mord mathnormal">u</span><span class="mord mathnormal">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">d</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">a</span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span><span class="mclose">}</span></span></span></span>.
<ul>
<li>To determine exactly which datacenters store the chain, consistent hashing is used with unique datacenter identifiers.</li>
</ul>
</li>
<li>Explicit Datacenters &amp; Global Chain Size <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>c</mi><mi>h</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo separator="true">,</mo><mi>d</mi><msub><mi>c</mi><mn>1</mn></msub><mo separator="true">,</mo><mi>d</mi><msub><mi>c</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>d</mi><msub><mi>c</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{chain\_size, dc_1, dc_2, ..., dc_N\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">{</span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord mathnormal">e</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>
<ul>
<li>The order of the chain is the same as specified with head within <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>c</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">dc_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and tail within <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>c</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">dc_N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</li>
<li>To determine which nodes within a datacenter store objects assigned to the chain, consistent hashing is used on the chain identifier.</li>
<li>Each datacenter <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">dc_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> has a node which connects to the tail of datacenter <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>c</mi><mrow><mi>i</mi><mtext>−</mtext><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">dc_{i−1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> and a node which connects to the head of datacenter <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>c</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">dc_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>.</li>
<li><code>chain_size</code> being <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> indicates that the chain should use all nodes within each datacenter.</li>
</ul>
</li>
<li>Explicit Datacenter Chain Sizes <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mi>d</mi><msub><mi>c</mi><mn>1</mn></msub><mo separator="true">,</mo><mi>c</mi><mi>h</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><msub><mi>e</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>d</mi><msub><mi>c</mi><mi>N</mi></msub><mo separator="true">,</mo><mi>c</mi><mi>h</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><msub><mi>e</mi><mi>N</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{dc_1, chain\_size_1, ..., dc_N, chain\_size_N\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mopen">{</span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">}</span></span></span></span>
<ul>
<li>This allows for non-uniformity in chain load balancing.</li>
<li>The chain nodes within each datacenter are chosen in the same manner as the previous method, and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>h</mi><mi>a</mi><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>i</mi><mi>z</mi><msub><mi>e</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">chain\_size_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-0.31em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">i</span><span class="mord mathnormal">n</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> can also be set to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="how-does-craq-handle-transient-failure-eg-partition-failure"><a class="markdownIt-Anchor" href="#how-does-craq-handle-transient-failure-eg-partition-failure"></a> How does CRAQ handle transient failure (e.g. partition failure)?</h3>
<ol>
<li>In methods 2 and 3 above, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><msub><mi>c</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">dc_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord mathnormal">d</span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> can be set as a master datacenter.
<ul>
<li>If a datacenter is the master for a chain, writes to the chain will only be accepted by that datacenter during transient failures.</li>
</ul>
</li>
<li>When a master is not defined,
<ul>
<li>Writes will only continue in a partition if the partition contains a majority of the nodes in the global chain.</li>
<li>The minority partition will become read-only for maximumbounded inconsistent read operations.</li>
</ul>
</li>
</ol>
<h3 id="how-should-craq-choose-nodes-within-a-datacenter"><a class="markdownIt-Anchor" href="#how-should-craq-choose-nodes-within-a-datacenter"></a> How should CRAQ choose nodes within a Datacenter?</h3>
<ol>
<li>In CRAQ’s current implementation, we place chains within a datacenter using consistent hashing, mapping potentially many chain identifiers to a single head node.</li>
<li>Another approach is to use the membership management service as a directory service in assigning and storing randomized chain membership, i.e., each chain can include some random set of server nodes.
<ul>
<li>This approach improves the potential for parallel system recovery.</li>
<li>But it would increase centralization and state, and require storing more metadata information in the coordination service.</li>
</ul>
</li>
</ol>
<h3 id="how-does-craq-support-mini-transaction-of-single-key-operations"><a class="markdownIt-Anchor" href="#how-does-craq-support-mini-transaction-of-single-key-operations"></a> How does CRAQ support mini-transaction of single-key operations?</h3>
<ol>
<li>For Prepend/Append and Increment/Decrement operations,
<ul>
<li>The head of the chain storing the key’s object can simply apply the operation to the latest version of the object, even if the latest version is dirty, and then propagate a full replacement write down the chain.</li>
<li>If these operations are frequent, the head can buffer the requests and batch the updates. These enhancements would be much more expensive using a traditional two-phase-commit protocol.</li>
</ul>
</li>
<li>For the test-and-set operation,
<ul>
<li>The head of the chain checks if its most recent committed version number equals the version number specified in the operation.</li>
<li>If there are no outstanding uncommitted versions of the object, the head accepts the operation and propagates an update down the chain.</li>
<li>If there are outstanding writes, we simply reject the test-and-set operation, and clients are careful to back off their request rate if continuously rejected.</li>
</ul>
</li>
<li>The optimistic two-phase protocol need only be implemented with the chain heads, not all involved nodes.
<ul>
<li>The chain heads can lock any keys involved in the minitransaction until it is fully committed.</li>
<li>It reduces the write throughput of CRAQ as writes to the same object can no longer be pipelined.</li>
</ul>
</li>
</ol>
<h3 id="how-does-craq-lower-write-latency-with-multicast"><a class="markdownIt-Anchor" href="#how-does-craq-lower-write-latency-with-multicast"></a> How does CRAQ lower write latency with multicast?</h3>
<ol>
<li>Within a datacenter, this would take the form of a network-layer multicast protocol, while application-layer multicast protocols may be better-suited for wide-area chains.</li>
<li>No ordering or reliability guarantees are required from these multicast protocols.</li>
<li>The actual value can be multicast to the entire chain. Then, only a small metadata message needs to be propagated down the chain to ensure that all replicas have received a write before the tail.</li>
<li>If a node does not receive the multicast for any reason, the node can fetch the object from its predecessor after receiving the write commit message and before further propagating the commit message.</li>
<li>When the tail receives a propagated write request, a multicast acknowledgment message can be sent to the multicast group instead of propagating it backwards along the chain.</li>
</ol>
<h3 id="how-does-craq-use-zookeeper-to-manage-configuration"><a class="markdownIt-Anchor" href="#how-does-craq-use-zookeeper-to-manage-configuration"></a> How does CRAQ use ZooKeeper to manage configuration?</h3>
<ol>
<li>
<p>During initialization, a CRAQ node creates an ephemeral file in <code>/nodes/dc_name/node_id</code>.</p>
<ul>
<li>
<p>The content of the file contains the node’s IP address and port number.</p>
</li>
<li>
<p>CRAQ nodes can query <code>/nodes/dc_name</code> to determine the membership list for its datacenter. They creates a watch on the children list of <code>/nodes/dc_name</code>.</p>
</li>
</ul>
</li>
<li>
<p>When a CRAQ node receives a request to create a new chain, a file is created in <code>/chains/chain_id</code>.</p>
<ul>
<li>The chain’s placement strategy determines the contents of the file, but it only includes this chain configuration information, not the list of a chain’s current nodes.</li>
<li>Instead of letting nodes register their membership for each chain they belong to (<em>i.e.</em>, chain metadata explicitly names the chain’s current members), any node participating in the chain will query the chain file and place a watch on it.</li>
<li>This is based on the assumption that the number of chains will generally be at least an order of magnitude larger than the number of nodes in the system, or that chain dynamism may be significantly greater than nodes joining or leaving the system.</li>
</ul>
</li>
</ol>
<h3 id="how-does-nodes-communicate-with-each-other"><a class="markdownIt-Anchor" href="#how-does-nodes-communicate-with-each-other"></a> How does nodes communicate with each other?</h3>
<ol>
<li>The nodes within each datacenter organize themselves into a one-hop DHT using the identifiers generated when joining the system.
<ul>
<li>A node’s chain predecessor and successor, the head node and the tail node are defined as its predecessor and successor in the DHT ring.</li>
</ul>
</li>
<li>All RPC-based communication between nodes, or between nodes and clients, is over TCP connections.
<ul>
<li>Each node maintains a pool of connected TCP connections with its chain’s predecessor, successor, and tail.</li>
<li>For chains that span across multiple datacenters, the last node of one datacenter maintains a connection to the first node of its successor datacenter.</li>
<li>Any node that maintains a connection to a node outside of its datacenter must also place a watch on the node list of the external datacenter.</li>
</ul>
</li>
</ol>
<h1 id="evaluation"><a class="markdownIt-Anchor" href="#evaluation"></a> Evaluation</h1>
<ol>
<li>
<p>The main contribution is that it support read from intermediate nodes. So the author measured the read throughput of CRAQ and compared it with basic CR.</p>
<p><img src="/imgs/Distributed/CRAQ/4.png" alt="" /><img src="/imgs/Distributed/CRAQ/6.png" alt="" /></p>
</li>
<li>
<p>The author also tested the factors affecting read throughput, i.e. number of clients, number of nodes, and writes.</p>
<p><img src="/imgs/Distributed/CRAQ/5.png" alt="" /><img src="/imgs/Distributed/CRAQ/7.png" alt="" /></p>
</li>
<li>
<p>Another important test in distributed system is that how long does the system take to recover, and how do they perform during the failure.</p>
<p><img src="/imgs/Distributed/CRAQ/10-13.png" alt="" /></p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Zookeeper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Zookeeper/" class="post-title-link" itemprop="url">Zookeeper</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:16:26" itemprop="dateCreated datePublished" datetime="2023-09-26T13:16:26+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:29:33" itemprop="dateModified" datetime="2023-10-04T16:29:33+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/zookeeper.pdf">ZooKeeper: Wait-free coordination for Internet-scale systems</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#zookeeper-service-overview">ZooKeeper service overview</a>
<ul>
<li><a href="#how-does-client-interact-with-zookeeper-server">How does client interact with ZooKeeper server?</a></li>
<li><a href="#what-are-the-flags-of-znodes">What are the flags of znodes?</a></li>
<li><a href="#what-are-the-differences-between-znodes-and-files-in-file-system">What are the differences between znodes and files in file system?</a></li>
<li><a href="#what-does-zookeeper-guarantee">What does ZooKeeper guarantee?</a></li>
<li><a href="#how-to-change-configuration">How to change configuration?</a></li>
<li><a href="#how-should-a-client-read-configurations">How should a client read configurations?</a></li>
</ul>
</li>
<li><a href="#implement-primitives">Implement primitives</a>
<ul>
<li><a href="#how-does-zookeeper-manage-configuration">How does ZooKeeper manage configuration?</a></li>
<li><a href="#how-does-zookeeper-manage-rendezvous">How does ZooKeeper manage rendezvous?</a></li>
<li><a href="#how-does-zookeeper-manage-group-mambership">How does ZooKeeper manage group mambership?</a></li>
<li><a href="#how-does-zookeeper-manage-mini-transaction">How does ZooKeeper manage Mini-transaction?</a></li>
<li><a href="#how-does-zookeeper-implement-simple-locks">How does ZooKeeper implement simple locks?</a></li>
<li><a href="#how-does-zookeeper-implement-simple-locks-without-herd-effect">How does ZooKeeper implement simple locks without herd effect?</a></li>
<li><a href="#how-does-zookeeper-implement-readwrite-locks">How does ZooKeeper implement Read/Write locks?</a></li>
<li><a href="#what-is-the-difference-between-zookeeper-locks-and-thread-mutex-locks">What is the difference between ZooKeeper locks and thread mutex locks?</a></li>
<li><a href="#how-does-zookeeper-implementa-double-barrier">How does ZooKeeper implementa double barrier?</a></li>
</ul>
</li>
<li><a href="#implementation-of-zookeeper">Implementation of ZooKeeper</a>
<ul>
<li><a href="#how-does-zookeeper-serve-requests">How does ZooKeeper serve requests?</a></li>
<li><a href="#how-does-zookeeper-manage-database">How does ZooKeeper manage database?</a></li>
<li><a href="#how-does-request-processor-handle-write-requests">How does request processor handle write requests?</a></li>
<li><a href="#how-does-servers-reach-agreement">How does servers reach agreement?</a></li>
<li><a href="#how-does-zookeeper-take-snapshot">How does ZooKeeper take snapshot?</a></li>
<li><a href="#how-does-zookeeper-handle-sync">How does ZooKeeper handle sync()</a></li>
<li><a href="#how-does-zookeeper-ensure-to-serve-data-at-least-as-update-as-last-server-served-that-data">How does ZooKeeper ensure to serve data at least as update as last server served that data?</a></li>
<li><a href="#how-to-detect-client-session-failures">How to detect client session failures?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#evaluation-and-results">Evaluation and results</a></li>
</ul>
</p>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Contribution
<ul>
<li>Moved away from implementing specific primitives on the server side
<ul>
<li>Opted for exposing an API that enables application developers to implement their own primitives.</li>
<li>It enables new primitives without requiring changes to the service core.</li>
</ul>
</li>
<li>Moved away from blocking primitives.
<ul>
<li>Manipulate simple wait-free data objects organized hierarchically as in file systems.</li>
</ul>
</li>
<li>Provide a per client guarantee of FIFO execution of requests and linearizability for all writes.
<ul>
<li>Read requests are satisfied by local servers.</li>
</ul>
</li>
</ul>
</li>
<li>Features
<ul>
<li>Provide a simple and high performance kernel for building more complex coordination primitives at the client.</li>
<li>Incorporates elements from group messaging, shared registers, and distributed lock services in a replicated, centralized service.</li>
<li>It has the wait-free aspects of shared registers with an event-driven mechanism.</li>
<li>Using a simple pipelined architecture that allows us to have hundreds or thousands of requests outstanding while still achieving low latency.</li>
<li>With asynchronous operations, a client is able to have multiple outstanding operations at a time.</li>
<li>Enables caching data on the client side with ZooKeeper watches to avoid the problem of delayed update caused by slow or faulty client.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="zookeeper-service-overview"><a class="markdownIt-Anchor" href="#zookeeper-service-overview"></a> ZooKeeper service overview</h2>
<p>Znode: an in-memory data node in the ZooKeeper data<br />
Data tree: a hierarchical namespace to organize znodes</p>
<h3 id="how-does-client-interact-with-zookeeper-server"><a class="markdownIt-Anchor" href="#how-does-client-interact-with-zookeeper-server"></a> How does client interact with ZooKeeper server?</h3>
<ol>
<li>
<p>Clients submit requests to ZooKeeper through a client API using a ZooKeeper client library.</p>
<ul>
<li><code>create(path, data, flags)</code>: returns the name of the new znode</li>
<li><code>delete(path, version)</code>: Deletes the znode path if that znode is at the expected version.</li>
<li><code>exists(path, watch)</code>: The watch flag enables a client to set a watch on the znode.</li>
<li><code>getData(path, watch)</code>: Returns the data and meta-data, such as version information, associated with the znode. ZooKeeper does not set the watch if the znode does not exist.</li>
<li><code>setData(path, data, version)</code>: Writes data[] to znode path if the version number is the current version of the znode.</li>
<li><code>getChildren(path, watch)</code></li>
<li><code>sync(path)</code>: Waits for all updates pending at the start of the operation to propagate to the server that the client is connected to. The path is currently ignored.</li>
</ul>
</li>
<li>
<p>All methods have both a synchronous and an asynchronous version available through the API.</p>
</li>
<li>
<p>If the actual version number of the znode does not match the expected version number the update fails with an unexpected version error.<br />
If the version number is −1, it does not perform version checking.</p>
</li>
<li>
<p>The client library also manages the network connections between the client and ZooKeeper servers.</p>
</li>
</ol>
<h3 id="what-are-the-flags-of-znodes"><a class="markdownIt-Anchor" href="#what-are-the-flags-of-znodes"></a> What are the flags of znodes?</h3>
<ol>
<li><em>Regular</em>: Clients manipulate regular znodes by creating and deleting them explicitly</li>
<li><em>Ephemeral</em>: Clients create such znodes, and they either delete them explicitly, or let the system remove them automatically when the session that creates them terminates</li>
<li><em>Sequential</em>: Those znodes in the same parent znode have a monotonically increasing counter appended to its name. The newer znode has larger sequence value.</li>
<li><em>Watch</em>
<ul>
<li>A read operation with a watch flag set completes as normal except that the server promises to notify the client when the information returned has changed.</li>
<li>Watches are one-time triggers associated with a session; they are unregistered once triggered or the session closes.</li>
<li>Watches indicate that a change has happened, but do not provide the change.</li>
<li>Session events, such as connection loss events, are also sent to watch callbacks so that clients know that watch events may be delayed.</li>
</ul>
</li>
</ol>
<h3 id="what-are-the-differences-between-znodes-and-files-in-file-system"><a class="markdownIt-Anchor" href="#what-are-the-differences-between-znodes-and-files-in-file-system"></a> What are the differences between znodes and files in file system?</h3>
<ol>
<li>
<p>Znodes map to abstractions of the client application, typically corresponding to meta-data used for coordination purposes.</p>
</li>
<li>
<p>Znodes allow clients to store some information that can be used for meta-data or configuration in a distributed computation, like the leadership of a replica group can be stored to a known location.</p>
</li>
<li>
<p>ZooKeeper does not use handles to access znodes. Each request instead includes the full path of the znode being operated on.</p>
<ul>
<li>
<p>It simplifies the API (no <code>open()</code> or <code>close()</code> methods)</p>
</li>
<li>
<p>It also eliminates extra state that the server would need to maintain.</p>
</li>
</ul>
</li>
</ol>
<h3 id="what-does-zookeeper-guarantee"><a class="markdownIt-Anchor" href="#what-does-zookeeper-guarantee"></a> What does ZooKeeper guarantee?</h3>
<ol>
<li>
<p>This definition of its linearizability is called A-linearizability (asynchronous linearizability) that allows a client to have multiple outstanding operations.</p>
<ul>
<li>
<p><em>Linearizable writes</em>: all requests that update the state of ZooKeeper are serializable and respect precedence.</p>
</li>
<li>
<p><em>FIFO client order</em>: all requests from a given client are executed in the order that they were sent by the client.</p>
</li>
</ul>
</li>
<li>
<p>A-linearizability can choose to guarantee no specific order for outstanding operations of the same client or to guarantee FIFO order.</p>
</li>
<li>
<p>A system that satisfies A-linearizability also satisfies linearizability. Because only update requests are A-linearizable, ZooKeeper processes read requests locally at each replica.</p>
</li>
</ol>
<h3 id="how-to-change-configuration"><a class="markdownIt-Anchor" href="#how-to-change-configuration"></a> How to change configuration?</h3>
<ol>
<li>
<p>Two requirements:</p>
<ul>
<li>As the new leader starts making changes, we do not want other processes to start using the configuration that is being changed.</li>
<li>If the new leader dies before the configuration has been fully updated, we do not want the processes to use this partial configuration.</li>
</ul>
</li>
<li>
<p>The new leader can designate a path as the ready znode; other processes will only use the configuration when that znode exists.</p>
<ul>
<li>The new leader makes the configuration change by deleting ready, updating the various configuration znodes, and creating ready.</li>
<li>All of these changes can be pipelined and issued asynchronously to quickly update the configuration state given the FIFO client order guarantee.</li>
</ul>
</li>
</ol>
<h3 id="how-should-a-client-read-configurations"><a class="markdownIt-Anchor" href="#how-should-a-client-read-configurations"></a> How should a client read configurations?</h3>
<ol>
<li>
<p>If a client sees the ready exists before the new leader starts to make a change, it could read the partial configuration in progress and cannot notice anything.</p>
<ul>
<li>The client need to set the watch flag when they check the existence of the ready znode.</li>
<li>Then it will see a notification informing the client of the change before it can read any of the new configuration.</li>
</ul>
</li>
<li>
<p>If A changes the shared configuration in ZooKeeper and tells B of the change through the shared communication channel, B would expect to see the change when it re-reads the configuration.</p>
<ul>
<li>If B’s ZooKeeper replica is slightly behind A’s, it may not see the new configuration.</li>
<li>B can make sure that it sees the most up-to-date information by issuing a write before re-reading the configuration.</li>
<li><code>sync</code> causes a server to apply all pending write requests before processing the read without the overhead of a full write.</li>
</ul>
</li>
</ol>
<h2 id="implement-primitives"><a class="markdownIt-Anchor" href="#implement-primitives"></a> Implement primitives</h2>
<h3 id="how-does-zookeeper-manage-configuration"><a class="markdownIt-Anchor" href="#how-does-zookeeper-manage-configuration"></a> How does ZooKeeper manage configuration?</h3>
<ol>
<li>Configuration is stored in a znode, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">z_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</li>
<li>Processes start up with the full pathname of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">z_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>. Starting processes obtain their configuration by reading zc with the watch flag set to true.</li>
<li>If the configuration in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">z_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is ever updated, the processes are notified and read the new configuration, again setting the watch flag to true.</li>
</ol>
<h3 id="how-does-zookeeper-manage-rendezvous"><a class="markdownIt-Anchor" href="#how-does-zookeeper-manage-rendezvous"></a> How does ZooKeeper manage rendezvous?</h3>
<ol>
<li>Use a rendezvous znode, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">z_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, which is an node created by the client.</li>
<li>The client passes the full pathname of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">z_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> as a startup parameter of the master and worker processes.</li>
<li>When the master starts, it fills in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">z_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> with information about addresses and ports it is using.</li>
<li>When workers start, they read <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">z_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> with watch set to true.
<ul>
<li>If <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">z_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> has not been filled in yet, the worker waits to be notified when <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">z_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is updated.</li>
<li>If <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">z_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is an ephemeral node, master and worker processes can watch for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>r</mi></msub></mrow><annotation encoding="application/x-tex">z_r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> to be deleted and clean themselves up when the client ends.</li>
</ul>
</li>
</ol>
<h3 id="how-does-zookeeper-manage-group-mambership"><a class="markdownIt-Anchor" href="#how-does-zookeeper-manage-group-mambership"></a> How does ZooKeeper manage group mambership?</h3>
<ol>
<li>
<p>Designate a znode, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">z_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> to represent the group.</p>
</li>
<li>
<p>When a process member of the group starts, it creates an <code>ephemeral</code> child znode under <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">z_g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:-0.04398em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>. Processes may put process information in the data of the child znode.</p>
</li>
<li>
<p>If each process has a unique name or identifier, then that name is used as the name of the child znode; otherwise, the process creates the znode with the <code>SEQUENTIAL</code> flag to obtain a unique name assignment.</p>
</li>
</ol>
<h3 id="how-does-zookeeper-manage-mini-transaction"><a class="markdownIt-Anchor" href="#how-does-zookeeper-manage-mini-transaction"></a> How does ZooKeeper manage Mini-transaction?</h3>
<ol>
<li>In a mini-transaction, we want the <code>getData</code> and <code>setData</code> to be atomic.</li>
<li>ZooKeeper can support mini-transaction using version number.</li>
</ol>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">true</span>:</span><br><span class="line">	X, v = <span class="title function_ invoke__">getData</span>(K)</span><br><span class="line">  <span class="keyword">if</span> <span class="title function_ invoke__">setData</span>(K, X+<span class="number">1</span>, v):</span><br><span class="line">		<span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="how-does-zookeeper-implement-simple-locks"><a class="markdownIt-Anchor" href="#how-does-zookeeper-implement-simple-locks"></a> How does ZooKeeper implement simple locks?</h3>
<ol>
<li>
<p>The lock is represented by a znode. To acquire a lock, a client tries to create the designated znode with the <code>EPHEMERAL</code> flag.</p>
</li>
<li>
<p>If the create succeeds, the client holds the lock. Otherwise, the client can read the znode with the watch flag set to be notified if the current leader dies.</p>
</li>
<li>
<p>A client releases the lock when it dies or explicitly deletes the znode.</p>
</li>
<li>
<p>Other clients that are waiting for a lock try again to acquire a lock once they observe the znode being deleted.</p>
</li>
</ol>
<h3 id="how-does-zookeeper-implement-simple-locks-without-herd-effect"><a class="markdownIt-Anchor" href="#how-does-zookeeper-implement-simple-locks-without-herd-effect"></a> How does ZooKeeper implement simple locks without herd effect?</h3>
<ol>
<li>
<p>Herd effect of simple locks: If there are many clients waiting to acquire a lock, they will all vie for the lock when it is released even though only one client can acquire the lock.</p>
</li>
<li>
<p>Define a lock znode l to implement such locks. Line up all the clients requesting the lock and each client obtains the lock in order of request arrival.</p>
<ul>
<li>Use the <code>SEQUENTIAL</code> flag to order the client’s attempt to acquire the lock with respect to all other attempts.</li>
<li>If the client’s znode has the lowest sequence number, the client holds the lock.</li>
<li>Otherwise, the client waits for deletion of the znode that either has the lock or will receive the lock before this client’s znode.</li>
</ul>
</li>
<li>
<p>By only watching the znode that precedes the client’s znode, the herd effect can be avoided by only waking up one process when a lock is released or a lock request is abandoned.</p>
</li>
<li>
<p>Once the znode being watched by the client goes away, the client must check if it now holds the lock.</p>
<ul>
<li>The previous lock request may have been abandoned and there is a znode with a lower sequence number still waiting for or holding the lock.</li>
</ul>
</li>
<li>
<p>Releasing a lock is as simple as deleting the znode n that represents the lock request.</p>
</li>
<li>
<p>We can see by browsing the ZooKeeper data the amount of lock contention, break locks, and debug locking problems.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Lock</span></span><br><span class="line">n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">C = getChildren(l, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">if</span> n is lowest znode in C, <span class="built_in">exit</span></span><br><span class="line">p = znode in C ordered just before n</span><br><span class="line"><span class="keyword">if</span> exists(p, <span class="literal">true</span>) wait <span class="keyword">for</span> watch event</span><br><span class="line"><span class="keyword">goto</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//Unlock</span></span><br><span class="line">delete(n)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="how-does-zookeeper-implement-readwrite-locks"><a class="markdownIt-Anchor" href="#how-does-zookeeper-implement-readwrite-locks"></a> How does ZooKeeper implement Read/Write locks?</h3>
<ol>
<li>
<p>The write locks is similar to the simple locks since they are all exclusive.</p>
</li>
<li>
<p>Since read locks may be shared, and only earlier write lock znodes prevent the client from obtaining a read lock, read locks only need to check that no lower write znode.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write lock</span></span><br><span class="line">n = create(l + “/write-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">C = getChildren(l, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">if</span> n is lowest znode in C, <span class="built_in">exit</span></span><br><span class="line">p = znode in C ordered just before n</span><br><span class="line"><span class="keyword">if</span> exists(p, <span class="literal">true</span>) wait <span class="keyword">for</span> event</span><br><span class="line"><span class="keyword">goto</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Read Lock</span></span><br><span class="line">n = create(l + “/read-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">C = getChildren(l, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">if</span> no write znodes lower than n in C, <span class="built_in">exit</span></span><br><span class="line">p = write znode in C ordered just before n</span><br><span class="line"><span class="keyword">if</span> exists(p, <span class="literal">true</span>) wait <span class="keyword">for</span> event</span><br><span class="line"><span class="keyword">goto</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="what-is-the-difference-between-zookeeper-locks-and-thread-mutex-locks"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-zookeeper-locks-and-thread-mutex-locks"></a> What is the difference between ZooKeeper locks and thread mutex locks?</h3>
<ol>
<li>In ZooKeeper lock, if lock holder fails, system automatically releases locks. So locks are not really enforcing atomicity of other activities.</li>
<li>To make writes atomic, use “ready” trick or mini-transactions.</li>
<li>For master/leader election, new leader must inspect state and clean up.</li>
<li>Or use soft locks like MapReduce, for performance but not correctness. Only one worker will do each task, and each task is OK to be done twice.</li>
</ol>
<h3 id="how-does-zookeeper-implementa-double-barrier"><a class="markdownIt-Anchor" href="#how-does-zookeeper-implementa-double-barrier"></a> How does ZooKeeper implementa double barrier?</h3>
<ol>
<li>Double barriers enable clients to synchronize the beginning and the end of a computation.
<ul>
<li>Enter barrier: At the beginning, a client need to wait until the number of waiting clients exceeds the threshold before it can execute the computation.</li>
<li>Leave barrier: At the end, a client need to wait until the number of finished clients exceeds the threshold before it can exit.</li>
</ul>
</li>
<li>Represent a barrier in ZooKeeper with a znode, referred to as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span>.</li>
<li>Every process <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span></span></span></span> registers with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span> on entry by creating a znode as a child of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span>, and unregisters when it is ready to leave by removing the child.
<ul>
<li>Processes can enter the barrier when the number of child znodes of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span> exceeds the barrier threshold.</li>
<li>Processes can leave the barrier when all of the processes have removed their children.</li>
</ul>
</li>
<li>We use watches to efficiently wait for enter and exit conditions to be satisfied.
<ul>
<li>To enter, processes watch for the existence of a ready child of b that will be created by the process that causes the number of children to exceed the barrier threshold.</li>
<li>To leave, processes watch for a particular child to disappear and only check the exit condition once that znode has been removed.</li>
</ul>
</li>
</ol>
<h2 id="implementation-of-zookeeper"><a class="markdownIt-Anchor" href="#implementation-of-zookeeper"></a> Implementation of ZooKeeper</h2>
<h3 id="how-does-zookeeper-serve-requests"><a class="markdownIt-Anchor" href="#how-does-zookeeper-serve-requests"></a> How does ZooKeeper serve requests?</h3>
<ol>
<li>Upon receiving a request, a server prepares it for execution in request processor.</li>
<li>If a write request requires coordination among the servers, they use atomic broadcast to reach an agreement , and finally servers commit changes to the ZooKeeper database fully replicated across all servers of the ensemble.
<ul>
<li>When a server processes a write request, it also sends out and clears notifications relative to any watch that corresponds to that update.</li>
<li>Only the server that a client is connected to tracks and triggers notifications for that client.</li>
<li>Servers process writes in order and do not process other writes or reads concurrently. This ensures strict succession of notifications.</li>
</ul>
</li>
<li>In the case of read requests, a server simply reads the state of the local database and generates a response to the request.
<ul>
<li>Each read request is processed and tagged with a <em>zxid</em> that corresponds to the last transaction seen by the server.</li>
<li><em>Zxid</em> defines the partial order of the read requests with respect to the write requests.</li>
<li>For applications that require that ZooKeeper do not serve stale data, they can call <code>sync()</code> followed by the read operation.</li>
</ul>
</li>
</ol>
<h3 id="how-does-zookeeper-manage-database"><a class="markdownIt-Anchor" href="#how-does-zookeeper-manage-database"></a> How does ZooKeeper manage database?</h3>
<ol>
<li>The replicated database is an in-memory database containing the entire data tree.</li>
<li>Each znode in the tree stores a maximum of 1MB of data by default, but this maximum value is a configuration parameter that can be changed in specific cases.</li>
<li>For recoverability,
<ul>
<li>Efficiently log updates to disk, and we force writes to be on the disk media before they are applied to the in-memory database.</li>
<li>A replay log (a write-ahead log) of committed operations is kept and periodically generate snapshots of the in-memory database.</li>
</ul>
</li>
</ol>
<h3 id="how-does-request-processor-handle-write-requests"><a class="markdownIt-Anchor" href="#how-does-request-processor-handle-write-requests"></a> How does request processor handle write requests?</h3>
<ol>
<li>When the leader receives a write request, it calculates what the state of the system will be when the write is applied and transforms it into a transaction that captures this new state.</li>
<li>The future state must be calculated because there may be outstanding transactions that have not yet been applied to the database.</li>
</ol>
<h3 id="how-does-servers-reach-agreement"><a class="markdownIt-Anchor" href="#how-does-servers-reach-agreement"></a> How does servers reach agreement?</h3>
<ol>
<li>All requests that update ZooKeeper state are forwarded to the leader.</li>
<li>The leader executes the request and broadcasts the change to the ZooKeeper state through Zab.
<ul>
<li>Zab uses by default simple majority quorums to decide on a proposal, so Zab and thus ZooKeeper can only work if a majority of servers are correct.</li>
<li>Zab guarantees that changes broadcast by a leader are delivered in the order they were sent and all changes from previous leaders are delivered to an established leader before it broadcasts its own changes.</li>
<li>Because idempotent transactions is used, multiple delivery is acceptable as long as they are delivered in order.</li>
<li>ZooKeeper requires Zab to redeliver at least all messages that were delivered after the start of the last snapshot.</li>
</ul>
</li>
<li>The server that receives the client request responds to the client when it delivers the corresponding state change.</li>
<li>Use TCP for transport so message order is maintained by the network.</li>
</ol>
<h3 id="how-does-zookeeper-take-snapshot"><a class="markdownIt-Anchor" href="#how-does-zookeeper-take-snapshot"></a> How does ZooKeeper take snapshot?</h3>
<ol>
<li>ZooKeeper snapshots is called fuzzy snapshots since we do not lock the ZooKeeper state to take the snapshot.<br />
Instead, we do a depth first scan of the tree atomically reading each znode’s data and meta-data and writing them to disk.</li>
<li>Since the resulting fuzzy snapshot may have applied some subset of the state changes delivered during the generation of the snapshot, the result may not correspond to the state of ZooKeeper at any point in time.</li>
<li>However, since state changes are idempotent, we can apply them twice as long as we apply the state changes in order.</li>
</ol>
<h3 id="how-does-zookeeper-handle-sync"><a class="markdownIt-Anchor" href="#how-does-zookeeper-handle-sync"></a> How does ZooKeeper handle sync()</h3>
<ol>
<li>It does not need to atomically broadcast sync as using a leader-based algorithm, and it simply places the <code>sync</code> operation at the end of the queue of requests between the leader and the server executing the call to sync.</li>
<li>The follower must be sure that the leader is still the leader.
<ul>
<li>If there are pending transactions that commit, then the server does not suspect the leader.</li>
<li>If the pending queue is empty, the leader needs to issue a null transaction to commit and orders the <code>sync</code> after that transaction.</li>
<li>Hence, when the leader is under load, no extra broadcast traffic is generated.</li>
<li>Timeouts are set such that leaders realize they are not leaders before followers abandon them, so it does not issue the null transaction.</li>
</ul>
</li>
</ol>
<h3 id="how-does-zookeeper-ensure-to-serve-data-at-least-as-update-as-last-server-served-that-data"><a class="markdownIt-Anchor" href="#how-does-zookeeper-ensure-to-serve-data-at-least-as-update-as-last-server-served-that-data"></a> How does ZooKeeper ensure to serve data at least as update as last server served that data?</h3>
<ol>
<li>Check the last zxid of the client against its last zxid.</li>
<li>If the client has a more recent view than the server, the server does not reestablish the session with the client until the server has caught up.</li>
</ol>
<h3 id="how-to-detect-client-session-failures"><a class="markdownIt-Anchor" href="#how-to-detect-client-session-failures"></a> How to detect client session failures?</h3>
<ol>
<li>The leader determines that there has been a failure if no other server receives anything from a client session within the session timeout.</li>
<li>If the client sends requests frequently enough, then there is no need to send any other message.<br />
Otherwise, the client sends heartbeat messages during periods of low activity.</li>
<li>If the client cannot communicate with a server to send a request or heartbeat, it connects to a different ZooKeeper server to re-establish its session.</li>
<li>To prevent the session from timing out, the ZooKeeper client library sends a heartbeat after the session has been idle for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi mathvariant="normal">/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">s/3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord">/</span><span class="mord">3</span></span></span></span> ms and switch to a new server if it has not heard from a server for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>s</mi><mi mathvariant="normal">/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">2s/3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">2</span><span class="mord mathnormal">s</span><span class="mord">/</span><span class="mord">3</span></span></span></span> ms, where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">s</span></span></span></span> is the session timeout in milliseconds.</li>
</ol>
<h1 id="evaluation-and-results"><a class="markdownIt-Anchor" href="#evaluation-and-results"></a> Evaluation and results</h1>
<ol>
<li>
<p>To show the scalability of the system, the author varied the number of servers that make up the ZooKeeper service, but always kept the number of clients the same.</p>
<p>The throughput performance of a saturated system as the ratio of reads to writes vary is as shown below.</p>
<img src="/imgs/Distributed/ZooKeeper/Throughput.png">
</li>
<li>
<p>There are two reasons for write requests taking longer than read requests.</p>
<ul>
<li>First, write requests must go through atomic broadcast, which requires some extra processing and adds latency to requests.</li>
<li>The other reason for longer processing of write requests is that servers must ensure that transactions are logged to non-volatile store before sending acknowledgments back to the leader.</li>
</ul>
</li>
<li>
<p>The atomic broadcast protocol does most of the work of the system and thus limits the performance of ZooKeeper more than any other component.</p>
<p>To benchmark its performance the author simulates clients by generating the transactions directly at the leader, so there is no client connections or client requests and replies.</p>
<p>The result is as shown below:</p>
<img src="/imgs/Distributed/ZooKeeper/AtomicBroadcast.png">
</li>
<li>
<p>The author also tested the throughput of the system when occurring different failure events.</p>
<p>The events are: ① Failure and recovery of a follower; ② Failure and recovery of a different follower; ③ Failure of the leader; ④Failure of two followers (a, b) in the first two marks, and recovery at the third mark ©; ⑤ Failure of the leader; ⑥Recovery of the leader.</p>
<img src="/imgs/Distributed/ZooKeeper/Failures.png">
</li>
<li>
<p>To assess the latency of requests, the author creates a worker process that simply sends a create, waits for it to finish, sends an asynchronous delete of the new node, and then starts the next create.</p>
<p>Then the throughput can be calculated by dividing the number of create requests completed by the total time it took for all the workers to complete.</p>
<img src="/imgs/Distributed/ZooKeeper/Latency.png">
</li>
<li>
<p>The author also measured the performance of primitives implemented with ZooKeeper. They measured the performance of barriers.</p>
<p>The time to process all barriers increase roughly linearly with the number of barriers, showing that concurrent access to the same part of the data tree did not produce any unexpected delay</p>
<p>Latency increases proportionally to the number of clients. This is a consequence of not saturating the ZooKeeper service due to clients waiting on other clients.</p>
<img src="/imgs/Distributed/ZooKeeper/Barrier.png">
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Raft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Raft/" class="post-title-link" itemprop="url">Raft</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:14:13" itemprop="dateCreated datePublished" datetime="2023-09-26T13:14:13+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:29:01" itemprop="dateModified" datetime="2023-10-04T16:29:01+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/raft-extended.pdf">In Search of an Understandable Consensus Algorithm</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#basics">Basics</a>
<ul>
<li><a href="#how-does-raft-implements-consensus-overall">How does Raft implements consensus overall?</a></li>
<li><a href="#what-are-the-states-of-each-server">What are the states of each server?</a></li>
<li><a href="#how-to-divide-the-terms">How to divide the terms?</a></li>
<li><a href="#how-does-terms-change">How does terms change?</a></li>
<li><a href="#how-does-raft-handle-follower-and-candidate-crashes">How does Raft handle follower and candidate crashes?</a></li>
<li><a href="#states-stored-on-servers">States stored on servers</a></li>
</ul>
</li>
<li><a href="#leader-election">Leader election</a>
<ul>
<li><a href="#how-does-the-servers-states-transit">How does the servers states transit?</a></li>
<li><a href="#how-to-elect-a-leader">How to elect a leader?</a></li>
<li><a href="#how-is-the-election-held">How is the election held?</a></li>
<li><a href="#how-to-determine-that-a-server-loses-the-election">How to determine that a server loses the election?</a></li>
<li><a href="#how-to-handle-a-split-vote">How to handle a split vote?</a></li>
<li><a href="#how-to-ensure-that-the-leader-of-any-given-term-contains-all-of-the-entries-committed-in-previous-terms">How to ensure that the leader of any given term contains all of the entries committed in previous terms?</a></li>
<li><a href="#what-is-the-limitation-of-broadcast-time-and-election-timeout">What is the limitation of broadcast time and election timeout?</a></li>
<li><a href="#requestvote-rpc">RequestVote RPC</a></li>
</ul>
</li>
<li><a href="#log-replication">Log replication</a>
<ul>
<li><a href="#how-is-clients-request-handled">How is clients request handled?</a></li>
<li><a href="#what-is-stored-in-a-log-entry">What is stored in a log entry?</a></li>
<li><a href="#how-to-apply-a-log-entry-to-the-state-machines">How to apply a log entry to the state machines?</a></li>
<li><a href="#how-to-determine-the-consistency-between-logs">How to determine the consistency between logs?</a></li>
<li><a href="#how-to-check-the-consistency-in-appendentries-rpcs">How to check the consistency in AppendEntries RPCs?</a></li>
<li><a href="#what-kinds-of-inconsistency-may-incur">What kinds of inconsistency may incur?</a></li>
<li><a href="#how-does-leader-handle-follower-inconsistencies">How does leader handle follower inconsistencies?</a></li>
<li><a href="#how-does-appendentries-rpc-perform-consistency-check">How does AppendEntries RPC perform consistency check?</a></li>
<li><a href="#how-to-handle-uncommited-entries-from-previous-leaders">How to handle uncommited entries from previous leaders?</a></li>
<li><a href="#appendentries-rpc">AppendEntries RPC</a></li>
</ul>
</li>
<li><a href="#log-compaction">Log compaction</a>
<ul>
<li><a href="#how-does-raft-compact-logs">How does Raft compact logs?</a></li>
<li><a href="#how-to-handle-the-first-appendentries-following-the-snapshot">How to handle the first AppendEntries following the snapshot?</a></li>
<li><a href="#when-will-leader-send-its-snapshot-to-followers">When will leader send its snapshot to followers?</a></li>
<li><a href="#how-to-install-the-snapshot-from-leader">How to install the snapshot from leader?</a></li>
<li><a href="#when-should-a-server-to-snapshot">When should a server to snapshot?</a></li>
<li><a href="#how-to-reduce-the-delays-of-normal-operations-caused-by-a-snapshot">How to reduce the delays of normal operations caused by a snapshot?</a></li>
<li><a href="#installsnapshot-rpc">InstallSnapshot RPC</a></li>
</ul>
</li>
<li><a href="#client-interaction">Client interaction</a>
<ul>
<li><a href="#how-does-client-find-cluster-leader">How does client find cluster leader?</a></li>
<li><a href="#how-to-prevent-raft-execute-a-command-multiple-times">How to prevent Raft execute a command multiple times?</a></li>
<li><a href="#how-to-prevent-returning-stale-date-to-a-read-only-operation">How to prevent returning stale date to a read-only operation?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiements-and-results">Experiements and results</a></li>
<li><a href="#reproduce-and-unmentioned-parts">Reproduce and unmentioned parts</a></li>
</ul>
</p>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Enhance the understandability of Paxos
<ul>
<li>Raft separates the key elements of consensus, such as leader election, log replication, and safety</li>
<li>It enforces a stronger degree of coherency to reduce the number of states that must be considered</li>
</ul>
</li>
<li>Novel features: strong leader, leader election, membership changes.</li>
<li><strong>What is the common properties of consensus algorithms?</strong>
<ul>
<li>Safety: never returning an incorrect result under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering.</li>
<li>Available as long as any majority of the servers are operational and can communicate with each other and with clients.</li>
<li>They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems.</li>
<li>A command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="basics"><a class="markdownIt-Anchor" href="#basics"></a> Basics</h2>
<h3 id="how-does-raft-implements-consensus-overall"><a class="markdownIt-Anchor" href="#how-does-raft-implements-consensus-overall"></a> How does Raft implements consensus overall?</h3>
<ol>
<li>
<p>First electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log.</p>
</li>
<li>
<p>The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines.</p>
</li>
<li>
<p>A leader can fail or become disconnected from the other servers, in which case a new leader is elected.</p>
</li>
</ol>
<h3 id="what-are-the-states-of-each-server"><a class="markdownIt-Anchor" href="#what-are-the-states-of-each-server"></a> What are the states of each server?</h3>
<ol>
<li>At any given time each server is in one of three states: leader, follower, or candidate.</li>
<li>In normal operation there is exactly one leader and all of the other servers are followers.</li>
<li>Followers are passive: they issue no requests on their own but simply respond to requests from leaders and candidates.</li>
<li>The leader handles all client requests. If a client contacts a follower, the follower redirects it to the leader.</li>
<li>The candidate is used to elect a new leader.</li>
</ol>
<h3 id="how-to-divide-the-terms"><a class="markdownIt-Anchor" href="#how-to-divide-the-terms"></a> How to divide the terms?</h3>
<ol>
<li>Terms are numbered with consecutive integers. Each election begins a new term.</li>
<li>If an election results in a split vote, the term will end with no leader; a new term with a new election will begin shortly.</li>
<li>Terms act as a logical clock in Raft, and they allow servers to detect obsolete information such as stale leaders.</li>
</ol>
<h3 id="how-does-terms-change"><a class="markdownIt-Anchor" href="#how-does-terms-change"></a> How does terms change?</h3>
<ol>
<li>Each server stores a current term number, which increases monotonically over time.</li>
<li>Current terms are exchanged whenever servers communicate; if one server’s current term is smaller than the other’s, then it updates its current term to the larger value.</li>
<li>If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state.</li>
<li>If a server receives a request with a stale term number, it rejects the request.</li>
</ol>
<h3 id="how-does-raft-handle-follower-and-candidate-crashes"><a class="markdownIt-Anchor" href="#how-does-raft-handle-follower-and-candidate-crashes"></a> How does Raft handle follower and candidate crashes?</h3>
<ol>
<li>
<p>If a follower or candidate crashes, then future RequestVote and AppendEntries RPCs sent to it will fail. Raft handles these failures by retrying indefinitely.</p>
</li>
<li>
<p>If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCs are idempotent, i.e. servers will ignore the RPCs that is already handled, so this causes no harm.</p>
</li>
</ol>
<h3 id="states-stored-on-servers"><a class="markdownIt-Anchor" href="#states-stored-on-servers"></a> States stored on servers</h3>
<ol>
<li>
<p>Persistent state on all servers: These states need to be updated on stable storage before responding to RPCs, i.e. communicating with outside.</p>
<ul>
<li><code>currentTerm</code>: latest term server has seen (initialized to 0 on first boot, increases monotonically)</li>
<li><code>votedFor</code>: candidateId that received vote in current term (or <code>null</code> if none)</li>
<li><code>log[]</code>: log entries</li>
</ul>
</li>
<li>
<p>Volatile state on all servers:</p>
<ul>
<li><code>commitIndex</code>: index of highest log entry known to be committed (initialized to 0, increases monotonically)</li>
<li><code>lastApplied</code>: index of highest log entry applied to state machine (initialized to 0, increases monotonically)</li>
</ul>
</li>
<li>
<p>Volatile state on leaders: These states need to be reinitialized after election</p>
<ul>
<li><code>nextIndex[]</code>: for each server, index of the next log entry to send to that server (initialized to leader last log index + 1)</li>
<li><code>matchIndex[]</code>: for each server, index of highest log entry known to be replicated on server (initialized to 0, increases monotonically)</li>
</ul>
</li>
</ol>
<h2 id="leader-election"><a class="markdownIt-Anchor" href="#leader-election"></a> Leader election</h2>
<h3 id="how-does-the-servers-states-transit"><a class="markdownIt-Anchor" href="#how-does-the-servers-states-transit"></a> How does the servers states transit?</h3>
<ol>
<li>
<p>When servers start up, they begin as followers. A server remains in follower state as long as it receives valid RPCs from a leader or candidate.</p>
</li>
<li>
<p>Leaders send periodic heartbeats (AppendEntries RPCs that carry no log entries) to all followers in order to maintain their authority.</p>
</li>
<li>
<p>If a follower receives no communication over a period of time called the election timeout, then it assumes there is no viable leader and become a candidate to initiate a new election.</p>
</li>
<li>
<p>A candidate that receives votes from a majority of the full cluster becomes the new leader.</p>
<p><img src="/imgs/Distributed/Raft/01.png" alt="" /></p>
</li>
</ol>
<h3 id="how-to-elect-a-leader"><a class="markdownIt-Anchor" href="#how-to-elect-a-leader"></a> How to elect a leader?</h3>
<ol>
<li>
<p>To begin an election, a follower increments its current term and transitions to candidate state.</p>
</li>
<li>
<p>It then votes for itself and issues RequestVote RPCs in parallel to each of the other servers in the cluster.</p>
</li>
<li>
<p>Candidate continues in this state until one of three things happens</p>
<ul>
<li>
<p>It wins the election</p>
</li>
<li>
<p>Another server establishes itself as leader</p>
</li>
<li>
<p>A period of time goes by with no winner.</p>
</li>
</ul>
</li>
</ol>
<h3 id="how-is-the-election-held"><a class="markdownIt-Anchor" href="#how-is-the-election-held"></a> How is the election held?</h3>
<ol>
<li>Each server will vote for at most one candidate in a given term, on a first-come-first-served basis to ensure that at most one candidate can win the election for a particular term.</li>
<li>A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term.</li>
<li>Once a candidate wins an election, it becomes leader. It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.</li>
</ol>
<h3 id="how-to-determine-that-a-server-loses-the-election"><a class="markdownIt-Anchor" href="#how-to-determine-that-a-server-loses-the-election"></a> How to determine that a server loses the election?</h3>
<ol>
<li>A candidate may receive an AppendEntries RPC from another server claiming to be leader.</li>
<li>If the leader’s term is at least as large as the candidate’s current term, then the candidate recognizes the leader as legitimate and returns to follower state.</li>
<li>If the term in the RPC is smaller than the candidate’s current term, then the candidate rejects the RPC and continues in candidate state.</li>
</ol>
<h3 id="how-to-handle-a-split-vote"><a class="markdownIt-Anchor" href="#how-to-handle-a-split-vote"></a> How to handle a split vote?</h3>
<ol>
<li>
<p>If many followers become candidates at the same time, votes could be split so that no candidate obtains a majority.</p>
</li>
<li>
<p>Each candidate will time out and start a new election by incrementing its term and initiating another round of RequestVote RPCs.</p>
</li>
<li>
<p>Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly.</p>
<ul>
<li>
<p>Election timeouts are chosen randomly from a fixed interval at the start of an election, and it waits for that timeout to elapse before starting the next election.</p>
</li>
<li>
<p>In most cases only a single server will time out; it wins the election and sends heartbeats before any other servers time out.</p>
</li>
</ul>
</li>
</ol>
<h3 id="how-to-ensure-that-the-leader-of-any-given-term-contains-all-of-the-entries-committed-in-previous-terms"><a class="markdownIt-Anchor" href="#how-to-ensure-that-the-leader-of-any-given-term-contains-all-of-the-entries-committed-in-previous-terms"></a> How to ensure that the leader of any given term contains all of the entries committed in previous terms?</h3>
<ol>
<li>
<p>A candidate must contact a majority of the cluster in order to be elected, which means that every committed entry must be present in at least one of those servers.</p>
</li>
<li>
<p>If the candidate’s log is at least as up-to-date as any other log in that majority， then it will hold all the committed entries.</p>
</li>
<li>
<p>In the RequestVote RPC, the voter denies its vote if its own log is more up-to-date than that of the candidate.</p>
</li>
<li>
<p>Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs.</p>
<ul>
<li>
<p>If the logs have last entries with different terms, then the log with the later term is more up-to-date.</p>
</li>
<li>
<p>If the logs end with the same term, then whichever log is longer is more up-to-date.</p>
</li>
</ul>
</li>
</ol>
<h3 id="what-is-the-limitation-of-broadcast-time-and-election-timeout"><a class="markdownIt-Anchor" href="#what-is-the-limitation-of-broadcast-time-and-election-timeout"></a> What is the limitation of broadcast time and election timeout?</h3>
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>r</mi><mi>o</mi><mi>a</mi><mi>d</mi><mi>c</mi><mi>a</mi><mi>s</mi><mi>t</mi><mi>T</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>≪</mo><mi>e</mi><mi>l</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>T</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo>≪</mo><mi>M</mi><mi>T</mi><mi>B</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">broadcastTime\ll electionTimeout\ll MTBF</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">b</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">i</span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">i</span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span></span></span></li>
<li>BbroadcastTime is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses. electionTimeout is the election timeout. MTBF is the average time between failures for a single server.</li>
<li>The broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbeat messages required to keep followers from starting elections.</li>
<li>The election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.</li>
</ol>
<h3 id="requestvote-rpc"><a class="markdownIt-Anchor" href="#requestvote-rpc"></a> RequestVote RPC</h3>
<ol>
<li>This is invoked by candidates to gather votes.</li>
<li>Arguments:
<ul>
<li><code>term</code>: candidate’s term</li>
<li><code>candidateId</code>: candidate requesting vote</li>
<li><code>lastLogIndex</code>: index of candidate’s last log entry</li>
<li><code>lastLogTerm</code>: term of candidate’s last log entry</li>
</ul>
</li>
<li>Results:
<ul>
<li><code>term</code>: currentTerm, for candidate to update itself</li>
<li><code>voteGranted</code>: true means candidate received vote</li>
</ul>
</li>
<li>Receiver implementation:
<ul>
<li>Reply <code>false</code> if <code>term &lt; currentTerm</code></li>
<li>If votedFor is <code>null</code> or <code>candidateId</code>, and candidate’s log is at least as up-to-date as receiver’s log, grant vote</li>
</ul>
</li>
</ol>
<h2 id="log-replication"><a class="markdownIt-Anchor" href="#log-replication"></a> Log replication</h2>
<h3 id="how-is-clients-request-handled"><a class="markdownIt-Anchor" href="#how-is-clients-request-handled"></a> How is clients request handled?</h3>
<ol>
<li>Each client request contains a command to be executed by the replicated state machines.</li>
<li>The leader appends the command to its log as a new entry, then issues AppendEntries RPCs in parallel to each of the other servers to replicate the entry.</li>
<li>When the entry has been safely replicated, the leader applies the entry to its state machine and returns the result of that execution to the client.</li>
<li>If followers crash or run slowly, or if network packets are lost, the leader retries AppendEntries RPCs indefinitely (even after it has responded to the client) until all followers eventually store all log entries.</li>
</ol>
<h3 id="what-is-stored-in-a-log-entry"><a class="markdownIt-Anchor" href="#what-is-stored-in-a-log-entry"></a> What is stored in a log entry?</h3>
<ol>
<li>A command for state machine</li>
<li>A term number when entry was received by leader.</li>
<li>An index to identify its position in the log. The index of the first log is 1.</li>
</ol>
<h3 id="how-to-apply-a-log-entry-to-the-state-machines"><a class="markdownIt-Anchor" href="#how-to-apply-a-log-entry-to-the-state-machines"></a> How to apply a log entry to the state machines?</h3>
<ol>
<li>
<p>The leader decides when it is safe to apply a log entry to the state machines.</p>
<ul>
<li>
<p>Such an entry is called <code>committed</code>.</p>
</li>
<li>
<p>Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines.</p>
</li>
</ul>
</li>
<li>
<p>A log entry is committed once the leader that created the entry has replicated it on a majority of the servers.</p>
</li>
<li>
<p>It also commits all preceding entries in the leader’s log, including entries created by previous leaders.</p>
</li>
<li>
<p>The leader keeps track of the highest index it knows to be committed, and it includes that index in future AppendEntries RPCs, including heartbeats, so that the other servers eventually find out that they should commit some new entries.</p>
</li>
<li>
<p>Once a follower learns that a log entry is committed, it applies the entry to its local state machine in log order.</p>
</li>
</ol>
<h3 id="how-to-determine-the-consistency-between-logs"><a class="markdownIt-Anchor" href="#how-to-determine-the-consistency-between-logs"></a> How to determine the consistency between logs?</h3>
<ol>
<li>
<p><strong>Log Matching property</strong>: If two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index.</p>
</li>
<li>
<p>The Log Matching property is maintained through the following properties:</p>
<ul>
<li>
<p>If two entries in different logs have the same index and term, then they store the same command.</p>
</li>
<li>
<p>If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.</p>
</li>
</ul>
</li>
</ol>
<h3 id="how-to-check-the-consistency-in-appendentries-rpcs"><a class="markdownIt-Anchor" href="#how-to-check-the-consistency-in-appendentries-rpcs"></a> How to check the consistency in AppendEntries RPCs?</h3>
<ol>
<li>When sending an AppendEntries RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries.</li>
<li>If the follower does not find an entry in its log with the same index and term, then it refuses the new entries.</li>
</ol>
<h3 id="what-kinds-of-inconsistency-may-incur"><a class="markdownIt-Anchor" href="#what-kinds-of-inconsistency-may-incur"></a> What kinds of inconsistency may incur?</h3>
<ol>
<li>Leader crashes can leave the logs inconsistent. The old leader may not have fully replicated all of the entries in its log.</li>
<li>A follower may be missing entries that are present on the leader, it may have extra entries that are not present on the leader, or both.</li>
</ol>
<h3 id="how-does-leader-handle-follower-inconsistencies"><a class="markdownIt-Anchor" href="#how-does-leader-handle-follower-inconsistencies"></a> How does leader handle follower inconsistencies?</h3>
<ol>
<li><strong>Leader Append-Only</strong> property: a leader never overwrites or deletes entries in its log.</li>
<li>The leader handles inconsistencies by forcing the followers’ logs to duplicate its own. Namely, conflicting entries in follower logs will be overwritten with entries from the leader’s log.</li>
<li>The leader must find the latest log entry where the two logs agree, delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point.</li>
<li>All of these actions happen in response to the consistency check performed by AppendEntries RPCs.</li>
<li>A leader does not need to take any special actions to restore log consistency when it comes to power. It just begins normal operation, and the logs automatically converge in response to failures of the AppendEntries consistency check.</li>
</ol>
<h3 id="how-does-appendentries-rpc-perform-consistency-check"><a class="markdownIt-Anchor" href="#how-does-appendentries-rpc-perform-consistency-check"></a> How does AppendEntries RPC perform consistency check?</h3>
<ol>
<li>The leader maintains a nextIndex for each follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log, i.e. assuming all followers are as up-to-date as itself.</li>
<li>If a follower’s log is inconsistent with the leader’s, the AppendEntries consistency check will fail in the next AppendEntries RPC.</li>
<li>After a rejection, the leader decrements nextIndex and retries the AppendEntries RPC.</li>
<li>Eventually nextIndex will reach a point where the leader and follower logs match. When this happens, AppendEntries will succeed, which removes any conflicting entries in the follower’s log and appends entries from the leader’s log (if any).</li>
<li>Once AppendEntries succeeds, the follower’s log is consistent with the leader’s, and it will remain that way for the rest of the term.</li>
</ol>
<h3 id="how-to-handle-uncommited-entries-from-previous-leaders"><a class="markdownIt-Anchor" href="#how-to-handle-uncommited-entries-from-previous-leaders"></a> How to handle uncommited entries from previous leaders?</h3>
<ol>
<li>If a leader crashes before committing an entry, future leaders will attempt to finish replicating the entry.</li>
<li>A leader cannot immediately conclude that an entry from a previous term <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">term_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> is committed once it is stored on a majority of servers.
<ul>
<li>There could have other entries in a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">term_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> between <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">term_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> and the leader’s current term <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">term_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</li>
<li>If nothing in the leaders current term has reached agreement, after the leader dies, that server with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">term_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> may become the new leader, and it can overwrite entries of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">term_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> although those entries are committed since <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>i</mi></msub><mo>&gt;</mo><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">term_i&gt;term_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
<li>Raft never commits log entries from previous terms by counting replicas.</li>
<li>Only log entries from the leader’s current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property.</li>
</ol>
<h3 id="appendentries-rpc"><a class="markdownIt-Anchor" href="#appendentries-rpc"></a> AppendEntries RPC</h3>
<ol>
<li>This is invoked by leader to replicate log entries; also used as heartbeat.</li>
<li>Arguments:
<ul>
<li><code>term</code>: leader’s term</li>
<li><code>leaderId</code>: so follower can redirect clients</li>
<li><code>prevLogIndex</code>: index of log entry immediately preceding new ones.</li>
<li><code>prevLogTerm</code>: term of <code>prevLogIndex</code> entry</li>
<li><code>entries[]</code>: log entries to store (empty for hearbeat; may send more than one for efficiency)</li>
<li><code>leaderCommit</code>: leader’s <code>commitIndex</code></li>
</ul>
</li>
<li>Results:
<ul>
<li><code>term</code>: <code>currentTerm</code>, for leader to update itself</li>
<li><code>success</code>: true if follower contrained entry matching <code>prevLogIndex</code> and <code>prevLogTerm</code></li>
</ul>
</li>
<li>Receiver implementation:
<ul>
<li>Reply <code>false</code> if <code>term &lt; currentTerm</code></li>
<li>Reply <code>false</code> if log doesn’t contrain an entry at <code>prevLogIndex</code> whose term matches <code>prevLogTerm</code></li>
<li>If an existing entry conflicts with a new one (same index but different terms), delete the existing entry and all that follow it.</li>
<li>Append any new entries not already in the log.</li>
<li>If <code>leaderCommit &gt; commitIndex</code>, set <code>commitIndex = min(leaderCommit, index of last new entry)</code>.</li>
</ul>
</li>
</ol>
<h2 id="log-compaction"><a class="markdownIt-Anchor" href="#log-compaction"></a> Log compaction</h2>
<h3 id="how-does-raft-compact-logs"><a class="markdownIt-Anchor" href="#how-does-raft-compact-logs"></a> How does Raft compact logs?</h3>
<ol>
<li>In snapshotting, the entire current system state is written to a snapshot on stable storage.</li>
<li>Once a server completes writing a snapshot, it may delete all log entries up through the last included index, as well as any prior snapshot.</li>
<li>Each server takes snapshots independently, covering just the committed entries in its log.</li>
<li>Data still only flows from leaders to followers, just followers can now reorganize their data.</li>
</ol>
<h3 id="how-to-handle-the-first-appendentries-following-the-snapshot"><a class="markdownIt-Anchor" href="#how-to-handle-the-first-appendentries-following-the-snapshot"></a> How to handle the first AppendEntries following the snapshot?</h3>
<ol>
<li>Raft also includes a small amount of metadata in the snapshot.</li>
<li>The <code>last included index</code> is the index of the last entry in the log that the snapshot replaces (the last entry the state machine had applied).</li>
<li>The <code>last included term</code> is the term of this entry.</li>
</ol>
<h3 id="when-will-leader-send-its-snapshot-to-followers"><a class="markdownIt-Anchor" href="#when-will-leader-send-its-snapshot-to-followers"></a> When will leader send its snapshot to followers?</h3>
<ol>
<li>When the leader has already discarded the next log entry that it needs to send to a follower.</li>
<li>This situation is unlikely in normal operation: a follower that has kept up with the leader would already have this entry.</li>
<li>However, an exceptionally slow follower or a new server joining the cluster would not.</li>
</ol>
<h3 id="how-to-install-the-snapshot-from-leader"><a class="markdownIt-Anchor" href="#how-to-install-the-snapshot-from-leader"></a> How to install the snapshot from leader?</h3>
<ol>
<li>
<p>The leader uses a new RPC called InstallSnapshot to send snapshots to followers that are too far behind.</p>
</li>
<li>
<p>When a follower receives a snapshot with this RPC, it must decide what to do with its existing log entries.</p>
</li>
<li>
<p>If the snapshot contains new information not already in the recipient’s log</p>
<ul>
<li>
<p>The follower discards its entire log</p>
</li>
<li>
<p>It is all superseded by the snapshot and may possibly have uncommitted entries that conflict with the snapshot.</p>
</li>
</ul>
</li>
<li>
<p>If instead the follower receives a snapshot that describes a prefix of its log</p>
<ul>
<li>
<p>This could be due to retransmission or by mistake.</p>
</li>
<li>
<p>Log entries covered by the snapshot are deleted but entries following the snapshot are still valid and must be retained.</p>
</li>
</ul>
</li>
</ol>
<h3 id="when-should-a-server-to-snapshot"><a class="markdownIt-Anchor" href="#when-should-a-server-to-snapshot"></a> When should a server to snapshot?</h3>
<ol>
<li>If a server snapshots too often, it wastes disk bandwidth and energy; if it snapshots too infrequently, it risks exhausting its storage capacity, and it increases the time required to replay the log during restarts.</li>
<li>One simple strategy is to take a snapshot when the log reaches a fixed size in bytes.</li>
<li>If this size is set to be significantly larger than the expected size of a snapshot, then the disk bandwidth overhead for snapshotting will be small.</li>
</ol>
<h3 id="how-to-reduce-the-delays-of-normal-operations-caused-by-a-snapshot"><a class="markdownIt-Anchor" href="#how-to-reduce-the-delays-of-normal-operations-caused-by-a-snapshot"></a> How to reduce the delays of normal operations caused by a snapshot?</h3>
<ol>
<li>The solution is to use copy-on-write techniques so that new updates can be accepted without impacting the snapshot being written.</li>
<li>The operating system’s copy-on-write support (e.g., fork on Linux) can be used to create an in-memory snapshot of the entire state machine.</li>
</ol>
<h3 id="installsnapshot-rpc"><a class="markdownIt-Anchor" href="#installsnapshot-rpc"></a> InstallSnapshot RPC</h3>
<ol>
<li>This is invoked by leader to send chunks of snapshot to a follower. Leaders always send chunks in order.</li>
<li>Arguments:
<ul>
<li><code>term</code>: leader’s term</li>
<li><code>leaderId</code>: so follower can redirect clients</li>
<li><code>lastIncludedIndex</code>: the snapshot replaces all entries up through and including this index</li>
<li><code>lastIncludedTerm</code>: term of <code>lastIncludedIndex</code></li>
<li><code>offset</code>: byte offset where chunk is positioned in the snapshot file
<ul>
<li>The whole snapshot file may be large, and hence divided into several chunks.</li>
</ul>
</li>
<li><code>data[]</code>: raw bytes of the snapshot chunk, starting at offset</li>
<li><code>done</code>: <code>true</code> if this is the last chunk</li>
</ul>
</li>
<li>Results:
<ul>
<li><code>term</code>: <code>currentTerm</code>, for leader to update itself</li>
</ul>
</li>
<li>Receiver implementation:
<ul>
<li>Reply immediately if <code>term &lt; currentTerm</code></li>
<li>Create new snapshot file if first chunk (offset is 0)</li>
<li>Write data into snapshot file at given offset</li>
<li>Reply and wait for more data chunks if done is <code>false</code>.</li>
<li>Save snapshot file, discard any existing or partial snapshot with smaller index</li>
<li>If existing log entry has same index and term as snapshot’s last included entry, retain log entries following it and reply</li>
<li>Discard the entire log</li>
<li>Reset state machine using snapshot contents (and load snapshot’s cluster configuration)</li>
</ul>
</li>
</ol>
<h2 id="client-interaction"><a class="markdownIt-Anchor" href="#client-interaction"></a> Client interaction</h2>
<h3 id="how-does-client-find-cluster-leader"><a class="markdownIt-Anchor" href="#how-does-client-find-cluster-leader"></a> How does client find cluster leader?</h3>
<ol>
<li>
<p>When a client first starts up</p>
<ul>
<li>
<p>It connects to a randomlychosen server.</p>
</li>
<li>
<p>If the client’s first choice is not the leader, that server will reject the client’s request and supply information about the most recent leader it has heard from.</p>
</li>
</ul>
</li>
<li>
<p>If the leader crashes</p>
<ul>
<li>
<p>Client requests will time out.</p>
</li>
<li>
<p>Clients then try again with randomly-chosen servers.</p>
</li>
</ul>
</li>
</ol>
<h3 id="how-to-prevent-raft-execute-a-command-multiple-times"><a class="markdownIt-Anchor" href="#how-to-prevent-raft-execute-a-command-multiple-times"></a> How to prevent Raft execute a command multiple times?</h3>
<ol>
<li>
<p>Our goal for Raft is to implement linearizable semantics, i.e. each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response.</p>
<ul>
<li>One case of executing a command multiple times is that if the leader crashes after committing the log entry but before responding to the client, the client will retry the command with a new leader, causing it to be executed a second time.</li>
</ul>
</li>
<li>
<p>The solution is for clients to assign unique serial numbers to every command.</p>
</li>
<li>
<p>Then, the state machine tracks the latest serial number processed for each client, along with the associated response.</p>
</li>
<li>
<p>If it receives a command whose serial number has already been executed, it responds immediately without re-executing the request.</p>
</li>
</ol>
<h3 id="how-to-prevent-returning-stale-date-to-a-read-only-operation"><a class="markdownIt-Anchor" href="#how-to-prevent-returning-stale-date-to-a-read-only-operation"></a> How to prevent returning stale date to a read-only operation?</h3>
<ol>
<li>
<p>The reason for stale reading is that the leader responding to the request might have been superseded by a newer leader of which it is unaware.</p>
</li>
<li>
<p>A leader must have the latest information on which entries are committed.</p>
<ul>
<li>
<p>The Leader Completeness Property guarantees that a leader has all committed entries, but at the start of its term, it may not know which those are.</p>
</li>
<li>
<p>To find out, it needs to commit an entry from its term.</p>
</li>
<li>
<p>Raft handles this by having each leader commit a blank <em>no-op</em> entry into the log at the start of its term.</p>
</li>
</ul>
</li>
<li>
<p>A leader must check whether it has been deposed before processing a read-only request, since its information may be stale if a more recent leader has been elected).</p>
<ul>
<li>
<p>Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests.</p>
</li>
<li>
<p>Alternatively, the leader could rely on the heartbeat mechanism to provide a form of lease, but this would rely on timing for safety (it assumes bounded clock skew).</p>
</li>
</ul>
</li>
</ol>
<h1 id="experiements-and-results"><a class="markdownIt-Anchor" href="#experiements-and-results"></a> Experiements and results</h1>
<ol>
<li>The main goal of Raft is to propose a consensus algorithm which is easier to understand than Paxos. Hence the author measured the understandability of this model through scores of learning students.</li>
<li>A most important measure of a new system is its correctness. The author proved the correctness of Raft with a formal specification.</li>
<li>Finally, the author also measured the performance of Raft, which is similar to other consensus algorithms.</li>
<li>The election timeout will effect the performance of the system through the performance of leader election. Hence, the author measured how will the randomization and base election timeout effect the performance.
<ul>
<li>A small amount of randomization in the election timeout is enough to avoid split votes in elections. Using more randomness improves worst-case behavior.</li>
<li>Downtime can be reduced by reducing the election timeout.
<ul>
<li>However, lowering the timeouts beyond 12 - 14 ms violates Raft’s timing requirement: leaders have difficulty broadcasting heartbeats before other servers start new elections. This can cause unnecessary leader changes and lower overall system availability.</li>
<li>The author recommends using a conservative election timeout such as 150–300ms; such timeouts are unlikely to cause unnecessary leader changes and will still provide good availability.</li>
</ul>
</li>
<li>
<img src="/imgs/Distributed/Raft/02.png" style="zoom:33%;" />
</li>
</ul>
</li>
</ol>
<h1 id="reproduce-and-unmentioned-parts"><a class="markdownIt-Anchor" href="#reproduce-and-unmentioned-parts"></a> Reproduce and unmentioned parts</h1>
<p>Reference to the Lab 2, 3 and 4 of MIT 6.824.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Fault-Tolerance-VM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Fault-Tolerance-VM/" class="post-title-link" itemprop="url">Fault Tolerance VM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:11:02" itemprop="dateCreated datePublished" datetime="2023-09-26T13:11:02+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:28:34" itemprop="dateModified" datetime="2023-10-04T16:28:34+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/vm-ft.pdf">The Design of a Practical System for Fault-Tolerance Virtual Machines</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#ft-design">FT design</a>
<ul>
<li><a href="#primary-backup-structure">Primary-backup structure</a>
<ul>
<li><a href="#what-is-the-usual-way-to-implement-fault-tolerance-via-primarybackup-approach">What is the usual way to implement fault-tolerance via primary/backup approach?</a></li>
<li><a href="#what-is-the-difference-between-physical-servers-and-vm-in-state-machine-level">What is the difference between physical servers and VM in state machine level?</a></li>
<li><a href="#what-is-the-basic-structure-of-ft-vms">What is the basic structure of FT VMs?</a></li>
</ul>
</li>
<li><a href="#ft-protocol">FT protocol</a>
<ul>
<li><a href="#how-does-vmware-backup-vm-replay">How does VMware backup VM replay?</a></li>
<li><a href="#what-if-the-backup-vm-executes-in-a-way-different-from-the-primary-vm">What if the backup VM executes in a way different from the primary VM?</a></li>
<li><a href="#will-the-output-rule-affect-vm-eg-stop-its-execution">Will the Output Rule affect VM, e.g. stop its execution?</a></li>
<li><a href="#what-is-the-subtleties-of-executing-disk-reads-on-the-backup-vm">What is the subtleties of executing disk reads on the backup VM?</a></li>
</ul>
</li>
<li><a href="#detecting-and-responding-to-failure">Detecting and responding to failure</a>
<ul>
<li><a href="#how-to-handle-duplicate-outputs">How to handle duplicate outputs?</a></li>
<li><a href="#how-to-handle-backup-vm-failure">How to handle backup VM failure?</a></li>
<li><a href="#how-to-handle-primary-vm-failure">How to handle primary VM failure?</a></li>
<li><a href="#after-a-failover-how-will-the-new-primary-vm-communicate-with-external-world">After a failover, how will the new primary VM communicate with external world?</a></li>
<li><a href="#how-to-detect-failure-of-primary-or-backup-vms">How to detect failure of primary or backup VMs?</a></li>
<li><a href="#how-to-avoid-split-brain-problems">How to avoid split-brain problems?</a></li>
</ul>
</li>
<li><a href="#alternative-non-shared-disk">Alternative: Non-shared disk</a>
<ul>
<li><a href="#what-is-the-difference-between-non-shared-disk-and-shared-disk">What is the difference between non-shared disk and shared disk?</a></li>
<li><a href="#in-what-case-non-shared-disk-will-be-useful">In what case non-shared disk will be useful?</a></li>
<li><a href="#what-is-the-disadvantage-of-non-shared-disk">What is the disadvantage of non-shared disk?</a></li>
<li><a href="#how-to-solve-the-split-brain-situation">How to solve the split-brain situation?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#starting-and-restarting">Starting and restarting</a>
<ul>
<li><a href="#what-requirements-need-to-be-satisfied-by-the-startup-mechanism">What requirements need to be satisfied by the startup mechanism?</a></li>
<li><a href="#how-to-implement-the-startup-mechanism">How to implement the startup mechanism?</a></li>
<li><a href="#how-to-choose-a-server-on-which-to-run-the-backup-vm">How to choose a server on which to run the backup VM?</a></li>
</ul>
</li>
<li><a href="#logging-channel">Logging channel</a>
<ul>
<li><a href="#how-to-control-primary-sending-log-entries-and-backup-receiving-entries">How to control primary sending log entries, and backup receiving entries?</a></li>
<li><a href="#what-if-the-log-buffer-of-the-primary-is-full">What if the log buffer of the primary is full?</a></li>
<li><a href="#what-is-the-main-cause-of-the-buffer-of-primary-being-full">What is the main cause of the buffer of primary being full?</a></li>
<li><a href="#how-to-prevent-the-backup-vm-from-getting-too-far-behind-the-primary">How to prevent the backup VM from getting too far behind the primary?</a></li>
</ul>
</li>
<li><a href="#special-operations">Special operations</a>
<ul>
<li><a href="#how-to-deal-with-control-operations">How to deal with control operations?</a></li>
<li><a href="#how-to-implement-the-vmotion-for-primary-and-backup-vms">How to implement the VMotion for primary and backup VMs?</a></li>
</ul>
</li>
<li><a href="#issues-for-disk-ios">Issues for disk IOs</a>
<ul>
<li><a href="#how-many-kind-of-races-may-occur">How many kind of races may occur?</a></li>
<li><a href="#how-to-solve-the-non-determinism-caused-by-several-io-operations">How to solve the non-determinism caused by several IO operations?</a></li>
<li><a href="#how-to-solve-the-non-determinism-caused-by-io-operations-and-applicationos">How to solve the non-determinism caused by IO operations and application/OS?</a></li>
<li><a href="#how-the-newly-promoted-primary-vm-handle-those-outstanding-ios">How the newly-promoted primary VM handle those outstanding IOs?</a></li>
</ul>
</li>
<li><a href="#issues-for-network-io">Issues for network IO</a>
<ul>
<li><a href="#how-to-solve-the-non-determinism-caused-by-asynchronous-updates">How to solve the non-determinism caused by asynchronous updates?</a></li>
<li><a href="#how-can-we-optimize-the-network-performance-while-running-ft">How can we optimize the network performance while running FT?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiments-and-results">Experiments and results</a></li>
</ul>
</p>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Contribution
<ul>
<li>This paper implemented a system providing fault tolerance virtual machine (VM) based on the approach of replicating the execution of a primary VM vis a backup VM on another server. The system automatically restores redundancy after faulure.</li>
<li>It reduces performance of real applications by less than 10%. The data bandwidth needed to keep the primary and secondary VM executing in lockstep is less than 20 Mb/s for several real applications, which allows for the possibility of implementing fault tolerance over longer distance.</li>
<li>The system automatically restores redundancy after a failure by starting a new backup viretual machine on any available server in the local cluster.</li>
</ul>
</li>
<li>Limitation
<ul>
<li>Only support uni-processor VMs. Recording and replaying the execution of a multi-processor VM have significant performance issues because nearly every access to shared memory can be a non-deterministic operation.</li>
<li>Only attempt to deal with fail-stop failure, which are server failures that can be detected before the failing server causes an incorrect externally visible action.</li>
</ul>
</li>
<li>Challenges
<ul>
<li>Correctly capturing all the input and non-determinism necessary to ensure deterministic execution of a backup virtual machine.</li>
<li>Correctly applying the inputs and non-determinism to the backup virtual machine.</li>
<li>Doing so in a manner that doesn’t degrade performance.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="ft-design"><a class="markdownIt-Anchor" href="#ft-design"></a> FT design</h2>
<h3 id="primary-backup-structure"><a class="markdownIt-Anchor" href="#primary-backup-structure"></a> Primary-backup structure</h3>
<h4 id="what-is-the-usual-way-to-implement-fault-tolerance-via-primarybackup-approach"><a class="markdownIt-Anchor" href="#what-is-the-usual-way-to-implement-fault-tolerance-via-primarybackup-approach"></a> What is the usual way to implement fault-tolerance via primary/backup approach?</h4>
<ol>
<li>
<p>The backup server is always available to take over is the primary server fails.</p>
<ul>
<li>The problem is that the state of the backup server must be kept nearly identical to the primary server at all times. We say that the two VMs are in virtual lock-step.</li>
</ul>
</li>
<li>
<p>One way is to ship changes to all state of the primary. The bandwidth needed to send can be very large.</p>
</li>
<li>
<p>Another method is the state-machine approach.</p>
<ul>
<li>
<p>The idea is to model the servers as deterministic state machcines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.</p>
</li>
<li>
<p>Some operations are not deterministic. Extra coordination must be used to ensure that they receive a primary and backup are kept in sync.</p>
</li>
<li>
<p>The extra information is far less than the amount of state (mainly memory updates) that is changing in the primary.</p>
</li>
</ul>
</li>
</ol>
<h4 id="what-is-the-difference-between-physical-servers-and-vm-in-state-machine-level"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-physical-servers-and-vm-in-state-machine-level"></a> What is the difference between physical servers and VM in state machine level?</h4>
<ol>
<li>
<p>Implementing coordication to ensure deterministic execution of physical servers is difficult, particularly as processor frequencies increase.</p>
</li>
<li>
<p>VM running on top of a hypervisor can be considered a well-defined state machine.</p>
</li>
<li>
<p>VMs still have non-deterministic operations. Hypervisor is able to capture all the necessary information about non-deterministic operations on the primary VM and to replay these operations correctly on the backup VM.</p>
</li>
</ol>
<h4 id="what-is-the-basic-structure-of-ft-vms"><a class="markdownIt-Anchor" href="#what-is-the-basic-structure-of-ft-vms"></a> What is the basic structure of FT VMs?</h4>
<ol>
<li>
<p>The virtual disks for the VMs are on shared storage, and accessible to the primary and backup VM for input and output.</p>
</li>
<li>
<p>Only the primary VM advertises its presence on the network, so all network inputs come to the primary VM. So does all other inputs.</p>
</li>
<li>
<p>All inputs, including incoming network packets, disk reads, keyboard and mouse, only come to the primary VM. And  the primary VM sends all inputs it received to the backup VM via a network connection known as the logging channel.</p>
<img src="/imgs/Distributed/FTVM/01.png" style="zoom:33%;" />
</li>
</ol>
<h3 id="ft-protocol"><a class="markdownIt-Anchor" href="#ft-protocol"></a> FT protocol</h3>
<h4 id="how-does-vmware-backup-vm-replay"><a class="markdownIt-Anchor" href="#how-does-vmware-backup-vm-replay"></a> How does VMware backup VM replay?</h4>
<ol>
<li>VMware deterministic replay records the inputs of a VM and all possible non-determinism associated with the VM execution in a stream of log entries written to a log file.</li>
<li>For non-deterministic operations, sufficient infomation is logged to allow the operation to be reproduced with the same state change and output.</li>
<li>For non-deterministic events such as timer or IO completion interrupts, the exact instruction at which the event occurred is also recorded. During replay, the event is delivered at the same point in the instruction stream.</li>
<li>VMware deterministic replay has no need to use epochs where non-deterministic events are only delievered at the end. Each interrupt is recorded as it occurs and effciently delivered at the appropriate instruction while being replayed.</li>
<li>Instead of writing the log entries to disk, we send them to the backup VM via the logging channel. The backup VM replays the entries in real time.</li>
</ol>
<h4 id="what-if-the-backup-vm-executes-in-a-way-different-from-the-primary-vm"><a class="markdownIt-Anchor" href="#what-if-the-backup-vm-executes-in-a-way-different-from-the-primary-vm"></a> What if the backup VM executes in a way different from the primary VM?</h4>
<ol>
<li>
<p>The <em>Output Requirement</em>: if the backup VM ever takes over after a failure of the primary, the backup VM will continue executing in a way that is entirely consistent with all outputs that the primary VM has sent to the external world.</p>
</li>
<li>
<p>The Output Requirement can be ensured by</p>
<ul>
<li>
<p>delaying any external output (typically a network packet) until the backup VM has received all information that will allow it to replay execution at least to the point of that output operation.</p>
</li>
<li>
<p>One necessary condition is that the backup VM must have received all log entries generated prior to the output operation.</p>
</li>
</ul>
</li>
<li>
<p>If we create a special log entry at each output operation. Then, the Output Requirement may be enforced by the Output Rule.</p>
<ul>
<li><em>Output Rule</em>: the primary VM may not send an output to the external world, until the backup VM has received and acknowledged the log entry associated with the operation producing the output.</li>
</ul>
</li>
</ol>
<h4 id="will-the-output-rule-affect-vm-eg-stop-its-execution"><a class="markdownIt-Anchor" href="#will-the-output-rule-affect-vm-eg-stop-its-execution"></a> Will the Output Rule affect VM, e.g. stop its execution?</h4>
<ol>
<li>It does not say anything about stopping the execution of the primary VM. We need only delay the sending of the output, but the VM itself can continue execution.</li>
<li>Since operating systems do non-blocking network and disk outputs with asynchronous interrupts to indicate completion, the VM can easily continue execution and will not necessarily be immediately affected by the delay in the output.</li>
</ol>
<h4 id="what-is-the-subtleties-of-executing-disk-reads-on-the-backup-vm"><a class="markdownIt-Anchor" href="#what-is-the-subtleties-of-executing-disk-reads-on-the-backup-vm"></a> What is the subtleties of executing disk reads on the backup VM?</h4>
<ol>
<li>
<p>By default, the primary VM will send the results of the disk read to the backup VM via the logging channel.</p>
</li>
<li>
<p>Executing disk read on the backup VM can greatly reduce the traffic on the logging channel for workloads that do a lot of disk reads. It may also slow down the backup VM’s execution.</p>
</li>
<li>
<p>Some extra work must be done to deal with failed disk read operations.</p>
<ul>
<li>
<p>If the primary succeeds while the backup fails, the backup needs to keep retrying until succeess.</p>
</li>
<li>
<p>If the backup succeeds while the primary fails, the contents of the target memory must be sent to the backup via the logging channel, since the contents of memory will be undetermined and not necessarily replicated by a successful disk read by the backup VM.</p>
</li>
</ul>
</li>
<li>
<p>If the primary VM does a read to a particular disk location followed fairly soon by a write to the same disk location, then the disk write must be delayed until the backup VM has executed the first disk read.</p>
</li>
</ol>
<h3 id="detecting-and-responding-to-failure"><a class="markdownIt-Anchor" href="#detecting-and-responding-to-failure"></a> Detecting and responding to failure</h3>
<h4 id="how-to-handle-duplicate-outputs"><a class="markdownIt-Anchor" href="#how-to-handle-duplicate-outputs"></a> How to handle duplicate outputs?</h4>
<ol>
<li>We cannot guarantee that all outputs are produced exactly once in a failover situation.</li>
<li>The network infrastructure (e.g. TCP) is designed to deal with lost packets and duplicate packets.</li>
</ol>
<h4 id="how-to-handle-backup-vm-failure"><a class="markdownIt-Anchor" href="#how-to-handle-backup-vm-failure"></a> How to handle backup VM failure?</h4>
<p>The primary VM will go live, i.e. leave recording mode, stop sending entries on the logging channel and start executing normally.</p>
<h4 id="how-to-handle-primary-vm-failure"><a class="markdownIt-Anchor" href="#how-to-handle-primary-vm-failure"></a> How to handle primary VM failure?</h4>
<ol>
<li>The backup VM will continue replaying its execution from the log entries until it has consumed the last log entry.</li>
<li>The backup VM will stop replaying mode and start executing as a normal VM. The backup VM has been promoted to the primary VM, and is now missing a backup VM.</li>
</ol>
<h4 id="after-a-failover-how-will-the-new-primary-vm-communicate-with-external-world"><a class="markdownIt-Anchor" href="#after-a-failover-how-will-the-new-primary-vm-communicate-with-external-world"></a> After a failover, how will the new primary VM communicate with external world?</h4>
<ol>
<li>VMware FT automatically advertises the MAC address of the new primary VM on the network, so that physical network switches will know  on what server that new primary VM is located.</li>
<li>The newly promoted primary VM may need to reissue some disk IOs.</li>
</ol>
<h4 id="how-to-detect-failure-of-primary-or-backup-vms"><a class="markdownIt-Anchor" href="#how-to-detect-failure-of-primary-or-backup-vms"></a> How to detect failure of primary or backup VMs?</h4>
<ol>
<li>VMware FT uses UDP heartbeating between servers that are running fault-tolerant VMs to detect when a server may have crashed.</li>
<li>In addition, VMware FT monitors the logging traffic that is sent from the primary to the backup VM and the acknowledgments sent from the backup VM to the primary VM. Because of regular timer interrupts, the logging traffic should be regular and never stop for a functioning guest OS.</li>
</ol>
<h4 id="how-to-avoid-split-brain-problems"><a class="markdownIt-Anchor" href="#how-to-avoid-split-brain-problems"></a> How to avoid split-brain problems?</h4>
<ol>
<li>When either a primary or backup VM wants to go live, it executes an atomic test-and-set operation on the shared virtual disk.</li>
<li>If the operation succeeds, the VM is allowed to go live.</li>
<li>If the operation fails, then the other VM must have already gone live, so the current VM actually halts itself (“commits suicide”).</li>
</ol>
<h3 id="alternative-non-shared-disk"><a class="markdownIt-Anchor" href="#alternative-non-shared-disk"></a> Alternative: Non-shared disk</h3>
<h4 id="what-is-the-difference-between-non-shared-disk-and-shared-disk"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-non-shared-disk-and-shared-disk"></a> What is the difference between non-shared disk and shared disk?</h4>
<ol>
<li>In shared disk, any write to the shared disk is considered a communication to external world. Writes to the shared disk must be delayed.</li>
<li>In non-shared disk, the virtual disks are essentially considered part of the internal state of each VM. Disk writes of the primary do not have to be delayed according to the Output Rule.</li>
</ol>
<h4 id="in-what-case-non-shared-disk-will-be-useful"><a class="markdownIt-Anchor" href="#in-what-case-non-shared-disk-will-be-useful"></a> In what case non-shared disk will be useful?</h4>
<ol>
<li>Shared storage is not accessible to the primary and backup VMs.</li>
<li>This may be the case because shared storage is unavailable or too expensive, or because the servers running the primary and backup VMs are far apart.</li>
</ol>
<h4 id="what-is-the-disadvantage-of-non-shared-disk"><a class="markdownIt-Anchor" href="#what-is-the-disadvantage-of-non-shared-disk"></a> What is the disadvantage of non-shared disk?</h4>
<ol>
<li>The two copies of the virtual disks must be explicitly synced up in some manner when fault tolerance is first enabled.</li>
<li>The disks can get out of sync after a failure, so they must be explicitly resynced when the backup VM is restarted after a failure.</li>
</ol>
<h4 id="how-to-solve-the-split-brain-situation"><a class="markdownIt-Anchor" href="#how-to-solve-the-split-brain-situation"></a> How to solve the split-brain situation?</h4>
<ol>
<li>
<p>There may be no shared storage to use for dealing with it. The system could use some other external tiebreaker.</p>
<ul>
<li>A third-party server that both servers can talk to.</li>
</ul>
</li>
<li>
<p>If the servers are part of a cluster with more than two nodes, the system could alternatively use a majority algorithm.</p>
<ul>
<li>A VM would only be allowed to go live if it is running on a server that is part of a communication sub-cluster that contains a majority of the original nodes.</li>
</ul>
</li>
</ol>
<h2 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h2>
<h3 id="starting-and-restarting"><a class="markdownIt-Anchor" href="#starting-and-restarting"></a> Starting and restarting</h3>
<h4 id="what-requirements-need-to-be-satisfied-by-the-startup-mechanism"><a class="markdownIt-Anchor" href="#what-requirements-need-to-be-satisfied-by-the-startup-mechanism"></a> What requirements need to be satisfied by the startup mechanism?</h4>
<ol>
<li>We also want to use it to restart a backup VM after a failure. Hence, this mechanism must be usable for a running primary VM that is in an arbitrary state.</li>
<li>We would prefer that the mechanism does not significantly disrupt that execution of the primary VM.</li>
</ol>
<h4 id="how-to-implement-the-startup-mechanism"><a class="markdownIt-Anchor" href="#how-to-implement-the-startup-mechanism"></a> How to implement the startup mechanism?</h4>
<ol>
<li>VMware FT adapted a modified VMware VMotion that allows the migration of a running VM from one server to another server with minimal disruption. However, after migration, the VMotion will destroy the local VM.</li>
<li>The FT VMotion clones a VM to a remote host rather than migrating it without destroying the local VM.</li>
<li>The FT VMotion also sets up a logging channel, and causes the source VM to enter logging mode as the primary, and the destination VM to enter replay mode as the new backup.</li>
</ol>
<h4 id="how-to-choose-a-server-on-which-to-run-the-backup-vm"><a class="markdownIt-Anchor" href="#how-to-choose-a-server-on-which-to-run-the-backup-vm"></a> How to choose a server on which to run the backup VM?</h4>
<ol>
<li>The primary Vm informs the clustering service that it needs a new backup.</li>
<li>The clustering service determines the best server on which to run the backup VM based on resource usage and other constraints and invokes an FT VMotion to create the new backup VM.</li>
<li>VMware FT typically can re-establish VM redundancy within minutes of a server failure, all without any noticeable interruption in the execution of a fault-tolerant VM.</li>
</ol>
<h3 id="logging-channel"><a class="markdownIt-Anchor" href="#logging-channel"></a> Logging channel</h3>
<h4 id="how-to-control-primary-sending-log-entries-and-backup-receiving-entries"><a class="markdownIt-Anchor" href="#how-to-control-primary-sending-log-entries-and-backup-receiving-entries"></a> How to control primary sending log entries, and backup receiving entries?</h4>
<ol>
<li>The hypervisors maintain a large buffer for logging entries for the primary and backup VMs.</li>
<li>The contents of the primary’s log buffer are flushed out to the logging channel as soon as possible, and log entries are read into the backup’s log buffer from the logging channel as soon as they arrive.</li>
<li>The backup sends acknowledgments back to the primary each time that it reads some log entries from the network into its log buffer.</li>
</ol>
<h4 id="what-if-the-log-buffer-of-the-primary-is-full"><a class="markdownIt-Anchor" href="#what-if-the-log-buffer-of-the-primary-is-full"></a> What if the log buffer of the primary is full?</h4>
<ol>
<li>It must stop execution until log entries can be flushed out.</li>
<li>This stop in execution is a natural flow-control mechanism that slows down the primary VM when it is producing log entries at too fast a rate.</li>
<li>This pause can affect clients of the VM, and we must minimize the possibility that the primary log buffer fills up.</li>
</ol>
<h4 id="what-is-the-main-cause-of-the-buffer-of-primary-being-full"><a class="markdownIt-Anchor" href="#what-is-the-main-cause-of-the-buffer-of-primary-being-full"></a> What is the main cause of the buffer of primary being full?</h4>
<ol>
<li>One biggest reason is that the backup VM is executing too slowly and therefore sonsuming log entries too slowly.</li>
<li>In general, the backup VM must be able to replay an execution at roughly the same speed as sthe primary VM is recording the execution.</li>
<li>The overhead of recording and replaying in VMware deterministic replay is roughly the same.</li>
<li>If the server hosting the backup VM is heavily loaded with other VMs (and hence overcommitted on resources), the backup VM may not be able to get enough CPU and memory resources to execute as fast as the primary VM.</li>
</ol>
<h4 id="how-to-prevent-the-backup-vm-from-getting-too-far-behind-the-primary"><a class="markdownIt-Anchor" href="#how-to-prevent-the-backup-vm-from-getting-too-far-behind-the-primary"></a> How to prevent the backup VM from getting too far behind the primary?</h4>
<ol>
<li>When sending acknowledgments, we also send additional information to determine the real-time execution lag between the primary and backup VMs.</li>
<li>Typically the execution lag is less than 100 milliseconds.</li>
<li>If the backup VM starts having a significant execution lag (e.g. more than 1 second), VMware FT starts slowing down the primary VM by informing the scheduler to give it a slightly smally amount of CPU.</li>
<li>Such slowdowns are very rare, and typically happen only when the system is under extreme stress.</li>
</ol>
<h3 id="special-operations"><a class="markdownIt-Anchor" href="#special-operations"></a> Special operations</h3>
<h4 id="how-to-deal-with-control-operations"><a class="markdownIt-Anchor" href="#how-to-deal-with-control-operations"></a> How to deal with control operations?</h4>
<ol>
<li>
<p>Most control operations shouls be applied to both machines.</p>
<ul>
<li>
<p>If the primary VM is explicitly powered off, the backup VM should be stopped as well, and not attempt to go live.</p>
</li>
<li>
<p>Any resource management change on the primary should be applied to the backup.</p>
</li>
</ul>
</li>
<li>
<p>The only operation that can be done independently on the primary and backup VMs is VMotion.</p>
<ul>
<li>
<p>The primary and backup VMs can be VMotioned independently to other hosts.</p>
</li>
<li>
<p>VMware FT ensures that neither VM is moved to the server where the other VM is.</p>
</li>
</ul>
</li>
</ol>
<h4 id="how-to-implement-the-vmotion-for-primary-and-backup-vms"><a class="markdownIt-Anchor" href="#how-to-implement-the-vmotion-for-primary-and-backup-vms"></a> How to implement the VMotion for primary and backup VMs?</h4>
<ol>
<li>
<p>For a normal VMotion, it requires that all outstanding disk IOs be quiesced just as the final switchover on the VMotion occurs.</p>
</li>
<li>
<p>For a primary VM,</p>
<ul>
<li>
<p>The quiescing is easily handled by waiting until the physical IOs completeand delivering these completions to the VM.</p>
</li>
<li>
<p>The backup VM must disconnect from the source primary and re-connect to the destination primary at the appropriate time.</p>
</li>
</ul>
</li>
<li>
<p>For a backup VM,</p>
<ul>
<li>
<p>There is no easy way to cause all IOs to be completed at any required point, since the backup VM must replay the primary VM’s execution and complete IOs at the same execution point.</p>
</li>
<li>
<p>When a backup VM is at the final switchover point for a VMotion, it requests via the logging channel that the primary VM temporarily quiesce all its IOs.</p>
</li>
</ul>
</li>
</ol>
<h3 id="issues-for-disk-ios"><a class="markdownIt-Anchor" href="#issues-for-disk-ios"></a> Issues for disk IOs</h3>
<h4 id="how-many-kind-of-races-may-occur"><a class="markdownIt-Anchor" href="#how-many-kind-of-races-may-occur"></a> How many kind of races may occur?</h4>
<ol>
<li>
<p>The first kind is caused by several IO operations.</p>
<ul>
<li>
<p>One reason is that disk operations are non-blocking and can execute in parallel. Simultaneous disk operations access the same disk location causing races.</p>
</li>
<li>
<p>The other reason is that DMA directly to/from the memory of the VM. Simultaneous disk operations access the same memory pages.</p>
</li>
</ul>
</li>
<li>
<p>The second kind is caused by IO operations and non-IO operations.</p>
<ul>
<li>The disk operations directly access the memory of a VM via DMA. Hence a disk operation accesses the same memory pages as an aplication or OS in a VM causing races.</li>
</ul>
</li>
</ol>
<h4 id="how-to-solve-the-non-determinism-caused-by-several-io-operations"><a class="markdownIt-Anchor" href="#how-to-solve-the-non-determinism-caused-by-several-io-operations"></a> How to solve the non-determinism caused by several IO operations?</h4>
<p>We should detect any such IO races, and force such racing disk operations to execute sequentially in the same way on the primary and backup.</p>
<h4 id="how-to-solve-the-non-determinism-caused-by-io-operations-and-applicationos"><a class="markdownIt-Anchor" href="#how-to-solve-the-non-determinism-caused-by-io-operations-and-applicationos"></a> How to solve the non-determinism caused by IO operations and application/OS?</h4>
<ol>
<li>
<p>We need to set up page protection termporarily on pages that are targets of disk operations.</p>
</li>
<li>
<p>The page protections result in a trap if the VM happens to make an access to a page that is also the target of an outstanding disk operation, and the VM can be paused until the disk operation completes.</p>
</li>
<li>
<p>Changing MMU protections on pages is expensive, we use bounce buffers.</p>
<ul>
<li>
<p>A bounce buffer is a temporary buffer that has the same size as the memory being accessed by a disk operation.</p>
</li>
<li>
<p>A disk read operation is modified to read the specified data to the bounce buffer, and the data is copied to guest memory only as the IO completion is delivered.</p>
</li>
<li>
<p>For a disk write operation, the data to besent is first copied to the bounce buffer, and the disk write is modified to write data from the bounce buffer.</p>
</li>
</ul>
</li>
<li>
<p>The bounce buffer can slow down disk operations, but noticeable performance loss is not seen.</p>
</li>
</ol>
<h4 id="how-the-newly-promoted-primary-vm-handle-those-outstanding-ios"><a class="markdownIt-Anchor" href="#how-the-newly-promoted-primary-vm-handle-those-outstanding-ios"></a> How the newly-promoted primary VM handle those outstanding IOs?</h4>
<ol>
<li>There is no way for the newly-promoted primary VM to be sure if the disk IOs were issued to the disk or completed  successfully.</li>
<li>We could send an error completion that indicates that each IO failed, since it is acceptable to return an error even if the IO completed successfully. However, the guest OS might not respond well to errors from its local disk.</li>
<li>We can re-issue the pending IOs during the go-live process of the backup VM. Because we have eliminated all races and all IOs specify derectly which memory and disk blocks are accessed, these disk operations can be re-issued even if they have already completed successfully.</li>
</ol>
<h3 id="issues-for-network-io"><a class="markdownIt-Anchor" href="#issues-for-network-io"></a> Issues for network IO</h3>
<h4 id="how-to-solve-the-non-determinism-caused-by-asynchronous-updates"><a class="markdownIt-Anchor" href="#how-to-solve-the-non-determinism-caused-by-asynchronous-updates"></a> How to solve the non-determinism caused by asynchronous updates?</h4>
<ol>
<li>
<p>In normal VM, the hypervisor asynchronously updating the state of the virtual machine’s network device.</p>
</li>
<li>
<p>For FT</p>
<ul>
<li>
<p>The code that asynchronously updates VM ring buffers with incoming packets has been modified to force the guest to trap to the hypervisor, where it can log the updates and then apply them to the VM.</p>
</li>
<li>
<p>The code that normally pull packets out of transmit queues asynchronously is diabled, and instead transmits are done through a trap to the hypervisor.</p>
</li>
</ul>
</li>
</ol>
<h4 id="how-can-we-optimize-the-network-performance-while-running-ft"><a class="markdownIt-Anchor" href="#how-can-we-optimize-the-network-performance-while-running-ft"></a> How can we optimize the network performance while running FT?</h4>
<ol>
<li>
<p>Reduce VM trapand interrupts with clustering optimizations.</p>
<ul>
<li>
<p>When the VM is streaming data at a sufficient bit rate, the hypervisor can do one transmit trap per group of packets and, in the best case, zero traps, since it can transmit the packets as part of receiving new packets.</p>
</li>
<li>
<p>The hypervisor can reduce the number of interrupts to the VM for incoming packets by only posting the interrupt for a group of packets.</p>
</li>
</ul>
</li>
<li>
<p>Reduce the delay for transmitted packets.</p>
<ul>
<li>
<p>The key is to reduce the time required to send a log message to the backup and get an acknowledgment.</p>
</li>
<li>
<p>It is ensured that sending and receiving log entries and acknowledgments can all be done without any thread context switch.</p>
</li>
<li>
<p>The VMware vSphere hypervisor allows functions to be registered with the TCP stack that will be called from a deferred-execution context (similar to a tasklet in Linux) whenever TCP data is received.</p>
</li>
<li>
<p>When the primary VM enqueues a packet to be transmitted, we force an immediate log flush of the associated output log entry by scheduling a defferred-execution context to do the flush.</p>
</li>
</ul>
</li>
</ol>
<h1 id="experiments-and-results"><a class="markdownIt-Anchor" href="#experiments-and-results"></a> Experiments and results</h1>
<p>One important banchmark is the performance ratio between non-FT and FT systems and logging bandwidth between primary and backup. The performance ratio can show how efficient the FT protocol is, and logging bandwidth is usually a bottleneck of the system. The author measured them all as the following table. We can see that FT protocol only decreases the performance by less than 10%</p>
<img src="/imgs/Distributed/FTVM/02.png" style="zoom:30%;" />
<p>The typical idle logging bandwidth is 0.5-1.5 Mbits/sec. The idle bandwidth is largely the result of recording the delivery of timer interrupts. For a VM with an active workload, the logging bandwidth is dominated by the network and disk inputs that must be sent to the backup - the network packets that are received and the disk blocks that are read from disk. Hence the logging bandwidth can be much higher than those measured in table for applications that have very high network receive or disk read bandwidth. For these kinds of applications, the bandwidth of the logging channel could be a bottleneck.</p>
<p>The author also measured the bandwidth of logging channels with different capacities, as shown below. When FT is enabled for receive workloads, the loging bandwidth is very large, since all incomming network packets must be sent on the logging channel. When FT is enabled for transmit workloads, the logging bandwidth is much lower. Overall, FT can limit network bandwidths significantly at very high transmit and receive rates, but high absolute rates are still achievable.</p>
<img src="/imgs/Distributed/FTVM/03.png" style="zoom:30%;" />

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/GFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/GFS/" class="post-title-link" itemprop="url">GFS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:06:36" itemprop="dateCreated datePublished" datetime="2023-09-26T13:06:36+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:27:02" itemprop="dateModified" datetime="2023-10-04T16:27:02+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/gfs.pdf">The Google File System</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#overview">Overview</a>
<ul>
<li><a href="#what-operations-are-supported">What operations are supported?</a></li>
<li><a href="#architecture">Architecture</a>
<ul>
<li><a href="#what-does-the-system-consist-of">What does the system consist of?</a></li>
<li><a href="#what-does-master-need-to-do">What does master need to do?</a></li>
<li><a href="#how-to-prevent-the-master-becoming-a-bottleneck">How to prevent the master becoming a bottleneck?</a></li>
<li><a href="#what-is-the-advantage-of-large-chunk-size">What is the advantage of large chunk size?</a></li>
<li><a href="#what-is-the-disadvantage-of-large-chunk-size">What is the disadvantage of large chunk size?</a></li>
<li><a href="#when-will-hot-spots-problem-emerge">When will hot spots problem emerge?</a></li>
</ul>
</li>
<li><a href="#consistency">Consistency</a>
<ul>
<li><a href="#how-do-we-define-a-file-region-being-consistent-or-defined">How do we define a file region being consistent or defined?</a></li>
<li><a href="#how-many-consistency-rules-should-be-considered">How many consistency rules should be considered?</a></li>
<li><a href="#how-do-gfs-guarantees-the-second-rule">How do GFS guarantees the second rule?</a></li>
<li><a href="#what-is-the-side-effect-of-clients-caching-chunk-locations">What is the side-effect of clients caching chunk locations?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#data-mutation">Data mutation</a>
<ul>
<li><a href="#control-data-flow">Control &amp; data flow</a>
<ul>
<li><a href="#how-do-we-minimize-management-overhead-at-the-master-of-data-mutation">How do we minimize management overhead at the master of data mutation?</a></li>
<li><a href="#what-does-leases-change">What does leases change?</a></li>
<li><a href="#how-does-the-control-flow">How does the control flow?</a></li>
<li><a href="#how-does-primary-and-secondary-servers-write-data">How does primary and secondary servers write data?</a></li>
<li><a href="#what-would-the-system-do-if-write-fails">What would the system do if write fails?</a></li>
<li><a href="#what-if-a-write-is-large-or-straddles-a-chunk-boundary">What if a write is large or straddles a chunk boundary?</a></li>
<li><a href="#how-to-prevent-primary-become-bottleneck-of-pushing-data">How to prevent primary become bottleneck of pushing data?</a></li>
<li><a href="#how-to-minimize-latency-of-pushing-data">How to minimize latency of pushing data?</a></li>
</ul>
</li>
<li><a href="#write-and-record-append">Write and record append</a>
<ul>
<li><a href="#what-is-the-difference-between-write-and-record-append">What is the difference between write and record append?</a></li>
<li><a href="#how-does-typical-writing-happen">How does typical writing happen?</a></li>
<li><a href="#how-does-readers-deal-with-occasional-padding-and-duplicates">How does readers deal with occasional padding and duplicates?</a></li>
</ul>
</li>
<li><a href="#atomic-record-appends">Atomic record appends</a>
<ul>
<li><a href="#what-if-appending-causes-the-current-chunk-to-exceed-the-maximum-size">What if appending causes the current chunk to exceed the maximum size?</a></li>
<li><a href="#what-if-appending-fails-at-some-chunkservers">What if appending fails at some chunkservers?</a></li>
</ul>
</li>
<li><a href="#snapshot">Snapshot</a>
<ul>
<li><a href="#what-does-snapshot-do">What does snapshot do?</a></li>
<li><a href="#how-does-snapshot-be-implemented">How does snapshot be implemented?</a></li>
<li><a href="#how-does-clients-write-a-chunk-after-snapshot">How does clients write a chunk after snapshot?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#master">Master</a>
<ul>
<li><a href="#basic-operations">Basic operations</a>
<ul>
<li><a href="#how-does-client-communicate-with-master-specifically">How does client communicate with master specifically?</a></li>
<li><a href="#what-metadata-does-master-need-to-store">What metadata does master need to store?</a></li>
<li><a href="#how-to-persist">How to persist?</a></li>
<li><a href="#how-does-gfs-manage-namespace">How does GFS manage namespace?</a></li>
<li><a href="#how-does-gfs-design-locking-scheme">How does GFS design locking scheme?</a></li>
</ul>
</li>
<li><a href="#replica-management">Replica management</a>
<ul>
<li><a href="#how-to-place-replicas">How to place replicas?</a></li>
<li><a href="#what-factors-are-considered-when-create-a-new-chunk">What factors are considered when create a new chunk?</a></li>
<li><a href="#what-if-the-number-of-available-replicas-of-a-chunk-falls-below-a-user-specified-goal">What if the number of available replicas of a chunk falls below a user-specified goal?</a></li>
<li><a href="#what-if-cloning-traffic-from-overwhelming-client-traffic">What if cloning traffic from overwhelming client traffic?</a></li>
<li><a href="#how-to-keep-the-placement-of-replicas-in-balance">How to keep the placement of replicas in balance?</a></li>
</ul>
</li>
<li><a href="#deletion">Deletion</a>
<ul>
<li><a href="#how-to-delete-a-file">How to delete a file?</a></li>
<li><a href="#what-are-the-advantages-and-disadvantages-of-lazy-deletion-over-eager-deletion">What are the advantages and disadvantages of lazy deletion over eager deletion?</a></li>
<li><a href="#how-to-address-the-issues-of-reusing">How to address the issues of reusing?</a></li>
<li><a href="#how-to-handle-the-possible-stale-replicas">How to handle the possible stale replicas?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#fault-tolerance">Fault tolerance</a>
<ul>
<li><a href="#how-to-handle-master-failure">How to handle master failure?</a></li>
<li><a href="#why-cannot-recover-data-using-other-chunk-replicas-why-each-chunkserver-must-independently-verify-the-integrity">Why cannot recover data using other chunk replicas? Why each chunkserver must independently verify the integrity?</a></li>
<li><a href="#how-to-ensure-data-integrity">How to ensure data integrity?</a></li>
<li><a href="#how-to-read-data-with-checksum">How to read data with checksum?</a></li>
<li><a href="#how-to-write-data-with-checksum">How to write data with checksum?</a></li>
<li><a href="#what-is-included-in-the-diagnostic-logs">What is included in the diagnostic logs?</a></li>
</ul>
</li>
<li><a href="#other-parts-unmentioned">Other parts (unmentioned)</a>
<ul>
<li><a href="#to-sum-up-what-is-the-metadata-of-master-and-where-are-they">To sum up, what is the metadata of master, and where are they?</a></li>
<li><a href="#what-is-the-cause-of-split-brain-how-to-solve-it">What is the cause of split brain? How to solve it?</a></li>
<li><a href="#why-gfs-doesnt-overwrite-those-failed-records-immediately-but-leaving-padding-and-duplicates">Why GFS doesn’t overwrite those failed records immediately, but leaving padding and duplicates?</a></li>
<li><a href="#gfs-is-a-weak-consistency-system-how-can-we-upgrade-it-to-a-strong-consistency-system">GFS is a weak consistency system, how can we upgrade it to a strong consistency system?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiments-and-results">Experiments and results</a>
<ul>
<li><a href="#micro-benchmarks">Micro-benchmarks</a>
<ul>
<li><a href="#read">Read</a></li>
<li><a href="#write">Write</a></li>
<li><a href="#record-append">Record append</a></li>
</ul>
</li>
<li><a href="#real-world-clusters">Real world clusters</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Contribution: provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.</li>
<li>Difference points in design space
<ul>
<li>This system integreted constant monitoring, error detection, fault tolerance, and automatic recovery.</li>
<li>Files are huge by traditional standards. Design assumptions and parameters such as I/O operation and block sizes have to be revisited.</li>
<li>Most files are mutated by appending new data rather than overwriting existing data. Random writes within a file are practically non-existent. Once written, the files are only read, and often only seuqentially.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> Overview</h2>
<h3 id="what-operations-are-supported"><a class="markdownIt-Anchor" href="#what-operations-are-supported"></a> What operations are supported?</h3>
<ol>
<li>Usual operations: <code>create</code>, <code>delete</code>, <code>open</code>, <code>close</code>, <code>read</code>, and <code>write</code></li>
<li><code>snapshot</code>: creates a copy of a file or a directory tree at low cost</li>
<li><code>record append</code>: allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity</li>
</ol>
<h3 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h3>
<img src="/imgs/Distributed/GFS/01.png" style="zoom:33%;" />
<h4 id="what-does-the-system-consist-of"><a class="markdownIt-Anchor" href="#what-does-the-system-consist-of"></a> What does the system consist of?</h4>
<ol>
<li>Consist of a signle <em>master</em> and multiple <em>chunkservers</em> and is accessed by multiple <em>clients</em>.</li>
<li>Files are divided into fixed-size chunks. Each chunk is identified by an immutable and globally unique 64 bit <em>chunk handle</em> assigned by the master at the timeof chunk creation.</li>
<li>Chunkservers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range. Each chunk is replicated on multiple chunkservers, three by default.</li>
<li>Neither the client nor the chunkserver caches file data. Caches offer little benefit while causing coherence issues. But clients do cache metadata.</li>
</ol>
<h4 id="what-does-master-need-to-do"><a class="markdownIt-Anchor" href="#what-does-master-need-to-do"></a> What does master need to do?</h4>
<ol>
<li>
<p>The master maintains all file system metadata, and controls system-wide activities.</p>
<ul>
<li>
<p>Metadata includes namespace, access control information, the mapping from files to chunks, and the current locations of chunks.</p>
</li>
<li>
<p>System-wide activities includes chunk lease management, garbage collection of orphaned chunks, and chunk migration between chunkservers.</p>
</li>
</ul>
</li>
<li>
<p>The master periodically comminicates with each chunkserver in HeartBeat messages to give it instructions and collect its state.</p>
</li>
<li>
<p>Clients interact with the master for metadata operations, but all data-bearing communication goes directly to the chunkservers.</p>
</li>
</ol>
<h4 id="how-to-prevent-the-master-becoming-a-bottleneck"><a class="markdownIt-Anchor" href="#how-to-prevent-the-master-becoming-a-bottleneck"></a> How to prevent the master becoming a bottleneck?</h4>
<ol>
<li>
<p>The idea is to minimize its involvement in reads and writes.</p>
</li>
<li>
<p>A client asks the master which chunkservers it should contact, and caches this information for a limited time and interacts with the chunkservers directly for subsequent operations.</p>
</li>
</ol>
<h4 id="what-is-the-advantage-of-large-chunk-size"><a class="markdownIt-Anchor" href="#what-is-the-advantage-of-large-chunk-size"></a> What is the advantage of large chunk size?</h4>
<ol>
<li>
<p>Reduce clients’ need to interact with the master because reads and writes on the same chunk require only one initial request to the master for chunk location information.</p>
</li>
<li>
<p>Reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time.</p>
</li>
<li>
<p>Reduce the size of the metadata stored on the master.</p>
</li>
</ol>
<h4 id="what-is-the-disadvantage-of-large-chunk-size"><a class="markdownIt-Anchor" href="#what-is-the-disadvantage-of-large-chunk-size"></a> What is the disadvantage of large chunk size?</h4>
<ol>
<li>
<p>Wasting space due to internal fragmentation. This can be eased through lazy space allocation.</p>
</li>
<li>
<p>A small file consists of a small number of chunks, perhaps just one. The chunkservers storing those chunks may become hot spots if many clients are accessing the same file. This have not been a major issue.</p>
</li>
</ol>
<h4 id="when-will-hot-spots-problem-emerge"><a class="markdownIt-Anchor" href="#when-will-hot-spots-problem-emerge"></a> When will hot spots problem emerge?</h4>
<ol>
<li>
<p>A more common case is that an executable was written to GFS as a single-chunk file and then started on hundreds of machines at the same time.</p>
</li>
<li>
<p>We can fix this problem by storing such executables with a higher replication factor.</p>
</li>
</ol>
<h3 id="consistency"><a class="markdownIt-Anchor" href="#consistency"></a> Consistency</h3>
<h4 id="how-do-we-define-a-file-region-being-consistent-or-defined"><a class="markdownIt-Anchor" href="#how-do-we-define-a-file-region-being-consistent-or-defined"></a> How do we define a file region being consistent or defined?</h4>
<ol>
<li>
<p>A file region is consistent if all clients will always see the same data, regardless of which replicas they read from.</p>
</li>
<li>
<p>A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety.</p>
</li>
<li>
<p>Concurrent successful mutations leave the region undefined but consistent: all clients see the same data, but it may not reflect what any one mutation has written. Typically, it consists of mingled fragments from multiple mutations.</p>
</li>
</ol>
<h4 id="how-many-consistency-rules-should-be-considered"><a class="markdownIt-Anchor" href="#how-many-consistency-rules-should-be-considered"></a> How many consistency rules should be considered?</h4>
<ol>
<li>File namespace mutations (e.g. file creation) are atomic.</li>
<li>After a sequence of successful mutations, the mutated file region is guaranteed to be defined and contain the data written by the last mutation.</li>
</ol>
<h4 id="how-do-gfs-guarantees-the-second-rule"><a class="markdownIt-Anchor" href="#how-do-gfs-guarantees-the-second-rule"></a> How do GFS guarantees the second rule?</h4>
<ol>
<li>Applying mutations to a chunk in the same order on all its replicas.</li>
<li>Using chunk version numbers to detect any replica that has become stale because it has missed mutations while its chunkserver was down.</li>
<li>Stale replicas will never be involved in a mutation or given to clients asking the master for chunk locations. They are garbage collected at the earliest opportunity.</li>
</ol>
<h4 id="what-is-the-side-effect-of-clients-caching-chunk-locations"><a class="markdownIt-Anchor" href="#what-is-the-side-effect-of-clients-caching-chunk-locations"></a> What is the side-effect of clients caching chunk locations?</h4>
<ol>
<li>They may read from a stale replica before that information is refreshed.</li>
<li>This window is limited by the cache entry’s timeout and dthe next open of the file.</li>
<li>As most of files are append-only, a stale replica usually returns a premature end of chunk rather than outdated data.</li>
</ol>
<h2 id="data-mutation"><a class="markdownIt-Anchor" href="#data-mutation"></a> Data mutation</h2>
<h3 id="control-data-flow"><a class="markdownIt-Anchor" href="#control-data-flow"></a> Control &amp; data flow</h3>
<h4 id="how-do-we-minimize-management-overhead-at-the-master-of-data-mutation"><a class="markdownIt-Anchor" href="#how-do-we-minimize-management-overhead-at-the-master-of-data-mutation"></a> How do we minimize management overhead at the master of data mutation?</h4>
<ol>
<li>We use leases to maintain a consistent mutation order across replicas.</li>
<li>The master grants a chunk lease to one of the replicas, which we call the primary. The primary picks a serial order for all mutations to the chunk. All replicas follow this order when applying mutations.</li>
<li>The client caches who is the lease of a certain chunk for future mutations. It needs to contact the master again only when the primary becomes unreachable or replies that it no longer holds a lease.</li>
</ol>
<h4 id="what-does-leases-change"><a class="markdownIt-Anchor" href="#what-does-leases-change"></a> What does leases change?</h4>
<ol>
<li>A lease has an initial timeout of 60 seconds. However, as long as the chunk is being mutated, the primary can request and typically receive extensions from the master indefinitely.</li>
<li>These extension requests and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunkservers.</li>
<li>The master may sometimes try to revoke a lease before it expires (e.g., when the master wants to disable mutations on a file that is being renamed).</li>
<li>Even if the master loses communication with a primary, it can safely grant a new lease to another replica after the old lease expires.</li>
</ol>
<h4 id="how-does-the-control-flow"><a class="markdownIt-Anchor" href="#how-does-the-control-flow"></a> How does the control flow?</h4>
<ol>
<li>
<p>the client asks the master which chunkserver holds the current lease for the chunk and the locations of the other replicas. If no one has a lease, the master grants one to a replica it chooses</p>
</li>
<li>
<p>the master replies with the identity of the primary and the locations of the other (secondary) replicas.</p>
</li>
<li>
<p>The client pushes the data to all the replicas in any order, instead of only sending to the lease.</p>
</li>
<li>
<p>once all the replicas have acknowledged receiving the data, the client sends a write request to the primary.</p>
</li>
<li>
<p>the primary forwards the write request to all secondary replicas.</p>
</li>
<li>
<p>the secondaries all reply to the primary indicating that they have completed the operation.</p>
</li>
<li>
<p>the primary replies to the client. Any errors encountered at any of the replicas are reported to the client.</p>
<img src="/imgs/Distributed/GFS/02.png" style="zoom:25%;" />
</li>
</ol>
<h4 id="how-does-primary-and-secondary-servers-write-data"><a class="markdownIt-Anchor" href="#how-does-primary-and-secondary-servers-write-data"></a> How does primary and secondary servers write data?</h4>
<ol>
<li>
<p>Each chunkserver will store the data from client in an internal LRU buffer cache until the data is used or aged out.</p>
</li>
<li>
<p>The write request from client to primary identifies the data pushed earlier to all of the replicas.</p>
</li>
<li>
<p>The primary assigns consecutive serial numbers to all the mutations it receives, possibly from multiple clients, which provides the necessary serialization.</p>
</li>
<li>
<p>The primary applies the mutation to its own local state in serial number order.</p>
</li>
</ol>
<h4 id="what-would-the-system-do-if-write-fails"><a class="markdownIt-Anchor" href="#what-would-the-system-do-if-write-fails"></a> What would the system do if write fails?</h4>
<ol>
<li>
<p>If it had failed at the primary, it would not have been assigned a serial number and forwarded.</p>
</li>
<li>
<p>In other cases, the write may have succeeded at the primary and an arbitrary subset of the secondary replicas. The client request is considered to have failed, and the modified region is left in an inconsistent state.</p>
</li>
<li>
<p>The client code handles such errors by retrying the failed mutation. It will make a few attempts at steps 3 through 7 before falling back to a retry from the beginning of the write.</p>
</li>
</ol>
<h4 id="what-if-a-write-is-large-or-straddles-a-chunk-boundary"><a class="markdownIt-Anchor" href="#what-if-a-write-is-large-or-straddles-a-chunk-boundary"></a> What if a write is large or straddles a chunk boundary?</h4>
<ol>
<li>GFS client code breaks it down into multiple write operations.</li>
<li>They all follow the control flow described above but may be interleaved with and overwritten by concurrent operations from other clients.</li>
<li>The shared file region may end up containing fragments from different clients, although the replicas will be identical because the individual operations are completed successfully in the same order on all replicas.</li>
</ol>
<h4 id="how-to-prevent-primary-become-bottleneck-of-pushing-data"><a class="markdownIt-Anchor" href="#how-to-prevent-primary-become-bottleneck-of-pushing-data"></a> How to prevent primary become bottleneck of pushing data?</h4>
<ol>
<li>While control flows from the client to the primary and then to all secondaries, data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion.</li>
<li>By decoupling the data flow from the control flow, we can improve performance by scheduling the expensive data flow based on the network topology regardless of which chunkserver is the primary.</li>
<li>Our goals are to fully utilize each machine’s network bandwidth, avoid network bottlenecks and high-latency links, and minimize the latency to push through all the data.</li>
<li>Each machine forwards the data to the “closest” machine in the network topology that has not received it.</li>
<li>Our network topology is simple enough that “distances” can be accurately estimated from IP addresses.</li>
</ol>
<h4 id="how-to-minimize-latency-of-pushing-data"><a class="markdownIt-Anchor" href="#how-to-minimize-latency-of-pushing-data"></a> How to minimize latency of pushing data?</h4>
<ol>
<li>We minimize latency by pipelining the data transfer over TCP connections. Once a chunkserver receives some data, it starts forwarding immediately.</li>
<li>Pipelining is especially helpful to us because we use a switched network with full-duplex links. Sending the data immediately does not reduce the receive rate.</li>
</ol>
<h3 id="write-and-record-append"><a class="markdownIt-Anchor" href="#write-and-record-append"></a> Write and record append</h3>
<h4 id="what-is-the-difference-between-write-and-record-append"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-write-and-record-append"></a> What is the difference between write and record append?</h4>
<ol>
<li>
<p>A write causes data to be written at an application-specified file offset.</p>
</li>
<li>
<p>A record append causes data (the “record”) to be appended atomically at least once even in the presence of concurrent mutations, but at an offset of GFS’s choosing.</p>
<ul>
<li>
<p>The offset is returned to the client and marks the beginning of a defined region that contains the record.</p>
</li>
<li>
<p>GFS may insert padding or record duplicates in between. They occupy regions considered to be inconsistent and are typically dwarfed by the amount of user data.</p>
</li>
</ul>
</li>
<li>
<p>A “regular” append is merely a write at an offset that the client believes to be the current end of file.</p>
</li>
</ol>
<h4 id="how-does-typical-writing-happen"><a class="markdownIt-Anchor" href="#how-does-typical-writing-happen"></a> How does typical writing happen?</h4>
<ol>
<li>A writer generates a file from beginning to end. It atomically renames the file to a permanent name after writing all the data, or periodically checkpoints how much has been successfully written.</li>
<li>Checkpoints may also include application-level checksums. Readers verify and process only the file region up to the last checkpoint, which is known to be in the defined state.</li>
<li>Checkpointing allows writers to restart incrementally and keeps readers from processing successfully written file data that is still incomplete.</li>
</ol>
<h4 id="how-does-readers-deal-with-occasional-padding-and-duplicates"><a class="markdownIt-Anchor" href="#how-does-readers-deal-with-occasional-padding-and-duplicates"></a> How does readers deal with occasional padding and duplicates?</h4>
<ol>
<li>Each record prepared by the writer contains extra information like checksums so that its validity can be verified.</li>
<li>A reader can identify and discard extra padding and record fragments using the checksums.</li>
<li>If it cannot tolerate the occasional duplicates, it can filter them out using unique identifiers in the records, which are often needed anyway to name corresponding application entities such as web documents.</li>
</ol>
<h3 id="atomic-record-appends"><a class="markdownIt-Anchor" href="#atomic-record-appends"></a> Atomic record appends</h3>
<h4 id="what-if-appending-causes-the-current-chunk-to-exceed-the-maximum-size"><a class="markdownIt-Anchor" href="#what-if-appending-causes-the-current-chunk-to-exceed-the-maximum-size"></a> What if appending causes the current chunk to exceed the maximum size?</h4>
<ol>
<li>The primary checks to see if appending the record to the current chunk would cause the chunk to exceed the maximum size.</li>
<li>If so, it pads the chunk to the maximum size, tells secondaries to do the same, and replies to the client indicating that the operation should be retried on the next chunk.</li>
<li>If the record fits within the maximum size, which is the common case, the primary appends the data to its replica, tells the secondaries to write the data at the exact offset where it has, and finally replies success to the client.</li>
</ol>
<h4 id="what-if-appending-fails-at-some-chunkservers"><a class="markdownIt-Anchor" href="#what-if-appending-fails-at-some-chunkservers"></a> What if appending fails at some chunkservers?</h4>
<ol>
<li>Replicas of the same chunk may contain different data possibly including duplicates of the same record in whole or in part.</li>
<li>GFS does not guarantee that all replicas are bytewise identical. It only guarantees that the data is written at least once as an atomic unit.</li>
<li>If there is any secondary chunkserver that can successfully append the record, the primary is succeed. Next time, the primary can choose an offset after the failed record.</li>
<li>Hence, after this, all replicas are at least as long as the end of record and therefore any future record will be assigned a higher offset or a different chunk even if a different replica later becomes the primary.</li>
</ol>
<h3 id="snapshot"><a class="markdownIt-Anchor" href="#snapshot"></a> Snapshot</h3>
<h4 id="what-does-snapshot-do"><a class="markdownIt-Anchor" href="#what-does-snapshot-do"></a> What does snapshot do?</h4>
<ol>
<li>The snapshot operation makes a copy of a file or a directory tree (the “source”) almost instantaneously, while minimizing any interruptions of ongoing mutations.</li>
<li>Users use it to quickly create branch copies of huge data sets (and often copies of those copies, recursively), or to checkpoint the current state before experimenting with changes that can later be committed or rolled back easily.</li>
</ol>
<h4 id="how-does-snapshot-be-implemented"><a class="markdownIt-Anchor" href="#how-does-snapshot-be-implemented"></a> How does snapshot be implemented?</h4>
<ol>
<li>
<p>It use standard copy-on-write techniques.</p>
</li>
<li>
<p>Master revokes leases on the chunks in the files it is about to snapshot.</p>
<ul>
<li>
<p>This ensures that any subsequent writes to these chunks will require an interaction with the master to find the lease holder.</p>
</li>
<li>
<p>And this will give the master an opportunity to create a new copy of the chunk first.</p>
</li>
</ul>
</li>
<li>
<p>Master logs the operation to disk.</p>
</li>
<li>
<p>It then applies this log record to its in-memory state by duplicating the metadata for the source file or directory tree. The newly created snapshot files point to the same chunks as the source files.</p>
</li>
</ol>
<h4 id="how-does-clients-write-a-chunk-after-snapshot"><a class="markdownIt-Anchor" href="#how-does-clients-write-a-chunk-after-snapshot"></a> How does clients write a chunk after snapshot?</h4>
<ol>
<li>The first time a client wants to write to a chunk C after the snapshot operation, it sends a request to the master to find the current lease holder.</li>
<li>The master notices that the reference count for chunk C is greater than one. It defers replying to the client request and instead picks a new chunk handle C’.</li>
<li>It then asks each chunkserver that has a current replica of C to create a new chunk called C’.</li>
<li>By creating the new chunk on the same chunkservers as the original, we ensure that the data can be copied locally, not over the network.</li>
<li>The master grants one of the replicas a lease on the new chunk C’ and replies to the client, which can write the chunk normally.</li>
</ol>
<h2 id="master"><a class="markdownIt-Anchor" href="#master"></a> Master</h2>
<h3 id="basic-operations"><a class="markdownIt-Anchor" href="#basic-operations"></a> Basic operations</h3>
<h4 id="how-does-client-communicate-with-master-specifically"><a class="markdownIt-Anchor" href="#how-does-client-communicate-with-master-specifically"></a> How does client communicate with master specifically?</h4>
<ol>
<li>The client translates the file name and byte offset specified by the application into a chunk index within the file.</li>
<li>It sends the master a requeust containing the file name and chunk index.</li>
<li>The master replies with the corresponding chunk handle and locations of the replicas.</li>
<li>The client caches this information using the file name and chunk index as the key.</li>
</ol>
<h4 id="what-metadata-does-master-need-to-store"><a class="markdownIt-Anchor" href="#what-metadata-does-master-need-to-store"></a> What metadata does master need to store?</h4>
<ol>
<li>
<p>Stored persistently: the file and chunk namespace, the mapping from files to chunks</p>
<ul>
<li>
<p>The master will scan periodically through its entire state in the background</p>
</li>
<li>
<p>Periodic scanning is to implement chunk garbage collection, re-replication in the presence of chunkserver failures, and chunk migration to balance load and disk space usage across chunkservers.</p>
</li>
<li>
<p>The number of chunks and hence the capacity of the whole system is limited by how much memory the master has. But not a serious limitation for less than <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> bytes of metadata for each chunk.</p>
</li>
</ul>
</li>
<li>
<p>No need to store persistently: the locations of each chunk’s replicas.</p>
<ul>
<li>
<p>The master asks each chunkserver about its chunks at master startup and whenever a chunkserver joins the cluster.</p>
</li>
<li>
<p>The master can keep itself up-to-date thereafter because it controls all chunk placement and monitors chunkserver status.</p>
</li>
</ul>
</li>
<li>
<p>Operation record</p>
<ul>
<li>
<p>The namespace and mapping are kept persistent by logging mutations to operation log</p>
</li>
<li>
<p>It is stored on the master’s local disk and replicated on remote machines</p>
</li>
</ul>
</li>
</ol>
<h4 id="how-to-persist"><a class="markdownIt-Anchor" href="#how-to-persist"></a> How to persist?</h4>
<ol>
<li>Operation log
<ul>
<li>The operation log contains a historical record of critical metadata changes. Not only is it the only persistent record of critical metadata, it also serves as a logical time line that defines the order of concurrent operations.</li>
<li>The system respond to a client operation only after flushing the corresponding log record to disk both locally and remotely.</li>
<li>The master batches several log records together before flushing thereby reducing the impact of flushing and replication on overall system thoughput.</li>
</ul>
</li>
<li>Checkpoint
<ul>
<li>To minimize startup time, we must keep the log small. The master checkpoints its state whenever the log grows beyond a certain size. It can recover by loading the latest checkpoint and replaying only the records after that.</li>
<li>The checkpoint is in a compact B-tree like form.</li>
<li>The master switches to a new log file and creates the new checkpoint in a separate thread.</li>
<li>Older checkpoints and log files can be freely deleted, though we keep a few around to guard against catastrophes. A failure during checkpointing does not affect correctness because the recovery code detects and skips incomplete checkpoints.</li>
</ul>
</li>
</ol>
<h4 id="how-does-gfs-manage-namespace"><a class="markdownIt-Anchor" href="#how-does-gfs-manage-namespace"></a> How does GFS manage namespace?</h4>
<ol>
<li>GFS does not have a per-directory data structure that lists all the files in that directory. Nor does it support aliases for the same file or directory.</li>
<li>GFS logically represents its namespace as a lookup table mapping full pathnames to metadata. With prefix compression, this table can be efficiently represented in memory.</li>
<li>Each node in the namespace tree (either an absolute file name or an absolute directory name) has an associated read-write lock.</li>
</ol>
<h4 id="how-does-gfs-design-locking-scheme"><a class="markdownIt-Anchor" href="#how-does-gfs-design-locking-scheme"></a> How does GFS design locking scheme?</h4>
<ol>
<li>If a master operation involves a certain file or directory, it will acquire read-locks on all the parent directories, and either a read-lock or a write-lock on the leaf file or directory that it will operate directly.</li>
<li>File creation does not require a write lock on the parent directory because there is no “directory”, or inode-like, data structure to be protected from modification.</li>
<li>This locking scheme allows concurrent mutations in the same directory.</li>
</ol>
<h3 id="replica-management"><a class="markdownIt-Anchor" href="#replica-management"></a> Replica management</h3>
<h4 id="how-to-place-replicas"><a class="markdownIt-Anchor" href="#how-to-place-replicas"></a> How to place replicas?</h4>
<ol>
<li>There are two purposes: maximize data reliability and availability, and maximize network bandwidth utilization.</li>
<li>It is not enough to spread replicas across machines, which only guards against disk or machine failures and fully utilizes each machine’s network bandwidth.</li>
<li>We must also spread chunk replicas across racks. This ensures that some replicas of a chunk will survive and remain available even if an entire rack is damaged or offline. It also means that traffic, especially reads, for a chunk can exploit the aggregate bandwidth of multiple racks.</li>
<li>On the other hand, write traffic has to flow through multiple racks, a tradeoff we make willingly.</li>
<li>An even safer way is to spread across data centers in different cities. It can guards against a city-level catastrophe.</li>
</ol>
<h4 id="what-factors-are-considered-when-create-a-new-chunk"><a class="markdownIt-Anchor" href="#what-factors-are-considered-when-create-a-new-chunk"></a> What factors are considered when create a new chunk?</h4>
<ol>
<li>We want to place new replicas on chunkservers with below-average disk space utilization. Over time this will equalize disk utilization across chunkservers.</li>
<li>We want to limit the number of “recent” creations on each chunkserver. Although creation itself is cheap, it reliably predicts imminent heavy write traffic because chunks are created when demanded by writes.</li>
<li>We want to spread replicas of a chunk across racks.</li>
</ol>
<h4 id="what-if-the-number-of-available-replicas-of-a-chunk-falls-below-a-user-specified-goal"><a class="markdownIt-Anchor" href="#what-if-the-number-of-available-replicas-of-a-chunk-falls-below-a-user-specified-goal"></a> What if the number of available replicas of a chunk falls below a user-specified goal?</h4>
<ol>
<li>
<p>The master would re-replicate the chunk.</p>
</li>
<li>
<p>If there are many chunks below their goal, the master picks the highest priority chunk considering some factors and “clones” it by instructing some chunkserver to copy the chunk data directly from an existing valid replica.</p>
<ul>
<li>
<p>How far it is from its replication goal.</p>
</li>
<li>
<p>Prefer to first re-replicate chunks for live files as opposed to chunks that belong to recently deleted files.</p>
</li>
<li>
<p>To minimize the impact of failures on running applications, we boost the priority of any chunk that is blocking client progress.</p>
</li>
</ul>
</li>
</ol>
<h4 id="what-if-cloning-traffic-from-overwhelming-client-traffic"><a class="markdownIt-Anchor" href="#what-if-cloning-traffic-from-overwhelming-client-traffic"></a> What if cloning traffic from overwhelming client traffic?</h4>
<ol>
<li>The master limits the numbers of active clone operations both for the cluster and for each chunkserver.</li>
<li>Each chunkserver limits the amount of bandwidth it spends on each clone operation by throttling its read requests to the source chunkserver.</li>
</ol>
<h4 id="how-to-keep-the-placement-of-replicas-in-balance"><a class="markdownIt-Anchor" href="#how-to-keep-the-placement-of-replicas-in-balance"></a> How to keep the placement of replicas in balance?</h4>
<ol>
<li>It examines the current replica distribution and moves replicas for better disk space and load balancing.</li>
<li>The master gradually fills up a new chunkserver rather than instantly swamps it with new chunks and the heavy write traffic that comes with them.</li>
<li>The master must also choose which existing replica to remove. It prefers to remove those on chunkservers with below-average free space.</li>
</ol>
<h3 id="deletion"><a class="markdownIt-Anchor" href="#deletion"></a> Deletion</h3>
<h4 id="how-to-delete-a-file"><a class="markdownIt-Anchor" href="#how-to-delete-a-file"></a> How to delete a file?</h4>
<ol>
<li>
<p>GFS does not immediately reclaim the available physical storage, it is just renamed to a hidden name that includes the deletion timestamp. It does so only lazily during regular garbage collection at both the file and chunk levels.</p>
</li>
<li>
<p>During the master’s regular scan of the file system namespace, it removes any such hidden files if they have existed for more than three days (the interval is configurable).</p>
<ul>
<li>
<p>Until then, the file can still be read under the new, special name and can be undeleted by renaming it back to normal.</p>
</li>
<li>
<p>When the hidden file is removed from the namespace, its in-memory metadata is erased. This effectively severs its links to all its chunks.</p>
</li>
</ul>
</li>
<li>
<p>In a similar regular scan of the chunk namespace, the master identifies orphaned chunks (i.e., those not reachable from any file) and erases the metadata for those chunks.</p>
<ul>
<li>
<p>In a HeartBeat message exchanged with the master, each chunkserver reports a subset of the chunks it has, and the master replies with the identity of all chunks that are no longer present in the master’s metadata.</p>
</li>
<li>
<p>The chunkserver is free to delete its replicas of such chunks.</p>
</li>
</ul>
</li>
</ol>
<h4 id="what-are-the-advantages-and-disadvantages-of-lazy-deletion-over-eager-deletion"><a class="markdownIt-Anchor" href="#what-are-the-advantages-and-disadvantages-of-lazy-deletion-over-eager-deletion"></a> What are the advantages and disadvantages of lazy deletion over eager deletion?</h4>
<ol>
<li>
<p>It is simple and reliable in a large-scale distributed system where component failures are common.</p>
</li>
<li>
<p>It merges storage reclamation into the regular background activities of the master.</p>
</li>
<li>
<p>The delay in reclaiming storage provides a safety net against accidental, irreversible deletion.</p>
</li>
<li>
<p>The delay sometimes hinders user effort to fine tune usage when storage is tight.</p>
</li>
<li>
<p>Applications that repeatedly create and delete temporary files may not be able to reuse the storage right away.</p>
</li>
</ol>
<h4 id="how-to-address-the-issues-of-reusing"><a class="markdownIt-Anchor" href="#how-to-address-the-issues-of-reusing"></a> How to address the issues of reusing?</h4>
<ol>
<li>
<p>Expediting storage reclamation if a deleted file is explicitly deleted again.</p>
</li>
<li>
<p>Allow users to apply different replication and reclamation policies to different parts of the namespace.</p>
</li>
</ol>
<h4 id="how-to-handle-the-possible-stale-replicas"><a class="markdownIt-Anchor" href="#how-to-handle-the-possible-stale-replicas"></a> How to handle the possible stale replicas?</h4>
<ol>
<li>
<p>For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas.</p>
</li>
<li>
<p>Whenever the master grants a new lease on a chunk, it increases the chunk version number and informs the up-to-date replicas. This occurs before any client is notified and therefore before it can start writing to the chunk.</p>
</li>
<li>
<p>If one replica is currently unavailable, its chunk version number will not be advanced. The master will detect that this chunkserver has a stale replica when the chunkserver restarts and reports its set of chunks and their associated version numbers.</p>
</li>
<li>
<p>The master removes stale replicas in its regular garbage collection. Before that, it effectively considers a stale replica not to exist at all when it replies to client requests for chunk information.</p>
</li>
<li>
<p>The master includes the chunk version number when it informs clients which chunkserver holds a lease on a chunk or when it instructs a chunkserver to read the chunk from another chunkserver in a cloning operation.</p>
</li>
</ol>
<h2 id="fault-tolerance"><a class="markdownIt-Anchor" href="#fault-tolerance"></a> Fault tolerance</h2>
<h3 id="how-to-handle-master-failure"><a class="markdownIt-Anchor" href="#how-to-handle-master-failure"></a> How to handle master failure?</h3>
<ol>
<li>
<p>The master state is replicated for reliability.</p>
<ul>
<li>
<p>When it fails, it can restart almost instantly.</p>
</li>
<li>
<p>When its machine or disk fails, monitoring infrastructure outside GFS starts a new master process elsewhere with the replicated operation log.</p>
</li>
<li>
<p>Clients use only the canonical name of the master, which is a DNS alias that can be changed if the master is relocated to another machine.</p>
</li>
</ul>
</li>
<li>
<p>“Shadow” masters provide read-only access to the file system even when the primary master is down.</p>
<ul>
<li>
<p>They enhance read availability for files that are not being actively mutated or applications that do not mind getting slightly stale results.</p>
</li>
<li>
<p>Since file content is read from chunkservers, applications do not observe stale file content. What could be stale within short windows is file metadata.</p>
</li>
<li>
<p>To keep itself informed, a shadow master reads a replica of the growing operation log and applies the same sequence of changes to its data structures exactly as the primary does.</p>
</li>
<li>
<p>It depends on the primary master only for replica location updates resulting from the primary’s decisions to create and delete replicas.</p>
</li>
</ul>
</li>
</ol>
<h3 id="why-cannot-recover-data-using-other-chunk-replicas-why-each-chunkserver-must-independently-verify-the-integrity"><a class="markdownIt-Anchor" href="#why-cannot-recover-data-using-other-chunk-replicas-why-each-chunkserver-must-independently-verify-the-integrity"></a> Why cannot recover data using other chunk replicas? Why each chunkserver must independently verify the integrity?</h3>
<ol>
<li>
<p>It would be impractical to detect corruption by comparing replicas across chunkservers.</p>
</li>
<li>
<p>Divergent replicas may be legal: the semantics of GFS mutations, in particular atomic record append, does not guarantee identical replicas.</p>
</li>
</ol>
<h3 id="how-to-ensure-data-integrity"><a class="markdownIt-Anchor" href="#how-to-ensure-data-integrity"></a> How to ensure data integrity?</h3>
<ol>
<li>
<p>Each chunkserver uses checksumming to detect corruption of stored data. A chunk is broken up into 64 KB blocks. Each has a corresponding 32 bit checksum.</p>
</li>
<li>
<p>Checksums are kept in memory and stored persistently with logging, separate from user data.</p>
</li>
<li>
<p>During idle periods, chunkservers can scan and verify the contents of inactive chunks.</p>
</li>
</ol>
<h3 id="how-to-read-data-with-checksum"><a class="markdownIt-Anchor" href="#how-to-read-data-with-checksum"></a> How to read data with checksum?</h3>
<ol>
<li>
<p>the chunkserver verifies the checksum of data blocks that overlap the read range before returning any data to the requester, whether a client or another chunkserver.</p>
</li>
<li>
<p>If a block does not match the recorded checksum, the chunkserver returns an error to the requestor and reports the mismatch to the master.</p>
</li>
<li>
<p>In response, the requestor will read from other replicas, while the master will clone the chunk from another replica.</p>
</li>
<li>
<p>After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica.</p>
</li>
</ol>
<h3 id="how-to-write-data-with-checksum"><a class="markdownIt-Anchor" href="#how-to-write-data-with-checksum"></a> How to write data with checksum?</h3>
<ol>
<li>
<p>For writes that append to the end of a chunk, we just incrementally update the checksum for the last partial checksum block, and compute new checksums for any brand new checksum blocks filled by the append.</p>
<ul>
<li>Even if the last partial checksum block is already corrupted and we fail to detect it now, the new checksum value will not match the stored data, and the corruption will be detected as usual when the block is next read.</li>
</ul>
</li>
<li>
<p>If a write overwrites an existing range of the chunk, we must read and verify the first and last blocks of the range being overwritten, then perform the write, and finally compute and record the new checksums.</p>
<ul>
<li>If we do not verify the first and last blocks before overwriting them partially, the new checksums may hide corruption that exists in the regions not being overwritten.</li>
</ul>
</li>
</ol>
<h3 id="what-is-included-in-the-diagnostic-logs"><a class="markdownIt-Anchor" href="#what-is-included-in-the-diagnostic-logs"></a> What is included in the diagnostic logs?</h3>
<ol>
<li>
<p>GFS servers generate diagnostic logs that record many significant events (such as chunkservers going up and down) and all RPC requests and replies.</p>
</li>
<li>
<p>The RPC logs include the exact requests and responses sent on the wire, except for the file data being read or written.</p>
</li>
</ol>
<h2 id="other-parts-unmentioned"><a class="markdownIt-Anchor" href="#other-parts-unmentioned"></a> Other parts (unmentioned)</h2>
<h3 id="to-sum-up-what-is-the-metadata-of-master-and-where-are-they"><a class="markdownIt-Anchor" href="#to-sum-up-what-is-the-metadata-of-master-and-where-are-they"></a> To sum up, what is the metadata of master, and where are they?</h3>
<ol>
<li>
<p>File name: this is an array of chunk handles. It is stored on disk.</p>
</li>
<li>
<p>Handle: it contains a list of chunkservers, version number, primary, and lease expiration.</p>
<ul>
<li>
<p>Only the version number is stored on disk, due to the rest can be restored by asking chunkservers when master is recovered.</p>
</li>
<li>
<p>Given that there might have stale chunks, we cannot ask chunkservers for the version number of a chunk.</p>
</li>
</ul>
</li>
<li>
<p>Lops and checkpoints are stored on disk.</p>
</li>
</ol>
<h3 id="what-is-the-cause-of-split-brain-how-to-solve-it"><a class="markdownIt-Anchor" href="#what-is-the-cause-of-split-brain-how-to-solve-it"></a> What is the cause of split brain? How to solve it?</h3>
<ol>
<li>Split brain is caused by network partition, the master cannot talk to primary while the primary can talk to clients. Hence the master mistakingly designates two primary for the same chunk.</li>
<li>The master knowswhen the lease will expire, so when the master cannot talk to the primary, it will wait until the lease expired before assign another primary.</li>
</ol>
<h3 id="why-gfs-doesnt-overwrite-those-failed-records-immediately-but-leaving-padding-and-duplicates"><a class="markdownIt-Anchor" href="#why-gfs-doesnt-overwrite-those-failed-records-immediately-but-leaving-padding-and-duplicates"></a> Why GFS doesn’t overwrite those failed records immediately, but leaving padding and duplicates?</h3>
<p>Because When it starts to write the next record, it may not know the fate of prior record.</p>
<h3 id="gfs-is-a-weak-consistency-system-how-can-we-upgrade-it-to-a-strong-consistency-system"><a class="markdownIt-Anchor" href="#gfs-is-a-weak-consistency-system-how-can-we-upgrade-it-to-a-strong-consistency-system"></a> GFS is a weak consistency system, how can we upgrade it to a strong consistency system?</h3>
<ol>
<li>Primary detects duplicate requests to ensure the failed write doesn’t show up twice</li>
<li>When primary asks a secondary to do something, the secondary actually does it and doesn’t just return error (except the secondary has a permanent damage, in which case, it should be removed)</li>
<li>The secondary doesn’t expose data to readers until the primary is sure that all the secondaries really will be execute the append.</li>
<li>When primary crashes, there will have been some last set of operations that primary had launched to the secondaries, but primary crashed before ensure all operations are done. The new primary need to explicitly resync with all secondaries.</li>
</ol>
<h1 id="experiments-and-results"><a class="markdownIt-Anchor" href="#experiments-and-results"></a> Experiments and results</h1>
<h2 id="micro-benchmarks"><a class="markdownIt-Anchor" href="#micro-benchmarks"></a> Micro-benchmarks</h2>
<p>The author first tested several micro-benchmarks, i.e. reads, writes, and record appends. These tests are that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> clients do those operation simultaneously. For reading, they read from the file system; for writing, they write to distinct files; for appending, they append to a single file. And test the aggregate throughputs of the system, comparing them with the network limit. The results are as following:</p>
<p><img src="/imgs/Distributed/GFS/03.png" alt="" /></p>
<h3 id="read"><a class="markdownIt-Anchor" href="#read"></a> Read</h3>
<p>The reading efficiency drops because as the number of readers increases, so does the probability that multiple readers simultaneously read from the same chunkserver.</p>
<h3 id="write"><a class="markdownIt-Anchor" href="#write"></a> Write</h3>
<p>The limit plateaus of write rate at 67 MB/s because we need to write each byte to 3 of the 16 chunkservers, each with a 12.5 MB/s input connection.</p>
<p>The main culprit for a low write rate with only one client is the network stack. It does not interact very well with the pipelining scheme we use for pushing data to chunk replicas. Delays in propagating data from one replica to another reduce the overall write rate.</p>
<p>As the number of clients grows, it becomes more likely that multiple clients write concurrently to the same chunkserver as the number of clients increases. Moreover, collision is more likely for 16 writers than for 16 readers because each write involves three different replicas.</p>
<p>Writes are slower than we would like. In practice this has not been a major problem because even though it increases the latencies as seen by individual clients, it does not significantly affect the aggregate write bandwidth delivered by the system to a large number of clients.</p>
<h3 id="record-append"><a class="markdownIt-Anchor" href="#record-append"></a> Record append</h3>
<p>The performance of record appends is limited by the network bandwidth of the chunkservers that store the last chunk of the file, independent of the number of clients.</p>
<p>The append rate drops mostly due to congestion and variances in network transfer rates seen by different clients.</p>
<p>The chunkserver network congestion in our experiment is not a significant issue in practice because a client can make progress on writing one file while the chunkservers for another file are busy.</p>
<h2 id="real-world-clusters"><a class="markdownIt-Anchor" href="#real-world-clusters"></a> Real world clusters</h2>
<p>The author also measured the performance of real world clusters. First, the author measured their storage usage and size of metadata. Then, the read rate, write rate and the rate of operations sent to the master were measured. The results show that master can easily keep up with this rate, and therefore is not a bottleneck for these workloads.</p>
<p><img src="/imgs/Distributed/GFS/04.png" style="zoom: 33%;" /><img src="/imgs/Distributed/GFS/05.png" style="zoom: 31%;" /></p>
<p>After a chunkserver fails, some chunks will become underreplicated and must be cloned to restore their replication levels. To test the recovery time, the author killed a single chunkserver containing 15000 chunks of 600 GB data.</p>
<p>To limit the impact on running applications and provide leeway for scheduling decisions, our default parameters limit this cluster to 91 concurrent clonings (40% of the number of chunkservers) where each clone operation is allowed to consume at most 6.25 MB/s (50 Mbps). All chunks were restored in 23.2 minutes, at an effective replication rate of 440 MB/s.</p>
<p>Finally, the author also measured the workload of chunkserver and master, and breakdown the workload of chunkserver by size and same of master by type. The table 4 shows the distribution of operations by size, and the table 5 shows the total amount of data transferred in operations of various size.</p>
<p><img src="/imgs/Distributed/GFS/06.png" style="zoom:25%;" /><img src="/imgs/Distributed/GFS/07.png" style="zoom:25%;" /><img src="/imgs/Distributed/GFS/08.png" style="zoom: 31%;" /></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/MapReduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/MapReduce/" class="post-title-link" itemprop="url">MapReduce</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:00:23" itemprop="dateCreated datePublished" datetime="2023-09-26T13:00:23+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:26:28" itemprop="dateModified" datetime="2023-10-04T16:26:28+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/mapreduce.pdf">MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#paper-ideas">Paper ideas</a>
<ul>
<li><a href="#what-does-users-need-to-do">What does users need to do?</a></li>
<li><a href="#what-does-run-time-system-need-to-do">What does run-time system need to do?</a></li>
<li><a href="#how-does-the-system-run">How does the system run?</a></li>
<li><a href="#what-does-master-need-to-do">What does master need to do?</a></li>
<li><a href="#how-to-handle-worker-failure">How to handle worker failure?</a></li>
<li><a href="#how-to-handle-master-failure">How to handle master failure?</a></li>
<li><a href="#how-to-partition-reduce-tasks">How to partition reduce tasks?</a></li>
<li><a href="#how-to-handle-straggler-problem">How to handle straggler problem?</a></li>
</ul>
</li>
<li><a href="#reproduce">Reproduce</a>
<ul>
<li><a href="#how-do-we-assign-map-tasks">How do we assign map tasks?</a></li>
<li><a href="#how-do-we-assign-reduce-tasks">How do we assign reduce tasks?</a></li>
<li><a href="#when-can-workers-stop-requesting-for-more-map-tasksreduce-tasks">When can workers stop requesting for more map tasks/reduce tasks?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiments-and-results">Experiments and results</a></li>
</ul>
</p>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Issues: Hard to parallel computation, distribute data, and handle server failures</li>
<li>Contribution: Proposed an interface where users only need to write relatively simple Map function and Reduce function, and the system will parallel and distribute automatically.</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="paper-ideas"><a class="markdownIt-Anchor" href="#paper-ideas"></a> Paper ideas</h2>
<h3 id="what-does-users-need-to-do"><a class="markdownIt-Anchor" href="#what-does-users-need-to-do"></a> What does users need to do?</h3>
<ol>
<li>Users need to provide a Map function and a Reduce function.</li>
<li>Map function will read the original data as key-value pairs, take one pair as input each time, and output intermediate key-value pairs, i.e. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>p</mi><mo stretchy="false">(</mo><mi>k</mi><mn>1</mn><mo separator="true">,</mo><mi>v</mi><mn>1</mn><mo stretchy="false">)</mo><mo>→</mo><mi>l</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>k</mi><mn>2</mn><mo separator="true">,</mo><mi>v</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">map(k1,v1)\rightarrow list(k2,v2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">2</span><span class="mclose">)</span></span></span></span></li>
<li>Reduce function will take the intermediate-key and a list of all intermediate-values for that key as input, and merge these values to form a smaller set of values, i.e. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>e</mi><mo stretchy="false">(</mo><mi>k</mi><mn>2</mn><mo separator="true">,</mo><mi>l</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>v</mi><mn>2</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>→</mo><mi>l</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>v</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">reduce(k2,list(v2))\rightarrow list(v2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">2</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">2</span><span class="mclose">)</span></span></span></span></li>
<li>Users need to implement the Mapper and Reducer as interface provided by the system, and pass to the MapReduce specification. After passing the input and output files, invoke the <code>MapReduce</code> function to execute.</li>
</ol>
<h3 id="what-does-run-time-system-need-to-do"><a class="markdownIt-Anchor" href="#what-does-run-time-system-need-to-do"></a> What does run-time system need to do?</h3>
<ol>
<li>Partition data</li>
<li>Schedule across a set of machines</li>
<li>Handle machine failures</li>
<li>Manage inter-machine communication.</li>
</ol>
<h3 id="how-does-the-system-run"><a class="markdownIt-Anchor" href="#how-does-the-system-run"></a> How does the system run?</h3>
<ol>
<li>
<p>When the <code>MapReduce</code> function is invoked, one <strong>master</strong> process and several <strong>worker</strong> processes will be forked.</p>
</li>
<li>
<p>Master will assign work to workers, either a Map work, or a Reduce work.</p>
</li>
<li>
<p>Master tries to make most of the <code>(3) read</code> run locally. In the <code>(5) remote read</code>, network communication is inevitable.</p>
<img src="/imgs/Distributed/MapReduce/01.png" style="zoom: 50%;" />
</li>
</ol>
<h3 id="what-does-master-need-to-do"><a class="markdownIt-Anchor" href="#what-does-master-need-to-do"></a> What does master need to do?</h3>
<ol>
<li>
<p>Master pings every worker periodically, and marks those no response in a certain amount of time as failed.</p>
</li>
<li>
<p>Track the state of each map task and reduce task (idle, in-progress or completed), and the identity of the worker machine (for non-idle tasks).</p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> is the number map tasks, while <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> is the number of reduce tasks. The master must make <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>M</mi><mo>+</mo><mi>R</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(M+R)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mclose">)</span></span></span></span> scheduling decisions, and keeps <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>M</mi><mo>∗</mo><mi>R</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(M*R)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mclose">)</span></span></span></span> state in memory (all map task/reduce task pair).</p>
</li>
</ol>
<h3 id="how-to-handle-worker-failure"><a class="markdownIt-Anchor" href="#how-to-handle-worker-failure"></a> How to handle worker failure?</h3>
<ol>
<li>
<p>What kinds of worker failure need re-execution?</p>
<ul>
<li>
<p>Any tasks in progress</p>
</li>
<li>
<p>Completed map tasks also need to be re-executed, since their output is stored on the local disks and is inaccessible.</p>
</li>
<li>
<p>Completed reduce tasks don’t need to be re-executed, since their output is stored on the global file system.</p>
</li>
</ul>
</li>
<li>
<p>Master will mark the state of those tasks that need re-execution to idle, and can assign them to other workers in the future.</p>
</li>
</ol>
<h3 id="how-to-handle-master-failure"><a class="markdownIt-Anchor" href="#how-to-handle-master-failure"></a> How to handle master failure?</h3>
<ol>
<li>
<p>One way is to make the master write periodic checkpoints of the master data structure.</p>
</li>
<li>
<p>Given that there is only a single master, its failure is unlikely. Therefore another way is to abort the MapReduce computation if the master fails, and clients can try again later. (This is the way the author takes)</p>
</li>
</ol>
<h3 id="how-to-partition-reduce-tasks"><a class="markdownIt-Anchor" href="#how-to-partition-reduce-tasks"></a> How to partition reduce tasks?</h3>
<p>The number of reduce tasks/output files (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>) is specified by the users. The default partitioning uses hashing, namely partition according to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>a</mi><mi>s</mi><mi>h</mi><mo stretchy="false">(</mo><mi>k</mi><mi>e</mi><mi>y</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>m</mi><mi>o</mi><mi>d</mi><mtext> </mtext><mi>R</mi></mrow><annotation encoding="application/x-tex">hash(key)\ mod\ R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace"> </span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>.</p>
<h3 id="how-to-handle-straggler-problem"><a class="markdownIt-Anchor" href="#how-to-handle-straggler-problem"></a> How to handle straggler problem?</h3>
<ol>
<li>Straggler: a machine that takes an unusually long time to complete on of the last few map or reduce tasks. This may be caused by a bad disk, its scheduling system scheduling it a different other tasks.</li>
<li>So when a MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks.</li>
</ol>
<h2 id="reproduce"><a class="markdownIt-Anchor" href="#reproduce"></a> Reproduce</h2>
<p>This reproduce part is based on the Lab 1 of MIT 6.824.</p>
<h3 id="how-do-we-assign-map-tasks"><a class="markdownIt-Anchor" href="#how-do-we-assign-map-tasks"></a> How do we assign map tasks?</h3>
<p>Each worker will request for more map tasks when it becomes idle, and the master will assign files directly to them.</p>
<h3 id="how-do-we-assign-reduce-tasks"><a class="markdownIt-Anchor" href="#how-do-we-assign-reduce-tasks"></a> How do we assign reduce tasks?</h3>
<ol>
<li>When worker is notified that there is no more map tasks, they will begin to request for reduce tasks. This time, the master won’t assign files directly, instead, master will only assign a number in the range from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">R-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>. Then each worker will try to read intermediate files from each workers according to its number automatically.</li>
<li>This requires those map workers store their output in a previously agreed file name for reduce workers to request.</li>
</ol>
<h3 id="when-can-workers-stop-requesting-for-more-map-tasksreduce-tasks"><a class="markdownIt-Anchor" href="#when-can-workers-stop-requesting-for-more-map-tasksreduce-tasks"></a> When can workers stop requesting for more map tasks/reduce tasks?</h3>
<ol>
<li>Only after all map tasks is completed, workers can stop requesting for more map tasks and begin to request for reduce tasks, since reduce tasks may depend on those unfinished map tasks. So the lifecycle of workers can be partitioned into two phase, the map phase and the reduce phase.</li>
<li>Also, only after all reduce tasks is completed, workers can stop requesting for more reduce tasks and quit the program. This is because those executing, yet uncompleted, tasks may fail, and when that happens, we need other workers to re-execute those tasks.</li>
<li>Similarly, reduce workers cannot delete those intermediate files right after they read them. Because if they fail, their successor need to read those files.</li>
</ol>
<h1 id="experiments-and-results"><a class="markdownIt-Anchor" href="#experiments-and-results"></a> Experiments and results</h1>
<ol>
<li>
<p><strong>What to notice when configurate cluster?</strong></p>
<ul>
<li>Need to reserve some memory for other tasks running on the cluster. The author reserved <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mn>1.5</mn></mrow><annotation encoding="application/x-tex">1-1.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">5</span></span></span></span> GB out of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span></span></span></span> GB.</li>
<li>Best test when the CPUs, disks, and network were mostly idle.</li>
</ul>
</li>
<li>
<p>The author tested two representative situations, grep and sort.</p>
</li>
<li>
<p>In the grep test, the execution time includes a minute of startup overhead over <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>150</mn></mrow><annotation encoding="application/x-tex">150</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">5</span><span class="mord">0</span></span></span></span> seconds of total time. The overhead is due to the propagation of the program to all worker machines, and delays interacting with GFS to open the set of input files and to get the information needed for the locality optimization.</p>
<img src="/imgs/Distributed/MapReduce/02.png" style="zoom: 33%;" />
</li>
<li>
<p>In the sort test,</p>
<ul>
<li>It only consists of less than <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn></mrow><annotation encoding="application/x-tex">50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">0</span></span></span></span> lines of user code</li>
<li>The entire computation time including startup overhead is similar to the best reported result at that time.</li>
<li>The author tested three kind of rates, the rate of reading by map workers (<em>input rate</em>), the rate of communicating intermediate files between map workers and reduce workers (<em>shuffle rate</em>), and the rate of writing output files by reduce workers (<em>output rate</em>). These are the I/O parts which affect the perfornmence significantly.
<ul>
<li>The input rate is less than for grep, because sort map tasks spend more time writing intermediate output to their local disks.</li>
<li>The input rate is higher than the shuffle rate and the output rate because of locality optimization.</li>
<li>The shuffle rate is higher than the output rate because the output phase writes replicas due to the mechanism for reliability  of the underlying file system.</li>
</ul>
</li>
<li>The author also tested when <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord">0</span></span></span></span> out of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1746</mn></mrow><annotation encoding="application/x-tex">1746</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">7</span><span class="mord">4</span><span class="mord">6</span></span></span></span> workers are killed several minutes. The underlying cluster scheduler immediately restarted new worker processes on these mechines (only the processes were killed, the machines were still functioning properly). The entire computation time increases of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="mord">5</span><span class="mord">%</span></span></span></span> over the normal execution time.</li>
</ul>
<img src="/imgs/Distributed/MapReduce/03.png" style="zoom:50%;" />
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/03/Courses/15445/17-Distributed-OLAP-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/03/Courses/15445/17-Distributed-OLAP-Databases/" class="post-title-link" itemprop="url">17 Distributed OLAP Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-03 12:05:46" itemprop="dateCreated datePublished" datetime="2023-09-03T12:05:46+08:00">2023-09-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:27" itemprop="dateModified" datetime="2023-09-26T14:01:27+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#how-should-we-divide-tables">How should we divide tables?</a></li>
<li><a href="#execution-models">Execution models</a>
<ul>
<li><a href="#how-to-execute-when-required-data-are-in-different-nodes">How to execute when required data are in different nodes?</a></li>
<li><a href="#how-to-handle-query-fault">How to handle query fault?</a></li>
<li><a href="#when-pushing-query-to-data-what-are-the-queries">When pushing query to data, what are the queries?</a></li>
</ul>
</li>
<li><a href="#distributed-join-algorithms">Distributed Join Algorithms</a>
<ul>
<li><a href="#how-to-perform-distributed-joins">How to perform distributed joins?</a></li>
<li><a href="#what-is-semi-join">What is semi-join?</a></li>
</ul>
</li>
<li><a href="#cloud-systems">Cloud systems</a>
<ul>
<li><a href="#what-is-the-difference-for-dbmss-running-in-cloud-systems">What is the difference for DBMSs running in cloud systems?</a></li>
<li><a href="#how-to-store-data-of-different-schema">How to store data of different schema?</a></li>
<li><a href="#how-to-share-data-between-systems">How to share data between systems?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="how-should-we-divide-tables"><a class="markdownIt-Anchor" href="#how-should-we-divide-tables"></a> How should we divide tables?</h1>
<ol>
<li>The first schema is star schema.
<ul>
<li>Star schemas contain two types of tables: fact tables and dimension tables.</li>
<li>The fact table contains multiple “events” that occur in the application. It will contain the minimal unique information per event.</li>
<li>The rest of the attributes will be foreign key references to outer dimension tables.</li>
<li>In a star schema, there can only be one dimension-level out from the fact table.</li>
</ul>
</li>
<li>The second schema is snowflake schema.
<ul>
<li>It allows for more than one dimension out from the fact table.</li>
</ul>
</li>
<li>Snowflake schemas take up less storage space. Denormalized data models may incur integrity and consistency violations.</li>
<li>Snowflake schemas require more joins to get the data needed for a query. Queries on star schemas will usually be faster.</li>
</ol>
<h1 id="execution-models"><a class="markdownIt-Anchor" href="#execution-models"></a> Execution models</h1>
<h2 id="how-to-execute-when-required-data-are-in-different-nodes"><a class="markdownIt-Anchor" href="#how-to-execute-when-required-data-are-in-different-nodes"></a> How to execute when required data are in different nodes?</h2>
<ol>
<li>The first approach is to push query to data.
<ul>
<li>Send the query (or a portion of it) to the node that contains the data.</li>
<li>The result is then sent back to where the query is being executed, which uses local data and the data sent to it, to complete the query.</li>
<li>Perform as much filtering and processing as possible where data resides before transmitting over network.</li>
<li>This is more common in a shared nothing system.</li>
</ul>
</li>
<li>The second approach is to pull data to query.
<ul>
<li>Bring the data to the node that is executing a query that needs it for processing.</li>
<li>This is normally what a shared disk system would do.</li>
<li>The problem with this is that the size of the data relative to the size of the query could be very different.</li>
</ul>
</li>
</ol>
<h2 id="how-to-handle-query-fault"><a class="markdownIt-Anchor" href="#how-to-handle-query-fault"></a> How to handle query fault?</h2>
<ol>
<li>The data that a node receives from remote sources are cached in the buffer pool.
<ul>
<li>When buffer pool run out of memory, the DBMS can write some pages out to disk in some ephemeral pages.</li>
<li>This allows the DBMS to support intermediate results that are large than the amount of memory available.</li>
<li>Ephemeral pages are not persisted after a restart.</li>
</ul>
</li>
<li>Most shared-nothing distributed OLAP DBMSs are designed to assume that nodes do not fail during query execution.
<ul>
<li>If one node fails during query execution, then the whole query fails.</li>
</ul>
</li>
<li>The DBMS could take a snapshot of the intermediate results for a query during execution to allow it to recover if nodes fail.
<ul>
<li>Most of the DMBSs do not want to pay the penalty of writing intermediate results to disk for quick queries.</li>
<li>This is adopted if the query takes a long time.</li>
</ul>
</li>
<li>In shared-disk distributed OLAP DMBSs, they can write results to the shared disk to prevent performing the same calculation again. However, the frequency of writing to disk is also a trade-off.</li>
</ol>
<h2 id="when-pushing-query-to-data-what-are-the-queries"><a class="markdownIt-Anchor" href="#when-pushing-query-to-data-what-are-the-queries"></a> When pushing query to data, what are the queries?</h2>
<ol>
<li>The DBMSs only need to push fragments of the query instead of the whole query.</li>
<li>The first approach is pushing physical operators.
<ul>
<li>Generate a single query plan and then break it up into partition-specific  fragments.</li>
<li>Most systems implement this approach.</li>
</ul>
</li>
<li>The second approach is pushing another SQL query.
<ul>
<li>The DBMS will rewrite original query into partition-specific queries.</li>
<li>This approach allows for local optimization at each node.</li>
</ul>
</li>
</ol>
<h1 id="distributed-join-algorithms"><a class="markdownIt-Anchor" href="#distributed-join-algorithms"></a> Distributed Join Algorithms</h1>
<h2 id="how-to-perform-distributed-joins"><a class="markdownIt-Anchor" href="#how-to-perform-distributed-joins"></a> How to perform distributed joins?</h2>
<ol>
<li>One approach is to put entire tables on a single node and then perform the join.
<ul>
<li>You lose the parallelism of a distributed DBMS.</li>
<li>It has expensive data transferring over the network.</li>
</ul>
</li>
<li>To join tables, the DBMS needs to get the proper tuples on the same node. Once the data is at the node, the DBMS then executes the single-node join algorithms.</li>
<li>The best scenario is that one table is replicated at every node.
<ul>
<li>Each node joins its local data in parallel and then sends their results to a coordinating node.</li>
</ul>
</li>
<li>The second scenario is that tables are partitioned on the join attribute.
<ul>
<li>Each node only needs to acquire tuples of the second table that will match the tuples of the left table that are already in it.</li>
<li>Each node performs the join on local data and then sends to a coordinator node for coalescing.</li>
</ul>
</li>
<li>The third scenario is that both tables are partitioned on different keys while one of the tables is small.
<ul>
<li>The DBMS will broadcast that table to all nodes.</li>
</ul>
</li>
<li>The worst scenario is that both tables are not partitioned on the join key.
<ul>
<li>The DBMS copies the tables by shuffling them across nodes.</li>
</ul>
</li>
</ol>
<h2 id="what-is-semi-join"><a class="markdownIt-Anchor" href="#what-is-semi-join"></a> What is semi-join?</h2>
<ol>
<li>If the result only contains columns sfrom the left table, the DBMSs use semi-join to minimize the amount of data sent during joins.</li>
<li>This is like a projection pushdown. The DBMS transmits only the necessary columns of the left table to other nodes.</li>
<li>For DBMSs that do not support <code>SEMI JOIN</code>, we can fake it with <code>EXISTS</code>.
<ul>
<li>Wrap the predicates of <code>WHERE</code>  clause with <code>EXISTS (SELECT 1 from S WHERE predicates)</code>.</li>
</ul>
</li>
</ol>
<h1 id="cloud-systems"><a class="markdownIt-Anchor" href="#cloud-systems"></a> Cloud systems</h1>
<h2 id="what-is-the-difference-for-dbmss-running-in-cloud-systems"><a class="markdownIt-Anchor" href="#what-is-the-difference-for-dbmss-running-in-cloud-systems"></a> What is the difference for DBMSs running in cloud systems?</h2>
<ol>
<li>The first approach is managed DBMS.
<ul>
<li>No significant modification to the DBMS to be aware that it is running in a cloud environment.</li>
</ul>
</li>
<li>The second approach is cloud-native DBMS.
<ul>
<li>The system is designed explicitly to run in a cloud environment.</li>
<li>Usually based on a shared-disk architecture.</li>
</ul>
</li>
<li>A “serverless” DBMS evicts tenants when they become idle, rather than always maintaining compute resources for each customer.</li>
</ol>
<h2 id="how-to-store-data-of-different-schema"><a class="markdownIt-Anchor" href="#how-to-store-data-of-different-schema"></a> How to store data of different schema?</h2>
<ol>
<li>A Data Lake is a centralized repository for storing large amounts of structured, semi-structured, and un- structured data without having to define a schema or ingest the data into proprietary internal formats.</li>
<li>Data lakes are usually faster at ingesting data, as they do not require transformation right away. Yet, when they are fetching data, they need to look up the catalog before transforming data into the desired schema.</li>
<li>They do require the user to write their own transformation piplines.</li>
</ol>
<h2 id="how-to-share-data-between-systems"><a class="markdownIt-Anchor" href="#how-to-share-data-between-systems"></a> How to share data between systems?</h2>
<ol>
<li>Most DBMSs use a proprietary on-disk binary file format for their databases.</li>
<li>The only way to share data between systems is to convert data into a common text-based format.</li>
<li>There are new open-source binary file formats that make it easier to access data across systems.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/02/Courses/15445/16-Distributed-OLTP-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/02/Courses/15445/16-Distributed-OLTP-Databases/" class="post-title-link" itemprop="url">16 Distributed OLTP Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-02 00:02:00" itemprop="dateCreated datePublished" datetime="2023-09-02T00:02:00+08:00">2023-09-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:19" itemprop="dateModified" datetime="2023-09-26T14:01:19+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#what-is-the-difference-between-oltp-and-olap">What is the difference between OLTP and OLAP?</a></li>
<li><a href="#2-phase-commit-2pc">2-Phase Commit (2PC)</a>
<ul>
<li><a href="#what-is-the-procedure-of-2pc">What is the procedure of 2PC?</a></li>
<li><a href="#what-would-happen-if-some-nodes-crash-during-2pc">What would happen if some nodes crash during 2PC?</a></li>
<li><a href="#how-to-recover-from-crash-during-2pc">How to recover from crash during 2PC?</a></li>
<li><a href="#how-can-we-optimize-2pc-to-reduce-communication">How can we optimize 2PC to reduce communication?</a></li>
<li><a href="#what-is-the-difference-betwen-2pc-and-paxos">What is the difference betwen 2PC and Paxos?</a></li>
</ul>
</li>
<li><a href="#replication">Replication</a>
<ul>
<li><a href="#how-can-we-execute-readwrite-with-replication">How can we execute read/write with replication?</a></li>
<li><a href="#when-should-notify-application-of-result">When should notify application of result?</a></li>
<li><a href="#when-should-propagate-updates-between-nodes">When should propagate updates between nodes?</a></li>
<li><a href="#what-should-be-sent-to-the-followers">What should be sent to the followers?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="what-is-the-difference-between-oltp-and-olap"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-oltp-and-olap"></a> What is the difference between OLTP and OLAP?</h1>
<ol>
<li>OLTP is the front-end databases that communicate and interact with the outside world, e.g. applications. And OLAP is the back-end databases that used to analyze the data in those front-end databases.</li>
<li>OLTP often executes repetitive short-lived read/write txns, while OLAP executes long-running, read-only queries involving complex joins.</li>
<li>Before going into OLAP system, data in OLTP databases need an intermediate step called ETL, or Extract, Transform, and Load, which combines the OLTP databases into a universal schema for the data warehouse.</li>
</ol>
<h1 id="2-phase-commit-2pc"><a class="markdownIt-Anchor" href="#2-phase-commit-2pc"></a> 2-Phase Commit (2PC)</h1>
<h2 id="what-is-the-procedure-of-2pc"><a class="markdownIt-Anchor" href="#what-is-the-procedure-of-2pc"></a> What is the procedure of 2PC?</h2>
<ol>
<li>When application sends a commit request to the coordinator, the coordinator tells other nodes to go into the first phase, prepare phase.</li>
<li>When participants are ready, they will reply to the coordinator with acknowledgement.</li>
<li>If the coordinator receives acknoledgement from all participants, they can begin the second phase, commit phase.
<ul>
<li>And the coordinator will response to application with success after receiving all acknowledgement of commit phase from all participants.</li>
</ul>
</li>
<li>If the coordinator receives abort messages from any of the participants, the coordinator responses to application with abort message and begins the abort phase.
<ul>
<li>As usual, participants need to reply acknowledgement to coordinator in abort phase.</li>
</ul>
</li>
</ol>
<h2 id="what-would-happen-if-some-nodes-crash-during-2pc"><a class="markdownIt-Anchor" href="#what-would-happen-if-some-nodes-crash-during-2pc"></a> What would happen if some nodes crash during 2PC?</h2>
<ol>
<li>If the coordinator crashes, participants must decide what to do after a timeout, either abort or elect a new coordinator.
<ul>
<li>The system is not available during this timeout.</li>
</ul>
</li>
<li>If one of the participants crashes, coordinator assumes that it responded with an abort if it hasn’t sent an acknowledgement yet.
<ul>
<li>Nodes use a timeout to determine that participant is dead.</li>
</ul>
</li>
</ol>
<h2 id="how-to-recover-from-crash-during-2pc"><a class="markdownIt-Anchor" href="#how-to-recover-from-crash-during-2pc"></a> How to recover from crash during 2PC?</h2>
<ol>
<li>Each node records the inbound/outbound messages and outcome of each phase in a non-volatile storage log.</li>
<li>On recovery, examine the log for 2PC messages
<ul>
<li>If local txn in prepared state, contact coordinator.</li>
<li>If local txn not in prepared, abort it.</li>
<li>If local txn was committing and node is the coordinator, send <code>COMMIT</code> message to nodes.</li>
</ul>
</li>
</ol>
<h2 id="how-can-we-optimize-2pc-to-reduce-communication"><a class="markdownIt-Anchor" href="#how-can-we-optimize-2pc-to-reduce-communication"></a> How can we optimize 2PC to reduce communication?</h2>
<ol>
<li>The first method is early prepare voting.
<ul>
<li>If you send a query to a remote node that you know will be the last one you execute there, then that node will also return their vote for the prepare phase with the query result.</li>
<li>This is rare due to rarely write database application with the idea of last query.</li>
<li>However, this can be used in RPC sorts of things where we can sure that this process will terminate after executed certain query.</li>
</ul>
</li>
<li>The second method is early ACK after prepare.
<ul>
<li>If all nodes vote to commit a txn, the coordinator can send the client an acknowledgement that their txn was successful before the commit phase finishes, i.e. send acknowledgement to client after receiving all acknowledgement from participants.</li>
<li>This could cause a small windown of client receiving success, yet cannot see modifications from servers due to commit phase is not finished yet.</li>
</ul>
</li>
</ol>
<h2 id="what-is-the-difference-betwen-2pc-and-paxos"><a class="markdownIt-Anchor" href="#what-is-the-difference-betwen-2pc-and-paxos"></a> What is the difference betwen 2PC and Paxos?</h2>
<ol>
<li>2-phase commit is a degenerate case of Paxos.
<ul>
<li>Paxos uses <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>F</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2F + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> coordinators and makes progress as long as at least <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">F + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> of them are working properly.</li>
<li>2-phase commit sets <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">F = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>.</li>
</ul>
</li>
<li>2-phase commit blocks if coordinator fails after the prepare message is sent, until coordinator recovers.<br />
Paxos remains non-blocking if a majority participants are alive, provided there is a sufficiently long period without further failures.</li>
<li>2-phase commit requires all nodes to agree on the commit while Paxos only requires majority agreement.</li>
<li>In 2-phase commit, each node may have different data, executing different commands. However, in Paxos, all nodes are only replications.</li>
</ol>
<h1 id="replication"><a class="markdownIt-Anchor" href="#replication"></a> Replication</h1>
<h2 id="how-can-we-execute-readwrite-with-replication"><a class="markdownIt-Anchor" href="#how-can-we-execute-readwrite-with-replication"></a> How can we execute read/write with replication?</h2>
<ol>
<li>The first approach is primary-replica.
<ul>
<li>All updates go to a designated primary for each object. The primary propagates updates to its replicas without an atomic commit protocol.</li>
<li>Read-only txns may be allowed to access replicas.</li>
<li>If the primary goes down, then hold an election to select a new primary.</li>
</ul>
</li>
<li>The second approach is multi-primary.
<ul>
<li>Txns can update data objects at any replica.</li>
<li>Replicas must synchronize with each other using an atomic commit protocol.</li>
</ul>
</li>
</ol>
<h2 id="when-should-notify-application-of-result"><a class="markdownIt-Anchor" href="#when-should-notify-application-of-result"></a> When should notify application of result?</h2>
<ol>
<li>When a txn commits on a replicated database, the DBMS decides whether it must wait for that txn’s changes to propagate to other nodes before it can send the acknowledgement to application.</li>
<li>The first propagation level is synchronous, which leads to strong consistency.
<ul>
<li>The primary sends updates to replicas and then waits for them to acknowledge that they fully applied (i.e., logged) the changes before sending acknowledgement to application.</li>
</ul>
</li>
<li>The second propagation level is asynchronous, which leads to eventual consistency.
<ul>
<li>The primary immediately returns the acknowledgement to the client without waiting for replicas to apply the changes.</li>
</ul>
</li>
</ol>
<h2 id="when-should-propagate-updates-between-nodes"><a class="markdownIt-Anchor" href="#when-should-propagate-updates-between-nodes"></a> When should propagate updates between nodes?</h2>
<ol>
<li>The first approach is continuous.
<ul>
<li>The DBMS sends log messages immediately as it generates them.</li>
<li>It also needs to send a commit/abort message.</li>
</ul>
</li>
<li>The second approach is on commit.
<ul>
<li>The DBMS only sends the log messages for a txn to the replicas once the txn is commits.</li>
<li>Do not waste time sending log records for aborted txns.</li>
<li>It assumes that a txn’s log records fits entirely in memory.</li>
</ul>
</li>
</ol>
<h2 id="what-should-be-sent-to-the-followers"><a class="markdownIt-Anchor" href="#what-should-be-sent-to-the-followers"></a> What should be sent to the followers?</h2>
<ol>
<li>The first choice is active-active.
<ul>
<li>A txn executes at each replica independently.</li>
<li>Need to check at the end whether the txn ends up with the same result at each replica.</li>
</ul>
</li>
<li>The second choice is active-passive.
<ul>
<li>Each txn executes at a single location and propagates the changes to the replica.</li>
<li>Can either do physical or logical replication.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/31/Courses/15445/15-Introduction-to-Distributed-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/31/Courses/15445/15-Introduction-to-Distributed-Databases/" class="post-title-link" itemprop="url">15 Introduction to Distributed Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-31 23:53:22" itemprop="dateCreated datePublished" datetime="2023-08-31T23:53:22+08:00">2023-08-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:14" itemprop="dateModified" datetime="2023-09-26T14:01:14+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#system-architecture">System architecture</a>
<ul>
<li><a href="#what-will-system-architecture-affect">What will system architecture affect?</a></li>
<li><a href="#what-is-shared-memory-architecture">What is shared memory architecture?</a></li>
<li><a href="#what-is-shared-disk-architecture">What is shared disk architecture?</a></li>
<li><a href="#what-is-shared-nothing-architecture">What is shared nothing architecture?</a></li>
</ul>
</li>
<li><a href="#design-issues">Design issues</a>
<ul>
<li><a href="#what-are-the-design-issues-of-distributed-database">What are the design issues of distributed database?</a></li>
<li><a href="#what-do-nodes-look-like">What do nodes look like?</a></li>
<li><a href="#how-to-coordinate-execution">How to coordinate execution?</a></li>
</ul>
</li>
<li><a href="#partitioning-schemes">Partitioning Schemes</a>
<ul>
<li><a href="#what-do-we-desire-when-partitioning-database">What do we desire when partitioning database?</a></li>
<li><a href="#how-can-we-partition-database">How can we partition database?</a></li>
<li><a href="#how-can-we-optimize-horizontal-partitioning">How can we optimize horizontal partitioning?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="system-architecture"><a class="markdownIt-Anchor" href="#system-architecture"></a> System architecture</h1>
<h2 id="what-will-system-architecture-affect"><a class="markdownIt-Anchor" href="#what-will-system-architecture-affect"></a> What will system architecture affect?</h2>
<ol>
<li>A distributed DBMS’s system architecture specifies what shared resources are directly accessible to CPUs.</li>
<li>This affects how CPUs coordinate with each other and where they retrieve/store objects in the database.</li>
<li>There are four architectures: shared everything, shared memory, shared disk and shared nothing.</li>
<li>In shared everything architecture, CPU, memories and disks are all in local. This is more of a parallel architecture than a distributed architecture.</li>
</ol>
<h2 id="what-is-shared-memory-architecture"><a class="markdownIt-Anchor" href="#what-is-shared-memory-architecture"></a> What is shared memory architecture?</h2>
<ol>
<li>CPUs have access to common memory address space via a fast interconnect.</li>
<li>Each processor has a global view of all the in-memory data structures.
<ul>
<li>Each process’s scope of memory is the same memory address space, which can be modified by multiple processes.</li>
</ul>
</li>
<li>Each DBMS instance on a processor must “know” about the other instances.</li>
<li>In practice, most DBMSs do not use this architecture, as it is provided at the OS / kernel level.</li>
</ol>
<img src="/imgs/15445/Distributed/shared_mem.png" width="30%">
<h2 id="what-is-shared-disk-architecture"><a class="markdownIt-Anchor" href="#what-is-shared-disk-architecture"></a> What is shared disk architecture?</h2>
<ol>
<li>All CPUs can access a single logical disk directly via an interconnect, but each have their own private memories.</li>
<li>It can scale execution layer independently from the storage layer.
<ul>
<li>The advantage of shared disk over shared nothing is that it can easily scale up with compute layer and storage layer independent.</li>
<li>What we want to persist after crash is in the storage layer.</li>
<li>Theoretically, we can kill or add front-end nodes without losing database or add storage disks / change storage layer with modfying compute nodes.</li>
</ul>
</li>
<li>It must send messages between CPUs to learn about their current state.</li>
<li>This architecture is commonly used in OLAP systems. Many DBSMs begin to think that shared disk architecture is better than shared nothing.</li>
</ol>
<img src="/imgs/15445/Distributed/shared_disk.png" width="30%">
<h2 id="what-is-shared-nothing-architecture"><a class="markdownIt-Anchor" href="#what-is-shared-nothing-architecture"></a> What is shared nothing architecture?</h2>
<ol>
<li>Each DBMS instance has its own CPU, memory, and local disk.</li>
<li>Nodes only communicate with each other via network.
<ul>
<li>When executing a query that requires data from different nodes, the DBMS can either send data up to the node connected with application, or that node can ask another node to execute the query and return the result.</li>
</ul>
</li>
<li>All data in the database are sharded into different nodes.
<ul>
<li>When adding a new node into the architecture, that node is initially empty. The DBMS must re-shard data so that they are distributed evenly.</li>
<li>It is more difficult to increase capacity because the DBMS has to physically move data to new nodes.</li>
<li>It might have small window for queries to receive false positive due to that part of data was in that node and now moving to another node.</li>
</ul>
</li>
<li>It is also difficult to ensure consistency across all nodes in the DBMS, since the nodes must coordinate with each other on the state of transactions.</li>
<li>However, it can potentially achieve better performance and are more efficient then other types of distributed DBMS architectures.</li>
</ol>
<img src="/imgs/15445/Distributed/shared_nothing.png" width="30%">
<h1 id="design-issues"><a class="markdownIt-Anchor" href="#design-issues"></a> Design issues</h1>
<h2 id="what-are-the-design-issues-of-distributed-database"><a class="markdownIt-Anchor" href="#what-are-the-design-issues-of-distributed-database"></a> What are the design issues of distributed database?</h2>
<ol>
<li>How does the application find data?</li>
<li>Where does the application send queries?</li>
<li>How to execute queries on distributed data? Push query to data? Or pull data to query?</li>
<li>How does the DBMS ensure correctness?</li>
<li>How do we divide the database across resources?</li>
</ol>
<h2 id="what-do-nodes-look-like"><a class="markdownIt-Anchor" href="#what-do-nodes-look-like"></a> What do nodes look like?</h2>
<ol>
<li>The first approach is homogenous nodes.
<ul>
<li>Every node in the cluster can perform the same set of tasks albeit on potentially different partitions of data.</li>
<li>Makes provisioning and failover “easier” since any node can replace other nodes.</li>
</ul>
</li>
<li>The second approach is heterogenous nodes.
<ul>
<li>Nodes are assigned specific tasks.</li>
<li>It can allow a single physical node to host multiple “virtual” node types for dedicated tasks.</li>
<li>A design of heterogenous nodes have two kinds of nodes router and config server.
<ul>
<li>Router can directly access shards of database yet it does not know where are that data they want.</li>
<li>Config server knows the data contained in each shards. However, it does not responsible for retrieving them.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="how-to-coordinate-execution"><a class="markdownIt-Anchor" href="#how-to-coordinate-execution"></a> How to coordinate execution?</h2>
<ol>
<li>If our DBMS supports multi-operation and distributed transactions, we need a way to coordinate their execution in the system.</li>
<li>The first approach is centerlized coordinator. There is a centralized coordinator that receives commands from application.
<ul>
<li>The first design requires applications to handle transactions.
<ul>
<li>The client communicates with the coordinator to acquire locks on the partitions that the client wants to access.</li>
<li>Once it receives an acknowledgement from the coordinator, the client sends its queries to those partitions.</li>
<li>Once all queries for a given transaction are done, the client sends a commit request to the coordinator.</li>
<li>The coordinator then communicates with the partitions involved in the transaction to determine whether the transaction is allowed to commit.</li>
</ul>
</li>
<li>Another design uses a middleware to accept query requests and routes queries to correct partitions.</li>
</ul>
</li>
<li>The second approach is decentralized.
<ul>
<li>The client directly sends queries to one of the partitions which will be the leader node of that transaction.</li>
<li>The leader node will coordinate the following communicating with other partitions and committing.</li>
</ul>
</li>
</ol>
<h1 id="partitioning-schemes"><a class="markdownIt-Anchor" href="#partitioning-schemes"></a> Partitioning Schemes</h1>
<h2 id="what-do-we-desire-when-partitioning-database"><a class="markdownIt-Anchor" href="#what-do-we-desire-when-partitioning-database"></a> What do we desire when partitioning database?</h2>
<ol>
<li>Applications should not be required to know where data is physically located in a distributed DBMS.
<ul>
<li>Any query that run on a single-node DBMS should produce the same result on a distributed DBMS.</li>
<li>The DBMS executes query fragments on each partition and then combines the results to produce a single answer.</li>
</ul>
</li>
<li>In practice, developers need to be aware of the communication costs of queries to avoid excessively “expensive” data movement.</li>
<li>The DBMS can partition a database physically for shared nothing or logically for shared disk.
<ul>
<li>In Logical partitioning, each node is responsible for certain designated data. They cannot access data out of their duty. Though data are actually stored in independent storage nodes which computation nodes all can access.</li>
<li>In physical partitioning, data out of their duty cannot be accessed directly since they are stored in the local disk of other nodes.</li>
</ul>
</li>
</ol>
<h2 id="how-can-we-partition-database"><a class="markdownIt-Anchor" href="#how-can-we-partition-database"></a> How can we partition database?</h2>
<ol>
<li>The first method is the naive table partitioning.
<ul>
<li>Assign an entire table to a single node.</li>
<li>It assumes that each node has enough storage space for an entire table.</li>
<li>This is ideal if queries never join data across tables stored on different nodes and access patterns are uniform.</li>
</ul>
</li>
<li>The second method is vertical partitioning.
<ul>
<li>Split a table’s attributes into separate partitions.</li>
<li>It must store tuple information to reconstruct the original record.</li>
</ul>
</li>
<li>The third method is horizontal partitioning.
<ul>
<li>Split a table’s tuples into disjoint subsets based on some partitioning key and scheme.</li>
<li>Choose column(s) that divides the database equally in terms of size, load, or usage.</li>
<li>It can partition based on hashing, ranges, or predicates.</li>
</ul>
</li>
</ol>
<h2 id="how-can-we-optimize-horizontal-partitioning"><a class="markdownIt-Anchor" href="#how-can-we-optimize-horizontal-partitioning"></a> How can we optimize horizontal partitioning?</h2>
<ol>
<li>The main problem is that when adding a new storage nodes, the DBMS needs to reshuffle data.</li>
<li>We can use the consistent hashing.
<ul>
<li>The hashing value is betwen <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> forming a circle.</li>
<li>Each nodes are assigned a value betwen <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>. Data are stored in the node with the closest value to its hashing values going in clock-wise order.</li>
<li>When adding a node, only one node needs to transmit data to the new node, instead of transmit data between all pairs of nodes.</li>
</ul>
</li>
<li>With consistent hashing, we can support replication easily.
<ul>
<li>Just store data in the first batch of nodes with closest value in clock-wise order.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
