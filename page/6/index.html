<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/page/6/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/10/Courses/CS149/11-Memory-Consistency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/10/Courses/CS149/11-Memory-Consistency/" class="post-title-link" itemprop="url">11. Memory Consistency</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-10 14:07:29" itemprop="dateCreated datePublished" datetime="2022-07-10T14:07:29+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:52" itemprop="dateModified" datetime="2024-02-09T12:57:52+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="consistency"><a class="markdownIt-Anchor" href="#consistency"></a> Consistency</h1>
<h2 id="definition"><a class="markdownIt-Anchor" href="#definition"></a> Definition</h2>
<ol>
<li>In a correct behaviored parallel memory hierachy, reading a location should return the latest value written by any thread.<br />
Side-effects of writes are only observable when reads occur, so we will focus on the values returned by reads.</li>
<li>Within a thread, “latest” can be defined by program order. But when it comes across threads, we don’t want it be physical time, because there is no way that the hardware can pull that off. If it takes &gt;10 cycles to communicate between processors, there is no way that processor 0 can know what processor 1 did 2 clock ticks ago.</li>
<li>Writes from any particular thread must be consistent with program order. And writes across threads must be consistent with a valid interleaving of threads.<br />
We define the memory model that each thread proceeds in program order, and memory accesses interleaved (one at a time) to a single-ported memory while rate of progress of each thread is unpredictable.<br />
“Latest” means consistent with some interleaving that matches this model</li>
</ol>
<h2 id="hide-memory-latency"><a class="markdownIt-Anchor" href="#hide-memory-latency"></a> Hide memory latency</h2>
<ol>
<li>Idea: overlap memory accesses with other accesses and computation</li>
<li>“Out of order” pipelining: When an instruction is stuck, perhaps there are subsequent instructions that can be executed.</li>
<li>We don’t need to wait for a conditional branch to be resolved before proceeding. Just predict the branch outcome and continue executing speculatively. If prediction is wrong, squash any side-effects and restart down correct path.</li>
<li>Modern processors fetch and graduate instructions in-order, but issue out-of-order. So intra-thread dependences are preserved, but memory accesses get reordered.</li>
<li>Hiding write latency is simple in uniprocessors, adding a write buffer. But this affects correctness in multiprocessors.<br />
In multiprocessor, write buffer or write-back cache might cause that later writes write earlier to memory, namely accesses issued in order may be observed out of order by other processors.</li>
</ol>
<h1 id="sequential-consistency-sc-model"><a class="markdownIt-Anchor" href="#sequential-consistency-sc-model"></a> Sequential consistency (SC) model</h1>
<ol>
<li>Accesses of each processor in program order, all accesses appear in sequential order. Any order implicitly assumed by programmer is maintained. Any order implicitly assumed by programmer is maintained.</li>
<li>How to implement sequential consistency:<br />
Implement cache coherence: writes to the same location are observed in same order by all processors<br />
For each processor, delay start of memory access until previous one completes, namely each processor has only one outstanding memory access at a time</li>
<li>A read completes when its return value is bound.<br />
A write completes when the new value is “visible” to other processors. “Visible” does not mean that other processors have necessarily seen the value yet. It means the new value is committed to the hypothetical serializable order (HSO).</li>
<li>The strict requirements of SC model severely restrict common hardware and compiler optimizations.<br />
Processor issues accesses one-at-a-time and stalls for completion, which results the Low processor utilization even with caching.</li>
<li>Total store ordering (TSO) model: Comparing to SC model, a read operation doesn’t need to stall for waiting an earlier write operation. This is similar to the architecture with a FIFO write buffer.</li>
<li>Partial store ordering (PSO) model: Comparing to TSO model, even a write operation doesn’t need to stall for waiting an earlier write operation. This is an architecture with a write buffer that doesn’t have to be FIFO.</li>
</ol>
<h1 id="optimization"><a class="markdownIt-Anchor" href="#optimization"></a> Optimization</h1>
<ol>
<li>Most programs don’t require strict ordering (all of the time) for correctness. Here correctness means same results as sequential consistency.</li>
<li>Two accesses conflict if they access same location, and at least one is a write.</li>
<li>We can order accesses by program order (po) and dependence order (do). Operation2 dependents on operation1 if operation2 reads operation1.</li>
<li>Data Race is that two conflicting accesses on different processors, not ordered by intervening accesses.</li>
<li>Properly Synchronized Programs is that all synchronizations are explicitly identified and all data accesses are ordered through synchronization.</li>
<li>Many parallel programs have mixtures of “private” and “public” parts.  The “private” parts must be protected by synchronization,  like locks and unlocks.<br />
Between synchronization operations, we can allow reordering of memory operations as long as intra-thread dependences are preserved.<br />
Just before and just after synchronization operations, thread must wait for all prior operations to complete</li>
<li>MFENCE does not begin until all prior reads &amp; writes from that thread have completed, and no subsequent read or write from that thread can start until after it finishes. Xchg does this implicitly.<br />
MFENCE operation does not push values out to other threads. It simply stalls the thread that performs the MFENCE until write buffer empty.<br />
MFENCE operations create partial orderings that are observable across threads.</li>
<li>In weak ordering model, we put MFENCEs before lock operation and after unlock operation.</li>
<li>Lock operation: only gains (“acquires”) permission to access data. Unlock operation: only gives away (“releases”) permission to access data.<br />
Release Consistency (RC) model make sure writes before the lock or in the critical section completed before exit critical section, and make sure reads/writes in the critical section or after exit critical section don’t access shared state until lock acquired.</li>
<li>LFENCE serializes only with respect to load operations, and SFENCE serializes only with respect to store operations. In practice MFENCE and xchg are the most likely used ones.</li>
<li>Don’t use only normal memory operations for synchronization, like Peterson’s algorithm. Do use either explicit synchronization operations, like xchg (atomic), or fences.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/07/Courses/CS149/10-Snooping-Implementation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/07/Courses/CS149/10-Snooping-Implementation/" class="post-title-link" itemprop="url">10. Snooping Implementation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-07 10:24:14" itemprop="dateCreated datePublished" datetime="2022-07-07T10:24:14+08:00">2022-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:47" itemprop="dateModified" datetime="2024-02-09T12:57:47+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="building-with-an-atomic-bus"><a class="markdownIt-Anchor" href="#building-with-an-atomic-bus"></a> Building with an atomic bus</h1>
<h2 id="transaction"><a class="markdownIt-Anchor" href="#transaction"></a> Transaction</h2>
<ol>
<li>There is a bus controller to do arbitration. If a processor wants to communicate on the bus, it has to make a request. If there are simultaneous requests from multiple processors, the arbiter will only grant one of them.</li>
<li>A transaction on an atomic bus generally need four steps.<br />
Client is granted bus access (result of arbitration). The client places command on bus (may also place data on bus).<br />
Response to command by another bus client placed on bus. Next client obtains bus access (arbitration)</li>
<li>In a multi-processor with atomic bus scenario, no other bus transactions is allowed between issuing address and receiving data when one processor want to read. Also, when flush is occurred, address and data are sent simultaneously, received by memory before any other transaction is allowed.</li>
<li>Both requests from processor and bus require to look the tag on cache.<br />
If bus receives priority, during bus transaction, processor is locked out from its own cache.<br />
If processor receives priority, during processor cache accesses, cache cannot respond with its snoop result. So it delays other processors even if no sharing of any form is present.</li>
<li>We can alleviate contention to allow simultaneous access by processor-side and snoop controllers through cache duplicate tags or multi-ported tag memory. In either case cost of the additional performance is additional hardware resources.<br />
Tags must stay in sync for correctness, so tag update by one controller will still need to block the other controller, but modifying tags is infrequent compared to checking them.</li>
</ol>
<h2 id="read-miss"><a class="markdownIt-Anchor" href="#read-miss"></a> Read miss</h2>
<ol>
<li>When a cache read miss occurred, memory needs to know what to do. If the line is dirty, memory should not respond. And the loading cache needs to know what to do. If the line is shared, cache should load into S state, not E.</li>
<li>If one cache controller find that the line is shared in its cache, the controller will send a message through the “shared” wire on bus. If that line is dirty, the controller will send a message through the “dirty” wire on bus.<br />
Everytime a processor has responded a snoop, the value in “snoop-pending” wire will be lower and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> value indicates that all processors have responded.</li>
<li>Memory controller could immediately start accessing DRAM, but not respond (squelch response). If a snoop result from another cache indicates it has copy of most recent data, then cache should provide data, not memory. Memory could assume one of the caches will service request until snoop results are valid. If snoop indicates no cache has data, then memory must respond</li>
</ol>
<h2 id="write-back"><a class="markdownIt-Anchor" href="#write-back"></a> Write back</h2>
<ol>
<li>Write backs involve two bus transactions, incoming line (line requested by processor) and outgoing line (evicted dirty line in cache that must be flushed).<br />
Ideally would like the processor to continue as soon as possible, namely it shouldn’t have to wait for the flush to complete.</li>
<li>The solution is write-back buffer.<br />
Stick line to be flushed in a write-back buffer. Immediately load requested line to allow processor to continue. Flush contents of write-back buffer at a later time.</li>
<li>If a request of another processor for the address of the data in the write-back buffer appears on the bus, snoop controller must check the write-back buffer addresses in addition to cache tags.<br />
If there is a write-back buffer match, the controller will respond with data from write- back buffer rather than cache and cancel outstanding bus access request.</li>
<li>A write commits when a read-exclusive transaction appears on bus and is acknowledged by all other caches. All future reads will reflect the value of this write, even if data from P has not yet been written to P’s dirty cache line, or to memory.<br />
Order of transactions on the bus defines the global order of writes in the parallel program.</li>
<li>“Commit” is not “complete”. A write completes when the updated value is in the cache line</li>
</ol>
<h2 id="race-conditions"><a class="markdownIt-Anchor" href="#race-conditions"></a> Race conditions</h2>
<ol>
<li>Coherence protocol state transition diagrams assumed that transitions between states were atomic. However, in practice state transitions are not atomic.</li>
<li>We’ve assumed the bus transaction itself is atomic, but all the operations the system performs as a result of a memory operation are not.</li>
<li>Processor, cache, and bus all are resources operating in parallel. They often contend for shared resources: processor and bus contend for cache while caches contend for bus access.</li>
<li>Cache must be able to handle requests while waiting to acquire bus AND be able to modify its own outstanding requests.</li>
<li>To avoid deadlock, processor must be able to service incoming transactions while waiting to issue requests.</li>
<li>To avoid livelock, a write that obtains exclusive ownership must be allowed to complete before exclusive ownership is relinquished.</li>
<li>Multiple processors competing for bus access must be careful to avoid (or minimize likelihood of) starvation.</li>
<li>Performance optimization often entails splitting operations into several, smaller transactions. Splitting costs in more hardware needed to exploit additional parallelism, and care needed to ensure abstractions still hold.</li>
</ol>
<h1 id="building-with-non-atomic-bus"><a class="markdownIt-Anchor" href="#building-with-non-atomic-bus"></a> Building with non-atomic bus</h1>
<ol>
<li>Problem with atomic bus: bus is idle while response is pending, which decreases effective bus bandwidth. The interconnect is a limited, shared resource in a multi-processor system. So it is important to use it as efficiently as possible.</li>
<li>Bus transactions are split into two transactions, the request and the response. Other transactions can intervene between a transaction’s request and response.</li>
<li>Basic design:<br />
Up to eight outstanding requests at a time (system wide)<br />
Responses need not occur in the same order as requests. But request order establishes the total order for the system<br />
Flow control via negative acknowledgements (NACKs). When a buffer is full, client can NACK a transaction, causing a retry</li>
<li>We can think of a split-transaction bus as two separate buses, a request bus and a response bus.<br />
The request bus has lines for command and address. The response bus has lines for data and response tag. Response tag has 3 bits to represent 8 requests.</li>
</ol>
<h2 id="read-miss-2"><a class="markdownIt-Anchor" href="#read-miss-2"></a> Read miss</h2>
<h3 id="phase-1"><a class="markdownIt-Anchor" href="#phase-1"></a> Phase 1</h3>
<ol>
<li>Request arbitration: cache controllers present request for address to bus (many caches may be doing so in the same cycle)</li>
<li>Request resolution: address bus arbiter grants access to one of the requestors. Request table entry allocated for request. Special arbitration lines indicate tag assigned to request.</li>
<li>Bus “winner” places command/address on the bus.</li>
<li>Caches perform snoop: look up tags, update cache state, etc. Memory operation commits here. (no bus traffic)</li>
<li>Caches acknowledge this snoop result is ready, or signal they could not complete snoop in time here.</li>
</ol>
<h3 id="phase-2"><a class="markdownIt-Anchor" href="#phase-2"></a> Phase 2</h3>
<ol>
<li>Data response arbitration: responder presents intent to respond to request with tag T. (many caches or memory may be doing so in the same cycle)</li>
<li>Data bus arbiter grants one responder bus access.</li>
<li>Original requestor signals readiness to receive response (or lack thereof: requestor may be busy at this time)</li>
<li>Then in phase 3, Responder places response data on data bus. Caches present snoop result for request with the data. Request table entry is freed. Those 3 actions can happen parallel.</li>
</ol>
<h2 id="pipelined-transactions"><a class="markdownIt-Anchor" href="#pipelined-transactions"></a> Pipelined transactions</h2>
<ol>
<li>Request bus and response bus can run parallel. So the response of the last transaction and the request of the next transaction can happen simultaneously. Pipelining may cause out-of-order completion.</li>
<li>Write backs and BusUpg transactions do not have a response component. Write backs acquire access to both request address bus and data bus as part of “request” phase BusUpg does not need any acknowledgement or data.</li>
<li>Avoid conflicting requests by disallowing them. Each cache has a copy of the request table. Caches do not make requests that conflict with requests in the request table.</li>
<li>Caches/memory have buffers for receiving data off the bus. If the buffer fills, client NACKs relevant requests or responses.</li>
<li>In parallel system, we use queues to accommodate variable (unpredictable) rates of production and consumption. As long as workers, on average, produce and consume at the same rate, all workers can run at full rate. Otherwise, some will stall waiting for others to accept or produce new input.</li>
<li>We have queues to track requests and responses between L1 and L2 caches and between L2 cache and bus. One queue is for requests and responses from closer (to processor) to farther (L1 to L2 or L2 to bus), and the other is from farther to closer (L2 to L1 or bus to L2).</li>
<li>This may rise deadlock due to full queue. Outgoing read request (initiated by processor) and incoming read request (due to another cache) both requests generate responses that require space in the other queue (circular dependency)</li>
<li>Sizing all buffers to accommodate the maximum number of outstanding requests on bus is one solution to avoiding deadlock. But a costly one.</li>
<li>Avoiding buffer deadlock with separate request/response queues. Namely, we distinguish whether it is a request or a response.<br />
Responses can be completed without generating further transactions. Requests increase queue length But responses reduce queue length. While stalled attempting to send a request, cache must be able to service responses. Responses will make progress, eventually freeing up resources for requests.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/06/Courses/CS149/09-Directory-Based-Cache-Cohurence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/06/Courses/CS149/09-Directory-Based-Cache-Cohurence/" class="post-title-link" itemprop="url">09. Directory-Based Cache Cohurence</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-06 17:10:17" itemprop="dateCreated datePublished" datetime="2022-07-06T17:10:17+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:41" itemprop="dateModified" datetime="2024-02-09T12:57:41+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="problems-to-solve"><a class="markdownIt-Anchor" href="#problems-to-solve"></a> Problems to solve</h1>
<ol>
<li>The snooping cache coherence protocols relied on broadcasting coherence information to all processors over the chip interconnect. Every time a cache miss occurred, the triggering cache communicated with all other caches, so the interconnect has a heavy traffic.</li>
<li>The efficiency of NUMA system does little good if the coherence protocol can’t also be scaled. Processor accesses nearby memory, but to ensure coherence still must broadcast to all other processors.</li>
<li>One possible solution is hierarchical snooping which arranges nodes in a tree and uses snooping coherence at each level. The interconnects involved in a communication is as low as possible and kept as local as possible.</li>
<li>The structure of hierarchical snooping is relatively simple to build.<br />
It uses a tree to reduce the conjunction at the center part, but if the workload is not nicely partitioned, then the root of the network can become a bottleneck.<br />
It also has larger latencies than direct communication and does not apply to more general network topologies (meshes, cubes)</li>
</ol>
<h1 id="directory"><a class="markdownIt-Anchor" href="#directory"></a> Directory</h1>
<ol>
<li>Snooping schemes broadcast coherence messages to determine the state of a line in the other caches. An alternative idea is to avoid broadcast by storing information about the status of the line in one place, namely a “directory”.</li>
<li>A line is a region of memory that would be cached as a single block. One directory entry corresponds to one line of memory.<br />
In a directory entry, there are a dirty bit that indicates line is dirty in one of the processors’ caches and P presence bits that indicate whether processor P has line in its cache.</li>
<li>The directory is used in NUMA system, each processor has a “local” memory and a directory.<br />
Home node of a line is the node with memory holding the corresponding data for the line.<br />
Requesting node is the node containing processor requesting line</li>
</ol>
<h2 id="read-and-write"><a class="markdownIt-Anchor" href="#read-and-write"></a> Read and write</h2>
<ol>
<li>When a read miss happens:<br />
The requesting node will send a read miss message to the home node of the requested line. Then home directory checks entry for line.<br />
If dirty bit for cache line is OFF, the home node will respond with contents from memory, set presence bit of the requesting node to true to indicate line is cached by the requesting processor.<br />
If dirty bit for cache line is ON, the home node will respond with message providing identity of line owner. Requesting node requests data from owner and owner changes state in cache to SHARED (read only), responds to requesting node. Owner also responds to home node, home clears dirty, updates presence bits (line is cached by both the requesting node and owner), updates memory.</li>
<li>When a write miss happens:<br />
The requesting node will send a write miss message to the home node of the requested line.<br />
The home node will response the sharer ids and data to the requesting node.<br />
The requesting node will send invalidation signal to all sharers. After receiving invalidation acks from all sharers, the requesting node can perform write.<br />
The home node will update presence bits (line is cached by only the requesting node) and dirty bit.</li>
<li>On reads, directory tells requesting node exactly where to get the line from, either from home node (if the line is clean) or from the owning node (if the line is dirty). Either way, retrieving data involves only point-to-point communication.</li>
<li>On writes, the advantage of directories depends on the number of sharers. In the limit, if all caches are sharing data, all caches must be communicated with, just like broadcast in a snooping protocol.</li>
<li>In general only a few processors share the line, namely only a few processors must be told of writes.  And the expected number of sharers typically increases slowly with P.</li>
</ol>
<h2 id="reduce-storage-overhead"><a class="markdownIt-Anchor" href="#reduce-storage-overhead"></a> Reduce storage overhead</h2>
<ol>
<li>Full bit vector directory storage is proportional to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">P\times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> is the number of nodes and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> is the number of lines in memory. The storage overhead of directory is too much, and we do not want it to be DRAM since we need it to run fast.</li>
<li>One way to reduce storage overhead is to optimize on full-bit vector.<br />
Increase cache line size to reduce <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> term.<br />
Group multiple processors into a single directory node to reduce <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> term. We could use snooping protocol to maintain coherence among processors in a node, directory across nodes.</li>
<li>Another way is to limit sharer pointer. Since data is expected to only be in a few caches at once, storage for a limited number of pointers per directory entry should be sufficient. Only need a list of the nodes holding a valid copy of the line.</li>
<li>When an overflow in limited pointer schemes occurs, we can revert to broadcast if broadcast mechanism exists. If no broadcast mechanism present on machine, newest sharer replaces an existing one (must invalidate line in the old sharer’s cache)</li>
<li>One more way is through sparse directory.<br />
The majority of memory is not resident in cache. And to carry out coherence protocol the system only needs sharing information for lines that are currently in some cache. So most directory entries are empty most of the time.<br />
We can add a tag for each directory line to indicate the memory held in some cache. The overhead is now <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">P\times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> is the number of lines in each cache.</li>
</ol>
<h2 id="reduce-number-of-message-sent"><a class="markdownIt-Anchor" href="#reduce-number-of-message-sent"></a> Reduce number of message sent</h2>
<ol>
<li>In a read miss to dirty line, there are five network transactions in total. But only four of the transactions are on the critical path.</li>
<li>In intervention forward, home node requests data from owner node (intervention read). After the owner has responded, home node updates directory, and responds to requesting node with data.<br />
Four network transactions in total (less traffic). But all four of the transactions are on the critical path.</li>
<li>In request forwarding, home node requests the owner to sent data to the requesting node. Then the owner will send data to requesting node and home node at the same time.<br />
Four network transactions in total, with only three of the transactions are on the critical path.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/06/19/Courses/CS149/08-Snooping-based-Cache-Coherence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/19/Courses/CS149/08-Snooping-based-Cache-Coherence/" class="post-title-link" itemprop="url">08. Snooping-based Cache Coherence</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-19 19:30:41" itemprop="dateCreated datePublished" datetime="2022-06-19T19:30:41+08:00">2022-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 13:58:37" itemprop="dateModified" datetime="2024-02-09T13:58:37+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="the-cache-coherence-problem"><a class="markdownIt-Anchor" href="#the-cache-coherence-problem"></a> The cache coherence problem</h1>
<ol>
<li>This problem happens in a shared memory multi-processor system. Reading a value at address X should return the last value written to address X by any processor.</li>
<li>This is a problem created by replicating the data stored at address X in local caches (a hardware implementation detail), and cannot be fixed by adding locks.</li>
<li>Memory coherence problem exists because there is both global storage (main memory) and per-processor local storage (processor caches) implementing the abstraction of a single shared address space.</li>
<li>In a cache hierarchy,<br />
L1 and L2 caches are private per core while L3 cahce is shared by cores in the same chip.<br />
L3 cache is split into sectors or banks. Each bank is physically associated with a core, but managed hardware-wise as a single coherent unit.<br />
L2 cache and L3 cache communicate through a ring interconnect where most inter-processor actions happens.</li>
</ol>
<h2 id="uniprocessor-case"><a class="markdownIt-Anchor" href="#uniprocessor-case"></a> Uniprocessor case</h2>
<ol>
<li>On a uniprocessor, providing coherence is fairly simple, since writes typically come from one client: the processor. Load operation must examine all pending stores in store buffer and select the last sequence.</li>
<li>There is one exception on a uniprocessor, which is device I/O via direct memory access (DMA).</li>
<li>One solution to DMA is that CPU writes to shared buffers using uncached stores.<br />
Another way is supported by OS which will mark virtual memory pages containing shared buffers as not-cachable, and explicitly flush pages from cache when I/O completes.</li>
<li>In practice, DMA transfers are infrequent compared to CPU loads and stores (so these heavyweight software solutions are acceptable)</li>
</ol>
<h2 id="coherence-definition"><a class="markdownIt-Anchor" href="#coherence-definition"></a> Coherence definition</h2>
<ol>
<li>Obeys program order as expected of a uniprocessor system: A read by processor P to address X that follows a write by P to address X, should return the value of the write by P (assuming no other processor wrote to X in between)</li>
<li>Write propagation: A read by processor P1 to address X that follows a write by processor P2 to X returns the written value, if the read and write are “sufficiently separated” in time (assuming no other write to X occurs in between)</li>
<li>Write serialization: Writes to the same address are serialized: two writes to address X by any two processors are observed in the same order by all processors.</li>
<li>Write propagation means that notification of a write must eventually get to the other processors. Note that precisely when information about the write is propagated is not specified in the definition of coherence.</li>
</ol>
<h1 id="implementing-choherence"><a class="markdownIt-Anchor" href="#implementing-choherence"></a> Implementing choherence</h1>
<ol>
<li>Software-based solution: OS uses page-fault mechanism to propagate writes. It can be used to implement memory coherence over clusters of workstations</li>
<li>Hardware-based solutions: “snooping”-based coherence implementations and directory-based coherence implementations</li>
<li>Most modern multi-core CPUs implement cache coherence<br />
Discrete GPUs do not implement cache coherence. Overhead of coherence deemed not worth it for graphics and scientific computing applications (NVIDIA GPUs provide single shared L2 + atomic memory operations)<br />
But the latest Intel Integrated GPUs do implement cache coherence</li>
</ol>
<h2 id="shared-caches"><a class="markdownIt-Anchor" href="#shared-caches"></a> Shared caches</h2>
<ol>
<li>One single cache shared by all processors eliminates problem of replicating state in multiple caches and makes coherence easy.</li>
<li>This has obvious scalability problems since the point of a cache is to be local and fast. It also causes interference and contention due to many clients.</li>
<li>Facilitates fine-grained sharing (overlapping working sets). Loads/stores by one processor might pre-fetch lines for another processor</li>
</ol>
<h2 id="snooping-cache-coherence-schemes"><a class="markdownIt-Anchor" href="#snooping-cache-coherence-schemes"></a> Snooping cache-coherence schemes</h2>
<ol>
<li>Main idea: all coherence-related activity is broadcast to all processors</li>
<li>Cache controllers monitor (“they snoop”) memory operations, and react accordingly to maintain memory coherence</li>
<li>Cache controller must respond to actions from both ends:<br />
It must respond the Load/Store requests from its local processor<br />
It also must respond coherence-related activity broadcast over the chip’s interconnect.</li>
<li>The interconnect is between memory and caches possessed by each processor. There is not only memory-cache information, but also cache-cache information, which will limit the scality of the system.</li>
</ol>
<h3 id="write-through-caches"><a class="markdownIt-Anchor" href="#write-through-caches"></a> Write-through caches</h3>
<ol>
<li>For the invalidation-based protocol, when one processor write into an address, cache controller broadcasts<br />
invalidation message for other caches to mark that line to invalidation.<br />
the next read from other processors will trigger cache miss</li>
<li>For the update-based protocol: other caches will update their local copies as the information is sent.</li>
<li>States: Valid (V) or Invalid (I)<br />
A local processor read (PrRd) always ends at valid. If the operation starts from an invalid state, a message will be sent (BusRd). If it starts from a valid state, no message will be sent.<br />
A local processor write (PrWr) always ends at the same state as before the operation (assumes write no-allocate policy), and always sends a message (BusWr).<br />
When a write message from other processor is received (BusWr), It always ends at invalid state.<br />
<img src="/imgs/CS149/08/1.jpeg" width="20%"></li>
<li>Requirements of the interconnect:<br />
All write transactions visible to all cache controllers.<br />
All write transactions visible to all cache controllers in the same order.</li>
<li>Simplifying assumptions here:<br />
Interconnect and memory transactions are atomic<br />
Processor waits until previous memory operations is complete before issuing next memory operation<br />
Invalidation applied immediately as part of receiving invalidation broadcast</li>
</ol>
<h1 id="write-back-caches-invalidation-based"><a class="markdownIt-Anchor" href="#write-back-caches-invalidation-based"></a> Write-back caches (Invalidation-based)</h1>
<ol>
<li>Dirty state of cache line now indicates exclusive ownership</li>
<li>Exclusive: cache is only cache with a valid copy of line (it can safely be written to)<br />
Owner: cache is responsible for supplying the line to other processors when they attempt to load it from memory (otherwise a load from another processor will get stale data from memory)</li>
<li>A line in the “exclusive” state can be modified without notifying<br />
the other caches<br />
Processor can only write to lines in the exclusive state. So they need a way to tell other caches that they want exclusive access to the line. They will do this by sending all the other caches messages<br />
When cache controller snoops a request for exclusive access to line it contains, it must invalidate the line in its own cache</li>
</ol>
<h2 id="msi-write-back-invalidation-protocol"><a class="markdownIt-Anchor" href="#msi-write-back-invalidation-protocol"></a> MSI write-back invalidation protocol</h2>
<ol>
<li>Three cache line states:<br />
Invalid (I): same as meaning of invalid in uniprocessor cache<br />
Shared (S): line valid in one or more caches<br />
Modified (M): line valid in exactly one cache (a.k.a. “dirty” or “exclusive” state)</li>
<li>The local processors have the same operations as write-through case.<br />
The coherence-related bus transactions from remote caches have three kinds:<br />
BusRd: obtain copy of line with no intent to modify<br />
BusRdX: obtain copy of line with intent to modify<br />
flush: write dirty line out to memory<br />
<img src="/imgs/CS149/08/2.png" width="50%"></li>
<li>When try to write an invalid line without reading it, the content of the current modified state line will be sent to the new writer.</li>
<li>Write propagation is achieved via combination of invalidation on BusRdX, and flush from M-state on subsequent BusRd/BusRdX from another processors</li>
<li>Write serialization<br />
Writes that appear on interconnect are ordered by the order they appear on interconnect (BusRdX)<br />
Reads that appear on interconnect are ordered by order they appear on interconnect (BusRd)<br />
Writes that don’t appear on the interconnect (PrWr to line already in M state):
<ul>
<li>Sequence of writes to line comes between two interconnect transactions for the line</li>
<li>All writes in sequence performed by same processor, P (that processor certainly observes them in correct sequential order)</li>
<li>All other processors observe notification of these writes only after a interconnect transaction for the line.</li>
<li>So all processors see writes in the same order.</li>
</ul>
</li>
</ol>
<h2 id="mesi-invalidation-protocol"><a class="markdownIt-Anchor" href="#mesi-invalidation-protocol"></a> MESI invalidation protocol</h2>
<ol>
<li>MSI requires two interconnect transactions for the common case of reading an address, then writing to it
<ul>
<li>Transaction 1: BusRd to move from I to S state</li>
<li>Transaction 2: BusRdX to move from S to M state</li>
</ul>
</li>
<li>Solution: add additional state E (“exclusive clean”) to mark the line that has not been modified, but only this cache has a copy of the line<br />
This state decouples exclusivity from line ownership (line not dirty, so copy in memory is valid copy of data)<br />
Upgrade from E to M does not require an interconnect transaction</li>
<li>
<img src="/imgs/CS149/08/3.jpeg" width="50%">
</li>
</ol>
<h2 id="5-stage-invalidation-based-protocol"><a class="markdownIt-Anchor" href="#5-stage-invalidation-based-protocol"></a> 5-stage invalidation-based protocol</h2>
<ol>
<li>Who should supply data on a cache miss when line is in the E or S state of another cache? <br />
Can get cache line data from memory or can get data from another cache? If source is another cache, which one should provide it?</li>
<li>Cache-to-cache transfers add complexity, but commonly used to reduce both latency of data access and reduce memory bandwidth required by application</li>
<li>MESIF: Like MESI, but one cache holds shared line in F state rather than S (F=”forward”). Cache with line in F state services miss<br />
Simplifies decision of which cache should service miss (basic MESI: all caches respond)<br />
Used by Intel processors</li>
<li>MOESI: Transition from M to O (O=”owned, but not exclusive”) and do not flush to memory (In MESI protocol, transition from M to S requires flush to memory).<br />
Other processors maintain shared line in S state, one processor maintains line in O state. Data in memory is stale, so cache with line in O state must service cache misses.<br />
Used in AMD Opteron</li>
</ol>
<h1 id="invalidation-based-vs-update-based"><a class="markdownIt-Anchor" href="#invalidation-based-vs-update-based"></a> Invalidation-based vs. Update-based</h1>
<ol>
<li>Invalidation-based protocol: To write to a line, cache must obtain exclusive access to it. All other caches must invalidate their copies</li>
<li>Update-based protocol: Can write to shared copy by broadcasting update to all other copies</li>
<li>Intuitively, update would seem preferable if other processors<br />
sharing data continue to access it after a write occurs<br />
But updates are overhead if data just sits in caches (and is never read by another processor again) or application performs many writes before the next read</li>
<li>Update can reduce cache miss rate since all shared copies remain valid.<br />
Update can suffer from high traffic due to multiple writes before the next read by another processor</li>
</ol>
<h1 id="snoop-for-a-cache-hierarchy"><a class="markdownIt-Anchor" href="#snoop-for-a-cache-hierarchy"></a> Snoop for a cache hierarchy</h1>
<ol>
<li>Challenge: changes made to data at L1 cache may not be visible to L2 cache controller than snoops the interconnect.</li>
<li>Inclusion property:<br />
All lines in closer to processor cache are also in farther from processor cache. Thus, all transactions relevant to L1 are also relevant to L2, so it is sufficient for only the L2 to snoop the interconnect.<br />
If line is in owned state (M in MSI/MESI) in L1, it must also be in owned state in L2. Allows L2 to determine if a bus transaction is requesting a modified cache line in L1 without requiring information from L1.</li>
<li>Even if L2 is larger than L1, the inclusion cannot be maintained automatically. L1 and L2 might choose to evict different lines because the access histories differ.</li>
<li>When line X is invalidated in L2 cache due to BusRdX from another cache. Must also invalidate line X in L1<br />
Solution: Each L2 line contains an additional state bit indicating if line also exists in L1. This bit tells the L2 invalidations of the cache line due to coherence traffic need to be propagated to L1.</li>
<li>When L1 write is hit, the corresponding line in L2 cache is in modified state in the coherence protocol, but L2 data is stale.<br />
When coherence protocol requires X to be flushed from L2, L2 cache must request the data from L1.<br />
Add another bit for “modified-but-stale” (flushing a “modified-but-stale” L2 line requires getting the real data from L1 first.)</li>
</ol>
<h1 id="false-sharing"><a class="markdownIt-Anchor" href="#false-sharing"></a> False sharing</h1>
<ol>
<li>Fasle sharing is that two processors write to different addresses, but those addresses map to the same cache line. The cache line keeps invalidating and request data from another processor, generating significant amounts of communication due to the coherence protocol.</li>
<li>We can split the line into two parts and each processor only write one part.</li>
<li>One way is to insert some paddings to make the data written by one processor take up a whole cache line. This can easily implemented in software level. But it causes memory waste.</li>
<li>Another way is mapping addresses handled by the same processor to the same cache line. This causes no waste, but break the successiveness of memory address within a cache line. Also it needs to know the addresses each processor will write, and is harder to implement.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/06/11/Courses/CS149/07-Workload-driven-Perfromance-Evaluation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/11/Courses/CS149/07-Workload-driven-Perfromance-Evaluation/" class="post-title-link" itemprop="url">07. Workload-driven Perfromance Evaluation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-11 21:39:38" itemprop="dateCreated datePublished" datetime="2022-06-11T21:39:38+08:00">2022-06-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:29" itemprop="dateModified" datetime="2024-02-09T12:57:29+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>We should compare parallel program speedup to the best sequential program, instead of parallel algorithm running on one core.<br />
The reason is that to allow for parallelism, we might change the algorithm, and make it slower when executed sequentially.</p>
<h1 id="scaling"><a class="markdownIt-Anchor" href="#scaling"></a> Scaling</h1>
<h2 id="why-consider-scaling"><a class="markdownIt-Anchor" href="#why-consider-scaling"></a> Why consider scaling?</h2>
<ol>
<li>
<p>Arithmetic intensity is determined by both problem size and the processors number. Small problem size or large processor number both yields low arithmetic intensity.</p>
</li>
<li>
<p>If the problem size is too small, it might already execute fast enough on a single core. Scaling the performance of small problem may not be all that important.<br />
Parallelism overheads dominate parallelism benefits, and may even result in slow downs.</p>
</li>
<li>
<p>If the problem size is too large for a single machine, working set may not fit in memory, and causing thrashing to disk. With enough processors, the key working set fits in per-processor cache.<br />
This may get a super-linear speedup and make speedup on a bigger parallel machine with more memory look amazing.</p>
</li>
<li>
<p>Another situation that we might get a super-linear speedup is when we try to search for a solution. With parallelism, we are trying more different variance of search, and more likely to find the solution earlier.</p>
</li>
<li>
<p>So we shouldn’t only consider a fixed problem size. Instead, it is desirable to scale problem size as machine sizes grow.</p>
</li>
<li>
<p>In architecture, scaling up considers how does performance scale with increasing core count, and will design scale to the high end?<br />
Scaling down considers how does performance scale with decreasing core count, and will desing scale to the low end?</p>
</li>
</ol>
<h2 id="different-scalings"><a class="markdownIt-Anchor" href="#different-scalings"></a> Different scalings</h2>
<ol>
<li>
<p>Strong scaling: scaling processors with a fixed problem size. Consider the ratio between the runtime of problem <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> processors and the runtime <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> processor.</p>
</li>
<li>
<p>The goal ratio is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span>. This kind of scaling tells us does having more processors get job done faster?</p>
</li>
<li>
<p>Weak scaling: scaling problem size and processors proportionally. Consider the ratio of the runtime of problem <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>×</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">P\times X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> processors and the runtime of problem <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> processor.</p>
</li>
<li>
<p>The goal ratio is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>. This kind of scaling tells us does having more procesors let me do bigger jobs?</p>
</li>
<li>
<p>Problem size is often determined by more than one parameter. So in weak scaling, we need to consider how should the parameter be changed.</p>
</li>
</ol>
<h2 id="scaling-constrains"><a class="markdownIt-Anchor" href="#scaling-constrains"></a> Scaling constrains</h2>
<ol>
<li>
<p>When scaling a probelm, we should first ask that in my situation, under what constraints should the problem be scaled?</p>
</li>
<li>
<p>Problem-constrained scaling focuses on using a parallel computer to solve the same problem faster<br />
Speedup <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mfrac><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">=\frac{time\ 1\ processor}{time\ P\ processors}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.38888em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.907772em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
<li>
<p>Time-constrained scaling focuses on completing more work in a fixed amount of time<br />
Speedup $ = \frac{work\ done\ by\ P\ processors}{work\ done\ by\ 1\ processor}$</p>
</li>
<li>
<p>“Work done” may not be linear function of problem inputs. One approach of defining “work done” is by execution time of same computation on a single processor (but consider effects of thrashing if problem too big)</p>
</li>
<li>
<p>Ideally, a measure of work is simple to understand and scales linearly with sequential run time (So ideal speedup remains linear in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span>)</p>
</li>
<li>
<p>Memory-constrained scaling (weak scaling) focuses on running the largest problem possible without overflowing main memory. Neither work or execution times are held constant.<br />
Speedup <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mfrac><mi mathvariant="normal">/</mi><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>p</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>o</mi><mi>n</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>p</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>o</mi><mi>n</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">= \frac{work\ (P\ processors)}{time\ (P\ processors)}/\frac{work\ (1\ processor)}{time\ (1\ processor)}=\frac{work\ per\ unit\ time\ on\ P\ processors}{work\ per\ unit\ time\ on\ 1\ processor}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">/</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">t</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">t</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>There are two assumptions: memory resources scale with processor count, and spilling to disk is infeasible behavior.</p>
</li>
</ol>
<h2 id="challenges-of-scaling-down-or-up"><a class="markdownIt-Anchor" href="#challenges-of-scaling-down-or-up"></a> Challenges of scaling down or up</h2>
<ol>
<li>
<p>Preserve ratio of time spent in different program phases.</p>
</li>
<li>
<p>Preserve important behavioral characteristics.</p>
</li>
<li>
<p>Preserve contention and communication patterns. Tough to preserve contention since contention is a function of timing and ratios.</p>
</li>
<li>
<p>Preserve scaling relationships between problem parameters.</p>
</li>
</ol>
<h1 id="simulation"><a class="markdownIt-Anchor" href="#simulation"></a> Simulation</h1>
<ol>
<li>
<p>Architects evaluate architectural decisions quantitatively using<br />
hardware performance simulators.</p>
</li>
<li>
<p>Architect runs simulations with new feature, runs simulations without new feature, compare simulated performance. Or simulate against a wide collection of benchmarks.</p>
</li>
<li>
<p>You can design detailed simulator to test new architectural feature. It would be very expensive to simulate a parallel machine in full detail.<br />
Often cannot simulate full machine configurations or realistic problem sizes (must scale down workloads significantly). Architects need to be confident scaled down simulated results predict reality</p>
</li>
<li>
<p>In trace-driven simulator, we instrument real code running on real machine to record a trace of all memory accesses. Then play back trace on simulator.<br />
It may lead to overfit the trace you have instead of having a better generalization.</p>
</li>
<li>
<p>In execution-driven simulator, we execute simulated program in software. Simulated processors generate memory references, which are processed by the simulated memory hierarchy.<br />
Performance of simulator is typically inversely proportional to level of simulated detail.</p>
</li>
<li>
<p>When dealing with large parameter space of machines (number of processors, cache sizes, cache line sizes, memory bandwidths, etc. ), we can use the architectural simulation state space.</p>
</li>
</ol>
<h1 id="understanding-the-performance"><a class="markdownIt-Anchor" href="#understanding-the-performance"></a> Understanding the performance</h1>
<ol>
<li>
<p>Always, always, always try the simplest parallel solution first, then measure performance to see where you stand.</p>
</li>
<li>
<p>Determine if your performance is limited by computation, memory bandwidth (or memory latency), or synchronization?<br />
Try and establish “high watermarks”. What’s the best you can do in practice? How close is your implementation to a best-case scenario?</p>
</li>
<li>
<p>Roofline model: Use microbenchmarks to compute peak performance of a machine as a function of arithmetic intensity of application. Then compare application’s performance to known peak values.</p>
</li>
<li>
<p>The x-axis means operational intensity (like Flops/Byte), and the y-axis means attenable GFlops/s.<br />
In the diagonal region, the y grows with x, which means the memory bendwidth is limited. In the horizontal region, the y stays the same as x grows, which means the compute is limited.</p>
</li>
<li>
<p>When compute is limited, we can make use of ILP or SIMD, or balance floating-point.<br />
When memory bandwidth is limited, we can limit accesses to unit stride accesses only, develope memory affinity, or use software prefetching.</p>
</li>
</ol>
<h2 id="establish-high-watermarks"><a class="markdownIt-Anchor" href="#establish-high-watermarks"></a> Establish high watermarks</h2>
<ol>
<li>
<p>Add “math” (non-memory instructions).<br />
Does execution time increase linearly with operation count as math is added? If so, this is evidence that code is instruction-rate limited</p>
</li>
<li>
<p>Remove almost all math, but load same data.<br />
How much does execution-time decrease? If not much, suspect memory bottleneck</p>
</li>
<li>
<p>The first two way need to avoid compiler optimization.</p>
</li>
<li>
<p>Change all array accesses to A[0].<br />
How much faster does your code get?<br />
This establishes an upper bound on benefit of improving locality of data access</p>
</li>
<li>
<p>Remove all atomic operations or locks.<br />
How much faster does your code get? (provided it still does approximately the same amount of work)<br />
This establishes an upper bound on benefit of reducing sync overhead.</p>
</li>
</ol>
<h2 id="profilersperformance-monitoring-tools"><a class="markdownIt-Anchor" href="#profilersperformance-monitoring-tools"></a> Profilers/performance monitoring tools</h2>
<ol>
<li>
<p>All modern processors have low-level event “performance counters”, which are registers that count important details such as: instructions completed, clock ticks, L2/L3 cache hits/misses, bytes read from memory controller, etc.</p>
</li>
<li>
<p>Intel’s Performance Counter Monitor Tool provides a C++ API for accessing these registers.</p>
</li>
<li>
<p>It can use <code>getIPC(begin, end)</code>, <code>getL3CacheHitRatio(begin, end)</code>, <code>getBytesReadFromMC(begin, end)</code>, etc. to get values of those information.</p>
</li>
<li>
<p>The <code>begin</code> and <code>end</code> is a <code>SystemCountState</code> instance acquired by <code>getSystemCounterState()</code> at the beginning and end of the code to analyze.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PCM *m = PCM::<span class="built_in">getInstance</span>();</span><br><span class="line">SystemCounterState begin = <span class="built_in">getSystemCounterState</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// code to analyze goes here</span></span><br><span class="line"></span><br><span class="line">SystemCounterState end = <span class="built_in">getSystemCounterState</span>();</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(“Instructions per clock: %f\n”, <span class="built_in">getIPC</span>(begin, end));</span><br><span class="line"><span class="built_in">printf</span>(“L3 cache hit ratio: %f\n”, <span class="built_in">getL3CacheHitRatio</span>(begin, end));</span><br><span class="line"><span class="built_in">printf</span>(“Bytes read: %d\n”, <span class="built_in">getBytesReadFromMC</span>(begin, end));</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/21/Courses/CS149/06-Locality-Communication-and-Contention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/21/Courses/CS149/06-Locality-Communication-and-Contention/" class="post-title-link" itemprop="url">06. Locality, Communication, and Contention</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-21 13:42:23" itemprop="dateCreated datePublished" datetime="2022-05-21T13:42:23+08:00">2022-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:24" itemprop="dateModified" datetime="2024-02-09T12:57:24+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="communication"><a class="markdownIt-Anchor" href="#communication"></a> Communication</h1>
<h2 id="reduce-communication-time"><a class="markdownIt-Anchor" href="#reduce-communication-time"></a> Reduce communication time</h2>
<ol>
<li>
<p>Total communication time = overhead + occupancy + network delay.</p>
</li>
<li>
<p>Overhead is the time spent on the communication by a processor, occupancy is the time for data to pass through slowest component of system, and network delay is everything else.</p>
</li>
<li>
<p>Reduce overhead of communication to sender/receiver:<br />
Reassign tasks in a better way that need to send fewer messages.<br />
Make messages larger to amortize overhead.<br />
Coalesce many small messages into large ones</p>
</li>
<li>
<p>Reduce delay:<br />
Application writer can restructure code to exploit locality.<br />
Hardware implementor can improve communication architecture.</p>
</li>
</ol>
<h2 id="reduce-communication-cost"><a class="markdownIt-Anchor" href="#reduce-communication-cost"></a> Reduce communication cost</h2>
<ol>
<li>
<p>Total communication cost = communication time - overlap</p>
</li>
<li>
<p>Overlap: portion of communication performed concurrently with other work (“other work” can be computation or other communication)</p>
</li>
<li>
<p>Cost is the part that your cannot get over with by changing the protocol. The communication time is obviously necessary, and we can hide the overlap part by pipelining.</p>
</li>
<li>
<p>Increase communication/computation overlap:<br />
Application writer can use asynchronous communication (e.g., async messages)<br />
Hardware implementor can use pipelining, multi-threading, pre-fetching, out-of-order execution<br />
Requires additional concurrency in application (more concurrency than number of execution units)</p>
</li>
<li>
<p>Instruction pipeline: Break execution of each instruction down into several smaller steps.<br />
Enables higher clock frequency, only a simple, short operation is done by each part of pipeline each clock</p>
</li>
<li>
<p>Non-piplined communication: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>T</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>n</mi><mi>B</mi></mfrac></mrow><annotation encoding="application/x-tex">T(n)=T_0+\frac{n}{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">T_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the start-up latency, n is bytes transferred in operation, B is transfer rate or bandwidth)<br />
Efficient bandwidth <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mfrac><mi>n</mi><mrow><mi>T</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">=\frac{n}{T(n)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.215392em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">n</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
</ol>
<h2 id="improve-arithmetic-intensity"><a class="markdownIt-Anchor" href="#improve-arithmetic-intensity"></a> Improve arithmetic intensity</h2>
<ol>
<li>
<p>Communication-to-computation ratio = amount of communication  / amount of computation<br />
The units can be different. If the denominator is the execution time of computation, the ratio gives the average bandwidth requirement</p>
</li>
<li>
<p>Arithmetic intensity = 1 / communication-to-computation ratio</p>
</li>
<li>
<p>Change the traversal order to reduce the time between accesses to same data. We want to do all the calculations related to the accessing data now.<br />
This way can improve the utilization of cache by preventing that those data get flushed out when we try to access them again.</p>
</li>
<li>
<p>Fuse loops to reduce the frequency of store operation.</p>
</li>
<li>
<p>Improve arithmetic intensity by sharing data. Co-locate tasks that operate on the same data.  Schedule threads working on the same data structure at the same time on the same processor. Reduces inherent communication</p>
</li>
</ol>
<h2 id="reduce-artifactual-communication"><a class="markdownIt-Anchor" href="#reduce-artifactual-communication"></a> Reduce artifactual communication</h2>
<ol>
<li>
<p>Inherent communication: Communication that must occur in a parallel algorithm. The communication is fundamental to the algorithm.<br />
Artifactual communication: all other communication that happens because we want to efficiently use resource.</p>
</li>
<li>
<p>Granularity of communication can be important because it may introduce artifactual communication.<br />
Assume that communication granularity is a cache line.</p>
</li>
<li>
<p>If we see data as row-major layout, when the data required is in column, each communication actually only provides one needed data.</p>
</li>
<li>
<p>When Threads access their assigned elements (no inherent communication exists), real machine triggers (artifactual) communication due to the cache line being written to by both processors.</p>
</li>
<li>
<p>Reducing artifactual comm by blocked data layout. Each communication only involves  the data in the same block. If communication granularity is larger than a block row, each communication will transfer multiple rows.</p>
</li>
</ol>
<h1 id="contention"><a class="markdownIt-Anchor" href="#contention"></a> Contention</h1>
<ol>
<li>
<p>Contention occurs when many requests to a resource are made within a small window of time. The resource is a hot spot.<br />
Contention for shared resource results in longer overall operation times.</p>
</li>
<li>
<p>Distributed work queues serve to reduce contention (contention in access to single shared work queue)</p>
</li>
<li>
<p>One way to reduce contention is to use finer-granularity locks. Instead of locking the whole data structure, we can only lock a part of that structure.</p>
</li>
<li>
<p>Another way is that each CUDA block computes partial results and merges them afterwards. But this requires extra work for merging, and each CUDA block need to store a partial result instead of all of them using the same result.</p>
</li>
<li>
<p>The best way is to stagger access to contended resources. For example, instead of calculate the final result directly, we can calculate several temporal results which has no contention, and get the final result from them.</p>
</li>
</ol>
<h1 id="understanding-the-performance"><a class="markdownIt-Anchor" href="#understanding-the-performance"></a> Understanding the performance</h1>
<ol>
<li>
<p>Always, always, always try the simplest parallel solution first, then measure performance to see where you stand.</p>
</li>
<li>
<p>We should compare parallel program speedup to the best sequential program, instead of parallel algorithm running on one core.<br />
The reason is that to allow for parallelism, we might change the algorithm, and make it slower when executed sequentially.</p>
</li>
<li>
<p>Determine if your performance is limited by computation, memory bandwidth (or memory latency), or synchronization?<br />
Try and establish “high watermarks”. What’s the best you can do in practice? How close is your implementation to a best-case scenario?</p>
</li>
<li>
<p>Roofline model: Use microbenchmarks to compute peak performance of a machine as a function of arithmetic intensity of application. Then compare application’s performance to known peak values</p>
</li>
</ol>
<h2 id="establish-high-watermarks"><a class="markdownIt-Anchor" href="#establish-high-watermarks"></a> Establish high watermarks</h2>
<ol>
<li>
<p>Add “math” (non-memory instructions).<br />
Does execution time increase linearly with operation count as math is added?<br />
If so, this is evidence that code is instruction-rate limited</p>
</li>
<li>
<p>Remove almost all math, but load same data.<br />
How much does execution-time decrease?<br />
If not much, suspect memory bottleneck</p>
</li>
<li>
<p>Change all array accesses to A[0].<br />
How much faster does your code get?<br />
This establishes an upper bound on benefit of improving locality of data access</p>
</li>
<li>
<p>Remove all atomic operations or locks.<br />
How much faster does your code get? (provided it still does approximately the same amount of work)<br />
This establishes an upper bound on benefit of reducing sync overhead.</p>
</li>
</ol>
<h2 id="scaling"><a class="markdownIt-Anchor" href="#scaling"></a> Scaling</h2>
<ol>
<li>
<p>Arithmetic intensity is determined by both problem size and the processors number. Small problem size or large processor number both yields low arithmetic intensity.</p>
</li>
<li>
<p>If the problem size is too small, it might already execute fast enough on a single core. Scaling the performance of small problem ay not be all that important.</p>
</li>
<li>
<p>If the problem size is too large for a single machine, working set may not fit in memory, and causing thrashing to disk. With enough processors, the key working set fits in per-processor cache.<br />
This may get a super-linear speedup and make speedup on a bigger parallel machine with more memory look amazing.</p>
</li>
<li>
<p>Desire to scale problem size as machine sizes grow (buy a bigger machine to compute more, rather than just compute the same problem faster)</p>
</li>
<li>
<p>Problem-constrained scaling focuses on using a parallel computer to solve the same problem faster<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>=</mo><mfrac><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Speedup=\frac{time\ 1\ processor}{time\ P\ processors}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.38888em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.907772em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
<li>
<p>Time-constrained scaling focuses on completing more work in a fixed amount of time<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>d</mi><mi>o</mi><mi>n</mi><mi>e</mi><mtext> </mtext><mi>b</mi><mi>y</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>d</mi><mi>o</mi><mi>n</mi><mi>e</mi><mtext> </mtext><mi>b</mi><mi>y</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Speedup = \frac{work\ done\ by\ P\ processors}{work\ done\ by\ 1\ processor}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br />
“Work done” may not be linear function of problem inputs. One approach of defining “work done” is by execution time of same computation on a single processor (but consider effects of thrashing if problem too big)</p>
</li>
<li>
<p>Memory-constrained scaling (wak scaling) focuses on running the largest problem possible without overflowing main memory. Neither work or execution times are held constant.<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mfrac><mi mathvariant="normal">/</mi><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>p</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>o</mi><mi>n</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>p</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>o</mi><mi>n</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Speedup = \frac{work\ (P\ processors)}{time\ (P\ processors)}/\frac{work\ (1\ processor)}{time\ (1\ processor)}=\frac{work\ per\ unit\ time\ on\ P\ processors}{work\ per\ unit\ time\ on\ 1\ processor}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">/</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">t</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">t</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/" class="post-title-link" itemprop="url">05. Graphic processing units and CUDA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-19 09:59:09" itemprop="dateCreated datePublished" datetime="2022-05-19T09:59:09+08:00">2022-05-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 13:51:23" itemprop="dateModified" datetime="2024-02-09T13:51:23+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="graphics"><a class="markdownIt-Anchor" href="#graphics"></a> Graphics</h1>
<ol>
<li>The first step to draw a graphic in screen is to describe the things (key entities) that are manipulated, whose surface is represented as a 3D triangle mesh.<br />
So the input of the calculating system is a list of vertices in 3D space and their connectivity into primitives.</li>
<li>The operations of system is as following:<br />
Given a scene camera position, compute where the vertices lie on screen<br />
Group vertices into primitives<br />
Generate one fragment for each pixel a primitive overlaps<br />
Compute color of primitive for each fragment based on scene lighting and primitive material properties<br />
Put color of the “closest fragment” to the camera in the output image</li>
<li>We can Abstract process of rendering a picture as a sequence of operations on vertices, primitives, fragments, and pixels.<br />
GPUs are very fast processors for performing the same computation (shader programs) on large collections of data (streams of vertices, fragments, and pixels), which sounds like data-parallelism.</li>
<li>To do GPU-based scientific computation, we need to set OpenGL output image size to be output array size, and render 2 triangles that exactly cover screen.</li>
</ol>
<h1 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h1>
<h2 id="grid-block-and-thread"><a class="markdownIt-Anchor" href="#grid-block-and-thread"></a> Grid, Block and Thread</h2>
<ol>
<li>CUDA thread IDs can be up to 3-dimensional. Multiple threads make up a block, and multiple blocks make up a grid.<br />
Each kernel has a grid. All the the blocks in a grid form a 3D matrix, and all the threads in a block also form a 3D matrix.<br />
We can get information about the shape of threads in a block matrix or block in a grid matrix in a block with <code>block_dim</code> and <code>grid_dim</code></li>
<li>When launching the CUDA threads, we need to specify the size of blocks and threads with <code>kernel_function&lt;&lt;&lt;block_dim, thread_dim&gt;&gt;&gt;(parameters)</code>.<br />
Here <code>block_dim</code> and <code>thread_dim</code> can be either a <code>dim3</code> value or a number of the total number of blocks and threads per-block to make the CUDA compiler figure out how to arrange blocks and threads.</li>
<li>So the coordinate to declare or locate a block in a grid, or a thread in a block is 3D. In the CUDA code, we can get the information about current block or thread with <code>blockIdx</code>, <code>threadIdx</code>, and their properties <code>x</code>, <code>y</code>, <code>z</code>.</li>
<li>The calculation object of CUDA is usually matrices. So we would prefer to cut matrices into blocks. Each CUDA block calculate one  block in matrices, and each thread calculate one element in matrices.</li>
<li>In <code>pthread</code>, there is stack space for thread, and need to allocate control block so OS can schedule thread.<br />
Unlike <code>pthread</code>, CUDA control those instances by thread blocks. If control by threads, there will be too many to control.</li>
<li>Major CUDA assumption: thread block execution can be carried out in any order (no dependencies between blocks)</li>
</ol>
<h2 id="kernel-function"><a class="markdownIt-Anchor" href="#kernel-function"></a> Kernel function</h2>
<ol>
<li>“Host” code: serial execution runs as part of normal C/C++ application on CPU<br />
“CUDA device” code: a kernel function runs on GPU, which is denoted by <code>__global__</code> before the definition of the kernel function.<br />
Device function: SPMD execution on GPU, which is denoted by <code>__device__</code>. These functions are called by kernels, and don’t generate new threads.<br />
Kernels and device functions only reference device memory. Host code can only reference host memory, but can have pointers to device memory.</li>
<li>In kernel function, we need to get the indices of the element a thread calculating. For a 2D matrix, we usually take <code>x</code> as column direction, and <code>y</code> as row direction.<br />
So to access <code>A[i][j]</code>, <code>i = blockIdx.y * blockDim * y + threadIdx.y</code> and <code>j = blockIdx.x * blockDim.x + threadIdx.x</code>.</li>
<li>Number of kernel invocations is not determined by size of data collection. So normally we want to do a test before accessing the vector values (array values). The overhead of the test can be ignored, although it does cause some threads are not used.</li>
<li><code>__syncthreads()</code> is a barrier that wait for all threads in the block to arrive at this point.<br />
Atomic operations on both global memory and shared memory variables is also provided, but they are very expensive, should only be used as the last resort.<br />
There is an implicit barrier across all threads at return of kernel.</li>
<li>A compiled CUDA device binary includes program text (instructions) and information about required resources (how many threads per block, how much space per thread, how much shared space per thread block)</li>
</ol>
<h2 id="memory-model"><a class="markdownIt-Anchor" href="#memory-model"></a> Memory Model</h2>
<ol>
<li>Host and device have distinct address spaces, so before the execution, we need to copy data into CUDA memory.<br />
<code>cudaMalloc(ptr, size)</code> can be used to allocate CUDA memory, and <code>cudaFree(ptr)</code> can free CUDA memory. The pointer in <code>cudaMalloc</code> can just be a host variable, but the pointer in <code>cudaFree</code> must be allocated by <code>cudaMalloc</code>.<br />
<code>cudaMemcpy(dest, src, size, kind)</code> can copy those data into CUDA memory. <code>kind</code> is the direction of copy, which can be <code>cudaMemcpyHostToDevice</code> or <code>curdaMemcpyDeviceToHost</code>.</li>
<li>There are three distinct types of memory visible to kernels, per-thread private memory, per-block shared memory, and device global memory.</li>
<li><code>cudaMalloc</code>, <code>cudaMemcpy</code> and <code>cudaFree</code> all operate on device global memory, and those local variables in kernel function are in perthread private memory.<br />
We can declare variables in per-block shared memory with <code>__share__</code>.</li>
<li>If multiple threads in a block need to access a common memory, they will all read that memory once. And if that memory is in the global memory, it would be really slow.</li>
<li>To avoid such efficient decrease, we can copy the common memory into the per-block shared memory. So we only need to access global memory once, and later accesses only in per-block shared memory.<br />
We just need to declare a <code>__share__</code> array, and assign it with values in global memory.</li>
<li>All threads copy data from global memory to per-block shared memory asynchronous, so we better ass a <code>_syncthread()</code> to  make sure later operations only start when all data have been copied.</li>
</ol>
<h2 id="hardware-implementation"><a class="markdownIt-Anchor" href="#hardware-implementation"></a> Hardware implementation</h2>
<ol>
<li>Those synchronization within a warp and scheduling are done by hardware, and programmers don’t need to worry about these things.</li>
<li>GPU implementation maps thread blocks (“work”) to cores using a dynamic scheduling policy that respects resource requirements.</li>
<li>In GPU implementation (not a CUDA abstraction), each SM has multiple groups of warps. However, the number of warp execution contexts is far more than the number of warps.<br />
And there is a warp selector for each warp to choose the instruction to be executed by that warp. Each warp selector usually has two (or more) Fectch/Decoder unit.</li>
<li>Shared per-block memory and L1-cache are inside each SM, but L2-cache is shared by all SMs.<br />
The devide global memory (GPU memory) only communicates with blocks through L2-cache.</li>
<li>Each clock, an SM will choose several warp contexts to fill all warps to use thread-level pallelism, and a warp will choose several instructions to fill all Fetch/Decoder units to use instruction-level pallelism.</li>
<li>When running a CUDA program, GPU work schedulor will map one block to multiple warps in the same SM core. CUDA threads numbered within block in row-major order.<br />
Inside a single thread block is SPMD shared address space programming.</li>
<li>When single warp accesses consecutive memory locations, do block read or write. When single warp accesses separated memory locations, requires gather (read) or scatter(write)</li>
<li>One SM core may have multiple blocks, so a SMM core is capable of concurrently executing multiple CUDA thread blocks.<br />
When all threads in block complete, block resources (shared memory allocations, warp execution contexts) become available for next block.</li>
<li>The CUDA program is not compiled to SIMD instructions like ISPC gangs. GPU hardware is dynamically checking whether 32 independent CUDA threads share an instruction, and if this is true, it executes all 32 threads in a SIMD manner, or performance can suer due to divergent execution.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/" class="post-title-link" itemprop="url">04. Work Distribution and Scheduling</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-17 15:35:33" itemprop="dateCreated datePublished" datetime="2022-05-17T15:35:33+08:00">2022-05-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:21" itemprop="dateModified" datetime="2024-02-09T12:57:21+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="balancing-the-workload"><a class="markdownIt-Anchor" href="#balancing-the-workload"></a> Balancing the workload</h1>
<p>Always implement the simplest solution first, then measure performance to determine if you need to do better.</p>
<h2 id="static-assignment"><a class="markdownIt-Anchor" href="#static-assignment"></a> Static assignment</h2>
<ol>
<li>The assignment of work to threads is pre-determined. But not necessarily determined at compile-time, may depend on runtime parameters.</li>
<li>Benefit: simple, essentially zero runtime overhead</li>
<li>Applicable situation: When the cost (execution time) of work and the amount of work is predictable. The followings are some most common situations:<br />
When it is known up front that all work has the same cost<br />
When work is predictable, but not all jobs have same cost<br />
When statistics about execution time are known</li>
<li>Semi-static assignment: When the cost of work is predictable for near-term future, we can periodically profile itself and re-adjust assignment.<br />
Assignment is “static” for the interval between re-adjustments</li>
</ol>
<h2 id="dynamic-assignment"><a class="markdownIt-Anchor" href="#dynamic-assignment"></a> Dynamic assignment</h2>
<ol>
<li>
<p>Program determines assignment dynamically at runtime to ensure a well distributed load. Often used when the execution time of tasks, or the total number of tasks, is unpredictable.</p>
</li>
<li>
<p>The ISPC task is implemented dynamically.</p>
</li>
</ol>
<h3 id="with-one-queue"><a class="markdownIt-Anchor" href="#with-one-queue"></a> With one queue</h3>
<ol>
<li>
<p>The programmers divide the whole problem into sub-problems (or “tasks”, “work”). A queue shared by all threads is a collection of work to do. Whenever a thread finished its task, it will grab another task from the queue.</p>
</li>
<li>
<p>Fine granularity partitioning: each task is small.<br />
This is likely to have a good workload balance, but potential for high synchronization cost.</p>
</li>
<li>
<p>Coarse granularity partitioning: each task is larger.<br />
This will decrease synchronization cost and the overhead, but may have a worse workload balance.</p>
</li>
<li>
<p>Long tasks should be scheduled first. Thread performing long task performs fewer overall tasks, but approximately the same amount of work as the other threads. This requires some knowledge of workload.</p>
</li>
</ol>
<h3 id="with-a-set-of-queues"><a class="markdownIt-Anchor" href="#with-a-set-of-queues"></a> With a set of queues</h3>
<ol>
<li>
<p>When assign with one queue, all threads have to communicate with each other about the queue.</p>
</li>
<li>
<p>Each thread has their own queue, and they only execute tasks in their queue. So there is no need to communication with other threads.</p>
</li>
<li>
<p>At the beginning, the programmer push tasks into queues arbitarily (a bit like static assignment).<br />
The dynamic is that when a queue is empty, that thread can steal from other threads that is  still working.<br />
It will steal from a random thread. Every time, it will steal a proportion of the tasks in the target queue, not all of them, and usually more than one task.<br />
A thread is terminated when there is no thread for it to steal, namely when a steal is failed, it will try to steal from other threads, util it have tried all of them.</p>
</li>
<li>
<p>Stealing involves communication, but in a lower frequency then one queue method. And in this way, the local queue access is fast.</p>
</li>
<li>
<p>Sometimes it is hard to have fully independent task, but work in task queues need not be independent.<br />
A task is not removed from queue and assigned to worker thread until all task dependencies are satisfied. Workers can submit new tasks (with optional explicit dependencies) to task system</p>
</li>
</ol>
<h1 id="scheduling"><a class="markdownIt-Anchor" href="#scheduling"></a> Scheduling</h1>
<p>In a divide-and-conquer algorithm, there is both dependencies and independencies. Like in quick-sort, both the divide is dependent on the partition, and those two divide is independent.<br />
With Cilk Plus, we can express divide-and-conquer easier.</p>
<h2 id="cilk_spawn"><a class="markdownIt-Anchor" href="#cilk_spawn"></a> cilk_spawn</h2>
<ol>
<li>
<p><code>cilk_spawn</code> is labelled before a function call, so that the called function can run with the code after the call concurrently.<br />
The call labelled <code>cilk_spawn</code> is the spawned child, and the rest of the code is the continuation.</p>
</li>
<li>
<p>In divide-and-conquer, there is always a time, when the problem size is small enough that overhead of spawn trumps benefits of<br />
potential parallelization. And then we will solve those problems sequentially.</p>
</li>
<li>
<p>The main idea is to expose independent work (potential parallelism) to the system using <code>cilk_spawn</code>.</p>
</li>
<li>
<p><code>cilk_spawn</code> is a bit like <code>pthread_create</code>, and <code>cilk_sync</code> is similar to <code>pthread_join</code>. But <code>pthread</code> has some problems when too many threads are spawned.<br />
The first is the heavyweight spawn operation. And many more concurrently running threads than cores, which will cause context switching overhead and larger working set than necessary, less cache locality.</p>
</li>
<li>
<p>The Cilk Plus runtime maintains pool of worker threads. All threads created at application launch.  There are exactly as many worker threads as execution contexts in the machine.<br />
If we labelled everything <code>cilk_spawn</code>, the main thread has nothing to do.</p>
</li>
<li>
<p>Each thread in the pool will maintain a work queue to store what word needs to be done.<br />
When a thread goes idle, it will look in busy thread’s queue for work, and moves work from busy thread’s queue to its own queue.</p>
</li>
<li>
<p>If caller thread runs the continuation first, the queue should record the child for later execution, and child is made available for stealing by other threads.<br />
In this method, caller thread will spawn as many child as it could, like BFS. If no stealing, execution order is very different than<br />
that of program withcilk_spawnremoved.</p>
</li>
<li>
<p>If caller thread runs the child first, the queue should record continuation for later execution, and continuation is made available for stealing by other threads.<br />
In this method, caller thread will only create one item to steal.<br />
If no stealing occurs, thread continually pops continuation from work queue, enqueues new continuation (like DFS). The order of execution is the same as for program with spawn removed.<br />
If continuation is stolen, stealing thread spawns next child.</p>
</li>
<li>
<p>If the continuation is run first, there will be more items to steal, and thus makes a better advantage of multi-thread.<br />
But if the continuation is run first, the work queue storage for system with T threads is no more than T times that of stack storage for single threaded execution, and thus saves more space.</p>
</li>
<li>
<p>Work queue is implemented as a dequeue (double ended queue).<br />
Local thread pushes/pops from the “tail” (bottom), while remote threads steal from “head” (top).<br />
Reduces contention with local thread: local thread is not accessing same part of dequeue that stealing threads do.<br />
Do larger work first: in divide-and-conquer, the top of the queue is usually at the beginning of call tree, and is a larger piece of work.<br />
Maximizes locality: in conjunction with run-child-first policy, local thread works on local part of call tree</p>
</li>
</ol>
<h2 id="cilk_sync"><a class="markdownIt-Anchor" href="#cilk_sync"></a> cilk_sync</h2>
<ol>
<li>
<p><code>cilk_sync</code> is used after those <code>cilk_spawn</code> call code. It will return when all calls spawned by current function have completed.</p>
</li>
<li>
<p>There is an implicitcilk_syncat the end of every function that contains a cilk_spawn, so when a Cilk function returns, all work associated with that function is complete.</p>
</li>
<li>
<p>If no work has been stolen by other threads, then there’s nothing to do at the sync point, <code>cilk_sync</code> is a no-op. But this is not a common situation.</p>
</li>
<li>
<p>One way to implement sync is with stalling joint.<br />
Thread that initiates the fork must perform the sync. Therefore it waits for all spawned work to be complete.<br />
descriptor for block A created<br />
When a stealing from the initial thread happens, a descriptor for that stolen work is created to track the number of outstanding spawns for the block, and the number of those spawns that have completed.<br />
When all the child spowned for the block is done, this block is considered done, and the descriptor is free. When all the blocks are done, sync is fullfilled.</p>
</li>
<li>
<p>Another way to implement sync is the greedy policy.<br />
When thread that initiates the fork goes idle, it can look to steal new work.  Last thread to reach the join point continues execution after sync. This will also create a decriptor for those stolen work.<br />
Worker thread that initiated spawn may not be thread that executes logic after <code>cilk_sync</code>.</p>
</li>
<li>
<p>In greedy policy, All threads always attempt to steal if there is nothing to do, and thread only goes idle if no work to steal is present in system. But in stalling policy, the initial thread doesn’t steal, and only waits when its work is done.</p>
</li>
<li>
<p>Overhead of bookkeeping steals and managing sync points only occurs when steals occur. If large pieces of work are stolen, this should occur infrequently. Most of the time, threads are pushing/popping local work from their local dequeue.</p>
</li>
<li>
<p>Cilk uses greedy join scheduling.</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/04/20/Courses/CS149/03-Parallel-Programming-Basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/20/Courses/CS149/03-Parallel-Programming-Basics/" class="post-title-link" itemprop="url">03. Parallel Programming Basics</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-20 14:27:45" itemprop="dateCreated datePublished" datetime="2022-04-20T14:27:45+08:00">2022-04-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:16" itemprop="dateModified" datetime="2024-02-09T12:57:16+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="decomposition"><a class="markdownIt-Anchor" href="#decomposition"></a> Decomposition</h1>
<ol>
<li>
<p>Decomposition: The problem to solve is usually a chunk of work. So the first step we need to do is to decomposite them into subproblems (a.k.a tasks, work to do)</p>
</li>
<li>
<p>We usually want the number subproblems to be at least as many as processors.</p>
</li>
<li>
<p>The key aspect of decomposition is to identify dependencies. We want subproblems to be independent, so that they can be paralleled.</p>
</li>
<li>
<p>Amdahl’s Law: dependencies limit maximum speedup due to parallelism<br />
Let S = the fraction of sequential execution that is inherently sequential (dependencies prevent parallel execution). Then maximum speedup due to parallel execution ≤ 1/S.</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>≤</mo><mfrac><mi>t</mi><mrow><mi>s</mi><mi>t</mi><mo>+</mo><mfrac><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>s</mi><mo stretchy="false">)</mo><mi>t</mi></mrow><mi>p</mi></mfrac></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mrow><mi>s</mi><mo>+</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>s</mi></mrow><mi>p</mi></mfrac></mrow></mfrac></mrow><annotation encoding="application/x-tex">speedup \le\frac{t}{st+\frac{(1-s)t}{p}}=\frac1{s+\frac{1-s}{p}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.6731879999999997em;vertical-align:-1.381108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.29208em;"><span style="top:-2.11em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.2399999999999998em;"><span class="pstrut" style="height:3.01em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.687em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord"><span class="mord mathnormal">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.381108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.537656em;vertical-align:-1.216216em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.264892em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.216216em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li>
<p>In most cases, the programmer is responsible for performing decomposition.</p>
</li>
<li>
<p>When we are doing the decomposition, it is better to think of partitioning computation instead of data.</p>
</li>
</ol>
<h1 id="assignment"><a class="markdownIt-Anchor" href="#assignment"></a> Assignment</h1>
<ol>
<li>
<p>Assignment: When the subproblems is more than processors, we then will group them together to form a larger chunk of work, and assign the grouped tasks to parallel threads.</p>
</li>
<li>
<p>One goal is to balance workload, so that each processor finish their work almost at the same time.<br />
Another goal is to reduce communication costs. Getting data from another processor is nontrivial expensive, either cache miss or waiting for a message.<br />
These two goals are at odds with each other.</p>
</li>
<li>
<p>This step can be performed statically, or dynamically during execution<br />
Static way: Before the processors begin to do the work, we have already decide how to divide things up.<br />
Dynamic way: We work out the way to divide on the way as processing.</p>
</li>
<li>
<p>We can choose the static way with the programCount and programIndex assignment or pthread</p>
</li>
<li>
<p>We can choose the dynamic way with foreach if the system choose the dynamic way, or the queue<br />
In the queue way, we arrange all tasks in a queue, and when a processor has done its job, it will grab another task from the queue. This is a good way for balancing workload, but maintain the queue and acquire task from it might cost some performance.</p>
</li>
</ol>
<h1 id="orchestration"><a class="markdownIt-Anchor" href="#orchestration"></a> Orchestration</h1>
<ol>
<li>
<p>Orchestration: When parallel threads are running, they may need to coorperate with each other. So we need to let them communicate correctly.</p>
</li>
<li>
<p>There are things that we will worry about, like structure communication, synchronization, orgnizing data structure in memory, and scheduling tasks</p>
</li>
<li>
<p>The goal is to reduce costs of communication/sync, preserve locality of data reference, reduce overhead of synchronization or communication, etc.</p>
</li>
<li>
<p>In shared address space model, lock/unlock is common used for preserving atomicity, and barriers can divide computation into phases. When threads execute to barriers, they will stop and wait. Until enough number of threads hit the barrier, those stalled threads are allowed to execute rest codes.</p>
</li>
<li>
<p>A common used optimization strategy is that try to use less lock/unlock and barriers.<br />
Everytime when we operate a shared variable, we need to use the lock/unlock to keep atomicity. But maybe we can operate on partial variable locally and then merge the partial results together.<br />
When we use barrier to keep a shared variable valid when it might be changed at the next phase, we can use different shared variable in successive phases. This is to trade off footprint for removing dependencies.</p>
</li>
</ol>
<h1 id="mapping"><a class="markdownIt-Anchor" href="#mapping"></a> Mapping</h1>
<ol>
<li>
<p>Mapping: Finally, we need to map each thread to physical hardware.</p>
</li>
<li>
<p>When we map pthreads to hardware execution context on a CPU core, this is done by the operating system<br />
When we map ISPC program instances to vector instruction lanes, this is done by the compiler<br />
When we map CUDA thread blocks to GPU cores, this is done by the hardware</p>
</li>
<li>
<p>Place related threads (cooperating threads) on the same processor to maximize locality, data sharing, minimize costs of comm/sync<br />
Place unrelated threads on the same processor (one might be bandwidth limited and another might be compute limited) to use machine more efficiently</p>
</li>
<li>
<p>Normally, we just let the OS do the mapping as it want. But sometimes we still want to control the way of mapping.</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/04/18/Courses/CS149/02-Parallel-programming-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/18/Courses/CS149/02-Parallel-programming-models/" class="post-title-link" itemprop="url">02. Parallel programming models</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-18 11:53:04" itemprop="dateCreated datePublished" datetime="2022-04-18T11:53:04+08:00">2022-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:56:58" itemprop="dateModified" datetime="2024-02-09T12:56:58+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ispc-intel-spmd-program-compiler"><a class="markdownIt-Anchor" href="#ispc-intel-spmd-program-compiler"></a> ISPC (Intel SPMD Program Compiler)</h1>
<h2 id="format-of-ispc"><a class="markdownIt-Anchor" href="#format-of-ispc"></a> Format of ISPC</h2>
<ol>
<li>
<p>ISPC is an SPMD compiler, not an SIMD.</p>
</li>
<li>
<p>The code that need to be paralleled will be written in a file with surfix of “.ispc” as a function. And we will call that function in the main.cpp.<br />
Call to ISPC function spawns gang of ISPC program instances. All instances run ISPC code concurrently. The ISPC function will return when all instances have completed.<br />
All code in main.cpp will be executed sequentially.</p>
</li>
<li>
<p><code>programCount</code>: the number of simultaneously executing instances in the gang. It is the same for all instances, thus called “uniform value”. This is not set by programmer, but by the run-time system. programmer can only read it but don’t set that.</p>
</li>
<li>
<p><code>programIndex</code>: the ID of the current instance in the gang. It is a non-uniform value, namely varying.<br />
This is used to assign work to each instance. If we don’t use the programIndex to control the work to be done by each instance, they will all do all the work. Thus there will be redundant and have no performance improve.</p>
</li>
<li>
<p><code>uniform</code>: a type modifier. All instances have a copy of the same value for this variable. Its use is purely an optimization. Not needed for correctness.<br />
We cannot add a non-uniform value to a uniform value directly, which will cause a compile-time type error. In order to do so, we need a reduce_add function from ISPC library.</p>
</li>
<li>
<p>Those ISPC program instances is not separate threads. The ISPC complier actually generates an SIMD thread. So the programCount is the vector width of the machine.<br />
So it can only run on one core. And “task” is used to achieve multi-core execution.</p>
</li>
</ol>
<h2 id="ways-of-assignment"><a class="markdownIt-Anchor" href="#ways-of-assignment"></a> Ways of assignment</h2>
<ol>
<li>
<p>Interleaved assignment: the data processed by each instance is discontinuous, namely the subsctipt is:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>I</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi><mo>+</mo><mi>i</mi><mo>×</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo separator="true">,</mo><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mfrac><mrow><mi>N</mi><mo>−</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>I</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi></mrow><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">programIndex+i\times programCount,i\in[0,\frac{N-programIndex}{programCount}) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.74285em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.25188em;vertical-align:-0.8804400000000001em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p>
</li>
<li>
<p>Block assignment: the data is split into several chunks, and each instance process one chunk. So the data is continuous, namely the subscript is:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>I</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi><mo>×</mo><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>+</mo><mi>i</mi><mo separator="true">,</mo><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>=</mo><mfrac><mi>N</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">programIndex\times count+i, i\in[0,count),count=\frac{N}{programCount}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69841em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.2407700000000004em;vertical-align:-0.8804400000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li>
<p>When using the block assignment, there might exist some situation when the cost of processing later data is more expensive than processing former data, which will make the assignment uneven. So the interleaved assignment is less risky.</p>
</li>
<li>
<p>Since ISPC only generate one thread, the continuous access of data in block assignment is not more cache-friend then the discontinuous access in interleaved assignment.<br />
If there are several threads, than block assignment might be more cache-friend.</p>
</li>
<li>
<p>The data requested by all instances at the same time is a vector. In interleaved assignment, the data in a vector is memory continuous, while it is not in block assignment. So in memory access, interleaved assignment is faster.</p>
</li>
<li>
<p><code>foreach (i = 0 ... N)</code> says that these loop iterations can be paralleled, and ISPC implementation assigns iterations to program instances in gang. Current ISPC implementation will perform a static interleaved assignment</p>
</li>
</ol>
<h2 id="system-layers"><a class="markdownIt-Anchor" href="#system-layers"></a> System layers</h2>
<ol>
<li>
<p>The layers from up to down is that parallel applications, compilers and/or parallel runtime, operating system, Micro-architecture (hardware implementation)</p>
</li>
<li>
<p>Different parallel models can have different combinations of concerned layers.</p>
</li>
<li>
<p>If we express parallelism with pthread, it goes through all layers. First the parallel application calls <code>pthread_create()</code> to access pthread library implementation. Then the pthread library uses <code>System call API</code> to ask OS support: kernel thread management. Finally, the OS uses <code>x86-64</code> to control modern multi-core CPU.</p>
</li>
<li>
<p>If we express parallelism with ISPC without “task”, it doesn’t need the support of OS. The parallel application uses ISPC language to ask for service of ISPC compiler. And the complier will produce machine language of x86-64 including <code>AVX vector instruction</code> to control a single-core of CPU.</p>
</li>
</ol>
<h1 id="communication"><a class="markdownIt-Anchor" href="#communication"></a> Communication</h1>
<h2 id="shared-address-space"><a class="markdownIt-Anchor" href="#shared-address-space"></a> Shared Address Space</h2>
<ol>
<li>
<p>The whole machine has a common space address. When threads aren’t working together, they just access their memory space. They can communicate with each by reading or writing the same data and manipulating synchronization primitives (like locks)</p>
</li>
<li>
<p>This model requires hardware support to implement efficiently. In hardware implementation, any processor can directly reference any memory location</p>
</li>
<li>
<p>Symmetric (shared-memory) multi-processor (SMP): all processors have uniform memory access time, namely the cost of accessing an uncached memory address is the same for all processors<br />
This is unscalarable since the access latency will increase fast with more and more processors and memory chips.<br />
The cores can share memory through a shared L3 cahce or a crossbar switch with die area of one core</p>
</li>
<li>
<p>Non-uniform memory access (NUMA): All processors can access any memory location, but the cost of memory access (latency and/or bandwidth) is different for different processors. Each processor has a memory chip that is close to it.<br />
This is more scalarable since the low latency and high bandwidth to local memory.<br />
Cost is the increased programmer effort for performance tuning. Finding, exploiting locality is important to performance (want most memory accesses to be to local memories)</p>
</li>
</ol>
<h2 id="message-passing"><a class="markdownIt-Anchor" href="#message-passing"></a> Message Passing</h2>
<ol>
<li>
<p>Threads operate within their own private address spaces, and they communicate by sending/receiving messages</p>
</li>
<li>
<p>send: specifies recipient, buffer to be transmitted, and optional message identifier (“tag”)<br />
receive: sender, specifies buffer to store data, and optional message identifier</p>
</li>
<li>
<p>With this model, we can easily build large scale of parallel machine by just interconnect them together. Hardware need not implement system-wide loads and stores to execute message passing programs, need only be able to communicate messages<br />
But the interconnect speed can be the bottleneck of the system.</p>
</li>
<li>
<p>We can implement the message passing model with shared memory space.<br />
Sending message is copying memory from message library buffers, while receiving message is copying data from message library buffers</p>
</li>
<li>
<p>We can also implement shared address space abstraction on machines that do not support it in hardware via less efficient software solution.<br />
Mark all pages with shared variables as invalid at first, and page-fault handler issues appropriate network requests</p>
</li>
<li>
<p>Synchronous (blocking) send and receive<br />
Send(): call returns when sender receives acknowledgement that message data resides in address space of receiver<br />
Recv(): call returns when data from received message is copied into address space of receiver and acknowledgement sent back to sender<br />
So when using message passing model, we need to be careful for the order of send() and recv() in each threads because it may easily raise a deadlock.<br />
If the first call of all threads is send(), then no one is receiving and all is waiting someone to receive what they have sent.<br />
One common way to program is to arrage one thread to send first and the receiver of that send to receive first.</p>
</li>
<li>
<p>Non-blocking asynchronous send/recv<br />
Send() call returns immediately, while recv() posts intent to receive in the future and returns immediately.<br />
We can use checksend(), checkrecv() to determine actual status of send/receipt.<br />
Buffer provided to send() cannot be modified by calling thread since message processing occurs concurrently with thread execution</p>
</li>
</ol>
<h2 id="data-parallel"><a class="markdownIt-Anchor" href="#data-parallel"></a> Data Parallel</h2>
<ol>
<li>
<p>Data parallel has a very rigid computation struture. If it works well, it will work very well. But sometimes it just won’t work.</p>
</li>
<li>
<p>Nowadays, data parallel usually takes the form of SPMD instead of SIMD. Programs perform same function on different data elements in a collection<br />
<code>map(function, collection)</code>: Synchronization is implicit at the end of the map. Map returns when function has been applied to all elements ofcollection</p>
</li>
<li>
<p>When the function is too complicated, the result might be non-deterministic.<br />
Data-parallel model (foreach) provides no specification of order in which iterations occur and no primitives for fine-grained mutual exclusion/synchronization.</p>
</li>
<li>
<p>Streams: collections of elements. Elements can be processed independently.<br />
Kernels: side-effect-free functions. Operate element-wise on collections.</p>
</li>
<li>
<p>A stream can be claimed by <code>stream&lt;ElemType&gt; name(N)</code><br />
When define the kernel function, its parameters are single elements instead of a whole collection. But when call the kernel function, we pass the streams into the function directly.</p>
</li>
<li>
<p>Benefits of stream programming:<br />
Functions really are side-effect free (cannot write a non-deterministic program)<br />
Program data flow is known by compiler: Inputs and outputs of each invocation are known in advance and thus prefetching can be employed to hide latency.<br />
When there are multiple kernels, and the output of the last kernel is the input of the next kernel, producer-consumer locality is known in advance.<br />
Implementation can be structured so outputs of first kernel are immediately processed by second kernel. The values are stored in on-chip buffers/caches and never written to memory, which saves bandwidth.<br />
These optimizations are responsibility of stream program compiler. Requires global program analysis.</p>
</li>
<li>
<p>Drawback of stream programming: Need library of operators to describe complex data flows, so it might go wrong.</p>
</li>
<li>
<p><code>stream_gather(input, indices, tmp_input)</code>: Put elements in input to tmp_input according to indices. This is called before the kernel function, and the kernel function will deal with gathered stream.<br />
<code>stream_scatter(tmp_output, indices, output)</code>: Similar to stream_gather, but it is called after the kernel function, and kernel function will deal with original stream.<br />
The parameters of both function are all stream.</p>
</li>
</ol>
<h2 id="synchronization-and-communication"><a class="markdownIt-Anchor" href="#synchronization-and-communication"></a> Synchronization and Communication</h2>
<ol>
<li>
<p>In shared address space, mutual exclusion is required for shared variables, and barriers are used to express dependencies between phases of computation.<br />
They can communicate through implicit loads/stores to shared variables.</p>
</li>
<li>
<p>In message passing model, the synchronizations and communications are both performed by sending and receiving messages.</p>
</li>
<li>
<p>In data parallel model, a single logical thread is in control, but iterations of forall loop may be parallelized by the system. There is an implicit barrier at end offorallloop body.<br />
They can also communicate throught implicit loads and stores, like shared address space. There is also some special built-in primitives for more complex communication patterns, e.g., reduce</p>
</li>
</ol>
<h2 id="modern-practice"><a class="markdownIt-Anchor" href="#modern-practice"></a> Modern practice</h2>
<ol>
<li>
<p>Use shared address space programming within a multi-core chip of a cluster, use message passing between chips<br />
Use convenience of shared address space where it can be implemented efficiently (within a chip), require explicit communication elsewhere.</p>
</li>
<li>
<p>Data-parallel-ish programming models support shared-memory style synchronization primitives in kernels. This could permit limited forms of inter-iteration communication.</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/7/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
