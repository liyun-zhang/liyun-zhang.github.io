<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/20/Courses/15445/11-Timestamp-Ordering-Concurrency-Control/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/20/Courses/15445/11-Timestamp-Ordering-Concurrency-Control/" class="post-title-link" itemprop="url">11 Timestamp Ordering Concurrency Control</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-20 23:59:40" itemprop="dateCreated datePublished" datetime="2023-08-20T23:59:40+08:00">2023-08-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-08-21 21:11:50" itemprop="dateModified" datetime="2023-08-21T21:11:50+08:00">2023-08-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="T-O-Protocols"><a href="#T-O-Protocols" class="headerlink" title="T/O Protocols"></a>T/O Protocols</h1><h2 id="What-is-the-difference-between-2PL-and-T-O"><a href="#What-is-the-difference-between-2PL-and-T-O" class="headerlink" title="What is the difference between 2PL and T/O?"></a>What is the difference between 2PL and T/O?</h2><ol>
<li>2PL determine serializability order of conflicting operations at runtime while transactions execute while T/O determine serializability order of transactions before they execute. <ul>
<li>If $TS(T_i) &lt; TS(T_j)$, then the DBMS must ensure that the execution schedule is equivalent to a serial schedule where $T_i$ appears before $T_j$. </li>
<li>Different schemes assign timestamps at different times during the transaction. </li>
<li>Timestamps can be implemented by different strategies: system/wall clock, logical counter, or hybrid. </li>
</ul>
</li>
<li>2PL is in a manner of pessimistic way. It assumes that conflicts between transactions are very common. <ul>
<li>Hence it uses locks to prevent conflicts. </li>
</ul>
</li>
<li>Timestamp  ordering (T/O) is a more optimistic way. It assumes that conflicts between transactions are rare. <ul>
<li>Hence it allows each transaction to execute all operations they want and validates their legitimate after each transaction committing and before applying anything to main database. </li>
</ul>
</li>
</ol>
<h2 id="Basic-T-O-protocol"><a href="#Basic-T-O-protocol" class="headerlink" title="Basic T/O protocol"></a>Basic T/O protocol</h2><h3 id="What-is-the-main-idea-of-basic-T-O-protocol"><a href="#What-is-the-main-idea-of-basic-T-O-protocol" class="headerlink" title="What is the main idea of basic T/O protocol?"></a>What is the main idea of basic T/O protocol?</h3><ol>
<li>Txns read and write objects without locks.</li>
<li>The timestamp of each transaction is assigned at the <code>BEGIN</code> command. </li>
<li>Every object $X$ is tagged with timestamp of the last transaction that successfully did read/write<ul>
<li>$W-TS(X)$: Write timestamp on $X$</li>
<li>$R-TS(X)$: Read timestamp on $X$</li>
</ul>
</li>
<li>Check timestamps for every operation. If transaction tries to access an object “from the future”, it aborts and restarts. </li>
</ol>
<h3 id="How-does-basic-T-O-check-each-operation"><a href="#How-does-basic-T-O-check-each-operation" class="headerlink" title="How does basic T/O check each operation?"></a>How does basic T/O check each operation?</h3><ol>
<li>When $T_i$ wants to read $X$:<ul>
<li>If $TS(T_i)&lt;W-TS(X)$, abort $T_i$ and restart it with a new TS to prevent it from starvation. <ul>
<li>This condition means that this $T_i$ is trying to read something from the future. </li>
</ul>
</li>
<li>Else, allow $T_i$ to read $X$, and update $R-TS(X)$ to $max(R-TS(X), TS(T_i))$. </li>
<li>A local copy of $X$ is made to ensure repeatable reads for $T_i$. </li>
</ul>
</li>
<li>When $T_i$ wants to write $X$:<ul>
<li>If $TS(T_i)&lt;R-TS(X)$ or $TS(T_i)&lt;W-TS(X)$, abort and restart $T_i$. <ul>
<li>The first condition means that another transaction from the future cannot see the write from $T_i$ in the past. </li>
<li>The second condition means that another transaction from the future already wrote this object and $T_i$ cannot overwrite it in the past. </li>
</ul>
</li>
<li>Else, allow $T_i$ to write $X$ and update $W-TS(X)$. </li>
<li>Also, a local copy is made. </li>
</ul>
</li>
</ol>
<h3 id="Can-we-optimize-the-write-rule-to-decrease-possibility-of-abort"><a href="#Can-we-optimize-the-write-rule-to-decrease-possibility-of-abort" class="headerlink" title="Can we optimize the write rule to decrease possibility of abort?"></a>Can we optimize the write rule to decrease possibility of abort?</h3><ol>
<li>Thomas write rule: If $TS(T_i) &lt; W-TS(X)$, ignore the write to allow the transaction to continue executing without aborting. <ul>
<li>The thought is that we can see this violation as an immediate write from a future transaction right after the successful write from $T_i$. </li>
<li>The effects are similar, i.e. no one sees what does $T_i$ write. </li>
</ul>
</li>
<li>If $TS(T_i)&lt;R-TS(X)$, we still need to abort $T_i$. </li>
</ol>
<h3 id="What-is-the-issues-of-basic-T-O"><a href="#What-is-the-issues-of-basic-T-O" class="headerlink" title="What is the issues of basic T/O?"></a>What is the issues of basic T/O?</h3><ol>
<li>High overhead from copying data to transaction’s workspace and from updating timestamps. Every read requires the transaction to write to the database. </li>
<li>Long running transactions can get starved. The likelihood that a transaction will read something from a newer transaction increases. </li>
<li>If you assume that conflicts between transactions are rare and that most transactions are short-lived, then forcing transactions to acquire locks or update timestamps adds unnecessary overhead. </li>
</ol>
<h2 id="Optimistic-concurrency-control"><a href="#Optimistic-concurrency-control" class="headerlink" title="Optimistic concurrency control"></a>Optimistic concurrency control</h2><h3 id="What-is-the-main-idea-of-OCC"><a href="#What-is-the-main-idea-of-OCC" class="headerlink" title="What is the main idea of OCC?"></a>What is the main idea of OCC?</h3><ol>
<li>OCC assumes that the number of conflicts is low. Especially when: <ul>
<li>All transactions are read-only (ideal).</li>
<li>Txns access disjoint subsets of data.</li>
<li>The database is large and the workload is not skewed. </li>
</ul>
</li>
<li>The DBMS creates a private workspace for each transaction. <ul>
<li>Any object read is copied into workspace. Modifications are applied to workspace. </li>
<li>When a transaction commits, the DBMS compares workspace write set to see whether it conflicts with other transactions. </li>
<li>If there are no conflicts, the write set is installed into the “global” database. </li>
</ul>
</li>
<li>OCC has three phases:<ul>
<li>Read Phase: Track the read/write sets of transactions and store their writes in a private workspace, i.e. execution of transaction content. </li>
<li>Validation Phase: When a transaction commits, check whether it conflicts with other transactions. </li>
<li>Write Phase: If validation succeeds, apply private changes to database. Otherwise abort and restart the transaction. <ul>
<li>Serial Commits: Use a global latch to limit a single transaction to be in the Validation/Write phases at a time. </li>
<li>Parallel Commits: Use fine-grained write latches to support parallel Validation/Write phases. Txns acquire latches in primary key order to avoid deadlocks.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="What-will-happen-in-validation-phase"><a href="#What-will-happen-in-validation-phase" class="headerlink" title="What will happen in validation phase?"></a>What will happen in validation phase?</h3><ol>
<li>When transaction $T_i$ invokes <code>COMMIT</code>, the DBMS checks if it conflicts with other transactions. <ul>
<li>The DBMS needs to guarantee only serializable schedules are permitted.</li>
<li>Checks other transactions for RW and WW conflicts and ensure that conflicts are in one direction (e.g., older→younger). </li>
</ul>
</li>
<li>There are two approaches to valid: <ul>
<li>In backward validation, check whether the committing transaction intersects its read/write sets with those of any transactions that have already committed.<br><img src="/imgs/15445/TO/backward.png" width="50%"></li>
<li>In forward validation, check whether the committing transaction intersects its read/write sets with any active transactions that have not yet committed.<br><img src="/imgs/15445/TO/forward.png" width="50%"></li>
</ul>
</li>
<li>Each transaction’s timestamp is assigned at the beginning of the validation phase. Check the timestamp ordering of the committing transaction with all other concerned transactions. When $TS(T_i)&lt;TS(T_j)$, there are only three cases: <ul>
<li>If $T_i$ completes all three phases before $T_j$ begins its execution. This just means that there is serial ordering. </li>
<li>If $T_i$ completes before $T_j$ starts its Write phase, then we require that $T_i$ does not write to any object read by $T_j$, i.e. $WriteSet(T_i)\cap ReadSet(T_j)=\empty$. <ul>
<li>At this condition, we can conclude that $T_j$ cannot see anythin written by $T_i$. Therefore, if $T_j$ read anything written by $T_i$, it is a violation. </li>
</ul>
</li>
<li>If $T_i$ completes its Read phase before $T_j$​ completes its Read phase, then we require that $T_i$ does not write to any object that is either read or written by $T_j$. <ul>
<li>$WriteSet(T_i) \cap ReadSet(T_j) = \empty$ and $WriteSet(T_i) \cap WriteSet(T_j) = \empty$. </li>
<li>Anything wrote by $T_i$ should be seen by $T_j$ and should not conflict with what $T_j$ intends to write. </li>
<li>OCC wants more than just serializable order. Similar with Thomas write rule, if we allow the write sets have something in common it still would be serializable, yet in conflict. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="What-are-the-issues-of-OCC"><a href="#What-are-the-issues-of-OCC" class="headerlink" title="What are the issues of OCC?"></a>What are the issues of OCC?</h3><ol>
<li>High overhead for copying data locally. </li>
<li>Validation/Write phase bottlenecks.</li>
<li>Aborts are more wasteful than in 2PL because they only occur after a transaction has already executed.</li>
</ol>
<h1 id="The-phantom-problem"><a href="#The-phantom-problem" class="headerlink" title="The phantom problem"></a>The phantom problem</h1><h2 id="What-is-the-phantom-problem"><a href="#What-is-the-phantom-problem" class="headerlink" title="What is the phantom problem?"></a>What is the phantom problem?</h2><ol>
<li>In the above transaction management protocols, we assume that the total number  of tuples in a table is fixed, i.e. transactions will not execute insertion or deletion. </li>
<li>Insertion or deletions result in different results for the same range scan queries, e.g. count, maximum. <ul>
<li>The reason is that transactions can only lock on existing records and not one under way. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-solve-the-phantom-problem"><a href="#How-can-we-solve-the-phantom-problem" class="headerlink" title="How can we solve the phantom problem?"></a>How can we solve the phantom problem?</h2><ol>
<li>The first approach is to re-execute scans. <ul>
<li>The DBMS tracks the <code>WHERE</code> clause for all queries that the transaction executes. Retain the scan set for every range query in a transaction. </li>
<li>Upon commit, re-execute just the scan portion of each query and check whether it generates the same result. </li>
<li>This could double the execute time for all queries, which may be unacceptable. </li>
</ul>
</li>
<li>The second approach is by predicate locking. <ul>
<li>Shared lock on the predicate in a <code>WHERE</code> clause of a <code>SELECT</code> query. </li>
<li>Exclusive lock on the predicate in a <code>WHERE</code> clause of any <code>UPDATE</code>, <code>INSERT</code>, or <code>DELETE</code> query. </li>
<li>Prevent any query changing the result of locked predicate from executing. </li>
</ul>
</li>
<li>The third approach is by index locking. <ul>
<li>Key-value locks only cover a single existing key-value in an index, while gap locks cover those virtual keys for non-existent values. </li>
<li>Key-range locks takes multiple key-value locks and gap locks to lock on a range. </li>
<li>Hierarchical locking allows for a transaction to hold wider key-range locks with different locking modes to reduce the number of visits to lock manager. </li>
</ul>
</li>
<li>If there is no suitable index, then the transaction must obtain: <ul>
<li>A lock on every page in the table to prevent a record’s attributes from being changed to fit the predicates. </li>
<li>The lock for the table itself to prevent records fit the predicates from being added or deleted. </li>
</ul>
</li>
</ol>
<h2 id="What-are-isolation-levels"><a href="#What-are-isolation-levels" class="headerlink" title="What are isolation levels?"></a>What are isolation levels?</h2><ol>
<li>We may want to use a weaker level of consistency to improve scalability. </li>
<li>Provides for greater concurrency at the cost of exposing transactions to uncommitted changes: dirty reads, unrepeatable reads and phantom reads. </li>
<li>The four isolation levels are as shown below:<br><img src="/imgs/15445/TO/isolation.png" width="50%"></li>
<li>Each isolation level requires different locks to implement:<ul>
<li><code>SERIALIZABLE</code>: Obtain all locks first; plus index locks, plus strict 2PL.</li>
<li><code>REPEATABLE READS</code>: Same as above, but no index locks.</li>
<li><code>READ COMMITTED</code>: Same as above, but S locks are released immediately.</li>
<li><code>READ UNCOMMITTED</code>: Same as above but allows dirty reads (no S locks). </li>
</ul>
</li>
<li><code>SNAPSHOT ISOLATION</code> is another isolation supported by Oracle. <ul>
<li>It guarantees that all reads made in a transaction see a consistent snapshot of the database that existed at the time the transaction started.</li>
<li>A transaction will commit only if its writes do not conflict with any concurrent updates made since that snapshot.</li>
<li>Susceptible to write skew anomaly.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/04/Courses/15445/10-Two-Phase-Locking/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/04/Courses/15445/10-Two-Phase-Locking/" class="post-title-link" itemprop="url">10 Two-Phase Locking</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-04 16:47:09" itemprop="dateCreated datePublished" datetime="2023-08-04T16:47:09+08:00">2023-08-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-08-05 17:36:14" itemprop="dateModified" datetime="2023-08-05T17:36:14+08:00">2023-08-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Two-phase-Locking"><a href="#Two-phase-Locking" class="headerlink" title="Two-phase Locking"></a>Two-phase Locking</h1><h2 id="How-can-we-guarantee-serializable-without-knowing-the-entire-schedule-ahead-of-time"><a href="#How-can-we-guarantee-serializable-without-knowing-the-entire-schedule-ahead-of-time" class="headerlink" title="How can we guarantee serializable without knowing the entire schedule ahead of time?"></a>How can we guarantee serializable without knowing the entire schedule ahead of time?</h2><ol>
<li><p>We can use locks to protect database objects. </p>
<ul>
<li><p>When a transaction wants to access some objects, it needs to acquire locks of that objects from a centralized lock manager. </p>
</li>
<li><p>Locks are issued by applications and handled in lock manager while latches are issued and acquired locally. Hence acquiring locks are more expensive than acquiring latches even if locks are free. </p>
</li>
</ul>
</li>
<li><p>There are <code>S-LOCK</code> and <code>X-LOCK</code>. </p>
<ul>
<li><p><code>S-LOCKs</code> are shared locks for reads while <code>X-LOCKs</code> are exclusive locks for writes. </p>
</li>
<li><p>Their compatibility matrix is as followed: </p>
<p><img src="/imgs/15445/2pl/sx_comp_matrix.png" width="50%"></p>
</li>
</ul>
</li>
<li><p>Lock manager keeps track of what transaction hold what locks and what transactions are waiting to acquire any locks. </p>
<ul>
<li>When transactions request or upgrade locks, lock manager grants or blocks requests. When transactions release or downgrade locks, lock manager updates its internal lock-table. </li>
<li>Lock manager is responsible for detecting deadlock and choosing some transactions to kill. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-problem-of-releasing-locks-and-acquiring-it-later-again"><a href="#What-is-the-problem-of-releasing-locks-and-acquiring-it-later-again" class="headerlink" title="What is the problem of releasing locks and acquiring it later again?"></a>What is the problem of releasing locks and acquiring it later again?</h2><ol>
<li>This may cause in-consistent reads for a transaction when another transaction modified the object during lock is available. </li>
<li>This problem can be solved by two-phase locking. <ul>
<li>The first phase is growing: <ul>
<li>Each transaction requests or upgrades the locks that it needs from the DBMS’s lock manager. The lock manager grants/denies lock requests.</li>
</ul>
</li>
<li>The second phase is shrinking: <ul>
<li>The transaction is allowed to only release or downgrade locks that it previously acquired. It cannot acquire new locks. </li>
</ul>
</li>
</ul>
</li>
<li>Two-phase locking on its own is sufficient to guarantee conflict serializability because it generates schedules whose precedence graph is acyclic.</li>
</ol>
<h2 id="What-is-the-problem-of-two-phase-locking"><a href="#What-is-the-problem-of-two-phase-locking" class="headerlink" title="What is the problem of two-phase locking?"></a>What is the problem of two-phase locking?</h2><ol>
<li>It is subject to cascading aborts caused by dirty reads. <ul>
<li>When a transaction modified an object, and released the lock before it is aborted, the modified object is exposed to other transactions. </li>
<li>When the modifier is aborted, all other transactions that have used the modified object needs to abort. </li>
</ul>
</li>
<li>This can be solved by strong strict two-phase locking (rigorous two-phase locking). <ul>
<li>The transaction is only allowed to release locks after it has ended, i.e., committed or aborted. </li>
</ul>
</li>
<li>A schedule is strict if a value written by a transaction is not read or overwritten by other transactions until that transaction finishes. <ul>
<li>Its advantages are that it does not incurcascading aborts, and aborted transactions can be undone by just restoring original values of modified tuples. </li>
<li>However, it allows only conflict serializable schedules, but it is often stronger than needed for some apps. Most DBMSs prefer correctness before performance. </li>
</ul>
</li>
</ol>
<h1 id="Deadlock"><a href="#Deadlock" class="headerlink" title="Deadlock"></a>Deadlock</h1><h2 id="How-to-detect-and-resolve-deadlocks"><a href="#How-to-detect-and-resolve-deadlocks" class="headerlink" title="How to detect and resolve deadlocks?"></a>How to detect and resolve deadlocks?</h2><ol>
<li>The two-phase locking may lead to deadlocks. </li>
<li>The DBMS creates a waits-for graph to keep track of what locks each transaction is waiting to acquire<ul>
<li>Nodes are transactions. Edge from $T_i$ to $T_j$ if $T_i$ is waiting for $T_j$ to release a lock. </li>
<li>The system periodically checks for cycles in waits- for graph and then decides how to break it. </li>
</ul>
</li>
<li>When the DBMS detects a deadlock, it will select a “victim” transaction to rollback to break the cycle. <ul>
<li>The victim transaction will either restart or abort (more common) depending on how it was invoked. </li>
<li>There is a trade-off between the frequency of checking for deadlocks and how long transactions wait before deadlocks are broken. </li>
</ul>
</li>
<li>Selecting the proper victim depends on a lot of different variables<ul>
<li>By age (lowest timestamp)</li>
<li>By progress (least/most queries executed)</li>
<li>By the number of items already locked</li>
<li>By the number of transactions that we have to rollback with it</li>
<li>We also should consider the # of times a transaction has been restarted in the past to prevent starvation. </li>
</ul>
</li>
<li>After selecting a victim transaction to abort, the DBMS can also decide on how far to rollback the transaction’s changes. <ul>
<li>The first approach is to rollback entire transaction and tell the application it was aborted. </li>
<li>The second approach is rolling back a portion of a transaction to break deadlock and then attempts to re-execute the undone queries. </li>
</ul>
</li>
</ol>
<h2 id="How-to-prevent-deadlocks"><a href="#How-to-prevent-deadlocks" class="headerlink" title="How to prevent deadlocks?"></a>How to prevent deadlocks?</h2><ol>
<li>When a transaction tries to acquire a lock that is held by another transaction, the DBMS kills one of them to prevent a deadlock. </li>
<li>Assign priorities based on timestamps, e.g older timestamp means higher priority. </li>
<li>The first kind of rule is Wait-Die (“Old Waits for Young”)<ul>
<li>If requesting transaction has higher priority than holding transaction, then requesting transaction waits for holding transaction. Otherwise requesting transaction aborts. </li>
</ul>
</li>
<li>The second kind of rule is Wound-Wait (“Young Waits for Old”)<ul>
<li>If requesting transaction has higher priority than holding transaction, then holding transaction aborts and releases lock. Otherwise requesting transaction waits. </li>
</ul>
</li>
<li>In this case, only one “type” of direction allowed when waiting for a lock. </li>
<li>When a transaction restarts, its new priority is still its original timestamp to prevent it from getting starved for resources like an old man at a corrupt senior center. </li>
</ol>
<h1 id="Lock-granularity"><a href="#Lock-granularity" class="headerlink" title="Lock granularity"></a>Lock granularity</h1><h2 id="What-are-the-database-objects"><a href="#What-are-the-database-objects" class="headerlink" title="What are the database objects?"></a>What are the database objects?</h2><ol>
<li>It can be attributes, tuples, pages, tables depending on the lock granularity. </li>
<li>The trade-off is between parallelism versus overhead of requesting and lock manager processing. </li>
<li>In a hierachical lock scheme, the objects from top-layer to lower-layer are database, table, page, tuple, attribute. </li>
</ol>
<h2 id="How-to-support-multiple-granularities"><a href="#How-to-support-multiple-granularities" class="headerlink" title="How to support multiple granularities?"></a>How to support multiple granularities?</h2><ol>
<li><p>With only <code>S-LOCK</code> and <code>X-LOCK</code>, we have to check the locks of all children when we try to lock a higher-level node. </p>
</li>
<li><p>An intention lock allows a higher-level node to be locked in shared or exclusive mode without having to check all descendent nodes. </p>
</li>
<li><p>If a node is locked in an intention mode, then some transaction is doing explicit locking at a lower level in the tree. </p>
<ul>
<li><p>Intention-Shared (<code>IS</code>) indicates explicit locking at lower level with shared locks. </p>
</li>
<li><p>Intention-Exclusive (<code>IX</code>) indicates explicit locking at lower level with exclusive locks. </p>
</li>
<li><p>Shared+Intention-Exclusive (<code>SIX</code>) indicates that the subtree rooted by that node is locked explicitly in shared mode and explicit locking is being done at a lower level with exclusive-mode locks. </p>
</li>
<li><p>Their compatibility matrix is as followed: </p>
<p><img src="/imgs/15445/2pl/intention.png" width="50%"></p>
</li>
</ul>
</li>
<li><p>Each transaction obtains appropriate lock at highest level of the database hierarchy. </p>
<ul>
<li>To get <code>S</code> or <code>IS</code> lock on a node, the transaction must hold at least <code>IS</code> on parent node. </li>
<li>To get <code>X</code>, <code>IX</code>, or <code>SIX</code> on a node, must hold at least <code>IX</code> on parent node. </li>
</ul>
</li>
<li><p>Multiple lock granularities is shown in the <code>S</code>, <code>X</code>, <code>SIX</code> locks on higher-level objects. </p>
<ul>
<li>Intention-Shared (<code>IS</code>): Intent to get <code>S</code> lock(s) at finer granularity. </li>
<li>Intention-Exclusive (<code>IX</code>): Intent to get <code>X</code> lock(s) at finer granularity. </li>
<li>Shared+Intention-Exclusive (<code>SIX</code>): Like <code>S</code> and <code>IX</code> at the same time. </li>
</ul>
</li>
</ol>
<h2 id="How-to-use-locks"><a href="#How-to-use-locks" class="headerlink" title="How to use locks?"></a>How to use locks?</h2><ol>
<li>Applications typically don’t acquire a transaction’s locks manually (i.e., explicit SQL commands). <ul>
<li>Sometimes you need to provide the DBMS with hints to help it to improve concurrency. </li>
<li>Explicit locks are also useful when doing major changes to the database. </li>
</ul>
</li>
<li>Lock escalation: The DBMS can automatically switch to coarser- grained locks when a transaction acquires too many low-level locks. This reduces the number of requests that the lock manager must process. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/02/OpenSource/BusTub/Project-3-Query-Execution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/02/OpenSource/BusTub/Project-3-Query-Execution/" class="post-title-link" itemprop="url">Project #3: Query Execution</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-02 00:52:07" itemprop="dateCreated datePublished" datetime="2023-08-02T00:52:07+08:00">2023-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-08-17 12:47:30" itemprop="dateModified" datetime="2023-08-17T12:47:30+08:00">2023-08-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Executors"><a href="#Executors" class="headerlink" title="Executors"></a>Executors</h1><h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><ol>
<li>The planner for each operation stores the necessary information to calculate the correct output. </li>
<li>The executor class for each operation stores the corresponding plan and other information for it to determine what tuple will be returned. <ul>
<li>All executors need to override the <code>Init()</code> and <code>Next(Tuple *tuple, RID *rid)</code>. </li>
<li><code>Init()</code> is used to initialize the information to determine what tuple will be returned. It is separated from the constructor because its parent executor may need to fetch the tuples of the current executor several times. </li>
</ul>
</li>
<li>The <code>ExecutorContext</code> in each executor stores the metadata of the database system, including catalog, buffer pool manager. <ul>
<li>Catalog maintains the tables and indexes in the current database. </li>
<li>We can register a new table or index through catalog or acquire metadata of a table (<code>TableInfo</code>) or index (<code>IndexInfo</code>) from catalog. </li>
</ul>
</li>
<li>The <code>TableInfo</code> stores the name, OID, schema of the table, and a pointer of <code>TableHeap</code>. <ul>
<li>We can manipulate a table through its <code>TableHeap</code>. </li>
<li>The <code>TableHeap</code> provides methods to insert or get tuples and update or get tuple metadata. <ul>
<li>To delete a tuple, we just need to mark the <code>is_deleted_</code> flag in its metadata as <code>true</code>. </li>
<li>To update a tuple, we need to delete it first and insert the new updated tuple into the table again. </li>
<li>To iterate through the table, <code>TableHeap</code> also provided <code>MakeIterator()</code>. </li>
</ul>
</li>
</ul>
</li>
<li>The <code>IndexInfo</code> stores the name and OID of the index, the schema for the index key, the name of the table and a pointer of <code>Index</code>. <ul>
<li>We can manipulate an index through its <code>Index</code>. </li>
<li>The <code>Index</code> provides methods to insert or delete an entry from the index, and get metadata of the index. <ul>
<li>The metadata includes the names of the index and table, mapping relation between key schema and tuple schema, and the schema of the indexed key. </li>
<li>The <code>Index</code> is the abstract class of all kinds of index implementations. To use a specific known index implementation, we can <code>dynamic_cast</code> it. </li>
</ul>
</li>
</ul>
</li>
<li>This system support <code>ArithmeticExpression</code>, <code>ColumnValueExpression</code>, <code>ComparisonExpression</code>, <code>ConstantValueExpression</code>, <code>LogicExpression</code>, <code>StringExpression</code>. <ul>
<li>The <code>AbstractExpression</code> provides an <code>Evaluate</code> method to calculate desired <code>Value</code> according to provided one <code>Tuple</code> and <code>Schema</code>. <ul>
<li>The <code>ArithmeticExpression</code> only supports <code>PLUS</code> and <code>MINUS</code> operation of two <code>Value</code>s. </li>
<li>The <code>ColumnValueExpression</code> returns the <code>Value</code> of the designated column. </li>
<li>The <code>ComparisonExpression</code> supports the comparison result of two <code>Values</code>s of <code>equal, not equal, less, less or equal, greater, greater or equal</code>. </li>
<li>The <code>ConstantValueExpression</code> returns a single constant <code>Value</code>. </li>
<li>The <code>LogicExpression</code> supports the <code>AND/OR</code> of two <code>Value</code>s. </li>
<li>The <code>StringExpression</code> converts a string  to lower-case or upper-case. </li>
</ul>
</li>
<li>The <code>AbstractExpression</code> also provides an <code>EvaluateJoin</code> method to calculate the join condition of two <code>Tuple</code>s. They support the same functions as <code>Evaluation</code> except that they consider two tuples. </li>
<li><code>ArithmeticExpression</code>, <code>ComparisonExpression</code>, <code>LogicExpression</code> and <code>LogicExpression</code> may have child-expressions. During evaluation, they will perform child expressions first recursively. </li>
</ul>
</li>
</ol>
<h2 id="Scan"><a href="#Scan" class="headerlink" title="Scan"></a>Scan</h2><ol>
<li>In sequential scan, we only need to know which table to scan. <ul>
<li>The object ID and name of the table is stored in the planner. </li>
<li>In the <code>Init()</code>, the metadata of the table (<code>TableInfo</code>) is fetched from catalog and the corresponding iterator of the table is created. </li>
<li>In the <code>Next(Tuple *tuple, RID *rid)</code>, as long as the iterator is not at the end, we need to find the first tuple with <code>is_deleted_</code> being <code>false</code> and matching with <code>filter_predicate_</code>. </li>
</ul>
</li>
<li>In the index scan, we need to know which index to scan and which table to fetch the tuple. <ul>
<li>The OID of the index is stored in the planner while the name of the table is stored in the <code>IndexInfo</code>. </li>
<li>To fetch the iterator of the index, we need to cast the <code>Index</code> into the specific implementation. </li>
<li>Each iterator points to a paire of the key and the RecordID. We can fetch the tuple with the <code>GetTuple</code> of the <code>TableHeap</code>. </li>
<li>As long as the iterator is not at the end, we need to find the first tuple with <code>is_deleted_</code> being <code>false</code>. </li>
</ul>
</li>
</ol>
<h2 id="Modification"><a href="#Modification" class="headerlink" title="Modification"></a>Modification</h2><ol>
<li>In the modification executors, we will modify the table and all the associated indexes. <ul>
<li>The table OIDs are stored in corresponding planner, while we can fetch all indexes of that table with the name of the table in <code>TableInfo</code>. </li>
<li>The indexed keys can be acquired with the <code>KeyFromTuple</code> method of the <code>Tuple</code>. The stored values are RecordIDs. </li>
</ul>
</li>
<li>All the modification executors only return one row representing the number of modified tuples. Hence in one <code>Next</code> call, the executor should handle all the modification. <ul>
<li>When no modification is performed, we should also return one row of $0$. </li>
<li>To distinguish between no modification and modification already finished in last call, we should have a flag representing whether or not we have already modified the table. </li>
</ul>
</li>
<li>In the insert executor and delete executor, the tuples to insert or delete are acquired from child executor. </li>
<li>In the update planner, there is a target expression for each output column. <ul>
<li>After acquired the original tuple from the child executor, we can evaluate each output column with target expressions. </li>
<li>The expressions can be any kind of expressions here. </li>
<li>When updating indexes, it needs to first delete the old indexes first before insert updated indexes. </li>
</ul>
</li>
</ol>
<h2 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h2><ol>
<li>The aggregations are implemented with a hash table. <ul>
<li>The hash table hashes the aggregate keys to the aggregate result. </li>
<li>When insert a tuple into the hash table, it updates the aggregate result in it according to the aggregate function and the aggregate values from the tuple. </li>
<li>To use <code>std::hash</code>, the key is <code>AggregateKey</code> containing a vector of <code>Value</code> describing the group-by keys while the value is <code>AggregateValue</code> containing another vector of <code>Value</code> describing aggregate values. <ul>
<li>The <code>AggregateKey</code> needs to override <code>==</code> operator and provide <code>()</code> operator in <code>struct std::hash&lt;bustub::AggregateKey&gt;</code> to calculate the hash value of <code>AggregateKey</code>. </li>
</ul>
</li>
</ul>
</li>
<li>Aggregations are pipeline breakers. Hence the build phase could be performed in the <code>Init()</code> where all tuples from child executor are read and inserted into the hash table to calculate the outputs. <ul>
<li>If no tuple is inserted, the aggregation executor still need to return the default values for each aggregate function. </li>
<li>The aggregation planner has two vectors of <code>AbstractExpressionRef</code> to fetch the aggregate keys and aggregate values from tuples received from child executor for aggregate computation. They are all <code>ColumnExpression</code>. </li>
</ul>
</li>
<li>In the <code>Next(Tuple *tuple, RID *rid)</code>, we just need to iterate over the hash table and output a tuple combining the aggregate keys and aggregate results. </li>
</ol>
<h2 id="NestedLoopJoin"><a href="#NestedLoopJoin" class="headerlink" title="NestedLoopJoin"></a>NestedLoopJoin</h2><ol>
<li><p>The executor needs to iterate over left child executor and right child executor. </p>
<ul>
<li>When the right child executor has emitted all tuples, the join executor will try to fetch a new tuple from the left child executor and re-initialize the right child executor for the following comparison. </li>
<li>Since we do not always fetch from left child executor when the <code>Next</code> is called, we need to record the current left tuple when it is fetched. </li>
<li>To distinguish between the status of no more left tuples and have not fetched any left tuples yet, we can fetch the first left tuple in <code>Init()</code>. </li>
<li>To mark the status of no more left tuples, i.e. no more tuples to emit, we need a flag to record the status of left tuples, which is the returned value of the <code>Next</code> of left child executor. </li>
</ul>
</li>
<li><p>For left join, if an outer tuple does not match with any inner tuple, we still need to emit the concatenation of that outer tuple with all-null inner tuple. </p>
</li>
<li><p>The predicate expression of the executor can be either a <code>LogicExpression</code> or <code>ComparisonExpression</code>. </p>
</li>
</ol>
<h2 id="HashJoin"><a href="#HashJoin" class="headerlink" title="HashJoin"></a>HashJoin</h2><ol>
<li>Similar with aggregations, we need a hash table for the outer table. We define <code>JoinKey</code> and <code>JoinBucket</code> to hash. <ul>
<li>We store all tuples with the same values in the attributes of join condition. </li>
<li>To support left join, we also need to record whether one tuple is used. <ul>
<li>When a tuple is matched with some inner tuple, all tuples in the same bucket must also matched. Hence we only need to record the usage of each <code>JoinBucket</code>. </li>
<li>For left join, when there is no more right tuples, the executor still need to iterate through the hash table to see if any bucket is unused. </li>
</ul>
</li>
<li>Similar with aggregation, <code>HashJoin</code> need to build the hash table based on the outer table in <code>Init()</code>. </li>
</ul>
</li>
<li>Different with <code>NestedLoopJoin</code>, the plan of <code>HashJoin</code> only need to know how to fetch columns from each tuples to determine whether those tuples are matched. <ul>
<li>The expressions of the <code>HashJoin</code> planner are only <code>ColumnValueExpression</code>. </li>
</ul>
</li>
</ol>
<h2 id="Sort-amp-Top-N"><a href="#Sort-amp-Top-N" class="headerlink" title="Sort &amp; Top-N"></a>Sort &amp; Top-N</h2><ol>
<li>The compare function needs specific expressions to fetch designated columns from each tuple. <ul>
<li>Hence we cannot only implement a non-static member function or a function with expressions being one of the parameters. </li>
<li>We need to implement a structure with override <code>()</code> operator. The expressions are passed to the object in initialization. </li>
<li>For <code>priority_queue</code>, two nodes will be swapped when the comparison function returns true. </li>
</ul>
</li>
<li>In the <code>Init()</code>, we need to fetch all tuples from child executor and sort them. <ul>
<li>For Top-N executor, the heap size should not be larger than $N$. The heap after the <code>Init()</code> is the counter-order of the output order, hence we still need a stack to support output in <code>Next</code>. </li>
</ul>
</li>
</ol>
<h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><ol>
<li>When trying to optimize a plan, the root plan is passed to the optimizer and the optimizer will perform a post-root-order DFS of the plan tree. </li>
<li>Each optimizer tries to recognize the pattern they need to handle and produce a new plan when it is matched. </li>
<li>To optimize limit followed by sort into top-N, we just need to check whether the child of a limit plan is a sort. </li>
</ol>
<h2 id="Optimize-nested-loop-join"><a href="#Optimize-nested-loop-join" class="headerlink" title="Optimize nested loop join"></a>Optimize nested loop join</h2><ol>
<li>To optimize nested loop join with hash join, the optimizer need to find a nested loop join and separate all the conditions from <code>LogicExpression</code> into <code>ComparisonExpression</code>. <ul>
<li>If all the <code>ComparisonExpression</code> are <code>ComparisonType::Equal</code>, we can create a new plan of <code>HashJoin</code> with <code>left_key_expressions</code> and <code>right_key_expressions</code> extracted from those <code>ComparisonExpression</code>. </li>
</ul>
</li>
<li>We can push down predicates of nested loop join to reduce the complexity of join. <ul>
<li>If a predicate is the conjunctions of <code>ComparisonExpression</code>s, we can decompose it into basic <code>ComparisonExpression</code> to examine them one by one to determine whether they can be pushed down. </li>
<li>If the two sides of a <code>ComparisonExpression</code> are from two different tables, it cannot be pushed down. </li>
<li>To push down a <code>ComparisonExpression</code>, we need to ajust the expressions of the two sides. <ul>
<li></li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/07/16/Courses/15445/09-Concurrency-Control/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/16/Courses/15445/09-Concurrency-Control/" class="post-title-link" itemprop="url">09 Concurrency Control</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-07-16 16:35:24 / Modified: 18:14:52" itemprop="dateCreated datePublished" datetime="2023-07-16T16:35:24+08:00">2023-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Concurency-control-amp-recovery"><a href="#Concurency-control-amp-recovery" class="headerlink" title="Concurency control &amp; recovery"></a>Concurency control &amp; recovery</h1><h2 id="What-do-we-want-from-concurrency-control-amp-recovery"><a href="#What-do-we-want-from-concurrency-control-amp-recovery" class="headerlink" title="What do we want from concurrency control &amp; recovery?"></a>What do we want from concurrency control &amp; recovery?</h2><ol>
<li>The concurrency control is responsible for lost update problem. <ul>
<li>How can we avoid race conditions when updating records at the same time? </li>
<li>This involves the buffer pool manager layer, access methods layer and operator execution layer. </li>
</ul>
</li>
<li>Recovery is responsible for durability problems. <ul>
<li>How can we ensure the correct state in case of a power failure? </li>
<li>This involves the disk manager layer and buffer pool manager layer. </li>
</ul>
</li>
</ol>
<h2 id="How-does-applications-issue-changes-to-a-DBMS"><a href="#How-does-applications-issue-changes-to-a-DBMS" class="headerlink" title="How does applications issue changes to a DBMS?"></a>How does applications issue changes to a DBMS?</h2><ol>
<li>A transaction is the execution of a sequence of one or more operations (e.g., SQL queries) on a database to perform some higher-level function. <ul>
<li>Transaction is the basic unit of change in a DBMS. Partial transactions are not allowed. </li>
<li>A new transaction starts with the <code>BEGIN</code> command. </li>
<li>The transaction stops with either <code>COMMIT</code> or <code>ABORT</code>:<ul>
<li>If commit, the DBMS either saves all the transaction’s changes <strong>or aborts</strong> it.</li>
<li>If abort, all changes are undone so that it’s like as if the transaction never executed at all. </li>
<li>Abort can be either self-inflicted or caused by the DBMS.</li>
</ul>
</li>
</ul>
</li>
<li>In a strawman system, each transaction is executed one-by-one as  they arrive at the DBMS. <ul>
<li>Before a transaction starts, copy the entire database to a new file and make all changes to that file.  <ul>
<li>If the txn completes successfully, overwrite the original file with the new one. </li>
<li>If the txn fails, just remove the dirty copy. </li>
</ul>
</li>
<li>The problem of this system is that there is no concurrency and copying the entire database can be expensive if the it is large. </li>
</ul>
</li>
<li>Besides correctness and fairness, we also want the DBMS to allow concurrent execution of independent transactions to provide better utilization / throughput and increase response times to users. </li>
</ol>
<h2 id="What-does-the-DBMS-want-to-prevent-when-supporting-concurrency"><a href="#What-does-the-DBMS-want-to-prevent-when-supporting-concurrency" class="headerlink" title="What does the DBMS want to prevent when supporting concurrency?"></a>What does the DBMS want to prevent when supporting concurrency?</h2><ol>
<li>Arbitrary interleaving of operations can lead to temporary inconsistency and permanent inconsistency. <ul>
<li>Temporary inconsistency is unavoidable and it is fine as long as no other transactions can see it. </li>
<li>Permanent inconsistency is unacceptable. </li>
</ul>
</li>
<li>The DBMS is only concerned about what data is read/written from/to the database. Changes to the “outside world”, e.g. sending an email, are beyond the scope of the DBMS. </li>
</ol>
<h1 id="Correctness"><a href="#Correctness" class="headerlink" title="Correctness"></a>Correctness</h1><h2 id="What-is-the-correctness-criteria"><a href="#What-is-the-correctness-criteria" class="headerlink" title="What is the correctness criteria?"></a>What is the correctness criteria?</h2><ol>
<li><strong>Atomicity</strong>: All actions in transaction happen, or none happen, i.e. “all or nothing”. </li>
<li><strong>Consistency</strong>: If each transaction is consistent and the DB starts consistent, then it ends up consistent. </li>
<li><strong>Isolation</strong>: Execution of one transaction is isolated from that of other transactions. </li>
<li><strong>Durability</strong>: If a txn commits, its effects persist. </li>
</ol>
<h2 id="How-to-ensure-atomicity"><a href="#How-to-ensure-atomicity" class="headerlink" title="How to ensure atomicity?"></a>How to ensure atomicity?</h2><ol>
<li>The first approach is logging (Write Ahead Log / WAL). <ul>
<li>DBMS logs all actions so that it can undo the actions of aborted transactions. </li>
<li>Maintain undo records both in memory and on disk. </li>
<li>When the DBMS come back from a crash, it need to undo partial transactions according to the undo records. </li>
</ul>
</li>
<li>Another approach is shadow paging. <ul>
<li>DBMS makes copies of pages and txns make changes to those copies. </li>
<li>Only when the txn commits is the page made visible to others by modifying the pointer at directory. </li>
<li>It does not need extra operations when come back from crash. </li>
</ul>
</li>
</ol>
<h2 id="What-is-consistency"><a href="#What-is-consistency" class="headerlink" title="What is consistency?"></a>What is consistency?</h2><ol>
<li>At a high level, consisitency means the “world” represented by the database is logically correct. All questions (i.e., queries) that the application asks about the data will return logically correct results. </li>
<li>There are two notions of consistency<ul>
<li>Database consistency means that the database accurately models the real world and follows integrity constraints. <ul>
<li>Transactions in the future see the effects of transactions committed in the past inside of the database. </li>
<li>The designer of DBMS should maintain this consistency. </li>
</ul>
</li>
<li>Transaction consistency means that if the database is consistent before the transaction starts, it will also be consistent after. <ul>
<li>The application programmer is responsible for this consistency. The DBMS does not know the semantics of correctness. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h2><h3 id="What-do-we-want-from-isolation"><a href="#What-do-we-want-from-isolation" class="headerlink" title="What do we want from isolation?"></a>What do we want from isolation?</h3><ol>
<li>Users submit txns, and each txn executes as if it was running by itself, i.e. it is executed in a strawman system where no other transaction is executing at the same time. </li>
<li>Isolation provides an easier programming model to reason about. </li>
<li>But the DBMS achieves concurrency by interleaving the actions (reads/writes of DB objects) of txns. Hence we need to schedule to interleave txns but still make it appear as if they ran one-at-a-time. </li>
<li>There is no guarantee that $T_1$ will execute before $T_2$ or vice-versa, if both are submitted together. The net effect must be equivalent to these two transactions running serially in some order.</li>
</ol>
<h3 id="How-to-decide-whether-a-schedule-matches-isolation"><a href="#How-to-decide-whether-a-schedule-matches-isolation" class="headerlink" title="How to decide whether a schedule matches isolation?"></a>How to decide whether a schedule matches isolation?</h3><ol>
<li>If the schedule is equivalent to some serial execution, we can consider it correct. </li>
<li>For any database state, if the effect of executing the first schedule is identical to the effect of executing the second schedule, we say these two schedules are equivalent. </li>
<li>A schedule is serializable schedule if it is equivalent to some serial execution of the transactions. <ul>
<li>If each transaction preserves consistency, every serializable schedule preserves consistency. </li>
</ul>
</li>
<li>There are two different levels of serializability: conflict serializability (most commonly used) and view serializability. </li>
</ol>
<h3 id="What-conflicts-do-we-want-to-prevent"><a href="#What-conflicts-do-we-want-to-prevent" class="headerlink" title="What conflicts do we want to prevent?"></a>What conflicts do we want to prevent?</h3><ol>
<li>Two operations conflict if: They are by different transactions, and they are on the same object while one of them is a write. </li>
<li>A read-write conflict will cause unrepeatable read. <ul>
<li>Transaction gets different values when reading the same object multiple times. </li>
<li>The conflict is between the write from one transaction and a repeated following read from another transaction, i.e. this is actually $read_1-write-read_2$ conflict where the conflict is between $write-read_2$. </li>
<li>If there is only one read, it is not a read-write conflict. The first read will never be a read-write conflict. </li>
</ul>
</li>
</ol>
<ul>
<li>A write-read conflict will cause dirty read. <ul>
<li>One transaction reads data written by another transaction that has not committed yet. </li>
<li>The problem will happen when the read transaction is commited before the write transaction aborts. </li>
<li>If the write transaction successfully commits, there is no problem. But we cannot know that when we commit the read transaction first. </li>
</ul>
</li>
<li>A write-write conflict will cause lost update. <ul>
<li>One transaction overwrites uncommitted data from another uncommitted transaction. </li>
<li>This may cause the result becoming combination of two partial transactions. </li>
<li>There is no problem when every data written by $T_2$ is the last write to that data, i.e. every data written by $T_2$ is not overwritten by $T_1$. The problem happens when $T_1$ overwrites some data of $T_2$ while $T_2$ overwrites some data of $T_1$. </li>
</ul>
</li>
</ul>
<h3 id="How-do-we-determine-whether-a-schedule-is-conflict-serializable"><a href="#How-do-we-determine-whether-a-schedule-is-conflict-serializable" class="headerlink" title="How do we determine whether a schedule is conflict serializable?"></a>How do we determine whether a schedule is conflict serializable?</h3><ol>
<li>When there are only two schedules: <ul>
<li>Two schedules are conflict equivalent if and only if they involve the same actions of the same transactions and every pair of conflicting actions is ordered the same way. </li>
<li>Schedule S is conflict serializable if S is conflict equivalent to some serial schedule. </li>
<li>We can transform S into a serial schedule by swapping consecutive non-conflicting operations of different transactions. </li>
</ul>
</li>
<li>For more schedules, we can use the dependency graphs. <ul>
<li>Create one node per txn in the graph. </li>
<li>Create an edge from $T_i$ to $T_j$ if an operation $O_i$ of $T_i$ conflicts with an<br>operation $O_j$ of $T_j$ and $O_i$ appears earlier in the schedule than $O_j$. </li>
<li>A schedule is conflict serializable if and only if its dependency graph is acyclic. </li>
</ul>
</li>
</ol>
<h3 id="How-do-we-determine-whether-a-schedule-is-view-serializable"><a href="#How-do-we-determine-whether-a-schedule-is-view-serializable" class="headerlink" title="How do we determine whether a schedule is view serializable?"></a>How do we determine whether a schedule is view serializable?</h3><ol>
<li>Schedules $S_1$ and $S_2$ are view equivalent if:<ul>
<li>If $T_1$ reads initial value of A in $S_1$, then $T_1$ also reads initial value of A in $S_2$. </li>
<li>If $T_1$ reads value of A written by $T_2$ in $S_1$, then $T_1$ also reads value of A written by $T_2$ in $S_2$. </li>
<li>If $T_1$ writes final value of A in $S_1$, then $T_1$ also writes final value of A in $S_2$. </li>
</ul>
</li>
<li>In a word, each transaction is different schedules read the same values written by the same transaction and at the final end of all transactions, all data are written by the same transaction in the same value. </li>
<li>View Serializability allows for (slightly) more schedules than Conflict Serializability does. Neither definition allows all serializable schedules. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/07/15/Courses/15445/08-Query-Planning-Optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/15/Courses/15445/08-Query-Planning-Optimization/" class="post-title-link" itemprop="url">08 Query Planning & Optimization</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-15 19:52:41" itemprop="dateCreated datePublished" datetime="2023-07-15T19:52:41+08:00">2023-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-16 02:02:27" itemprop="dateModified" datetime="2023-07-16T02:02:27+08:00">2023-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Query-planning"><a href="#Query-planning" class="headerlink" title="Query planning"></a>Query planning</h1><h2 id="What-are-the-logical-plans-and-physical-plans"><a href="#What-are-the-logical-plans-and-physical-plans" class="headerlink" title="What are the logical plans and physical plans?"></a>What are the logical plans and physical plans?</h2><ol>
<li>The optimizer generates a mapping of a logical algebra expression to the optimal equivalent physical algebra expression. </li>
<li>Physical operators define a specific execution strategy using an access path, i.e. a specific algorithm. <ul>
<li>They depend on the physical format of the data that they process, i.e., sorting, compression. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-process-flow-of-query-execution"><a href="#What-is-the-process-flow-of-query-execution" class="headerlink" title="What is the process flow of query execution?"></a>What is the process flow of query execution?</h2><ol>
<li>The application connected to the database system and sends a SQL query, which may be rewritten to a different format in SQL rewriter. </li>
<li>The SQL string is parsed into tokens that make up the syntax tree. </li>
<li>The binder converts named objects in the syntax tree to internal identifiers by consulting the system catalog. </li>
<li>The binder emits a logical plan which may be fed to a tree rewriter for additional schema info. </li>
<li>The logical plan is given to the optimizer which selects the most efficient procedure to execute the plan. </li>
</ol>
<p><img src="/imgs/15445/Optimizer/architecture.png" width="50%"></p>
<h2 id="How-does-optimizer-work"><a href="#How-does-optimizer-work" class="headerlink" title="How does optimizer work?"></a>How does optimizer work?</h2><ol>
<li>One way is heuristics / rules. <ul>
<li>Rewrite the query to remove stupid / inefficient things. </li>
<li>These techniques may need to examine catalog, but they do not need to examine data. </li>
</ul>
</li>
<li>Another way is the cost-based search. <ul>
<li>We need to use a model to estimate the cost of executing a plan. </li>
<li>Enumerate multiple equivalent plans for a query and pick the one with the lowest cost. </li>
</ul>
</li>
</ol>
<h1 id="Heuristics-optimization"><a href="#Heuristics-optimization" class="headerlink" title="Heuristics optimization"></a>Heuristics optimization</h1><h2 id="How-should-we-optimize-logical-plans"><a href="#How-should-we-optimize-logical-plans" class="headerlink" title="How should we optimize logical plans?"></a>How should we optimize logical plans?</h2><ol>
<li>Split conjunctive predicates. <ul>
<li>Decompose predicates into their simplest forms to make it easier for the optimizer to move them around. </li>
</ul>
</li>
<li>Predicate pushdown<ul>
<li>Move the predicate to the lowest applicable point in the plan. </li>
</ul>
</li>
<li>Replace cartesian products with joins<ul>
<li>Replace all Cartesian Products with inner joins using the join predicates. </li>
</ul>
</li>
<li>Projection pushdown<ul>
<li>This is to eliminate redundant attributes before pipeline breakers to reduce materialization cost and the data passed aroung. </li>
</ul>
</li>
</ol>
<h2 id="How-should-we-optimize-for-nested-sub-queries"><a href="#How-should-we-optimize-for-nested-sub-queries" class="headerlink" title="How should we optimize for nested sub-queries?"></a>How should we optimize for nested sub-queries?</h2><ol>
<li>Rewrite to de-correlate and/or flatten them<ul>
<li>E.g. an <code>EXISTS</code> sub-query in <code>WHERE</code> clause may be rewrited as an inner-join. </li>
</ul>
</li>
<li>Decompose nested query and store result to temporary table. <ul>
<li>For those sub-queries uncorrelated with outer query, the optimizer breaks up queries into blocks and then concentrates on one block at a time. </li>
<li>Sub-queries are written to a temporary table that are discarded after the query finishes. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-rewrite-expression"><a href="#How-can-we-rewrite-expression" class="headerlink" title="How can we rewrite expression?"></a>How can we rewrite expression?</h2><ol>
<li>This is implemented using if/then/else clauses or a pattern-matching rule engine. <ul>
<li>Search for expressions that match a pattern. When a match is found, rewrite the expression. Halt if there are no more rules that match. </li>
</ul>
</li>
<li>One approach is replacing impossible or unnecessary predicates by false. </li>
<li>Another approach is merging predicates, e.g. numeric ranging predicates. </li>
</ol>
<h1 id="Cost-based-search"><a href="#Cost-based-search" class="headerlink" title="Cost-based search"></a>Cost-based search</h1><h2 id="Cost-estimation"><a href="#Cost-estimation" class="headerlink" title="Cost estimation"></a>Cost estimation</h2><h3 id="What-cost-do-we-care"><a href="#What-cost-do-we-care" class="headerlink" title="What cost do we care?"></a>What cost do we care?</h3><ol>
<li>Physical Costs<ul>
<li>Predict CPU cycles, I/O, cache misses, RAM consumption, network messages. </li>
<li>This cost depends heavily on hardware. </li>
</ul>
</li>
<li>Logical Costs<ul>
<li>Estimate output size per operator. </li>
<li>This cost is independent of the operator algorithm since algorithms are physical. </li>
<li>It need estimations for operator result sizes. </li>
</ul>
</li>
<li>Algorithmic Costs<ul>
<li>Mainly the complexity of the operator algorithm implementation. </li>
</ul>
</li>
<li>We may use a combination of multiple costs that are weighted by magic constant factors. <ul>
<li>Some assumptions is that processing a tuple in memory is $400\times$ faster than reading a tuple from disk, and sequential I/O is $4\times$ faster than random I/O. </li>
<li>Most commonly used cost is the combination of the physical costs and logical costs. </li>
</ul>
</li>
</ol>
<h3 id="How-do-DBMS-estimate-the-costs"><a href="#How-do-DBMS-estimate-the-costs" class="headerlink" title="How do DBMS estimate the costs?"></a>How do DBMS estimate the costs?</h3><ol>
<li>The DBMS stores internal statistics about tables, attributes, and indexes in its internal catalog. <ul>
<li>Different systems update them at different times. </li>
</ul>
</li>
<li>Then DBMS derives the <strong>selection cardinality</strong> (<strong>selectivity</strong>) of a predicate which is the fraction of tuples that qualify. </li>
<li>We can make some assumptions to estimate selectivity<ul>
<li>Uniform data: The distribution of values (except for the heavy hitters) is the same. May maintain a heavy hitter list that stores most common values and assume that the occurrence of the rest data is the same. </li>
<li>Independent predicates: The predicates on attributes are independent, i.e. the conjuction of predicates can result in multiplication or addition of probabilities. </li>
<li>Inclusion principle: The domain of join keys overlap such that each key in the inner relation will also exist in the outer table.</li>
<li>These assumptions may not be true. </li>
</ul>
</li>
</ol>
<h3 id="What-statistics-does-the-DBMS-maintain"><a href="#What-statistics-does-the-DBMS-maintain" class="headerlink" title="What statistics does the DBMS maintain?"></a>What statistics does the DBMS maintain?</h3><ol>
<li>Histograms: <ul>
<li>The naive and most accurate way is to maintain an occurrence count per value in a column. </li>
<li>Equi-width histograms maintain counts for a group of values. All buckets have the same width, i.e. the same number of values. </li>
<li>Equi-depth histograms vary the width of buckets so that the total number of occurrences for each bucket is roughly the same. </li>
<li>Equi-width or equi-depth histograms use the total count of a bucket dividing by the number of values in that bucket as the count of each values. </li>
</ul>
</li>
<li>Sketches: <ul>
<li>Probabilistic data structure that gives an approximate count for a given value. </li>
<li>Cost-model can replace histograms with sketches to improve its selectivity estimate accuracy. </li>
</ul>
</li>
<li>Sampling: <ul>
<li>DBMS maintains a small subset of each table that it then uses to evaluate expressions to compute selectivity. </li>
<li>The selectivity is estimated by running the same query on the sample table. </li>
<li>Sample table is updated when the underlying tables changes significantly. </li>
</ul>
</li>
</ol>
<h2 id="Query-optimization"><a href="#Query-optimization" class="headerlink" title="Query optimization"></a>Query optimization</h2><h3 id="How-do-we-perform-cost-based-optimization"><a href="#How-do-we-perform-cost-based-optimization" class="headerlink" title="How do we perform cost-based optimization?"></a>How do we perform cost-based optimization?</h3><ol>
<li>After performing rule-based rewriting, the DBMS will enumerate different plans for the query and estimate their costs. <ul>
<li>It chooses the best plan it has seen for the query after exhausting all plans or some timeout. </li>
<li>The time spent on search should be significantly smaller than the time of executing query. DBMS can set a time threshold to end search. </li>
</ul>
</li>
<li>DBMS mainly enumerates the access methods (sequential scan, binary search / clustered indexes, index scan) and evaluation ordering. </li>
<li>Query planning for OLTP queries is easy because they are <strong>sargable</strong> (Search Argument Able). <ul>
<li>It is usually just picking the best index. </li>
<li>Joins are almost always on foreign key relationships with a small cardinality. </li>
</ul>
</li>
<li>For multi-relation query planning, there are two choices. <ul>
<li>Bottom-up optimization: Start with nothing and then build up the plan to get to the outcome that you want. </li>
<li>Top-down optimization: Start with the outcome that you want, and then work down the tree to find the optimal plan that gets you to that goal. </li>
</ul>
</li>
</ol>
<h3 id="How-does-bottom-up-optimization-work"><a href="#How-does-bottom-up-optimization-work" class="headerlink" title="How does bottom-up optimization work?"></a>How does bottom-up optimization work?</h3><ol>
<li>Break query up into blocks and generate the logical operators for each block. For each logical operator, generate a set of physical operators that implement it. </li>
<li>The whole diagram can be layered by relations or temporary relations, i.e. results of logical operators. </li>
<li>We can visualize the whole optimization diagram as a tree with different layers. <ul>
<li>The top layer is the output of the query and the bottom layer is all the relations. </li>
<li>The middle layers are the enumerations of different ordering. Each layer only performs one more operator than its last layer. </li>
<li>Hence each pair of layers is connected with an undetermined physical operator. </li>
</ul>
</li>
<li>From bottom layer up, we enumarate the possible physical algorithm of each logical operator. <ul>
<li>Then estimate the cost of all possible physical algorithms. </li>
<li>Leave only the more efficient physical algorithm for each logical operator after comare with only the possible physical algorithms of the same logical operator. </li>
</ul>
</li>
<li>When reaches the top layer, we can determine the most efficient path of all possible paths. </li>
<li>Then iteratively construct a “left-deep” join tree that minimizes the estimated amount of work to execute the plan. <ul>
<li>Generate a left-deep tree is to take advantages of pipeline. </li>
</ul>
</li>
</ol>
<h3 id="How-does-top-down-optimization-work"><a href="#How-does-top-down-optimization-work" class="headerlink" title="How does top-down optimization work?"></a>How does top-down optimization work?</h3><ol>
<li>Start with a logical plan of what we want the query to be. </li>
<li>Perform a branch-and-bound search to traverse the plan tree by converting logical operators into physical operators. <ul>
<li>When traversing from logical operator to logical operators, it is enumarating different ordering. </li>
<li>When traversing from logical operator to physical operators, it is enumarating different physical algorithm. </li>
<li>The layers are similar with bottom-up optimization. </li>
<li>When we meet a logical operator, we need to estimate the cost of its all possible physical algorithms. <ul>
<li>So for each physical algorithm, we need to go deeper until the bottom to calculate the estimation. </li>
<li>For the sub-logical-operators in the physical algorithm, we will enumarate its optimal execution in the lower levels. </li>
</ul>
</li>
<li>During the search, we can cut-off a branch if its cost is already more expensive then another branch we have already seen. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/07/12/Courses/15445/07-Query-Execution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/12/Courses/15445/07-Query-Execution/" class="post-title-link" itemprop="url">07 Query Execution</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-12 16:51:12" itemprop="dateCreated datePublished" datetime="2023-07-12T16:51:12+08:00">2023-07-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-15 16:52:19" itemprop="dateModified" datetime="2023-07-15T16:52:19+08:00">2023-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Sequential-Execution"><a href="#Sequential-Execution" class="headerlink" title="Sequential Execution"></a>Sequential Execution</h1><h2 id="Processing-model"><a href="#Processing-model" class="headerlink" title="Processing model"></a>Processing model</h2><h3 id="What-does-processing-model-do"><a href="#What-does-processing-model-do" class="headerlink" title="What does processing model do?"></a>What does processing model do?</h3><ol>
<li>Processing model defines how the system executes a query plan, i.e. how to traverse the query plan tree. </li>
<li>There are two processing directions: <ul>
<li>Top-to-Bottom: Start with the root and “pull” data up from its children. Tuples are always passed with function calls. </li>
<li>Bottom-to-Top: Start with leaf nodes and push data to their parents. Allows for tighter control of caches/registers in pipelines. </li>
</ul>
</li>
<li>There are three commonly used model: iterator model (volcano/pipeline model), materialization model and vectorized/batch model. </li>
</ol>
<h3 id="How-does-iterator-model-execute"><a href="#How-does-iterator-model-execute" class="headerlink" title="How does iterator model execute?"></a>How does iterator model execute?</h3><ol>
<li><p>In iterator model, operators are executed top-to-down. </p>
</li>
<li><p>Each query plan operator implements a <code>Next()</code> function. On each invocation, the operator returns either a single tuple or a <code>null</code> marker if there are no more tuples. </p>
<ul>
<li><p>The operator implements a loop that calls <code>Next()</code> on its children to retrieve their tuples and then process them. </p>
</li>
<li><p><code>Next()</code> is first called on the root operator and the nodes on the tree will call the <code>Next()</code> on their children recursively. </p>
</li>
</ul>
</li>
<li><p>This model allows for uple pipelining. Although some operators must block until their children emit all their tuples. </p>
<ul>
<li>Joins must wait until all tuples in the leaf child are processed and built the hash table to further process tuples from right child. Hence, the tuples from leaft child need to block the pipeline while the tuples from right child can enable pipeline. </li>
<li>Subqueries and ordering (<code>Order By</code>) clauses also need to block pipeline. These are called pipeline breaker. </li>
</ul>
</li>
<li><p>Output control works easily with this approach. We only need to add constraints on the root operator. </p>
</li>
</ol>
<h3 id="How-does-materialization-model-execute"><a href="#How-does-materialization-model-execute" class="headerlink" title="How does materialization model execute?"></a>How does materialization model execute?</h3><ol>
<li><p>In materialization model, operators are called bottom-to-up. </p>
</li>
<li><p>Each query plan operator implements a <code>Output()</code> function. </p>
<ul>
<li><p>On each invocation, the operator processes its input all at once and then emits its output all at once. </p>
</li>
<li><p>The operator materializes its output as a single result. </p>
</li>
<li>The operators can send either a materialized row or a single column. </li>
</ul>
</li>
<li><p>The DBMS can push down hints (e.g., <code>LIMIT</code>) to avoid scanning too many tuples. </p>
</li>
<li><p>The output can be either whole tuples (NSM) or subsets of columns (DSM). </p>
</li>
<li><p>This model is better for OLTP workloads because queries only access a small number of tuples at a time which means lower execution and coordination overhead and fewer function calls. </p>
<ul>
<li>It is not good for OLAP queries with large intermediate results. </li>
</ul>
</li>
</ol>
<h3 id="How-does-vectorization-model-execute"><a href="#How-does-vectorization-model-execute" class="headerlink" title="How does vectorization model execute?"></a>How does vectorization model execute?</h3><ol>
<li>The problem of iterator model is that it can only process one tuple at a time when we can take multiple tuples and vectorize them to process in parallel (SIMD). </li>
<li>Vectorization model is similar with iterator model except that each operator emits a batch of tuples instead of a single tuple. </li>
<li>This is ideal for OLAP queries because it greatly reduces the number of invocations per operator. <ul>
<li>It allows for operators to more easily use vectorized (SIMD) instructions to process batches of tuples. </li>
</ul>
</li>
</ol>
<h2 id="Access-methods"><a href="#Access-methods" class="headerlink" title="Access methods"></a>Access methods</h2><h3 id="How-can-we-optimize-sequential-scan-with-data-skipping"><a href="#How-can-we-optimize-sequential-scan-with-data-skipping" class="headerlink" title="How can we optimize sequential scan with data skipping?"></a>How can we optimize sequential scan with data skipping?</h3><ol>
<li><p>The first approach is approximate queries. </p>
<ul>
<li><p>This method is lossy, which means that it may return incorrect results, but it is OK. </p>
</li>
<li><p>Execute queries on a sampled subset of the entire table to produce approximate results. </p>
</li>
</ul>
</li>
<li><p>The second approach is zone maps. </p>
<ul>
<li>This method is lossless. </li>
<li>Pre-computed aggregates for the attribute values in a page. DBMS checks the zone map first to decide whether it wants to access the page. </li>
<li>The trade-off is between page size and filter efficacy. </li>
</ul>
</li>
</ol>
<h3 id="What-is-multi-index-scan"><a href="#What-is-multi-index-scan" class="headerlink" title="What is multi-index scan?"></a>What is multi-index scan?</h3><ol>
<li>If there are multiple indexes that the DBMS can use for a query, one method for DBMS to execute is try to filter tuples with index that has least number of tuples matches and filter other indexes based on the filtered tuples of previous indexes. <ul>
<li>This is ideal in the case that some indexes has little tuples that matches. Filtering those indexes first can significantly reduce the number of tuples to process in following indexes. </li>
</ul>
</li>
<li>If all indexes has a lot of matching tuples, we can use another method:<ul>
<li>Compute sets of Record IDs using each matching index. Combine these sets based on the query’s predicates (union or intersect). Retrieve the records and apply any remaining predicates. </li>
<li>In this way, we can reduce a log of I/O and memory space by only fetching Record IDs in the first phase instead of fetching entire tuple. </li>
</ul>
</li>
</ol>
<h2 id="Modification-queries"><a href="#Modification-queries" class="headerlink" title="Modification queries"></a>Modification queries</h2><h3 id="How-should-we-execute-update-and-delete-queries"><a href="#How-should-we-execute-update-and-delete-queries" class="headerlink" title="How should we execute update and delete queries?"></a>How should we execute update and delete queries?</h3><ol>
<li>Operators that modify the database (<code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) are responsible for modifying the target table and its indexes. <ul>
<li>The output of these operators can either be Record Ids or tuple data (i.e., <code>RETURNING</code>). </li>
</ul>
</li>
<li>For update or delete, child operators pass Record IDs for target tuples. <ul>
<li>The operator should remove the corresponding item from index. </li>
<li>Then the operator can modify tuple or remove tuple. </li>
<li>Update should re-insert modified tuple into index again. </li>
</ul>
</li>
<li>Updates have a possible Halloween problem: <ul>
<li>When we re-inserted the modified tuple, its new place may be ahead of cursor, i.e. we will pass the new tuple again in the future. <ul>
<li>E.g. when the index is sorted according to an attribute, and the modification increases that attribute. </li>
</ul>
</li>
<li>An update operation changes the physical location of a tuple, which causes a scan operator to visit the tuple multiple times. It can occur on clustered tables or index scans. </li>
<li>The solution is to keep track of previously seen tuples (e.g. through Record IDs). </li>
</ul>
</li>
</ol>
<h3 id="How-should-we-execute-insert-queries"><a href="#How-should-we-execute-insert-queries" class="headerlink" title="How should we execute insert queries?"></a>How should we execute insert queries?</h3><ol>
<li>The first choice is to materialize tuples inside of the operator. <ul>
<li>The insert operator needs to implement its own method of how to materialize tuples. </li>
</ul>
</li>
<li>The second choice is that operator inserts any tuple passed in from child operators. <ul>
<li>The insert operator only need to accept tuples from its children instead of implementing itself. </li>
</ul>
</li>
</ol>
<h2 id="How-to-evaluate-expressions"><a href="#How-to-evaluate-expressions" class="headerlink" title="How to evaluate expressions?"></a>How to evaluate expressions?</h2><ol>
<li>The DBMS represents a <code>WHERE</code> clause as an expression tree. </li>
<li>The nodes in the tree represent different expression types: Comparisons (<code>=, &lt;, &gt;, !=</code>), conjunction (<code>AND</code>), disjunction (<code>OR</code>), arithmetic operators (<code>+, -, *, /, %</code>), constant values, tuple attribute references. </li>
<li>When evaluate the expression, DFS through the expression tree from the root. <ul>
<li>The performance is poor due to that the DBMS traverses the tree and for each node that it visits it must figure out what the operator needs to do. </li>
</ul>
</li>
<li>A better approach is to just evaluate the expression directly. <ul>
<li>Compile a function of the express (e.g. JIT compilation). In evaluation, DBMS just execute the compiled function instead of traversing through the tree. </li>
</ul>
</li>
</ol>
<h1 id="Parallel-execution"><a href="#Parallel-execution" class="headerlink" title="Parallel execution"></a>Parallel execution</h1><h2 id="Parallel-and-distributed"><a href="#Parallel-and-distributed" class="headerlink" title="Parallel and distributed"></a>Parallel and distributed</h2><h3 id="Why-care-about-parallel-execution"><a href="#Why-care-about-parallel-execution" class="headerlink" title="Why care about parallel execution?"></a>Why care about parallel execution?</h3><ol>
<li>It increased performance for potentially the same hardware resources, i.e. to gain higher throughput and lower latency. </li>
<li>It increased the responsiveness of the system. </li>
<li>It potentially lower total cost of ownership (TCO). <ul>
<li>Fewer machines means less parts / physical footprint / energy consumption. </li>
</ul>
</li>
</ol>
<h3 id="What-are-the-similarities-and-differences-between-parallel-and-distributed-DBMS"><a href="#What-are-the-similarities-and-differences-between-parallel-and-distributed-DBMS" class="headerlink" title="What are the similarities and differences between parallel and distributed DBMS?"></a>What are the similarities and differences between parallel and distributed DBMS?</h3><ol>
<li>Similarities:<ul>
<li>Database is spread out across multiple resources to improve different aspects of the DBMS. </li>
<li>They both need to make database  to appear as a single logical database instance to the application, regardless of physical organization. </li>
<li>SQL query for a single-resource DBMS should generate same result on a parallel or distributed DBMS. </li>
</ul>
</li>
<li>Differences:<ul>
<li>For parallel DBMSs, resources are physically close to each other. Hence resources communicate over high-speed interconnect. And communication is assumed to be cheap and reliable. </li>
<li>For distributed DBMSs, resources can be far from each other. Resources communicate using slow(er) interconnect. Therefore, communication cost and problems cannot be ignored. And communication is considered unreliable. </li>
</ul>
</li>
</ol>
<h2 id="Process-model"><a href="#Process-model" class="headerlink" title="Process model"></a>Process model</h2><h3 id="What-does-process-model-of-parallel-DBMSs-need-to-do"><a href="#What-does-process-model-of-parallel-DBMSs-need-to-do" class="headerlink" title="What does process model of parallel DBMSs need to do?"></a>What does process model of parallel DBMSs need to do?</h3><ol>
<li>It defines how the system is architected to support concurrent requests from a multi-user application. </li>
<li>A worker is the DBMS component that is responsible for executing tasks on behalf of the client and returning the results. </li>
<li>There are three approaches: process per DBMS worker, thread per DBMS worker and embedded DBMS</li>
</ol>
<h3 id="What-is-process-per-worker-model"><a href="#What-is-process-per-worker-model" class="headerlink" title="What is process per worker model?"></a>What is process per worker model?</h3><ol>
<li>Each worker is a separate OS process. Hence, this model relies on OS scheduler entirely. </li>
<li>When an application connect with DBMS, it connect with a dispatcher process. The dispatcher picks on the processes for the application. Then the application communicate with the process directly. </li>
<li>The processes can use shared-memory for global data structures. </li>
<li>The advantage of this model is that a process crash does not take down entire system. </li>
</ol>
<h3 id="What-is-thread-per-worker-model"><a href="#What-is-thread-per-worker-model" class="headerlink" title="What is thread per worker model?"></a>What is thread per worker model?</h3><ol>
<li>In this model, the whole DBMS is a single process with multiple worker threads. </li>
<li>DBMS (mostly) manages its own scheduling by controlling what each threads is doing. <ul>
<li>This also means less overhead per context switch and that DBMS does not have to manage shared memory. </li>
</ul>
</li>
<li>There may or may not have a dispatcher thread in the front. <ul>
<li>Applications may connect to dispatcher, and dispatcher immediately forward request to another thread while application does not know about it. </li>
<li>Or applications can use the same scheme as process per worker model. </li>
</ul>
</li>
<li>In this model, thread crash may kill the entire system. </li>
</ol>
<h3 id="What-is-considered-when-DBMS-scheduling-threads"><a href="#What-is-considered-when-DBMS-scheduling-threads" class="headerlink" title="What is considered when DBMS scheduling threads?"></a>What is considered when DBMS scheduling threads?</h3><p>For each query plan, the DBMS decides where, when, and how to execute it. </p>
<ol>
<li>How many tasks should it use?</li>
<li>How many CPU cores should it use?</li>
<li>What CPU core should the tasks execute on? </li>
<li>Where should a task store its output?</li>
</ol>
<h3 id="What-is-embedded-DBMS-model"><a href="#What-is-embedded-DBMS-model" class="headerlink" title="What is embedded DBMS model?"></a>What is embedded DBMS model?</h3><ol>
<li>In aforementioned systems and most common systems, applications are in separate machines. They are connected through TCP or socket. Even if the applications crashed, DBMS still remains running. </li>
<li>In embedded DBMS, DBMS runs inside of the same address space as the application. Application is (mostly) responsible for threads and scheduling. </li>
<li>The application may support outside connections. </li>
</ol>
<h2 id="Query-level-parallelism"><a href="#Query-level-parallelism" class="headerlink" title="Query-level parallelism"></a>Query-level parallelism</h2><h3 id="What-are-the-query-level-parallelisms"><a href="#What-are-the-query-level-parallelisms" class="headerlink" title="What are the query-level parallelisms?"></a>What are the query-level parallelisms?</h3><ol>
<li>Inter-Query: Execute multiple disparate queries simultaneously. <ul>
<li>This parallelism increases throughput and reduces latency. It improves overall performance by allowing multiple queries to execute simultaneously. </li>
<li>If queries are read-only, then this requires almost no explicit coordination between queries. Buffer pool can handle most of the sharing if necessary. </li>
<li>If multiple queries are updating the database at the same time, then this is hard to do correctly. </li>
</ul>
</li>
<li>Intra-Query: Execute the operations of a single query in parallel. <ul>
<li>This parallelism decreases latency for long-running queries, especially for OLAP queries. It improves the performance of a single query by executing its operators in parallel. </li>
<li>Organize operators in terms of a producer/consumer paradigm. </li>
</ul>
</li>
</ol>
<h3 id="How-can-we-achieve-intra-query-parallelism"><a href="#How-can-we-achieve-intra-query-parallelism" class="headerlink" title="How can we achieve intra-query parallelism?"></a>How can we achieve intra-query parallelism?</h3><ol>
<li>The first approach is using intra-operator (horizontal) parallelism. <ul>
<li>Decompose operators into independent fragments that perform the same function on different subsets of data. </li>
<li>In the generated query plan, those decomposed operators are copied for each thread. </li>
<li>The DBMS inserts an exchange operator into the query plan to coalesce/split results from multiple children/parent operators. The exchange operators are similar with barriers stating that data cannot be sent up to parent until received all results. </li>
<li>There are three kinds of exchange operators:<ul>
<li>Gather: Combine the results from multiple workers into a single output stream. </li>
<li>Distribute: Split a single input stream into multiple output streams. </li>
<li>Repartition: Shuffle multiple input streams across multiple output streams. </li>
</ul>
</li>
</ul>
</li>
<li>The second approach is using inter-operator (vertical / pipeline) parallelism. <ul>
<li>Operations are overlapped in order to pipeline data from one stage to the next without materialization. </li>
<li>Each operator is a worker. Workers execute operators from different segments of a query plan at the same time. </li>
</ul>
</li>
<li>We can also combine these two approaches, which is call bushy parallelism. </li>
</ol>
<h2 id="I-O-paralleism"><a href="#I-O-paralleism" class="headerlink" title="I/O paralleism"></a>I/O paralleism</h2><h3 id="What-is-the-problem-of-query-level-parallelism"><a href="#What-is-the-problem-of-query-level-parallelism" class="headerlink" title="What is the problem of query-level parallelism?"></a>What is the problem of query-level parallelism?</h3><ol>
<li>Using additional processes/threads to execute queries in parallel won’t help if the disk is always the main bottleneck. </li>
<li>It can sometimes make the DBMS’s performance worse if worker is accessing different segments of the disk at the same time. </li>
</ol>
<h3 id="How-can-we-parallel-I-Os-with-multi-disk"><a href="#How-can-we-parallel-I-Os-with-multi-disk" class="headerlink" title="How can we parallel I/Os with multi-disk?"></a>How can we parallel I/Os with multi-disk?</h3><ol>
<li><p>Split the DBMS across multiple storage devices to improve disk bandwidth latency. </p>
<ul>
<li>There are many options<ul>
<li>Multiple disks per database, one database per disk, one relation per disk, split relation across multiple disks. </li>
<li>The main trade-off is the number of disks and I/O parallelism. </li>
</ul>
</li>
</ul>
</li>
<li><p>Configure OS/hardware to store the DBMS’s files across multiple storage<br>devices, e.g. storage appliances, RAID configuration. </p>
<ul>
<li><p>This is transparent to the DBMS. </p>
</li>
<li><p>RAID 0 strips data into different disks. Each disk stores different data. </p>
</li>
<li>RAID 1 mirrors data in different disks. Each disk stores the same data. </li>
</ul>
</li>
</ol>
<h3 id="How-can-we-partition-database"><a href="#How-can-we-partition-database" class="headerlink" title="How can we partition database?"></a>How can we partition database?</h3><ol>
<li>Some DBMSs allow you to specify the disk location of each individual database. <ul>
<li>The buffer pool manager maps a page to a disk location. </li>
<li>This is also easy to do at the filesystem level if the DBMS stores each database in a separate directory. </li>
<li>The DBMS recovery log file might still be shared if transactions can update multiple databases. </li>
</ul>
</li>
<li>Logical splitting is to split single logical table into disjoint physical segments that are stored/managed separately. <ul>
<li>Partitioning should (ideally) be transparent to the application. </li>
<li>The application should only access logical tables and not have to worry about how things are physically stored. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/07/07/OpenSource/BusTub/Project-2-B-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/07/OpenSource/BusTub/Project-2-B-Tree/" class="post-title-link" itemprop="url">Project #2: B+Tree</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-07 13:28:28" itemprop="dateCreated datePublished" datetime="2023-07-07T13:28:28+08:00">2023-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-09 22:07:54" itemprop="dateModified" datetime="2023-07-09T22:07:54+08:00">2023-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/BusTub/" itemprop="url" rel="index"><span itemprop="name">BusTub</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@<a href="c">toc</a></p>
<h1 id="B-Tree-Page"><a href="#B-Tree-Page" class="headerlink" title="B+Tree Page"></a>B+Tree Page</h1><ol>
<li><p>The <code>size_</code> in each page means the number of stored values not keys, i.e. in internal nodes, <code>size_</code> is $1$ larger than the number of keys. </p>
</li>
<li><p><code>KeyComparator</code> accept two keys to compare, which will return $0$ when they are equal, $-1$ for the first key “smaller” than the second key, $1$ for the second key being larger. </p>
</li>
<li><p>We need to design the sematic of the binary search that will be used in search, insert and delete a key inside a node. </p>
<ul>
<li>In the leaf node: <ul>
<li>For search and delete, we want this function to tell us the index of the key is it exists. </li>
<li>For insert, we want this function to indicate the index we need to place the key. </li>
</ul>
</li>
<li>In the internal node:<ul>
<li>For search, we only want it to inform us the child that might have the given key. </li>
<li>For insert, we want it to return the page ID to find a proper leaf page to store the key in forward search and give the index we need to place the split key when backward split is required. </li>
<li>For delete, we want it the same as for insert in the forward search and to provide the index to help merge two children when the backward merge is required. </li>
</ul>
</li>
<li>In the following implemetation of binary search, it will return the index of the key if it exists, or it will return the largest index with smaller key. <ul>
<li>Notably that if the given key is smaller than all keys in the node, it will return $-1$. </li>
<li>In internal node, we would expect the smallest possible result is $0$ since the first key is <code>NULL</code> which should be smaller than any other keys. </li>
</ul>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">BinarySearch</span><span class="params">(KeyType key, KeyComparator comparator)</span> <span class="type">const</span> -&gt; <span class="type">int</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> lo = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> hi = <span class="built_in">GetSize</span>();</span><br><span class="line">  <span class="keyword">while</span> (lo &lt; hi) &#123;</span><br><span class="line">    <span class="type">int</span> mid = (lo + hi) / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="number">0</span> &lt; mid &amp;&amp; <span class="built_in">comparator</span>(key, array_[mid].first) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      hi = mid;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      lo = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> lo - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>FindNextPageID</code> will provide the proper child for the caller to access to find a certain key. </p>
<ul>
<li>For leaf node, this will only be used in B+Tree search. If the binary search result is $-1$, we can conclude that that key does not exist and should tell caller that. </li>
<li>For internal node, this will be used in B+Tree search or the forward search in insert and delete. If the binary search result is $-1$, then we should return the first value to indicate the key is smaller than all keys. </li>
</ul>
</li>
<li><p><code>InsertKey</code> should </p>
</li>
</ol>
<h1 id="B-Tree-search"><a href="#B-Tree-search" class="headerlink" title="B+Tree search"></a>B+Tree search</h1><ol>
<li>The thought is simple: we just go down from the root until hit a leaf node. If the key is in the leaf node, return the value. Otherwise, the key does not exists. </li>
<li>For concurrency control, we can only release the latch of a page after acquired the latch of its child. <ul>
<li>This can be implemented with the move assignment operation of <code>PageGuard</code>. In the execution, the right expression will first acquire the latch of next page, then destroy the original page guard of the left variable. </li>
</ul>
</li>
</ol>
<h1 id="B-Tree-insert"><a href="#B-Tree-insert" class="headerlink" title="B+Tree insert"></a>B+Tree insert</h1><ol>
<li><p>For concurrency control, I implemented both the optimistic scheme and the pessimistic scheme. </p>
<ul>
<li><p>The program will first try with optimistic search with only write latch on leaf node. </p>
</li>
<li><p>If the leaf node may overflow, release all latches and start again trying to acquire write latch for all nodes. </p>
</li>
</ul>
</li>
<li><p>In the optimistic search, we can use the queue in <code>Context</code>. </p>
<ul>
<li>When we fetched a new page guard, we push it to the back of <code>ctx.read_set_</code> and pop the front element out of the queue. Every time we just need to access the last element of the queue to find the next page to read. </li>
<li>When we realised that we have reached the leaf node, we are holding a read latch for that page. Hence, we still need to release the read latch and re-acquire the write latch, i.e. we cannot release the latch of last internal page when we first acquire the leaf page. </li>
<li>The solution is to release the latch before read the “grand-child” of it. </li>
</ul>
</li>
<li><p>In the pessimistic search, we need to acquire write latch for all pages we want to access. </p>
<ul>
<li>We can check whether a node is safe after its write latch is acquired. If the node is safe, we can release all latches acquired before it, including the header page. </li>
</ul>
</li>
<li><p>In the insert function, there are three cases to handle: </p>
<ul>
<li>The first case is that this is an empty tree, i.e. the <code>root_page_id_</code> in header page is invalid. We just need to create a new page for the node and update header page. </li>
<li>The second case is that the leaf node won’t overflow where a simple insertion is enough. </li>
<li>The last case is that the leaf node might overflow. </li>
</ul>
</li>
<li><p>When the leaf node might overflow: </p>
<ul>
<li>After acquired the write latches, we need to check whether there is an overflow again in case that other thread already handled overflow causing unexpected split. </li>
<li><code>SolveLeafOverflow</code><ul>
<li>When a leaf reaches max size after insertion, it will immediately split. So this is used after the insertion. </li>
<li>It will create a new page to store the larger half the nodes and return the first key in the new page to insert to its parent node to indicate to this page. </li>
<li>Also, it need to take care of the sibling pointers between leaf nodes. The new page will point to what the original page points to. And the original page will point to the new page. </li>
</ul>
</li>
<li><code>SolveInternalOverflow</code><ul>
<li>Internal nodes won’t split immediately when it reaches max size. This means that internal nodes must split first before insertion, otherwise the address will overflow. </li>
<li>The process is similar with the leaf case, except that it will choose a proper page to insert the key after split. (Or it just does not need to split if it is a safe node). </li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="B-Tree-delete"><a href="#B-Tree-delete" class="headerlink" title="B+Tree delete"></a>B+Tree delete</h1><ol>
<li>The concurrency control is similar with the insert operation, except that the condition of whether a node is safe is different. </li>
<li>Compare with insertion, <ul>
<li>if the tree is empty, there is nothing else to do; </li>
<li>if the leaf node won’t underflow, a simple deletion is enough; </li>
<li>if the leaf node might underflow, we need to handle it and possible following cascade underflow. </li>
</ul>
</li>
<li>When the leaf node might overflow:<ul>
<li>Similar with the insertion situation, we need to re-acquire write latches, and check whether the underflow is handled by another thread. </li>
<li>If the underflowed leaf is the root, i.e. the tree has only one node, we do not need to do anything further. </li>
<li><code>SolveLeafUnderflow</code><ul>
<li>There are three situations: left sibling can borrow a key, right sibling can borrow a key or neither siblings can borrow a key. </li>
<li>When we can borrow a key, the underflow is solved easily, no more cascading. We need to modify the key in parent node that separates the two involving node to the new first key of the right node. </li>
<li>When we need to merge with one of the siblings, we also need to handle the pointers between leaf nodes. If we move the data of the left node to the right node and delete the left node, it is hard for us to modify the pointer of the left of the left node. Instead, if we move delete the right node, we only need to modify the pointer of the left node to the original pointer of the right node. </li>
<li>If a leaf only has left sibling or right sibling to merge with, then we do not have a choice. </li>
</ul>
</li>
<li><code>SolveInternalUnderflow</code><ul>
<li>The process is similar with <code>SolveLeafUnderflow</code>, except how to borrow a key. </li>
<li>When a node is borrowing a key to its left sibling, it will borrow the first valid key and the fire value, i.e. <code>array_[1].first</code> and <code>array_[0].second</code>. </li>
<li>When a node is borrowing a key to its right sibling, it will borrow the last key-value pair. And the node accpeting those keys will use the key as its first valid key and the value as its first valid value. </li>
<li>We need to set the corresponding key in parent node to the borrowed key. </li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/06/28/Courses/15445/06-Operator-Algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/28/Courses/15445/06-Operator-Algorithms/" class="post-title-link" itemprop="url">06 Operator Algorithms</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-28 13:16:33" itemprop="dateCreated datePublished" datetime="2023-06-28T13:16:33+08:00">2023-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-12 16:54:01" itemprop="dateModified" datetime="2023-07-12T16:54:01+08:00">2023-07-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Execute-queries"><a href="#Execute-queries" class="headerlink" title="Execute queries"></a>Execute queries</h1><h2 id="How-are-queries-planned"><a href="#How-are-queries-planned" class="headerlink" title="How are queries planned?"></a>How are queries planned?</h2><ol>
<li>The operators are arranged in an abstract syntax tree. Data flows from the leaves of the tree up towards the root. </li>
<li>The leaf nodes are access methods, e.g scanning index and scanning table, feeding data up along its path to its parent node to do processing. </li>
<li>The output of the root node is the result of the query. </li>
<li>This tree only describes a logical plan, i.e. instead of telling what implementation to use, it is just the logical flow we want. SQL only declare logical plan while it is the database system’s job to figure out the optimal way to execute. </li>
</ol>
<h2 id="What-is-the-assumption-of-algorithms-in-database-system"><a href="#What-is-the-assumption-of-algorithms-in-database-system" class="headerlink" title="What is the assumption of algorithms in database system?"></a>What is the assumption of algorithms in database system?</h2><ol>
<li>Just like it cannot assume that a table fits entirely in memory, a disk-oriented DBMS cannot assume that query results fit in memory. </li>
<li>We will use the buffer pool to implement algorithms that need to spill to disk. </li>
<li>We are also going to prefer algorithms that maximize the amount of sequential I/O.</li>
</ol>
<h1 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h1><h2 id="What-implementation-should-we-use-to-execute-a-query-containing-an-ORDER-BY-with-a-LIMIT"><a href="#What-implementation-should-we-use-to-execute-a-query-containing-an-ORDER-BY-with-a-LIMIT" class="headerlink" title="What implementation should we use to execute a query containing an ORDER BY with a LIMIT?"></a>What implementation should we use to execute a query containing an ORDER BY with a LIMIT?</h2><ol>
<li>The DBMS only needs to scan the data once to find the top-N elements. </li>
<li>The ideal scenario for heapsort is when the top-N elements fit in memory, so that the DBMS only has to maintain an in-memory sorted priority queue while scanning the data. </li>
</ol>
<h2 id="What-if-the-data-is-too-large-to-fit-in-memory-for-any-clause-including-ORDER-BY-with-a-LIMIT"><a href="#What-if-the-data-is-too-large-to-fit-in-memory-for-any-clause-including-ORDER-BY-with-a-LIMIT" class="headerlink" title="What if the data is too large to fit in memory? (for any clause including ORDER BY with a LIMIT)"></a>What if the data is too large to fit in memory? (for any clause including ORDER BY with a LIMIT)</h2><ol>
<li>We do not want to use quick sort in this scenario since data spilling to disk will cause too many random access. </li>
<li>We can use external merge sort that splits data into separate runs, sorts them individually, and then combines them into longer sorted runs. </li>
<li>A run is a list of key/value pairs. <ul>
<li>Keys are the attribute(s) to compare to compute the sort order. </li>
<li>Values have two choices: it can either be the actual tuple data (i.e. early materialization) or be the Record IDs (i.e. late materialization)<ul>
<li>The advantage of early materialization is that it can be faster to produce result while the disadvantage is that it need to copy more data during the procedule. </li>
<li>The advantage of late materialization is that it can only fetch wanted data while it needs to find the actual data else where (probably involving another disk I/O). </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="How-to-perform-an-external-merge-sort"><a href="#How-to-perform-an-external-merge-sort" class="headerlink" title="How to perform an external merge sort?"></a>How to perform an external merge sort?</h2><ol>
<li>Data is broken up into $N$ pages. The DBMS has a finite number of $B$ buffer pool pages to hold input and output data.</li>
<li>In the first phase, sort chunks of data that fit in memory and then write back the sorted chunks to a file on disk. <ul>
<li>In the first pass, we can read all $B$ pages of the table into memory and sort pages into runs and write them back to disk. </li>
<li>Do not sort in buffer since we do not wish other threads seeing some partially sorted data which may cause errors. Copy the data to somewhere else and copy back to buffer after sorted. </li>
</ul>
</li>
<li>In the second phase, combine sorted runs into larger chunks. <ul>
<li>In the following passes, each pass use $B-1$ pages for input and $1$ page for output. </li>
<li>Recursively merge pairs of runs into runs $B-1$-times as long. </li>
</ul>
</li>
<li>In general, the first pass create $\lceil N/B\rceil$ sorted runs of size $B$ and the following passes are $(B-1)$-way merge. <ul>
<li>The number of passes is $1+\lceil\log_{B-1}\lceil N/B\rceil\rceil$. </li>
<li>The total I/O cost is $2N\cdot (number\ of\ passes)$. </li>
</ul>
</li>
<li>In a 2-way external merge sort, instead of sort the $B$ pages into a run in the first pass, each page is sorted into a run. <ul>
<li>So the number of passes is $1+\lceil\log_2N\rceil$. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-hide-the-disk-I-O"><a href="#How-can-we-hide-the-disk-I-O" class="headerlink" title="How can we hide the disk I/O?"></a>How can we hide the disk I/O?</h2><ol>
<li>A typical setup is using $3$ pages ($2$ for input pages and $1$ for output page). Even if we have more buffer space available ($B&gt;3$), it does not effectively utilize them if the worker must block on disk I/O. </li>
<li>We can prefetch the next run in the background and store it in a second buffer while the system is processing the current run. </li>
<li>This method reduces the wait time for I/O requests at each step by continuously utilizing the disk. </li>
</ol>
<h2 id="How-can-we-optimize-comparison"><a href="#How-can-we-optimize-comparison" class="headerlink" title="How can we optimize comparison?"></a>How can we optimize comparison?</h2><ol>
<li>The first approach is code specialization. <ul>
<li>Instead of providing a comparison function as a pointer to sorting algorithm, create a hardcoded version of sort that is specific to a key type. </li>
</ul>
</li>
<li>For string keys, the second approach is suffix truncation. <ul>
<li>First compare a binary prefix of keys instead of slower string comparison. </li>
<li>Fallback to slower version if prefixes are equal. </li>
</ul>
</li>
</ol>
<h2 id="How-can-use-use-B-Tree-for-sorting-since-it-is-sorted"><a href="#How-can-use-use-B-Tree-for-sorting-since-it-is-sorted" class="headerlink" title="How can use use B+Tree for sorting since it is sorted?"></a>How can use use B+Tree for sorting since it is sorted?</h2><ol>
<li>If the table that must be sorted already has a B+Tree index on the sort attribute(s), then we can use that to accelerate sorting. </li>
<li>Retrieve tuples in desired sort order by simply traversing the leaf pages of the tree. <ul>
<li>For clustered B+Tree, this is always better than external sorting because there is no computational cost, and all disk access is sequential. </li>
<li>For non-clustered B+Tree, this is almost always a bad idea. In general, one I/O per data record. </li>
</ul>
</li>
</ol>
<h1 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h1><h2 id="How-can-we-implement-aggregations"><a href="#How-can-we-implement-aggregations" class="headerlink" title="How can we implement aggregations?"></a>How can we implement aggregations?</h2><ol>
<li>The DBMS needs a way to quickly find tuples with the same distinguishing attributes for grouping. </li>
<li>For aggregations specified with a <code>ORDER BY</code> clause, we can directly use the aforementioned sorting algorithms. </li>
<li>For queries do not need the data to be ordered, e.g. forming groups in <code>GROUP BY</code> or removing duplicates in <code>DISTINCT</code>, hashing is a better alternative. Hashing can be computationally cheaper than sorting. <ul>
<li>For each record, check whether there is already an entry in the hash table. For <code>DISTINCT</code>, just discard duplicate. For <code>GROUP BY</code>, perform aggregate computation. </li>
</ul>
</li>
</ol>
<h2 id="How-to-do-hashing-when-spill-data-to-disk"><a href="#How-to-do-hashing-when-spill-data-to-disk" class="headerlink" title="How to do hashing when spill data to disk?"></a>How to do hashing when spill data to disk?</h2><ol>
<li>The first phase is partition. Divide tuples into buckets based on hash key. Write them out to disk when they get full. <ul>
<li>Use a hash function $h_1$ to split tuples into partitions on disk. </li>
<li>A partition is one or more pages that contain the set of keys with the same hash value. </li>
<li>Assume that we have $B$ buffers. We will use $B-1$ buffers for the partitions and $1$ buffer for the input data. </li>
</ul>
</li>
<li>The second phase is ReHash. Build in-memory hash table for each partition and compute the aggregation. <ul>
<li>For each partition on disk, read it into memory and build an in-memory hash table based on a second hash function $h_2$. </li>
<li>Then go through each bucket of this hash table to bring together matching tuples. </li>
<li>Each time we can use $B-1$ pages as input pages and $1$ page as output page. <ul>
<li>In each round, we can read in $B-1$ partitions. </li>
<li>After each round is finished, we can clear the hash table since the next $B-1$ partitions definitely won’t have the same keys as last round. </li>
</ul>
</li>
</ul>
</li>
<li>During the ReHash phase, store pairs of the form $(GroupKey→RunningVal)$. <ul>
<li>When we want to insert a new tuple into the hash table, if we find a matching $GroupKey$, just update the$RunningVal$ appropriately. Else insert a new $(GroupKey→RunningVal)$. </li>
<li>The running totals of different aggregation function is as followed:<ul>
<li>$AVG(col)\rightarrow (COUNT,SUM)$</li>
<li>$SUM(col)\rightarrow(SUM)$</li>
<li>$COUNT(col)\rightarrow(COUNT)$</li>
<li>$MIN(col)\rightarrow(MIN)$</li>
<li>$MAX(col)\rightarrow(MAX)$</li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h1><h2 id="Why-do-we-need-to-join"><a href="#Why-do-we-need-to-join" class="headerlink" title="Why do we need to join?"></a>Why do we need to join?</h2><ol>
<li>Tables are normalized in a relational database to avoid unnecessary repetition of information. </li>
<li>Join operator is used to reconstruct the original tuples without any information loss. </li>
<li>Join is an important operator in both OLAP and OLTP systems. Especially for OLAP system, join could take up to $15\sim 50\%$ of time. </li>
</ol>
<h2 id="What-are-join-algorithms-doing"><a href="#What-are-join-algorithms-doing" class="headerlink" title="What are join algorithms doing?"></a>What are join algorithms doing?</h2><ol>
<li>Most important kind is binary joins using inner equijoin algorithms. <ul>
<li>Binary means that the operator takes two tables as input. </li>
<li>Inner means that it matches certain tuple of left table with another tuple in the right table. </li>
<li>Equijoin means that the condition of matching two tuples is the equivalence of some attributes. </li>
</ul>
</li>
<li>There are also other joins. <ul>
<li>Multi-way joins take more than two tables as input, which exists primarily in research literature. </li>
<li>Beside equijoin, there could also be anti-join, non-equijoin, etc. </li>
</ul>
</li>
<li>Compare with cross-product, join is more efficient and can be carefully optimized. </li>
</ol>
<h2 id="What-are-the-outputs-of-join-algorithms"><a href="#What-are-the-outputs-of-join-algorithms" class="headerlink" title="What are the outputs of join algorithms?"></a>What are the outputs of join algorithms?</h2><ol>
<li>In <code>R JOIN S</code>, for tuple $r \in R$ and tuple $s \in S$ that match on join attributes, concatenate $r$ and $s$ together into a new tuple. </li>
<li>The output contents can vary depends on processing model, storage model or data requirements in query. </li>
<li>Basically, there are two choises similar with sort. <ul>
<li>Early materialization<ul>
<li>Copy the values for the attributes in outer and inner tuples into a new output tuple. </li>
<li>Subsequent operators in the query plan never need to go back to the base tables to get more data.</li>
</ul>
</li>
<li>Late materialization<ul>
<li>Only copy the joins keys along with the Record IDs of the matching tuples. </li>
<li>This is ideal for column stores because the DBMS does not copy data that is not needed for the query. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="How-can-we-measure-the-cost-of-join"><a href="#How-can-we-measure-the-cost-of-join" class="headerlink" title="How can we measure the cost of join?"></a>How can we measure the cost of join?</h2><ol>
<li>We can measure the cost of join by the number of I/Os to compute join. </li>
<li>Output costs are ignored since that depends on the data. </li>
<li>In the following analysis, we assume there are $m$ tuples stored in $M$ pages in table $R$, $n$ tuples stored in $N$ pages in table $S$. </li>
</ol>
<h2 id="Join-algorithms"><a href="#Join-algorithms" class="headerlink" title="Join algorithms"></a>Join algorithms</h2><h3 id="Nested-loop-join"><a href="#Nested-loop-join" class="headerlink" title="Nested loop join"></a>Nested loop join</h3><h4 id="What-is-the-most-naive-algorithm"><a href="#What-is-the-most-naive-algorithm" class="headerlink" title="What is the most naive algorithm?"></a>What is the most naive algorithm?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> S:</span><br><span class="line">    emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure>
<ol>
<li>For $R\bowtie S$, he left table $R$ in the outer loop is called outer table and $S$ is called the inner table. </li>
<li>For every tuple in $R$, it scans $S$ once. The cost is $M+(m\cdot N)$. </li>
<li>If we use the smaller table with less tuples as the outer table, we can have a better performance since the number of pages is significant smaller than the number of tuples. </li>
</ol>
<h4 id="How-can-we-better-use-the-data-already-read-from-disk"><a href="#How-can-we-better-use-the-data-already-read-from-disk" class="headerlink" title="How can we better use the data already read from disk?"></a>How can we better use the data already read from disk?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> block B_R <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> block B_S <span class="keyword">in</span> S:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> B_R:</span><br><span class="line">      <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> B_S:</span><br><span class="line">        emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure>
<ol>
<li>For every read block $B_R$ and $B_S$, we try to compute as much as possible, i.e. pair all tuples in $B_R$ with all tuples in $B_S$. </li>
<li>For every block in R, it scans S once. The cost is $M+(M\cdot N)$. </li>
<li>$M\cdot N$ won’t be affected by the order of tables. However, if we let the smaller table with less pages as the outer table, the first term can be smaller. </li>
</ol>
<h4 id="How-can-we-take-advantages-of-more-buffer-space"><a href="#How-can-we-take-advantages-of-more-buffer-space" class="headerlink" title="How can we take advantages of more buffer space?"></a>How can we take advantages of more buffer space?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> B-<span class="number">2</span> pages p_R <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> page p_S <span class="keyword">in</span> S:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> B_2 pages:</span><br><span class="line">      <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> p_S:</span><br><span class="line">        emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure>
<ol>
<li>Use $B-2$ buffers for scanning the outer table. Use one buffer for the inner table, one buffer for storing<br>output. </li>
<li>The cost is $M+(\lceil M/(B-2)\rceil\cdot N)$. </li>
<li>If the outer relation completely fits in memory, the cost is $M+N$. </li>
</ol>
<h4 id="Can-we-avoid-sequential-scans-by-using-an-index"><a href="#Can-we-avoid-sequential-scans-by-using-an-index" class="headerlink" title="Can we avoid sequential scans by using an index?"></a>Can we avoid sequential scans by using an index?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> Index(r_i = s_j):</span><br><span class="line">    emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure>
<p>Assume the cost of each index probe is some constant $C$ per tuple. The cost is $M+(m\cdot C)$</p>
<h3 id="Sort-merge-join"><a href="#Sort-merge-join" class="headerlink" title="Sort-merge join"></a>Sort-merge join</h3><h4 id="What-is-the-process-of-sort-merge-join"><a href="#What-is-the-process-of-sort-merge-join" class="headerlink" title="What is the process of sort-merge join?"></a>What is the process of sort-merge join?</h4><ol>
<li>The first phase is to sort both tables on the join key(s). </li>
<li>In the second phase, we step through the two sorted tables with cursors and emit matching tuples. </li>
<li>The sort cost of outer table is $2M\cdot(1+\lceil \log_{B-1}\lceil M/B\rceil\rceil)$ and the sort cost of inner table is $2N\cdot (1+\lceil\log_{B-1}\lceil N/B \rceil \rceil)$. </li>
<li>The merge cost is $M+N$. <ul>
<li>The worst case for the merging phase is when the join attribute of all the tuples in both relations contains the same value.</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sort R,S on join keys</span><br><span class="line">cursor_R points to R_sorted, cursor_S points to S_sorted</span><br><span class="line"><span class="keyword">while</span> cursor_R <span class="keyword">and</span> cursor_S:</span><br><span class="line">  <span class="keyword">if</span> cursor_R &gt; cursor_S:</span><br><span class="line">    increment cursor_S</span><br><span class="line">  <span class="keyword">if</span> cursor_R &lt; cursor_S:</span><br><span class="line">    increment cursor_R</span><br><span class="line">    possible backtrack cursor_S</span><br><span class="line">  <span class="keyword">elif</span> cursor_R <span class="keyword">and</span> surcor_s <span class="keyword">match</span>:</span><br><span class="line">    emit</span><br><span class="line">    increment cursor_s</span><br></pre></td></tr></table></figure>
<h4 id="How-does-the-two-cursors-move"><a href="#How-does-the-two-cursors-move" class="headerlink" title="How does the two cursors move?"></a>How does the two cursors move?</h4><ol>
<li>The cursor of outer table will only move forward. Specifically, it will only move forward when we can assure that we have already match the current tuple with all possible tuples, i.e. when we occurred a larger inner tuple. </li>
<li>The cursor of inner table may move both forward and backward. <ul>
<li>It moves forward when some later tuple in inned table may match with the current tuple in outer tuple, i.e. when inner tuple is smaller than outer tuple or when they matches (there are possible more matches in the following). </li>
<li>It moves backward when there might have some missing matches in the past, i.e. when the outer cursor moving forward, the key is the same as the last one, we need to backtrack to the earliest tuple that matches with the last outer tuple. </li>
</ul>
</li>
</ol>
<h4 id="When-is-sort-merge-join-useful"><a href="#When-is-sort-merge-join-useful" class="headerlink" title="When is sort-merge join useful?"></a>When is sort-merge join useful?</h4><ol>
<li>When one or both tables are already sorted on join key, we can save some sort cost. </li>
<li>When output must be sorted on join key, if the output of join is larger, we might use sort-merge join to produce sorted output directly. <ul>
<li>If the output has a small amout, hash join might still be the better way. </li>
</ul>
</li>
</ol>
<h3 id="Hash-join"><a href="#Hash-join" class="headerlink" title="Hash join"></a>Hash join</h3><h4 id="How-does-hash-join-work"><a href="#How-does-hash-join-work" class="headerlink" title="How does hash join work?"></a>How does hash join work?</h4><ol>
<li><p>The thought behind hash join is that: </p>
<ul>
<li>If tuple $r \in R$ and a tuple$s \in S$ satisfy the join condition, then they have the same value for the join attributes. </li>
<li>If that value is hashed to some partition $i$, the $R$ tuple must be in $r_i$ and the $S$ tuple in $s_i$. </li>
<li>Therefore, $R$ tuples in $r_i$ need only to be compared with $S$ tuples in $s_i$. </li>
</ul>
</li>
<li><p>In the first phase (build), we scan the outer relation and populate a hash table using the hash function $h_1$ on the join attributes. </p>
<p>In the second phase (probe), scan the inner relation and use $h_1$ on each tuple to jump to a location in the hash table and find a matching tuple. </p>
</li>
<li><p>The keys stored in the hash table is the attribute(s) that the query is joining the tables on. We always need the original key to verify that we have a correct match in case of hash collisions. </p>
<p>The values stored varies per implementation, which depends on what the operators above the join in the query plan expect as its input. </p>
</li>
<li><p>Assume that we have enough buffers, we need to read and write both tables with cost of $2(M+N)$ in partitioning phase, and read both tables with cost of $M+N$ in probing phase. </p>
<ul>
<li>We can see that there is no constraint on the size of inner table. </li>
</ul>
</li>
</ol>
<h4 id="How-big-of-a-table-can-we-hash-using-this-approach"><a href="#How-big-of-a-table-can-we-hash-using-this-approach" class="headerlink" title="How big of a table can we hash using this approach?"></a>How big of a table can we hash using this approach?</h4><ol>
<li>In the first phase of building hash table, we can use at most $B-1$ spill partitions leaving one page as input buffer. When one partition is full, we should write it out to disk and clear it. </li>
<li>total number of both outer and inner table of each partition should be no more than $B$ blocks big so that in the second phase, we can store all tuples in the same partition in memory. </li>
<li>The total page used is $B\cdot (B-1)$. A hash table of $N$ pages needs about $\sqrt{N}$ buffers if the hash distribution is even. </li>
<li>When including the fudge factor $f&gt;1$ when hash distribution is skewed, we need $B\cdot\sqrt{f\cdot N}$. </li>
</ol>
<h4 id="Can-we-optimized-the-search-for-tuples-that-does-not-have-any-match"><a href="#Can-we-optimized-the-search-for-tuples-that-does-not-have-any-match" class="headerlink" title="Can we optimized the search for tuples that does not have any match?"></a>Can we optimized the search for tuples that does not have any match?</h4><ol>
<li>We can create a Bloom filter during the build phase when the key is likely to not exist in the hash table. This method is called Bloom filter or sideways information passing. </li>
<li>The Bloom filter is a probabilistic data structure (bitmap) that answers set membership queries. <ul>
<li>False negatives will never occur while false positives can sometimes occur. </li>
<li>To insert a key into the filter, we use $k$ hash functions to set all $k$ bits to $1$. </li>
<li>During lookup a key, the key may exist if all $k$ bits hashed by the same $k$ hash function are all $1$. The key definitely does not exists if one of the $k$ bits is $0$. </li>
</ul>
</li>
</ol>
<h4 id="What-if-we-do-not-have-enough-memoty-to-fit-the-entire-hash-table"><a href="#What-if-we-do-not-have-enough-memoty-to-fit-the-entire-hash-table" class="headerlink" title="What if we do not have enough memoty to fit the entire hash table?"></a>What if we do not have enough memoty to fit the entire hash table?</h4><ol>
<li>We can use the recursive hash join (GRACE hash join). </li>
<li>Similar with aforementioned algorithm, hash both tables into same number of buckets with the same hash function. </li>
<li>Perform regular hash join on each pair of matching buckets in the same level between two tables. </li>
<li>If the buckets do not fit in memory, then use recursive partitioning to split the tables into chunks that will fit. <ul>
<li>Build another hash table for $bucket_{R,i}$ using hash function $h_2$ (with $h_2≠h_1$). </li>
<li>Then probe it for each tuple of the other table’s bucket at that level. </li>
</ul>
</li>
<li><strong>Hybrid hash join</strong>: If the keys are skewed, then the DBMS keeps the hot partition in-memory and immediately perform the comparison instead of spilling it to disk. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/06/27/OpenSource/BusTub/Project-1-Buffer-Pool/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/27/OpenSource/BusTub/Project-1-Buffer-Pool/" class="post-title-link" itemprop="url">Project #1: Buffer Pool</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-27 15:29:31" itemprop="dateCreated datePublished" datetime="2023-06-27T15:29:31+08:00">2023-06-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-09 22:07:13" itemprop="dateModified" datetime="2023-07-09T22:07:13+08:00">2023-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/BusTub/" itemprop="url" rel="index"><span itemprop="name">BusTub</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@<a href="c">toc</a></p>
<h1 id="Task-1-LRU-K-Replacement-Policy"><a href="#Task-1-LRU-K-Replacement-Policy" class="headerlink" title="Task #1 - LRU-K Replacement Policy"></a>Task #1 - LRU-K Replacement Policy</h1><ol>
<li><p>Evict rule: </p>
<ul>
<li>When all evictable frames have more than $K$ access records, evict the one whose backward k-distance is maximum. </li>
<li>When some frames only have less than $K$ access records, evict the one with earliest first access record among those less than $K$ frames. </li>
</ul>
</li>
<li><p>Comparison: </p>
<ul>
<li>When each frame is created or stored with a new page, push an access record of $0$ to its history to represent $+\inf$. </li>
<li>Each comparison use the first two records in the list. If the first record (the earliest backward most k-distance) is the same, it must be $0$ causing comparing second record where represent true first access of that frame. </li>
<li>Remember to update second comparison timestamp when the first time find $0$ in first timestamp. </li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> k_timestamp = frame.second.<span class="built_in">GetKTimestamp</span>();</span><br><span class="line"><span class="type">size_t</span> sec_timestamp = frame.second.<span class="built_in">GetSecondTimestamp</span>();</span><br><span class="line"><span class="keyword">if</span> (k_timestamp &lt; cmp_k_timestamp) &#123;</span><br><span class="line">	victim = frame.first;</span><br><span class="line">	cmp_k_timestamp = k_timestamp;</span><br><span class="line">	<span class="keyword">if</span> (k_timestamp == <span class="number">0</span>) &#123;</span><br><span class="line">		cmp_sec_timestamp = sec_timestamp;</span><br><span class="line">	&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (k_timestamp == cmp_k_timestamp &amp;&amp; sec_timestamp &lt; cmp_sec_timestamp) &#123;</span><br><span class="line">	victim = frame.first;</span><br><span class="line">	cmp_sec_timestamp = sec_timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>After successfully choosing a victim, we need to clear its history (leaving the $0$ as sentinel), set it to non-evictable, and recude the current size of replacer. </p>
</li>
</ol>
<h1 id="Task-2-Buffer-Pool-Manager"><a href="#Task-2-Buffer-Pool-Manager" class="headerlink" title="Task #2 - Buffer Pool Manager"></a>Task #2 - Buffer Pool Manager</h1><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><ol>
<li>When the page that is asked to fetch is already in the buffer pool, we still need to do several things:<ul>
<li>Set it to non-evictable. </li>
<li>Record its access in replacer. </li>
<li>Increase its pin count. </li>
</ul>
</li>
<li><p>When the page is not in buffer pool, or creating a new page: </p>
<ul>
<li>First, we need to acquire a free frame. <ul>
<li>Erase it from page table when we are evicting one. </li>
<li>Write the content to disk when the old page is dirty. </li>
</ul>
</li>
<li>Then, similar to the other situation, we need to: <ul>
<li>Set it to non-evictable</li>
<li>Record its access in replacer</li>
<li>Put it in page table</li>
<li>Set its pin count to $1$</li>
<li>Set <code>is_dirty_</code> to <code>false</code></li>
<li>Set its page ID to frame. </li>
</ul>
</li>
<li>Even in <code>NewPage</code>, the <code>is_dirty_</code> is false due to it can directly “write” empty to file by increasing offset which is done when larger page ID is written. There is no need to actually write empty content. </li>
</ul>
</li>
<li><p>When setting <code>is_dirty_</code> in <code>Unpin</code>, if the page is already dirty, we should not set it to clean no matter what we are told to. </p>
</li>
</ol>
<h2 id="Leaderboard-optimization"><a href="#Leaderboard-optimization" class="headerlink" title="Leaderboard optimization"></a>Leaderboard optimization</h2><ol>
<li>Fine-grain lock<ul>
<li>A coarse-grain lock is easier to program while sacrificing performance. </li>
<li>In fine-grain lock, a <code>core_latch_</code> is used to protect the core data of buffer pool manager, i.e. <code>page_table_</code>, <code>free_list_</code>, <code>next_page_id_</code>). Each frame has its own latch to protect its data, i.e. <code>pin_count_</code>, <code>is_dirty_</code>, <code>page_id_</code>. </li>
<li>In each function thread, it will at most grab the <code>core_latch_</code> and one of frame latches. </li>
<li>To avoid deadlock, each procedule is designed that a thread will try to acquire a page latch only if it already has a core_latch, or it won’t want a core_latech later. </li>
<li>To obtain atomic when switching from <code>core_latch_</code> to a page latch, we need to acquire the page latch before release the <code>core_latch_</code>. <ul>
<li>If release <code>core_latch_</code> first, some other thread may change the frame we want to use (e.g. evict the frame, modify its metadata) before we can acquire the frame latch. </li>
</ul>
</li>
</ul>
</li>
<li>Delay write out: <ul>
<li>Disk I/Os consume a large amount of time. So we need to avoid holding a lock while communicate with disk. </li>
<li>When we need to write data into disk, instead of immediately call the <code>disk_manager_-&gt;WritePage</code>, we create a temporary buffer in memory, and copy data into the buffer. After all locks are released, we actually write those content in buffer into disk. </li>
<li>A similar thought is to delay read data until all locks are released. However, <code>ReadData</code> are call in <code>FetchPage</code> where  we can only release the frame latch after the whole frame is ready for others to visit. Still, we can <code>ReadData</code> after <code>core_latch_</code> is released since the <code>core_latch_</code> could be the bottleneck of the manager. </li>
<li>However, there is a problem: <ul>
<li>When a page is evicted, and only a short time later, that exact page is fetched again. Since all latches are release before write dirty data to disk, there is a chance that <code>FetchPage</code> read stale data from disk before the up-to-date data is written to disk. </li>
<li>Then my another thought is to maintain a list of page IDs in the writing buffer. If the page ID of <code>FetchPage</code> is in the list, it just copy data from the writing buffer and erase that page from writing buffer. </li>
<li>The problem here is that we need to guarantee the atomic between getting the writing content and writing them to disk. Otherwise, tricky concurrent operations may cause the written content to be wrong. </li>
<li>Hence we cannot unlock the latches until we are sure the content is written, which also makes fine-grained latches meaningless given that disk I/O is the key problem of performance. </li>
</ul>
</li>
</ul>
</li>
<li>Pre-fetch<ul>
<li>To better suit the scan case, when we accessing three consecutive page IDs in a row, the buffer pool manager is allowed to pre-fetch several following pages. </li>
<li>Different from fetching pages through <code>FetchPage</code>, pre-fetched frames need to set <code>pin_count_</code> to $0$ and evictable. </li>
<li>Pre-fetch should not stall <code>FetchPage</code> from returning, so it must be run in a separate thread. </li>
<li>The problem is that pre-fetch must be an independent background thread. It won’t know whether the buffer pool manager is destroyed, which will cause heap-use-after-free error. </li>
</ul>
</li>
</ol>
<h1 id="Task-3-Read-Write-Page-Guards"><a href="#Task-3-Read-Write-Page-Guards" class="headerlink" title="Task #3 - Read/Write Page Guards"></a>Task #3 - Read/Write Page Guards</h1><ol>
<li>Any <code>PageGuard</code> will automatically unpin itself when it is deconstructed. But they don’t need to pin themselves since they are pinned before <code>PageGuard</code> is created when fetching or creating page. </li>
<li>If a <code>Read/WritePageGuard</code> is acquired through <code>FetchPageRead</code> or <code>FetchPageWrite</code>, the latch is acquired inside these functions. However, if a <code>PageGuard</code> is acquired through it construction function, the latch won’t be acquired automatically. </li>
<li>No matter how <code>Read/WritePageGuard</code> is acquired, the latch will always be released automatically when the <code>PageGuard</code> is deconstructed by deconstructor or move operation. </li>
<li>In the move assignment, we need to first drop the original page, copy from right reference and finally deconstruct right reference. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/06/25/Courses/15445/05-Index-Concurrency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/25/Courses/15445/05-Index-Concurrency/" class="post-title-link" itemprop="url">05 Index Concurrency</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-25 00:40:22" itemprop="dateCreated datePublished" datetime="2023-06-25T00:40:22+08:00">2023-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-07-09 22:00:27" itemprop="dateModified" datetime="2023-07-09T22:00:27+08:00">2023-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Concurrency-control"><a href="#Concurrency-control" class="headerlink" title="Concurrency control"></a>Concurrency control</h1><h2 id="What-is-the-correctness-criteria-of-concurrency-control"><a href="#What-is-the-correctness-criteria-of-concurrency-control" class="headerlink" title="What is the correctness criteria of concurrency control?"></a>What is the correctness criteria of concurrency control?</h2><ol>
<li>Logical Correctness: <ul>
<li>Can a thread see the data that it is supposed to see? For example, correctly control data race. </li>
</ul>
</li>
<li>Physical Correctness: <ul>
<li>Is the internal representation of the object sound? For example, when fetched a ptr, it will points to a correct address, and it won’t be freed between fetching and accessing. </li>
</ul>
</li>
</ol>
<h2 id="What-is-more-specific-difference-between-locks-and-latches"><a href="#What-is-more-specific-difference-between-locks-and-latches" class="headerlink" title="What is more specific difference between locks and latches?"></a>What is more specific difference between locks and latches?</h2><ol>
<li>What are they used to separate? What are they protecting? How long will they be held?<ul>
<li>Locks are used to separate user transactions accessing the same tuples in database contents. Locks are held in the entire transaction. </li>
<li>Latches are used to separate threads accessing the same in-memory data structures. Latchs are held in the critical section. </li>
</ul>
</li>
<li>How many modes do they have?<ul>
<li>Locks have four modes: shared, excusive, update, intention. </li>
<li>Latches only have two modes: read and write. </li>
</ul>
</li>
<li>How do they solve deadlock?<ul>
<li>Locks use detection and resolution by waits-for, timeout or abort. </li>
<li>Latches can only avoid deadlock by code discipline. </li>
</ul>
</li>
<li>Where are they kept?<ul>
<li>Locks are kept in Lock Manager while latches are kept in protected data structure. </li>
</ul>
</li>
</ol>
<h2 id="What-are-the-two-latch-modes"><a href="#What-are-the-two-latch-modes" class="headerlink" title="What are the two latch modes?"></a>What are the two latch modes?</h2><ol>
<li><p>Read mode means that Multiple threads can read the same object at the same time. </p>
<ul>
<li>A thread can acquire the read latch if another thread has it in read mode. </li>
</ul>
</li>
<li><p>Write mode onely allows one thread to access the object. </p>
<ul>
<li>A thread cannot acquire a write latch if another thread has it in any mode. </li>
</ul>
</li>
<li>In the compatibility matrix, only two read mode access is allowed. </li>
</ol>
<h2 id="What-are-the-different-latch-implementations"><a href="#What-are-the-different-latch-implementations" class="headerlink" title="What are the different latch implementations?"></a>What are the different latch implementations?</h2><ol>
<li>The first is blocking OS Mutex. <ul>
<li>It is simple to use. </li>
<li>It takes about $25\ ns$ per lock/unlock invocation, which means that it is non-scalable. <ul>
<li><code>std::mutex</code> is slower than <code>pthread_mutex</code> and <code>futex</code> is faster than both of them. </li>
</ul>
</li>
<li><code>futex</code> stands for fast userspace mutex. <ul>
<li>It has a userspace spinlock and a heavy-weight OS latch. </li>
<li>Threads will first try to acquire the userspace lock. If success, then that is good. </li>
<li>But if failed, the thread will fall back to the OS latch. And OS takes control of the thread with when to schedule it and DBMS can do nothing with the thread. Also <code>syscall</code> is expensive. </li>
</ul>
</li>
</ul>
</li>
<li>The second approach is reader-writer latches. <ul>
<li>It allows for concurrent readers. Must manage read/write queues to avoid starvation. </li>
<li>It can be implemented on top of spinlock. </li>
<li>Still <code>std::shared_mutex</code> is slower than <code>pthread_rwlock</code>. </li>
</ul>
</li>
</ol>
<h1 id="Latching-scheme"><a href="#Latching-scheme" class="headerlink" title="Latching scheme"></a>Latching scheme</h1><h2 id="Hash-table-latching"><a href="#Hash-table-latching" class="headerlink" title="Hash table latching"></a>Hash table latching</h2><h3 id="Why-are-deadlocks-not-possible-in-hash-table"><a href="#Why-are-deadlocks-not-possible-in-hash-table" class="headerlink" title="Why are deadlocks not possible in hash table?"></a>Why are deadlocks not possible in hash table?</h3><ol>
<li>All threads move in the same direction and only access a single page/slot at a time. <ul>
<li>Hence there are no loop waiting in this scenario. </li>
</ul>
</li>
<li>To resize the table, take a global write latch on the entire table. </li>
</ol>
<h3 id="How-can-we-design-hash-table-latching"><a href="#How-can-we-design-hash-table-latching" class="headerlink" title="How can we design hash table latching?"></a>How can we design hash table latching?</h3><ol>
<li>The coarser-grain approach is to use page latches. <ul>
<li>Each page has its own reader-writer latch that protects its entire contents. </li>
</ul>
</li>
<li>The finer-grain approach is to use slot latches. <ul>
<li>Each slot has its own latch. </li>
<li>It can use a single-mode latch to reduce meta-data and computational overhead. </li>
</ul>
</li>
</ol>
<h2 id="B-Tree-latching"><a href="#B-Tree-latching" class="headerlink" title="B+Tree latching"></a>B+Tree latching</h2><h3 id="What-should-we-use-latching-to-prevent"><a href="#What-should-we-use-latching-to-prevent" class="headerlink" title="What should we use latching to prevent?"></a>What should we use latching to prevent?</h3><ol>
<li>Threads trying to modify the contents of a node at the same time. (This is logical correctness, i.e. data race)</li>
<li>One thread traversing the tree while another thread splits/merges nodes. <ul>
<li>This is physical correctness. Splitting/merging will causing free pointers, nodes or in-node entries. </li>
<li>This will also cause a problem with logical correctness, i.e. false negative. <ul>
<li>If a thread get the pointer of node which possess the key it want, then before it accesses the node, the key is borrowed by a sibling causing the thread thought the key does not exists. </li>
</ul>
</li>
</ul>
</li>
<li>When introducing sibling pointers, we may have a deadlock situation. </li>
</ol>
<h3 id="How-should-we-achieve-physical-correctness"><a href="#How-should-we-achieve-physical-correctness" class="headerlink" title="How should we achieve physical correctness?"></a>How should we achieve physical correctness?</h3><ol>
<li>The most naive method is to hold all locks until the entire process is done. But its performance is a disaster. <ul>
<li>We must release some latches when we are sure it is safe. </li>
<li>According to our goal, a node is safe when we know that it won’t be changed (split, merge or redistribute) when updated. </li>
<li>We can know a child is safe when its child is not full on insertion or more than half-full on deletion, i.e. any later opereations can be isolated on or below its level. </li>
</ul>
</li>
<li>For find operation, there won’t have any updates. Hence we can always unlatch parent when acquired a R latch on child. </li>
<li>For insert or delete operation, we need to obtaining W latches as needed. Once child is latched, check if it is safe. <ul>
<li>If the child is safe, we can release all latches on ancestors. The latches should be released from top to bottom to have a better performance. </li>
</ul>
</li>
</ol>
<h3 id="What-is-the-problem-of-aforementioned-strategy"><a href="#What-is-the-problem-of-aforementioned-strategy" class="headerlink" title="What is the problem of aforementioned strategy?"></a>What is the problem of aforementioned strategy?</h3><ol>
<li>Every insert/delete operation will take a write latch on the root, which makes the root a bottleneck with higher concurrency. </li>
<li>We can make the assumption that most modifications to a B+Tree will not require a split or merge. </li>
<li>Hence, instead of assuming that there will be a split/merge as aforementioned, optimistically traverse the tree using read latches. If the guess is wrong in the end, just repeat traversal with pessimistic algorithm. </li>
<li>For insert/delete operations, set latches as if for search, get to leaf, and set W latch on leaf. If leaf is not safe, release all latches, and restart thread using previous insert/delete protocol with write latches. </li>
</ol>
<h3 id="When-will-a-deadlock-occur"><a href="#When-will-a-deadlock-occur" class="headerlink" title="When will a deadlock occur?"></a>When will a deadlock occur?</h3><ol>
<li>With sibling pointers, we may move from one leaf node to another leaf node where deadlock could occur. </li>
<li>Latches cannot detect deadlock, so the only solution is to kill one thread. </li>
<li>The leaf node sibling latch acquisition protocol must support a “no-wait” mode. The DBMS’s data structures must cope with failed latch acquisitions. </li>
<li>Though some scenario is not a deadlock, the waiting thread cannot know what the other thread is doing, which means it can only kill itself to avoid deadlock. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
