<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">21</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/27/Paper/Sys4AI/SiloD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/27/Paper/Sys4AI/SiloD/" class="post-title-link" itemprop="url">SiloD</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-27 15:46:11" itemprop="dateCreated datePublished" datetime="2023-09-27T15:46:11+08:00">2023-09-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:25:00" itemprop="dateModified" datetime="2023-10-04T16:25:00+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Sys4AI/" itemprop="url" rel="index"><span itemprop="name">Sys4AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3552326.3567499">SiloD: A Co-design of Caching and Scheduling for Deep Learning Clusters</a></p>
<p>@[toc]</p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="How-to-separate-storage-and-computing-in-DL-training"><a href="#How-to-separate-storage-and-computing-in-DL-training" class="headerlink" title="How to separate storage and computing in DL training?"></a>How to separate storage and computing in DL training?</h2><ol>
<li>Separate storage and computing. The training executes on a compute cluster equipped with GPUs/TPUs while reading data from a separate cluster hosting the storage service. </li>
<li>When submitting a DL job, users can simply specify the storage account and the location of the desired dataset, and the job can directly load the data through remote IO. </li>
<li>The remote IO between compute and storage services could become a bottleneck. The worst case is to read the entire training dataset remotely in each epoch. </li>
<li>The data access pattern and the computation pattern, despite being highly diverse for different training jobs, are both highly stable and predictable within each individual job. </li>
</ol>
<h2 id="How-to-levarage-cache-subsystem-for-DL-training"><a href="#How-to-levarage-cache-subsystem-for-DL-training" class="headerlink" title="How to levarage cache subsystem for DL training?"></a>How to levarage cache subsystem for DL training?</h2><ol>
<li>Leverage local disks of GPU servers, instead of the small cache memory in traditional systems, to cache a subset of data to reduce the demands to remote IO. </li>
<li>The first type of cache subsystem is built into a data loading library. <ul>
<li>The cache is built with the processes of a training job, and is statically allocated. </li>
<li>DL training jobs have diverse demands on cache and remote IO. Isolated cache with a static allocation can neither satisfy nor exploit such diversity. </li>
</ul>
</li>
<li>The second type of cache subsystem is distributed cache which consolidates the local storage of all cluster servers into a large storage pool shared by all jobs. <ul>
<li>Modern GPU cluster usually has a high-speed storage fabric (separate from the InfiniBand network used for distributed training) that supports accessing data from peer servers as fast as local disk. </li>
<li>A distributed cache across the local cluster can generally satisfy the IO demands of training jobs. </li>
</ul>
</li>
</ol>
<h2 id="How-to-cache-data-for-DL-training"><a href="#How-to-cache-data-for-DL-training" class="headerlink" title="How to cache data for DL training?"></a>How to cache data for DL training?</h2><ol>
<li>Due to the random-and-exactly-once data access pattern, it has been shown that uniform caching is optimal for single training job. </li>
<li>In uniform caching, accessed data items are cached until the cache capacity is reached, and will not be evicted thereafter. There is no eviction unless the cache capacity is reduced. <ul>
<li>Other cache eviction policies like LRU (Least-Recently-Used) may evict useful items, leading to the thrashing issue. </li>
<li>In uniform caching, part of the dataset is staying at the local disk permanently and can be used in each epoch. </li>
<li>LRU will only reserve data used recently while they won’t be used in near future. </li>
</ul>
</li>
<li>It is noteworthy that the cached data are evenly distributed in each batch, instead of cache some batches entirely. <ul>
<li>Each data item has a unique ID. The missed data items are fetched from the remote storage. Because each epoch shuffles the data loading order, the expected cache hit ratio is uniform for all items. </li>
<li>In this way, we can improve the performance of the pipeline of data loading. </li>
</ul>
</li>
<li><p>For deep learning training, uniform caching leads to a constant and predictable cache hit ratio w.r.t. the cache capacity regardless of which items being cached. </p>
<p><img src="/imgs/Sys4ai/SiloD/cache.png" width="50%"></p>
</li>
</ol>
<h2 id="What-is-the-problem-of-static-allocation"><a href="#What-is-the-problem-of-static-allocation" class="headerlink" title="What is the problem of static allocation?"></a>What is the problem of static allocation?</h2><ol>
<li>When there are multiple jobs in a cluster, uniform caching transforms the cache management from cache eviction problem to a cache space allocation problem. </li>
<li>The job’s cache efficiency as the amount of remote IO (in MB/s) saved per GB of cache allocated. </li>
<li>$𝑓^\ast$ and $d$ are the IO demand to achieve the ideal training speed and the dataset size, respectively. A job’s cache efficiency is $\frac{𝑓^\ast}{d}$. </li>
<li>The cache efficiency of different dataset can varies largely. A static cache allocation could not take advantage of the diverse cache-efficiency of DL jobs.</li>
</ol>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Issue:<ul>
<li>Existing deep learning schedulers do not manage storage resources thus fail to consider the diverse caching effects across different training jobs. </li>
<li>State-of-the-art deep learning schedulers focus on arbitrating compute resources (e.g., GPUs and CPUs) with different optimization objectives like job completion time (JCT), fairness, or cluster utilization. </li>
</ul>
</li>
<li>Challenge:<ul>
<li>Deep learning schedulers have diverse scheduling objectives. An ad-hoc solution to every scheduling policy increases design complexity and is hard to scale. </li>
<li>Deep learning training exhibits highly diverse performance patterns: different jobs impose different cache and IO demands. This further complicates the system design. </li>
<li>Even deep learning-aware cache systems could exhibit poor performance because of caching policies that ignore scheduling impacts. </li>
</ul>
</li>
<li>Contribution:<ul>
<li>Co-design the cluster scheduler and the cache subsystems for deep learning training. </li>
<li>The job performance estimator is enhanced. <ul>
<li>To help different schedulers to jointly consider the impact of storage and compute resource allocation while preserving their respective scheduling objectives. </li>
<li>SiloD derives a unified way of performance estimation by further leveraging the pipelined execution of data loading and computation. </li>
<li>SiloD is able to augment different state-of-the-art deep learning schedulers to jointly perform cache and remote IO allocation while preserving the original objectives of these scheduling policies. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="What-does-SiloD-do"><a href="#What-does-SiloD-do" class="headerlink" title="What does SiloD do?"></a>What does SiloD do?</h2><ol>
<li>SiloD allocates compute and cache-related resources jointly to training jobs. SiloD treats cache and remote IO as first-class citizens. <ul>
<li>Existing multi-resource schedulers can just treat storage resources as yet another resource types whose impact has already been captured by the performance estimator</li>
</ul>
</li>
<li>The scheduling problem can be generally abstracted as allocating cluster resources defined in <code>totalResource</code> to jobs with the help of a performance estimator <code>perf(j, R)</code>. </li>
<li>SiloD further enhances the estimator <code>perf</code> with <code>SiloDPerf</code> to estimate the joint impact of compute and storage resources. <ul>
<li>The SiloD-augmented performance estimator transforms a joint performance estimation into a two-step process. </li>
<li>It first estimates whether data loading will become the bottleneck of the entire training. </li>
<li>If so, SiloD will use <code>IOPerf</code>, a performance estimator we introduce to analyze the impact of storage to estimate the job performance under IO bottleneck. </li>
</ul>
</li>
</ol>
<h2 id="How-does-SiloD-estimate-performance"><a href="#How-does-SiloD-estimate-performance" class="headerlink" title="How does SiloD estimate performance?"></a>How does SiloD estimate performance?</h2><ol>
<li>The end-to-end throughput is then determined by the bottleneck stage, i.e. <code>SiloDPerf</code>$=min\{f^\ast,f\}$. <ul>
<li>$f^\ast$ is a job’s computation throughput when IO is not the bottleneck, i.e., <code>perf</code>, which is the original estimator used by an existing scheduler. </li>
<li>$f$ is the throughput of data loading, i.e., <code>IOPerf</code>, which is the estimator for IO given some cache allocation. </li>
</ul>
</li>
<li>A job’s remote IO demand can be calculated as $b=f\cdot (1-\frac{c}{d})$. <ul>
<li>$𝑐$ is the allocated cache space and $𝑑$ the size of the training dataset. The expected cache hit ratio of a job is $\frac{c}{d}$. </li>
<li>$𝑏$ is the remote IO demand of a job, which equals to the data loading throughput multiplied by the cache miss ratio. </li>
</ul>
</li>
<li>A job’s IO throughput 𝑓 (i.e., <code>IOPerf</code>) can be estimated by $f=\frac{b}{1-c/d}$. </li>
<li>When a job is fetching data at its ideal throughput (i.e., $f = f^\ast$), its cache efficiency is exactly the negative derivative of $b$, i.e. Cache Efficiency$=-\frac{\partial b}{\partial c}=\frac{f^\ast}{d}$. <ul>
<li>The different computation throughput $f^\ast$ of different neural model and dataset size $d$ is the sources of the heterogeneity. </li>
</ul>
</li>
<li><p>The policy assumes the ideal throughput of a job $𝑓^\ast$ (when IO is not a bottleneck) can be profiled offline.</p>
<h2 id="How-to-integrate-SiloD-with-existing-scheduler"><a href="#How-to-integrate-SiloD-with-existing-scheduler" class="headerlink" title="How to integrate SiloD with existing scheduler?"></a>How to integrate SiloD with existing scheduler?</h2></li>
<li><p>In Shortest Job First (SJF), each job will have a performance score defined as its weighted sum of resource demand of all resource types multiplied by its duration. </p>
<ul>
<li>$score=\displaystyle\min_{R}\sum_{t}w_t\cdot R_t\cdot (\frac{j.numSteps\cdot j.stepDataSize}{perf(j,\bold{R})})$</li>
<li>$w_t$ is the weight of the $t$-th resource type, $\bold{R}$ is a vector of allocation of all resource types, and $R_{t}$ is the allocation of the $t$-th resource type in $\bold{R}$, $𝑗.numSteps$ is the total number of steps and $𝑗.stepDataSize$ is the size of data consumed per step of job $j$. </li>
<li>The jobs with the least score will be scheduled first by the multi-resource SJF policy. </li>
</ul>
</li>
<li>The vanilla programming in Gavel’s max-min fairness is $\displaystyle\max_{R}\min_{j}\frac{perf(j,R[j])}{perf(j,R^{equal})},s.t. Sum(R)≤totalResource$. <ul>
<li>$𝑅[𝑗]$ is the resource allocated to job $j$ and $R^{equal}$ is the equal resource division among all jobs. </li>
<li>The max-min fairness objective maximizes the job with the least performance improvement over the equal resource division. </li>
</ul>
</li>
<li>When intergrate SiloD, $\bold{R}$ includes cache and remote IO as another two types of resources in addition to compute resources and the performance estimator function $perf(j,\bold{R})$ is replaced by $SiloDPerf(j,\bold{R})$. </li>
</ol>
<h2 id="How-to-use-SiloD-without-modifying-existing-scheduler"><a href="#How-to-use-SiloD-without-modifying-existing-scheduler" class="headerlink" title="How to use SiloD without modifying existing scheduler?"></a>How to use SiloD without modifying existing scheduler?</h2><ol>
<li>The greedy policy minimizes the remote IO consumption in a best-effort manner so that the impact of IO to original scheduling objectives can be minimized. </li>
<li>It can be done by allocating more cache to the most cache-efficient jobs. </li>
<li>Each job first calculates its cache efficiency. The datasets with the highest cache efficiency are first cached until the cache space is full. <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> job j <span class="keyword">in</span> <span class="keyword">all</span> jobs do</span><br><span class="line">  j.CacheEfficiency <span class="operator">=</span> j.fStar <span class="operator">/</span> j.datasetSize</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> job j <span class="keyword">in</span> descending <span class="keyword">order</span> <span class="keyword">of</span> j.CacheEfficiency do</span><br><span class="line">  alloc.Cache[j] <span class="operator">=</span> <span class="built_in">min</span>(j.datasetSize, totalCache)</span><br><span class="line">  totalCache <span class="operator">-</span><span class="operator">=</span> alloc.Cache[j]</span><br><span class="line"> </span><br><span class="line"><span class="keyword">return</span> alloc</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="How-does-SiloD-allocate-resources"><a href="#How-does-SiloD-allocate-resources" class="headerlink" title="How does SiloD allocate resources?"></a>How does SiloD allocate resources?</h2><ol>
<li>SiloD Data Manager serves in the storage layer to enforce the allocations made by the scheduler. <ul>
<li>A cluster scheduler uses the interface to allocate cache and remote IO to two types of entities: jobs and datasets. <ul>
<li>Remote IO is allocated to jobs directly, while cache is allocated to datasets, and to the associated jobs indirectly.</li>
<li>Multiple jobs can transparently share the same cache space for the same dataset. In contrast, since jobs using the same dataset still read data items in different order, remote IO is exclusive to each job. </li>
<li>The cache consumption is charged by only once for each dataset instead for every jobs. The cache efficiency is defined at dataset-level, which is the sum of all jobs’ cache efficiency using the same dataset. </li>
<li>For distributed data-parallel training, the remote IO allocation is equally distributed to each worker of the job. </li>
</ul>
</li>
<li>SiloD data manager sets up FUSE (Filesystem in USErspace) clients co-located with training tasks to manage the cache, throttling remote IO and maintaining the metadata of datasets on each server. </li>
</ul>
</li>
<li>SiloD Scheduler extends the responsibility of the compute-only resource scheduler from job scheduling to compute-storage joint allocation.<br><img src="/imgs/Sys4ai/SiloD/structure.png" width="50%"></li>
</ol>
<h2 id="How-to-handle-delayed-data-access-and-irregular-data-access"><a href="#How-to-handle-delayed-data-access-and-irregular-data-access" class="headerlink" title="How to handle delayed data access and irregular data access?"></a>How to handle delayed data access and irregular data access?</h2><ol>
<li>Since DL training reads each data item exactly once per epoch, any newly cached data items will never be accessed again until the next epoch. <ul>
<li>Even though the newly cached item consumes the cache space, but it does not help to reduce the remote IO until the next epoch. Therefore, accurate estimation of job performance should use the effective cache size. </li>
<li>However, since multiple jobs may use the same dataset, it is unknown beforehand if a newly cached item by one job is effective or not for other jobs. </li>
<li>The delayed effectiveness only has a limited impact that lasts for at most one epoch for newly cached items. A DL job usually trains a model for tens of epochs, thus for most of the time, the cached data are effective. </li>
<li>SiloD also supports fine-grained management for policies to inspect the effective cache size and the instantaneous remote IO demand, by maintaining a bitset for each job to track its accessed items. </li>
</ul>
</li>
<li>When a cluster is mixed by regular jobs satisfying SiloD’s assumptions and irregular jobs, we partition the cache and remote IO into two parts for all regular jobs and irregular jobs, respectively. <ul>
<li>Allocate resources to the regular jobs in the first partition still using <code>SiloDPerf</code>. </li>
<li>The irregular jobs in the second partition fall back to the original scheduling policy and estimator, and share the cache and remote IO within the partition. </li>
<li>In this way, the regular DL jobs can still benefit from exploiting the heterogeneous cache efficiency without being impacted by potential anomalies due to mis-estimation of irregular jobs. </li>
</ul>
</li>
</ol>
<h2 id="How-does-SiloD-support-fault-tolerance"><a href="#How-does-SiloD-support-fault-tolerance" class="headerlink" title="How does SiloD support fault tolerance?"></a>How does SiloD support fault tolerance?</h2><ol>
<li>The allocation of remote IO and cache is stored in “pod annotation” for the pods of each job, which is kept reliably by Kubernetes. </li>
<li>For the job with multiple pods, the remote IO allocation is proportionally divided to each pod and the cache allocation is same for all pods. </li>
<li>When SiloD Data Manager recovers from crashes, it reconstructs the status by collecting the information from pods. </li>
<li>The cache content on each server is stored on local disk thus can be reliably restored when the server restarts. </li>
<li>SiloD does not add stateful information into the cluster scheduler, thus their fault tolerance is handled by their original approach. </li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ol>
<li><p>First, the author showed the evidence of the issues: </p>
<ul>
<li>They presented the increasing of dataset size and the GPU performance as the indirect evidence of remote IO bottleneck.<br><img src="/imgs/Sys4ai/SiloD/indirect.png" width="50%"></li>
<li><p>Then the required remote IO to reach the optimal speed for GPU is calculated and the GPU cluster’s aggregate IO demand is profiled to prove that remote IO without cache is slowing down the training prosedure.<br><img src="/imgs/Sys4ai/SiloD/optimal.png" width="25%"><br><img src="/imgs/Sys4ai/SiloD/demand.png" width="25%"></p>
</li>
<li><p>When discussing design for cache subsystem, the author also provided the sub-optimal evidence of current scheduler without knowing the cache subsystem.<br><img src="/imgs/Sys4ai/SiloD/quiver.png" width="50%"></p>
</li>
</ul>
</li>
<li>When further exploring the cache efficiency, the author showed the variance between difference datasets and effective cache size.<br><img src="/imgs/Sys4ai/SiloD/variance.png" width="35%" /> <img src="/imgs/Sys4ai/SiloD/effective.png" width="35%" /></li>
<li>To evaluate SiloD in a large-scale cluster of fast V100 GPUs with a lower cost, the authors design an approach to accelerating the training on a K80 GPU cluster to investigate the data loading performance of running the same trace in a V100 GPU cluster. <ul>
<li>In the experiment, they first profile the training speed of selected models on real V100 GPUs. </li>
<li>Then, execute the same model on K80 GPUs by processing the same training pipeline of data loading, preprocessing and model aggregation, but replacing the model execution (forward pass and backward pass) with “<code>sleep()</code>” for the profiled duration from V100. </li>
<li>Since deep learning training usually has a very stable mini-batch duration, the IO behaviour in accelerated K80 GPUs is almost the same as real training of V100 GPUs. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/OpenSource/6.824/6-824-Labs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/OpenSource/6.824/6-824-Labs/" class="post-title-link" itemprop="url">6.824 Labs</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-09-26 13:57:11 / Modified: 13:59:06" itemprop="dateCreated datePublished" datetime="2023-09-26T13:57:11+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/6-824/" itemprop="url" rel="index"><span itemprop="name">6.824</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#lab-2-raft">Lab 2: Raft</a>
<ul>
<li><a href="#2a-leader-election">2A: Leader election</a></li>
<li><a href="#2b-log">2B: Log</a></li>
<li><a href="#2c-persistence">2C: Persistence</a></li>
<li><a href="#2d-log-compaction">2D: Log compaction</a></li>
</ul>
</li>
<li><a href="#lab-3-fault-tolerant-keyvalue-service">Lab 3: Fault-tolerant Key/Value Service</a></li>
<li><a href="#lab-4-shardkv">Lab 4: ShardKV</a></li>
</ul>
</p>
<h1 id="lab-2-raft"><a class="markdownIt-Anchor" href="#lab-2-raft"></a> Lab 2: Raft</h1>
<h2 id="2a-leader-election"><a class="markdownIt-Anchor" href="#2a-leader-election"></a> 2A: Leader election</h2>
<ol>
<li><strong>How to count the votes a candidate gotten?</strong>
<ul>
<li>The candidate begins a new goroutine to request vote from each server.</li>
<li>Use a shared variable to track how many votes has the candidate gotten. Each goroutine monitor that after this vote, has the candidate gotten enough votes to become a leader independently.</li>
<li>When each goroutine received reply from other peers, it needs to check whether the reply is still in the same term as the term where it is now and whether it is still a candidate.
<ul>
<li>Only check its state is insufficient. The check of term is to prevent delayed replies arrives in the future election initialed by this server.</li>
</ul>
</li>
<li>My initial thought
<ul>
<li>The election goroutine will monitor the process of votes instead of the request goroutines.</li>
<li>Then the check loop in election goroutine need to sleep after each time it failed. Or the election would be hard to converge.</li>
<li>I GUESS that the reason is the for-loop never ends and takes too many resources, causing the CPU cannot schedule those RequestVote goroutine and later eletion goroutine in time.</li>
</ul>
</li>
</ul>
</li>
<li><strong>How to does each server initial an election?</strong>
<ul>
<li>A timestamp is used to record the last time heard from leader or candidate. And <code>electionTimeout</code> is set randomly in startup and beginning of election.
<ul>
<li>Each time the server wants to change its state into follower, it needs to reset timestamp before switch state, or it may trigger a new election due to the stale timeStamp.</li>
<li>When the replied term is larger than the term of sending <code>AppendEntries</code>, the leader should known that itself is out-of-date. But before any further settings, it should check that whether the replied term is larger than the term where it is now to prevent this is a stale reply processed with a stale term, causing <code>currectTerm</code> decreasing.</li>
</ul>
</li>
<li><code>ticker()</code> will check whether the time since timestamp is larger than the <code>electionTimeout</code> periodically, and if so, an new election is initiated.
<ul>
<li>When the checking is failed, this goroutine should sleep for a short time. But it cannot simply sleep as long as <code>electionTimeout</code>, because the next sleep may be shorter then <code>electionTimeout</code>.</li>
</ul>
</li>
<li>Another way is to set the <code>electionTimeout </code> when checking the condition instead of fixed <code>electionTimeout</code> between two elections.
<ul>
<li>But this will cause multiple peers initiate election in short gap. In addition to the unreliable network and the burden of scheduling goroutines, the <code>RequestVote()</code> may not be executed by others immediately and causing severe split brain and re-elections.</li>
<li>The reason, I GUESS, is that it only need one short sleep time to kick off election. And with a new random sleep time every <code>CHECKTIMEOUT</code> ms, there is a greater probability that one sleep time is short and it actually shortened the <code>electionTimeout</code> I want.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="2b-log"><a class="markdownIt-Anchor" href="#2b-log"></a> 2B: Log</h2>
<ol>
<li>
<p><strong>How does the leader sending log entries to followers?</strong></p>
<ul>
<li><code>nextIndex</code> is the lowest indices of entries missed by each peers known by leader.</li>
<li>There are two situations of sending log entries: the first is when there are some un-replicated entries for some follower, and the second is the heartbeat message.
<ul>
<li><code>SyncLogWithFollower(x int)</code> is implemented to check whether server x has some missing entries.</li>
<li><code>Heartbeat()</code> is implemented to send heartbeat message.</li>
<li><code>SendEntriesOnceTo(x int)</code> is implemented to actually send log entries to server x.</li>
<li>The <code>SyncLogWIthFollower()</code> goroutine for each server and the <code>Heartbeat()</code> goroutine is initiated when the server is elected to be leader.</li>
</ul>
</li>
<li>The difference between <code>SyncLogWithFollower()</code> and <code>Heartbeat()</code>
<ul>
<li><code>SyncLogWithFollower()</code> sends entries only when there are missing entries in server x. But <code>Heartbeat()</code> always sends entries even when there is no missing entries in which case an empty log is sent.</li>
<li>The check cycle in <code>SyncLogWithFollower()</code> is way more short than the sending cycle in <code>Heartbeat()</code> to ensure the missing entries can be replicated as soon as possible.</li>
</ul>
</li>
<li>When <code>SyncLogWithFollower()</code> calls <code>SendEntriesOnceTo()</code>, it cannot create a goroutine.
<ul>
<li>Because the check cycle in <code>SyncLogWithFollower()</code> is quite short, if the <code>SyncLogWithFollower()</code> goroutine keeps being scheduled, the <code>SendEntriesOnceTo()</code> cannot send entries to follower. And thus <code>SyncLogWithFollower()</code> goroutine keeps creating too many <code>SendEntriesOnceTo()</code> goroutine.</li>
</ul>
</li>
<li>My initial thought
<ul>
<li>Start a <code>SendEntriesOnceTo()</code> goroutine for every follower after <code>Start()</code> has added a new log entry to leader’s logs instead of using <code>SyncLogWithFollower()</code> goroutine.</li>
<li>But when there are concurrent <code>Start()</code>, this may cause mulitple <code>SendEntriesOnceTo()</code> goroutine sending the same RPC to the same follower.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>How to optimize the consistency check protocol?</strong></p>
<ul>
<li>Additional AppenEntries RPC results for fast roll back:
<ul>
<li><code>Xterm</code>: the term of the conflicting entry.</li>
<li><code>Xindex</code>: the first index that is in <code>Xterm</code>.</li>
<li><code>Xlen</code>: the length of log</li>
</ul>
</li>
<li>Fast roll back implementation:
<ul>
<li>If the leader doesn’t have <code>Xterm</code>, then every entries of <code>Xterm</code> in follower’s log will causing conflict. Hence the <code>nextIndex</code> can backup to <code>Xindex</code>.</li>
<li>If the leader has <code>Xterm</code>, the matching entry in leader’s log must have a term no larger than <code>Xterm</code>. Hence, the <code>nextIndex</code> should backup to the next entry of the last <code>Xterm</code> in leader’s log.</li>
<li>If the follower’s conflicting is due to empty in <code>prevLogTerm</code>, then <code>Xterm</code> is set to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">−</span><span class="mord">1</span></span></span></span>, and leader should backup to <code>Xlen</code>.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>How should a follower accept log entries after passing all consistency check?</strong></p>
<ul>
<li>The <code>nextIndex</code> and <code>logs</code> of the leader can be updated by different goroutines of <code>SendEntriesOnce()</code> and <code>Start()</code>. So it is possible that <code>args.Entries</code> are chosen through a stale <code>nextIndex</code> and an up-to-date <code>log</code>, or even a stale <code>nextIndex</code> and a stale <code>log</code>.</li>
<li>If the <code>nextIndex</code> is stale while the <code>log</code> is up-to-date, i.e. there are some entries after the <code>nextIndex</code> is already replicated by follower.
<ul>
<li>Then we need to drop those replicated entries but not all the entries since there still could have some new entries.</li>
<li>We need to drop those entries with the same index and the same term according to the Log Matching property and append only those different entries.</li>
<li>I didn’t consider the case of <code>nextIndex</code> being larger than the follower’s replicated indices. Since in this case, transmission won’tsuccess and a fast backup will be triggered.</li>
</ul>
</li>
<li>If both the <code>nextIndex</code> and <code>logs</code> are stale, it is similar to the former situation, since the only additional problem is that it cannot be brought up-to-date by one <code>AppendEntries</code>.</li>
</ul>
</li>
<li>
<p><strong>How will each server commit index?</strong></p>
<ul>
<li><code>commitIndex</code> is the highest index of entries that can commit now. This is included in the <code>AppendEntries</code> arguments from leader to followers.</li>
<li><code>lastApplied</code> is the highest index of entries that is committed. If <code>lastApplied</code> no larger than <code>commitIndex</code>, then a server can commit the entry next to <code>lastApplied</code>.</li>
<li>Followers cannot modify <code>commitIndex</code> before the logs are synchronized, since there may have some entries need to wipe out by the leader.</li>
</ul>
</li>
<li>
<p><strong>How do the leader check which entries can be committed?</strong></p>
<ul>
<li>
<p>My solution</p>
<ul>
<li>
<p><code>matchIndex</code> is used to track the highest indices of entries replicated by each peers.</p>
</li>
<li>
<p>When a group of entries that ends with index <code>i</code> is accepted by one peer, the leader first set the corresponding <code>matchIndex</code> to <code>i</code>.</p>
</li>
<li>
<p>Than check whether a majority of <code>matchIndex</code> is larger than <code>i</code>. If so, than the leader can commit at least until <code>i</code>. Or, it won’t commit any entry.</p>
</li>
</ul>
</li>
<li>
<p>Improvement</p>
<ul>
<li>When a lagged peer replicated a lot of entries, it may be insufficient to commit the last entry it replicated. But it could be sufficient to commit some earlier entries.</li>
<li>We only need to commit the k-th largest index in <code>matchIndex</code>.</li>
</ul>
</li>
<li>
<p>My initial thought</p>
<ul>
<li>Track the replication state of each entry instead each peer.</li>
<li>But the entries are a lot more than peers causing higher time complexity to maintain when a lot of entries are by peers.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>What is the constraint of committing uncommitted entries of earlier terms?</strong></p>
<ul>
<li>A constraint on commit entries is that each leader can only commit entries added in its term. And by committing such entries, they also commit all entries before it. Hence the entries of earlier terms are committed indirectly.</li>
<li>But there is a corner case that a leader doesn’t receive any entry at the beginning of its term. And those uncommitted entries of earlier terms cannot be committed until the leader received its first entry.</li>
<li>My solution is that when a server becomes the leader, it will add an empty entry, and by this empty entry, those uncomitted entries can be committed.</li>
<li>But the test system of 6.824 didn’t consider this case. This implementation will cause all tests failed.</li>
</ul>
</li>
</ol>
<h2 id="2c-persistence"><a class="markdownIt-Anchor" href="#2c-persistence"></a> 2C: Persistence</h2>
<ol>
<li><code>persist()</code> is called only when <code>rf.currentTerm</code>, <code>rf.votedFor</code> or <code>rf.logs</code> is changed. And when they’re changed, the <code>rf.mu</code> lock must be hold by the caller of <code>persist()</code>. In order to make the states stored as soon as possible, I won’t release the lock util <code>persist()</code> is finished.</li>
<li><strong>When a server restarts, what information should it restore?</strong>
<ul>
<li>Naturally, it need to restore its <code>currentTerm</code>, <code>votedFor</code> and <code>logs</code> from the persisted state.</li>
<li>Then it need to set its <code>lastLogIndex</code> and <code>lastLogTerm</code> appropriately.</li>
<li>The <code>lastApplied</code>, <code>commitIndex</code> and <code>lastIncludedIndex</code> will be set to the index of the sentinel entry.
<ul>
<li>Because the current state is the same state executed until the sentinel entry.</li>
<li>In the future, if the server finds that other peers have committed later entries, it need to re-execute those entries to achieve the same state.</li>
</ul>
</li>
<li>It should update its <code>lastApplied</code> to the last committed entry according to the commit state in each log entry.</li>
</ul>
</li>
<li>**How to invoke persist()? **
<ul>
<li><code>persist()</code> is called only when <code>rf.currentTerm</code>, <code>rf.votedFor</code> or <code>rf.logs</code> is changed.</li>
<li>When they’re changed, the <code>rf.mu</code> lock must be hold by the caller of <code>persist()</code>. In order to make the states stored as soon as possible, I won’t release the lock util <code>persist()</code> is finished.</li>
<li>If it is possible to modify the three variables several times before communicate with outside, or release the lock, we just use a variable to mark whether need to persist, instead of really call <code>persist()</code> each time. And only call the <code>persist()</code> at the end.</li>
</ul>
</li>
</ol>
<h2 id="2d-log-compaction"><a class="markdownIt-Anchor" href="#2d-log-compaction"></a> 2D: Log compaction</h2>
<ol>
<li><strong>How to deal with those deleted entries when a server restarts?</strong>
<ul>
<li>When a server restores its state from <code>readPersist()</code>, we want it to re-execute those persisted logs.</li>
<li>But we don’t require to re-execute those deleted entries since we can get to the state of last deleted log by restore snapshot without execution.</li>
<li>The effect is the same as we have committed those missing entries. So we also need to modify the <code>lastApplied</code> and <code>commitIndex</code> to the last deleted log index before re-executing.</li>
</ul>
</li>
<li><strong>How to take a snapshot?</strong>
<ul>
<li>In this lab, the snapshot is naive. It simply stores all the log entries.</li>
<li>Hence, the server just need to set <code>lastIncludedIndex</code> and <code>lastIncludedTerm</code> appropriately, and remove all the snapshotted entries.</li>
<li>Also, if the log is empty after removal, we need to insert the sentinel entry back to the entry. Its command is still unimportant.</li>
</ul>
</li>
<li><strong>How to install a snapshot?</strong>
<ul>
<li>The snapshot data only contain commands in log entries, so we need to recover the log entries with the snapshot.
<ul>
<li>Their indices and commands are easy to understand.</li>
<li>We only know the <code>lastIncludedterm</code>. Also these entries are already committed in leader. Thus no matter what will happen in the future, these entries will not be overwriten and no <code>AppendEntry</code> will need to compare with these entries except for the last one. Hence for convenient, I set all their terms to the <code>LastIncludedTerm</code>.</li>
<li>As aforementioned, these entries are already reflected in the snapshot. Thus, there is no need to re-commit them again.</li>
</ul>
</li>
<li>Then we need to determine whether the snapshot contains new information.
<ul>
<li><code>FirstUncover</code> is to indicate the first entry in <code>logs</code> that is more up-to-date than the last entry in snapshot.
<ul>
<li>If the snapshot contains new information not already in the recipient’s log, then <code>firstUncover == len(rf.logs)</code>.</li>
<li>If the snapshot describes a prefix of its log, then <code>firstUncover &lt; len(rf.logs)</code>, then we only need to delete the former entries.</li>
</ul>
</li>
<li>I thought that maybe I just need to compare <code>lastLogIndex</code> in server and <code>LastIncludedIndex</code> of snapshot, but this is not sufficient.
<ul>
<li>Because some server need to discard the last few entries from ealier leaders, yet not in the logs of the current leader.</li>
<li>We need to remove the entries whose term is smaller than the <code>LastIncludedTerm</code> of snapshot while whose index is larger than <code>LastIncludedIndex</code> of snapshot.</li>
</ul>
</li>
</ul>
</li>
<li>If the snapshot contains new information
<ul>
<li>We will discard the whole log entries, and insert a new sentinel entry with index equals to <code>LastIncludedIndex</code> and term equals to <code>LastIncludedTerm</code>.</li>
<li>Then <code>commitIndex</code> and <code>lastApplied</code> need to be updated to <code>LastIncludedIndex</code> since we won’t re-execute those new entries in snapshot.</li>
<li>It need to be persisted.</li>
</ul>
</li>
<li>If the snapshot describes only a prefix of logs, then discard the covered entries. But don’t discard those covered, yet uncommitted entries and need to leave one entry as sentinel.</li>
</ul>
</li>
<li>If PrevLog is already trimmed, we should find the entry with the same index as the sentinel. And make it the new PrevLog, only accept the entries following it.</li>
</ol>
<h1 id="lab-3-fault-tolerant-keyvalue-service"><a class="markdownIt-Anchor" href="#lab-3-fault-tolerant-keyvalue-service"></a> Lab 3: Fault-tolerant Key/Value Service</h1>
<ol>
<li><strong>How does Key/Value server execute command from clerk?</strong>
<ul>
<li>It will call the <code>Start()</code> function of its associated Raft server. It can safely execute the command until the Raft server commits that command.</li>
<li>Only the KV server associated with leader Raft server can successfully use <code>Start()</code> to append command to logs.</li>
<li>But there could be a situation that the network is partitioned, and the clerk connects to a partition leader whose log entry can never be successfully committed. So we need to set a timeout for each command, if it cannot commit in time, the clerk should be informed to find another server.</li>
</ul>
</li>
<li><strong>Can clerk read from follower?</strong>
<ul>
<li>In the design described above, clerks can only read or write through the leader, which can make leader the bottleneck of the whole system.</li>
<li>With another design, read-only operation can be executed through followers, thus providing a speedup for the Get operation.
<ul>
<li>When a KV server received a Get operation request, it will request ReadIndex from the Raft leader server.</li>
<li>Then the raft leader server need to broadcast heartbeat to all followers to confirm that it is still the rightful leader.</li>
<li>When it heard from a majority followers, it can reply its last CommitIndex as the ReadIndex.</li>
<li>Then the KV server can execute the Get operation after it has applied at least as up-to-date as the ReadIndex.</li>
</ul>
</li>
<li>In the Raft protocol, a leader can only commit the entries appended in its term. Similarly, a leader can only grant ReadIndex pointing to an entry of its term. Or the result of read may become unlinearizable.
<ul>
<li>Consider the case that a leader committed index of <em>x</em>, and granted a ReadIndex of <em>x</em> to its associated KV server. So the KV server returned the result up to index <em>x</em>. But it crashed before sending commit message to other followers. Then a new leader is elected who received a request for ReadIndex. It does not know anything about committing entry <em>x</em>, thus granting a ReadIndex lower than <em>x</em>. Hence the read will return a result unseeing the execution result of entry of index <em>x</em>, which is violated with the linearizablility scheme.</li>
<li>So the solution is to append an empty entry to the log if the leader hasn’t appended any entry in its term.</li>
</ul>
</li>
</ul>
</li>
<li><strong>How can we prevent re-execution of an executed command?</strong>
<ul>
<li>Each clerk needs to maintain a monotonically increasing index to mark the command it issued.</li>
<li>Each Key/Value server needs to track the highest index it has executed for each clerk.</li>
<li>When a Key/Value server receives a new command, it compares the index of the new command with the highest executed index of that clerk. If the command’s index is lower, it can know that this is an executed command, and return the result directly.</li>
</ul>
</li>
<li><strong>How many kind of error could occur when client issues a command?</strong>
<ul>
<li>For read-only operation, there could be a key error.</li>
<li>Except for the read-only operation with follower read, the client may connect to a follower who cannot append a command.</li>
<li>The client may connect to a server in a minority parition of servers.
<ul>
<li>The PutAppend command is appended to the log but cannot commit.</li>
<li>For the read-only operation with follower read, it connects to its leader, but its leader cannot receive majority response. Hence it cannot acquire a ReadIndex.</li>
</ul>
</li>
<li>For read-only operation with follower read,
<ul>
<li>The KV Server is associated with a leader, and its leader set a higher commit index when communicating with majority. But the Raft leader crashed before commit that entry, leaving the KV server keep waiting to apply as up-to-date as the ReadIndex.</li>
<li>The client may connect to a partition without leader, hence cannot acquire ReadIndex.</li>
</ul>
</li>
</ul>
</li>
<li><strong>What should be stored in a snapshot?</strong>
<ul>
<li>Of course, we need to store the state of all key-value pairs.</li>
<li>We also need to store the last index executed for each client so far, in order to be able to recognize duplicated commands even after crashed.</li>
<li>The index of last applied entry is also stored to prevent that the first command after recovery is a read-only operation.</li>
</ul>
</li>
<li><strong>What need to do to recover the state after crashed?</strong>
<ul>
<li>First, we need to install the latest snapshot persisted.</li>
<li>But that is not enough if only the KV server is crashed while the associated Raft server is still running since there might still have some committed entries which is not included in the snapshot, yet applied before crash. The KV server needs to acquire those committed entries to truely bring itself back to the state right before crash.</li>
</ul>
</li>
<li>Execution speed of Get operation:
<ul>
<li>When running test without enabling snapshot:
<ul>
<li>The time spent for each operation of the un-optimzied implementation will become slower as the number of operation grows. This is because the log of each Raft server becomes larger and larger and becomes slower to persist.</li>
<li>When executing the Get operation, the log of optimized implementation won’t increase. Hence it won’t slow down the execution.</li>
<li>When executing <em>3000</em> Get operations, the un-optimized implementation needs about <em>50 ms/op</em>, while the optimized implementation only needs <em>1.7 ms/op</em>, which is about <em>29</em> times speedup.</li>
</ul>
</li>
<li>When running test enabling snapshot:
<ul>
<li>The time spent for the un-optimized implementation becomes stable since the log size is now stable. However, when only executing the Get operation, the oprimized implementation does not need the benefit of trimming logs.</li>
<li>When executing <em>3000</em> Get operations, the un-optimized imlementation need about <em>14.7 ms/op</em>, while the optimized imlementation is still <em>1.7 ms/op</em>, which is about <em>8.6</em> times speedup.</li>
</ul>
</li>
<li>I think that the speedup of optimized read-only scheme comes from two parts: follower concurrency that increased the total bandwidth between clients and servers, and the persisting time saved by eliminating Get operation entries from logs.</li>
</ul>
</li>
<li>Execution speed of PutAppend operation:
<ul>
<li>When executing <em>1000</em> Put operations without enabling snapshot, both implementations needs about <em>29.5 ms/op</em>. Executing Append operations needs about the same amount of time.</li>
<li>When executing <em>1000</em> Put operations enabling snapshot, both implementations needs about <em>13.9 ms/op</em>. Executing Append operations needs about the same amount of time.</li>
<li>Thus, with snapshot enabled, it can achieve about <em>2.1</em> times speedup. I think the speedup mainly comes from the persisting time saved by shrinking log.</li>
</ul>
</li>
</ol>
<h1 id="lab-4-shardkv"><a class="markdownIt-Anchor" href="#lab-4-shardkv"></a> Lab 4: ShardKV</h1>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/COPS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/COPS/" class="post-title-link" itemprop="url">COPS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:54:58" itemprop="dateCreated datePublished" datetime="2023-09-26T13:54:58+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:32:40" itemprop="dateModified" datetime="2023-10-04T16:32:40+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/cops.pdf">Don’t Settle for Eventual: Scalable Causal Consistency for Wide-Area Storage with COPS</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Issues<ul>
<li>Systems often sacrifice strong consistency to achieve these goals, exposing inconsistencies to their clients and necessitating complex application logic. </li>
<li>The author referred to systems with these four properties—Availability, low Latency, Partition-tolerance, and high Scalability—as ALPS systems. </li>
<li>Eventually consistent systems may expose versions out of order. </li>
</ul>
</li>
<li>Contribution:<ul>
<li>The author identified and defined a consistency model—causal consistency with convergent conflict handling, or causal+ —that is the strongest achieved under ALPS systems. <ul>
<li>The convergent conflict handling component of causal+ consistency ensures that replicas never permanently diverge and that conflicting updates to the same key are dealt with identically at all sites. </li>
<li>When combined with causal consistency, this property ensures that clients see only progressively newer versions of keys. </li>
</ul>
</li>
<li>The scalability of Clusters of Order-Preserving Servers (COPS) system can enforce causal dependencies between keys stored across an entire cluster, rather than a single server like previous systems. </li>
<li>In COPS-GT, the author introduced get transactions in order to obtain a consistent view of multiple keys without locking or blocking. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="ALPS-systems"><a href="#ALPS-systems" class="headerlink" title="ALPS systems"></a>ALPS systems</h2><h3 id="What-are-the-desirable-properties-of-ALPS-systems"><a href="#What-are-the-desirable-properties-of-ALPS-systems" class="headerlink" title="What are the desirable properties of ALPS systems?"></a>What are the desirable properties of ALPS systems?</h3><ol>
<li><strong>Availability</strong>: <ul>
<li>All operations issued to the data store complete successfully. </li>
<li>No operation can block indefinitely or return an error signifying that data is unavailable. </li>
</ul>
</li>
<li><strong>Low Latency</strong>: Client operations complete “quickly.” <ul>
<li>Commercial service-level objectives suggest average performance of a few milliseconds and worse-case performance (i.e., 99.9th percentile) of 10s or 100s of milliseconds. </li>
</ul>
</li>
<li><strong>Partition Tolerance</strong>: The data store continues to operate under network partitions. </li>
<li><strong>High Scalability</strong>: The data store scales out linearly. Adding N resources to the system increases aggregate throughput and storage capacity by $O(N)$. </li>
<li><strong>Stronger Consistency</strong>: <ul>
<li>Linearizability dictates that operations appear to take effect across the entire system at a single instance in time between the invocation and completion of the operation. </li>
<li>Eventual consistency models not only might subsequent reads not reflect the latest value, reads across multiple objects might reflect an incoherent mix of old and new values. </li>
</ul>
</li>
</ol>
<h3 id="What-is-the-problem-of-linearizability"><a href="#What-is-the-problem-of-linearizability" class="headerlink" title="What is the problem of linearizability?"></a>What is the problem of linearizability?</h3><ol>
<li>The CAP Theorem proves that a shared-data system that has availability and partition tolerance cannot achieve linearizability. </li>
<li>Low latency—defined as latency less than the maximum widearea delay between replicas—has also been proven incompatible with linearizability and sequential consistency. </li>
</ol>
<h2 id="Causal-consistency"><a href="#Causal-consistency" class="headerlink" title="Causal+ consistency"></a>Causal+ consistency</h2><h3 id="What-is-causal-consistency"><a href="#What-is-causal-consistency" class="headerlink" title="What is causal consistency?"></a>What is causal consistency?</h3><ol>
<li>Values are stored and retrieved from logical replicas, each of which hosts the entire key space. </li>
<li>The potential causality between operations denoted by $\leadsto$: <ul>
<li><strong>Execution Thread</strong>: If $a$ and $b$ are two operations in a single thread of execution, then $a \leadsto b$ if operation $a$ happens before operation $b$. </li>
<li><strong>Gets From</strong>: If $a$ is a put operation and $b$ is a get operation that returns the value written by $a$, then $a \leadsto b$. </li>
<li><strong>Transitivity</strong>: For operations $a$, $b$, and $c$, if $a \leadsto b$ and $b \leadsto c$, then $a \leadsto c$. </li>
</ul>
</li>
<li>Causal consistency requires that values returned from get operations at a replica are consistent with the order defined by $\leadsto$ (causality). <ul>
<li>It must appear the operation that writes a value occurs after all operations that causally precede it. </li>
<li>If $a \not\leadsto b$ and $b \not\leadsto a$, then $a$ and $b$ are concurrent. Causal consistency does not order concurrent operations. </li>
</ul>
</li>
</ol>
<h3 id="How-to-handle-conflicts"><a href="#How-to-handle-conflicts" class="headerlink" title="How to handle conflicts?"></a>How to handle conflicts?</h3><ol>
<li>If $a$ and $b$ are both puts to the same key, then they are in conflict. <ul>
<li>Conflicts are unordered by causal consistency, and allow replicas to diverge forever. </li>
<li>Conflicts may represent an exceptional condition that requires special handling. </li>
</ul>
</li>
<li>Convergent conflict handling requires that all conflicting puts be handled in the same manner at all replicas, using a handler function $h$. <ul>
<li>This handler function $h$ must be <strong>associative</strong> and <strong>commutative</strong>, so that replicas can handle conflicting writes in the order they receive them and that the results of these handlings will converge. <ul>
<li>The last-writer-wins rule (Thomas’s write rule): one of the conflicting writes as having occurred later and has it overwrite the “earlier” write. </li>
<li>The default COPS system avoids conflict detection using a last-writer-wins strategy. The “last” write is determined by comparing version numbers. </li>
<li>Another way is to mark them as conflicting and require their resolution by some other means. </li>
</ul>
</li>
</ul>
</li>
<li>All potential forms of convergent conflict handling avoid the first issue by ensuring that replicas reach the same result after exchanging operations. </li>
<li>the second issue with conflicts is only avoided by the use of more explicit conflict resolution procedures.<ul>
<li>These explicit procedures provide greater flexibility for applications, but require additional programmer complexity and/or performance overhead. </li>
</ul>
</li>
</ol>
<h3 id="How-to-decide-which-write-is-the-last-write"><a href="#How-to-decide-which-write-is-the-last-write" class="headerlink" title="How to decide which write is the last write?"></a>How to decide which write is the last write?</h3><ol>
<li>It uses last-write-win strategy. So the problem is how to decide which write is the last write. </li>
<li>The decision is made by attaching the current wall-clock time as version number on each put. <ul>
<li>Local shard server assigns <code>version number (v#) = time</code> when it receives client <code>put()</code></li>
<li>Remote datacenter receives <code>put(k, -, v#)</code><ul>
<li>If <code>v#</code> is larger than version of currently stored value for <code>k</code>, then it replaces the current value with new value, and update <code>v#</code>. </li>
<li>Otherwise, it just ignores new value. </li>
</ul>
</li>
</ul>
</li>
<li>If two <code>put(k)</code> happen at exactly the same time at different datacenters, we can break tie with a unique ID in the low bits of <code>v#</code>. </li>
<li>COPS uses Lamport clocks to assign <code>v#</code><ul>
<li>Each server implements a “Lamport clock” or “logical clock” <ul>
<li><code>Tmax = highest v# seen</code> (from self and others)</li>
<li><code>T = max(Tmax + 1, wall-clock time)</code></li>
</ul>
</li>
</ul>
</li>
<li>In the naive strategy, if one datacenter’s (or server’s) clock is fast by an hour, it will cause that datacenter’s values to win. In the worst case, it prevents any other update for an hour. <ul>
<li>But in  COPS, if some server has a fast clock, everyone who sees a version from that server will advance their Lamport clock. </li>
</ul>
</li>
</ol>
<h3 id="What-are-other-consistency-models"><a href="#What-are-other-consistency-models" class="headerlink" title="What are other consistency models?"></a>What are other consistency models?</h3><ol>
<li><p>Linearizability (or strong consistency) maintains a global, real-time ordering. </p>
</li>
<li><p>Sequential consistency ensures at least a global ordering. </p>
</li>
<li><p>Causal consistency ensures partial orderings between dependent operations. </p>
</li>
<li><p>FIFO (PRAM) consistency only preserves the partial ordering of an execution thread, not between threads. </p>
</li>
<li><p>Per-key sequential consistency ensures that, for each individual key, all operations have a global order. </p>
</li>
<li><p>Eventual consistency, a “catch-all” term used today suggesting eventual convergence to some type of agreement. </p>
</li>
<li><p>The strength of those models is as shown below:</p>
<p><img src="/imgs/Distributed/COPS/models.png" width="50%"></p>
</li>
</ol>
<h3 id="How-is-causal-ensured-in-COPS"><a href="#How-is-causal-ensured-in-COPS" class="headerlink" title="How is causal+ ensured in COPS?"></a>How is causal+ ensured in COPS?</h3><ol>
<li><p><strong>Progressing property</strong></p>
<ul>
<li><p>Different values a key has is referred as the versions of a key, which is denote $key_{version}$. </p>
</li>
<li><p>In COPS, versions are assigned in a manner that ensures that if $x_i \leadsto y_j$ then $i &lt; j$. </p>
</li>
<li><p>Once a replica in COPS returns version $i$ of a key, $x_i$, causal+ consistency ensures it will then only return that version or a causally later version. </p>
</li>
<li><p>The handling of a conflict is causally later than the conflicting puts it resolves. </p>
<ul>
<li><p>Assume a replica first returns $x_i$ and then $x_k$, where $i \ne k$ and $x_i \not\leadsto x_k$. </p>
</li>
<li><p>Causal consistency ensures that if $x_k$ is returned after $x_i$, then $x_k \not\leadsto x_i$, and so $x_i$ and $x_k$ conflict. </p>
</li>
<li><p>But, if $x_i$ and $x_k$ conflict, then convergent conflict handling ensures that as soon as both are present at a replica, their handling $h(x_i,x_k)$, which is causally after both, will be returned instead of either $x_i$ or $x_k$, which contradicts our assumption. </p>
</li>
</ul>
</li>
<li><p>Thus, each replica in COPS always returns non-decreasing versions of a key. </p>
</li>
</ul>
</li>
<li><p>$y_j$ depends on $x_i$ if and only if $put(x_i) \leadsto put(y_j)$. </p>
<ul>
<li>These dependencies are in essence the reverse of the causal ordering of writes. </li>
<li>COPS provides causal+ consistency during replication by writing a version only after writing all of its dependencies. </li>
</ul>
</li>
</ol>
<h3 id="Why-previous-systems-cannot-provide-scalability"><a href="#Why-previous-systems-cannot-provide-scalability" class="headerlink" title="Why previous systems cannot provide scalability?"></a>Why previous systems cannot provide scalability?</h3><ol>
<li><p>They all use a form of log serialization and exchange. </p>
<ul>
<li><p>All operations at a logical replica are written to a single log in serialized order, commonly marked with a version vector. </p>
</li>
<li><p>Log-exchange-based serialization inhibits replica scalability, as it relies on a single serialization point in each replica to establish ordering. </p>
</li>
<li>Either causal dependencies between keys are limited to the set of keys that can be stored on one node, or a single node (or replicated state machine) must provide a commit ordering and log for all operations across a cluster.</li>
</ul>
</li>
<li><p>In COPS, nodes in each datacenter are responsible for different partitions of the keyspace, but the system can track and enforce dependencies between keys stored on different nodes. </p>
<ul>
<li>COPS explicitly encodes dependencies in metadata associated with each key’s version. </li>
<li>When keys are replicated remotely, the receiving datacenter performs dependency checks before committing the incoming version.</li>
</ul>
</li>
</ol>
<h2 id="COPS-system"><a href="#COPS-system" class="headerlink" title="COPS system"></a>COPS system</h2><h3 id="What-are-the-components-of-COPS"><a href="#What-are-the-components-of-COPS" class="headerlink" title="What are the components of COPS?"></a>What are the components of COPS?</h3><ol>
<li>Key-value store<ul>
<li>Each key-value pair has associated metadata. <ul>
<li>In COPS, this metadata is a version number. </li>
<li>In COPS-GT, it is both a version number and a list of dependencies (other keys and their respective versions). </li>
</ul>
</li>
<li>The key-value store exports three additional operations as part of its key-value interface: <code>get_by_version</code>, <code>put_after</code>, and <code>dep_check</code>. </li>
<li>For COPS-GT, the system keeps around old versions of key-value pairs, not just the most recent put, to ensure that it can provide get transactions. </li>
</ul>
</li>
<li>Client library<ul>
<li>The client library exports two main operations to applications: reads via <code>get</code> (in COPS) or <code>get_trans</code> (in COPS-GT), and writes via <code>put</code>. </li>
<li>The client library also maintains state about a client’s current dependencies through a <code>context</code> parameter in the client library API. </li>
</ul>
</li>
<li>A client of COPS is an application that uses the COPS client library to call directly into the COPS key-value store. </li>
<li>Clients communicate only with their local COPS cluster running in the same datacenter.</li>
</ol>
<h3 id="What-consistency-is-achieved-in-COPS"><a href="#What-consistency-is-achieved-in-COPS" class="headerlink" title="What consistency is achieved in COPS?"></a>What consistency is achieved in COPS?</h3><ol>
<li>Each local COPS cluster is set up as a linearizable (strongly consistent) key-value store. <ul>
<li>Linearizable systems can be implemented scalably by partitioning the keyspace into N linearizable partitions. </li>
<li>The composability of linearizability ensures that the resulting system as a whole remains linearizable. </li>
<li>Linearizability is acceptable locally because we expect very low latency and no partitions within a cluster. </li>
</ul>
</li>
<li>Replication between COPS clusters happens asynchronously to ensure low latency for client operations and availability in the face of external partitions. </li>
<li>The COPS design strives to provide causal+ consistency with resource and performance overhead similar to existing eventually consistent systems. <ul>
<li>COPS and COPS-GT need to minimize overhead of consistency-preserving replication<ul>
<li>A naive implementation, however, would require checks on all of a value’s dependencies. </li>
</ul>
</li>
<li>COPS-GT needs to minimize space requirements</li>
<li>COPS-GT needs to ensure fast <code>get_trans</code> operations<ul>
<li>A naive algorithm could block and/or take an unbounded number of get rounds to complete. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="Key-value-store"><a href="#Key-value-store" class="headerlink" title="Key-value store"></a>Key-value store</h3><h4 id="What-is-stored-in-COPS-storage"><a href="#What-is-stored-in-COPS-storage" class="headerlink" title="What is stored in COPS storage?"></a>What is stored in COPS storage?</h4><ol>
<li>COPS must track the versions of written values, as well as their dependencies in the case of COPS-GT. </li>
<li>In COPS, the system stores the most recent version number and value for each key. </li>
<li>In COPS-GT, the system maps each key to a list of version entries, each consisting of <code>&lt;version, value, deps&gt;</code>. <ul>
<li>The deps field is a list of the version’s zero or more dependencies; each dependency is a <code>&lt;key, version&gt;</code> pair. </li>
</ul>
</li>
</ol>
<h4 id="How-does-COPS-support-scalability-in-KV-storage"><a href="#How-does-COPS-support-scalability-in-KV-storage" class="headerlink" title="How does COPS support scalability in KV-storage?"></a>How does COPS support scalability in KV-storage?</h4><ol>
<li>It partitions the keyspace across a cluster’s nodes using consistent hashing. </li>
<li>Every key stored in COPS has one primary node in each cluster. <ul>
<li>The set of primary nodes for a key across all clusters are termed as the <strong>equivalent nodes</strong> for that key. </li>
<li>After a write completes locally, the primary node places it in a replication queue, from which it is sent asynchronously to remote equivalent nodes. </li>
<li>Those nodes, in turn, wait until the value’s dependencies are satisfied in their local cluster before locally committing the value. </li>
<li>This dependency checking mechanism ensures writes happen in a causally consistent order and reads never block. </li>
</ul>
</li>
<li>In practice, COPS’s consistent hashing assigns each node responsibility for a few different key ranges. <ul>
<li>Key ranges may have different sizes and node mappings in different datacenters</li>
<li>The total number of equivalent nodes with which a given node needs to communicate is proportional to the number of datacenters (i.e., communication is not all-to-all between nodes in different datacenters).</li>
</ul>
</li>
</ol>
<h3 id="Client-library"><a href="#Client-library" class="headerlink" title="Client library"></a>Client library</h3><h4 id="What-is-provided-in-the-client-API"><a href="#What-is-provided-in-the-client-API" class="headerlink" title="What is provided in the client API?"></a>What is provided in the client API?</h4><ol>
<li>$ctx_id\leftarrow createContext()$</li>
<li>$bool\leftarrow deleteContext(ctx_id)$</li>
<li>$bool\leftarrow put(key,value,ctx_id)$</li>
<li>$value\leftarrow get(key,ctx_id)$ for COPS, or $\langle values\rangle\leftarrow get_trans(\langle keys\rangle,ctx_id)$ for COPS-GT<ul>
<li>COPS-GT provides <code>get_trans</code>, which returns a consistent view of multiple key-value pairs in a single call. </li>
</ul>
</li>
</ol>
<h4 id="What-is-the-context-in-library"><a href="#What-is-the-context-in-library" class="headerlink" title="What is the context in library?"></a>What is the context in library?</h4><ol>
<li>All functions take a context argument, which the library uses internally to track causal dependencies across each client’s operations. </li>
<li>The context defines the causal+ “thread of execution.” A single process may contain many separate threads of execution. </li>
<li>By separating different threads of execution, COPS avoids false dependencies that would result from intermixing them. </li>
</ol>
<h4 id="What-does-COPS-GT-store-in-context"><a href="#What-does-COPS-GT-store-in-context" class="headerlink" title="What does COPS-GT store in context?"></a>What does COPS-GT store in context?</h4><ol>
<li><p>The client library in COPS-GT stores the client’s context in a table of $\langle key, version, deps\rangle$ entries. </p>
<ul>
<li>Clients reference their context using a context ID ($ctx_id$) in the API. </li>
</ul>
</li>
<li>When a client gets a key from the data store, the library adds this key and its causal dependencies to the context. </li>
<li>When a client puts a value, the library sets the put’s dependencies to the most recent version of each key in the current context. <ul>
<li>A successful put into the data store returns the version number v assigned to the written value. </li>
<li>The client library then adds this new entry, $\langle key, v, D\rangle$, to the context. </li>
</ul>
</li>
</ol>
<h4 id="What-are-the-concerns-of-COPS-GT-context"><a href="#What-are-the-concerns-of-COPS-GT-context" class="headerlink" title="What are the concerns of COPS-GT context?"></a>What are the concerns of COPS-GT context?</h4><ol>
<li>The context therefore includes all values previously read or written in the client’s session, as well as all of those dependencies’ dependencies. </li>
<li>State requirements for storing these dependencies, both in the client library and in the data store. <ul>
<li>To mitigate the client and data-store state required to track dependencies, COPS-GT provides garbage collection. </li>
</ul>
</li>
<li>The number of potential checks that must occur when replicating writes between clusters, in order to ensure causal consistency. <ul>
<li>The dependencies that must be checked are termed the nearest dependencies. <ul>
<li>If the storage node committing a node determins that its direct dependencies are all committed, then it can infer that all former dependencies are also committed. </li>
<li>Hence, each dependency check only need to check nodes within $1$ step in the graph of causal dependencies. </li>
</ul>
</li>
<li>The nearest dependencies are sufficient for the key-value store to provide causal+ consistency. </li>
<li>The full dependency list is only needed to provide <code>get_trans</code> operations in COPS-GT. </li>
</ul>
</li>
</ol>
<h4 id="What-does-COPS-store-in-context"><a href="#What-does-COPS-store-in-context" class="headerlink" title="What does COPS store in context?"></a>What does COPS store in context?</h4><ol>
<li>It does not store or even retrieve the dependencies of any value it gets<ul>
<li>The retrieved value is nearer than any of its dependencies, rendering them unnecessary. </li>
<li>The COPS client library stores only $\langle key, version\rangle$ entries. </li>
</ul>
</li>
<li>For a get operation, the retrieved $\langle key, version\rangle$ is added to the context. </li>
<li>For a put operation, the library uses the current context as the nearest dependencies, clears the context, and then repopulates it with only this put. <ul>
<li>This put depends on all previous key-version pairs and thus is nearer than them. </li>
</ul>
</li>
</ol>
<h4 id="How-does-COPS-or-COPS-GT-write-to-local-cluster"><a href="#How-does-COPS-or-COPS-GT-write-to-local-cluster" class="headerlink" title="How does COPS (or COPS-GT) write to local cluster?"></a>How does COPS (or COPS-GT) write to local cluster?</h4><ol>
<li>All writes in COPS first go to the client’s local cluster and then propagate asynchronously to remote clusters. </li>
<li>The key-value store exports a single API call to provide both operations: $\langle bool,vers\rangle \leftarrow put_after(key,val,[deps],nearest,vers=\empty)$</li>
<li>When a client calls $put (key,val,ctx_id)$, <ul>
<li>The library computes the complete set of dependencies deps, and identifies some of those dependency tuples as the value’s nearest ones. </li>
<li>The library then calls put after without the version argument (i.e., it sets $version=\empty$). </li>
<li>In COPS-GT, the library includes deps in the <code>put_after</code> call because dependencies must be stored with the value. </li>
<li>In COPS, the library only needs to include nearest and does not include deps. </li>
</ul>
</li>
<li>The key’s primary storage node in the local cluster assigns the key a version number and returns it to the client library. </li>
<li>Each client is restricted to a single outstanding put; this is necessary because later puts must know the version numbers of earlier puts so they may depend on them. </li>
<li>The put after operation ensures that val is committed to each cluster only after all of the entries in its dependency list have been written. <ul>
<li>In the client’s local cluster, this property holds automatically, as the local store provides linearizability. </li>
<li>If $y$ depends on $x$, then $put(x)$ must have been committed before $put(y)$ was issued. </li>
</ul>
</li>
</ol>
<h4 id="How-to-replica-writes-between-clusters"><a href="#How-to-replica-writes-between-clusters" class="headerlink" title="How to replica writes between clusters?"></a>How to replica writes between clusters?</h4><ol>
<li>After a write commits locally, the primary storage node asynchronously replicates that write to its equivalent nodes in different clusters using a stream of <code>put_after</code> operations. <ul>
<li>The primary node includes the key’s version number in the put after call. </li>
<li>The deps argument is included in COPS-GT, and not included in COPS. </li>
</ul>
</li>
<li>It requires the remote nodes receiving updates to commit an update only after its dependencies have been committed to the same cluster. <ul>
<li>A node that receives a put after request from another cluster must determine if the value’s nearest dependencies have already been satisfied locally. </li>
<li>It does so by issuing a check to the local nodes responsible for the those dependencies: $bool\leftarrow dep_check(key, version)$</li>
<li>The way that nearest dependencies are computed ensures that all dependencies have been satisfied before the value is committed, which in turn ensures causal consistency. </li>
</ul>
</li>
</ol>
<h4 id="How-to-read-values-in-COPS"><a href="#How-to-read-values-in-COPS" class="headerlink" title="How to read values in COPS?"></a>How to read values in COPS?</h4><ol>
<li>Reads are satisfied in the local cluster. </li>
<li>The library issues a read to the node responsible for the key in the local cluster: $\langle value, version,deps\rangle\leftarrow get_by_version(key,version=LATEST)$</li>
<li>This read can request either the latest version of the key or a specific older one. Requesting a specific version is necessary to enable get transactions. </li>
<li>Upon receiving a response, the client library adds the $\langle key,version,[deps]\rangle$ tuple to the client context, and returns value to the calling code. </li>
<li>The deps are stored only in COPS-GT, not in COPS. </li>
</ol>
<h4 id="Why-does-COPS-GT-need-to-proviced-get-trans-What-is-wrong-with-the-get-interface"><a href="#Why-does-COPS-GT-need-to-proviced-get-trans-What-is-wrong-with-the-get-interface" class="headerlink" title="Why does COPS-GT need to proviced get_trans? What is wrong with the get interface?"></a>Why does COPS-GT need to proviced get_trans? What is wrong with the get interface?</h4><ol>
<li>Reading a set of dependent keys using a single-key get interface cannot ensure causal+ consistency, even though the data store itself is causal+ consistent. </li>
<li>It may have a “time-to-check-to-time-to-use” race condition, i.e. check operation and usage is not an atomic operation. </li>
<li>The standard way to achieve such a guarantee is to read and write all related keys in a transaction<ul>
<li>This requires a single serialization point for all grouped keys, which COPS avoids for greater scalability and simplicity. </li>
</ul>
</li>
</ol>
<h4 id="How-does-get-trans-work"><a href="#How-does-get-trans-work" class="headerlink" title="How does get_trans work?"></a>How does get_trans work?</h4><ol>
<li><p>To retrieve multiple values in a causal+ consistent manner, a client calls get trans with the desired set of keys. </p>
</li>
<li><p>In the first round, the library issues n concurrent <code>get_by_version</code> operations to the local cluster, one for each key the client listed in <code>get_trans</code>. </p>
<ul>
<li>Because COPS-GT commits writes locally, the local data store guarantees that each of these explicitly listed keys’ dependencies are already satisfied</li>
<li><p>All listed keys have been written locally and reads on them will immediately return. </p>
</li>
<li><p>Each <code>get_by_version</code> operation returns a $\langle value, version, deps\rangle$ tuple, where deps is a list of keys and versions. </p>
</li>
</ul>
</li>
<li><p>The client library then examines every dependency entry $\langle key, version\rangle$. </p>
<ul>
<li>The causal dependencies for that result are satisfied<ul>
<li>If either the client did not request the dependent key. </li>
<li>Or if it did, the version it retrieved was $≥$ the version in the dependency list. </li>
</ul>
</li>
</ul>
</li>
<li><p>For all keys that are not satisfied, the library issues a second round of concurrent get by version operations. </p>
<ul>
<li>The version requested will be the newest version seen in any dependency list from the first round. </li>
<li>These versions satisfy all causal dependencies from the first round because they are $≥$ the needed versions. </li>
<li>Because dependencies are transitive and these second-round versions are all depended on by versions retrieved in the first round, they do not introduce any new dependencies that need to be satisfied. </li>
</ul>
</li>
<li><p>The second round happens only when the client must read newer versions than those retrieved in the first round. </p>
<ul>
<li>This case occurs only if keys involved in the get transaction are updated during the first round.</li>
</ul>
</li>
</ol>
<h3 id="Garbage-collection"><a href="#Garbage-collection" class="headerlink" title="Garbage collection"></a>Garbage collection</h3><h4 id="How-does-COPS-GT-collect-version-garbage"><a href="#How-does-COPS-GT-collect-version-garbage" class="headerlink" title="How does COPS-GT collect version garbage?"></a>How does COPS-GT collect version garbage?</h4><ol>
<li>The <code>get_trans</code> algorithm limits the number of versions needed to complete a get transaction. <ul>
<li>COPS-GT limits the total running time of get trans through a configurable parameter, <code>trans_time</code>. </li>
<li>If the timeout fires, the client library will restart the get trans call and satisfy the transaction with newer versions of the keys. </li>
</ul>
</li>
<li>After a new version of a key is written, COPS-GT only needs to keep the old version around for <code>trans_time</code> plus a small delta for clock skew. </li>
<li>The space overhead is bounded by the number of old versions that can be created within the <code>trans_time</code>. <ul>
<li>This number is determined by the maximum write throughput that the node can sustain. </li>
<li>This overhead is per-machine and does not grow with the cluster size or the number of datacenters. </li>
</ul>
</li>
</ol>
<h4 id="How-does-COPS-GT-collect-dependency-garbage"><a href="#How-does-COPS-GT-collect-dependency-garbage" class="headerlink" title="How does COPS-GT collect dependency garbage?"></a>How does COPS-GT collect dependency garbage?</h4><ol>
<li>COPS-GT can garbage collect these dependencies once the versions associated with old dependencies are no longer needed for correctness in get transaction operations. </li>
<li>After <code>trans_time</code> seconds after a value has been committed in all datacenters, COPS-GT can clean a value’s dependencies. <ul>
<li>To clean dependencies each remote datacenter notifies the originating datacenter when the write has committed and the timeout period has elapsed.</li>
<li>Once all datacenters confirm, the originating datacenter cleans its own dependencies and informs the others to do likewise. </li>
</ul>
</li>
<li>To minimize bandwidth devoted to cleaning dependencies, a replica only notifies the originating datacenter if this version of a key is the newest after <code>trans_time</code> seconds<ul>
<li>If it is not, there is no need to collect the dependencies because the entire version will be collected. </li>
</ul>
</li>
<li>Under normal operation, dependencies are garbage collected after <code>trans_time</code> plus a round-trip time. </li>
<li>During a partition, dependencies on the most recent versions of keys cannot be collected. </li>
</ol>
<h4 id="How-does-COPS-collect-client-metadata-garbage"><a href="#How-does-COPS-collect-client-metadata-garbage" class="headerlink" title="How does COPS collect client metadata garbage?"></a>How does COPS collect client metadata garbage?</h4><ol>
<li>The COPS client library tracks all operations during a client session (single thread of execution) using the ctx id passed with all operation. <ul>
<li>In both systems, each get since the last put adds another nearest dependency. </li>
<li>Additionally in COPS-GT, all new values and their dependencies returned in get trans operations and all put operations add normal dependencies. </li>
</ul>
</li>
<li>Clients need to track dependencies only until they are guaranteed to be satisfied everywhere. </li>
<li>Once a <code>put_after</code> commits successfully to all datacenters, COPS flags that key version as <em>never-depend</em>, in order to indicate that clients need not express a dependence upon it. <ul>
<li>The client library will immediately remove a never-depend item from the list of dependencies in the client context. </li>
<li>Anything that a never-depend key depended on must have been flagged never-depend, so it too can be garbage collected from the context. </li>
</ul>
</li>
<li>The COPS storage nodes remove unnecessary dependencies from <code>put_after</code> operations. <ul>
<li>When a node receives a <code>put_after</code>, it checks each item in the dependency list and removes items with version numbers older than a global checkpoint time. </li>
</ul>
</li>
</ol>
<h3 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault tolerance"></a>Fault tolerance</h3><h4 id="How-does-COPS-handle-client-failures"><a href="#How-does-COPS-handle-client-failures" class="headerlink" title="How does COPS handle client failures?"></a>How does COPS handle client failures?</h4><ol>
<li>From the storage system’s perspective, if a client fails, it simply stops issuing new requests; no recovery is necessary. </li>
<li>From a client’s perspective, COPS’s dependency tracking makes it easier to handle failures of other clients, by ensuring properties such as referential integrity. </li>
</ol>
<h4 id="How-does-COPS-handle-key-value-node-failures"><a href="#How-does-COPS-handle-key-value-node-failures" class="headerlink" title="How does COPS handle key-value node failures?"></a>How does COPS handle key-value node failures?</h4><ol>
<li><p>COPS can use any underlying faulttolerant linearizable key-value store. </p>
<ul>
<li>The author uses chain replication within a cluster to mask node failures. </li>
</ul>
</li>
<li><p>Dependency garbage collection follows a similar pattern of interlocking chains. </p>
<p>Version garbage collection is done locally on each node and can operate as in the single node case. </p>
<p>Calculation of the global checkpoint time, for client metadata garbage collection, operates normally with each tail updating its corresponding key range minimums. </p>
</li>
</ol>
<h4 id="What-will-happen-when-datacenter-failed"><a href="#What-will-happen-when-datacenter-failed" class="headerlink" title="What will happen when datacenter failed?"></a>What will happen when datacenter failed?</h4><ol>
<li>Any put after operations that originated in the failed datacenter, but which were not yet copied out, will be lost. </li>
<li>The storage required for replication queues in the active datacenters will grow. <ul>
<li>They will be unable to send put after operations to the failed datacenter, and thus COPS will be unable to garbage collect those dependencies. </li>
<li>Solutions: Allow the queues to grow if the partition is likely to heal soon, or reconfigure COPS to no longer use the failed datacenter. </li>
</ul>
</li>
</ol>
<h4 id="How-does-COPS-with-conflict-detection-COPS-CD-detect-conflict"><a href="#How-does-COPS-with-conflict-detection-COPS-CD-detect-conflict" class="headerlink" title="How does COPS with conflict detection (COPS-CD) detect conflict?"></a>How does COPS with conflict detection (COPS-CD) detect conflict?</h4><ol>
<li>All put operations carry with them previous version metadata<ul>
<li>It indicates the most recent previous version of the key that was visible at the local cluster at the time of the write. </li>
</ul>
</li>
<li>All put operations now have an implicit dependency on that previous version<ul>
<li>This ensures that a new version will only be written after its previous version. </li>
</ul>
</li>
<li>COPS-CD has an applicationspecified convergent conflict handler that is invoked when a conflict is detected. <ul>
<li>A put operation $new$ to a key (with previous version $prev$) is in conflict with the key’s current visible version $curr$: $prev \neq curr$ if and only if $new$ and $curr$ conflict. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Memcache/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Memcache/" class="post-title-link" itemprop="url">Memcache</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:52:21" itemprop="dateCreated datePublished" datetime="2023-09-26T13:52:21+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:32:01" itemprop="dateModified" datetime="2023-10-04T16:32:01+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/memcache-fb.pdf">Scaling Memcache at Facebook</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li><p>Issues: </p>
<ul>
<li>Users consume an order of magnitude more content than they create. <ul>
<li>This behavior results in a workload dominated by fetching data and suggests that caching can have significant advantages. </li>
</ul>
</li>
<li>Read operations fetch data from a variety of sources<ul>
<li>This heterogeneity requires a flexible caching strategy able to store data from disparate sources. </li>
</ul>
</li>
</ul>
</li>
<li><p>The requirement of a social network’s infrastructure?</p>
<ul>
<li>Allow near realtime communication</li>
<li><p>Aggregate content on-the-fly from multiple sources</p>
</li>
<li><p>Be able to access and update very popular shared content</p>
</li>
<li><p>Scale to process millions of user requests per second</p>
</li>
</ul>
</li>
<li><p>Memcached is an open-source implementation of an in-memory hash table, which provides low latency access to a shared storage pool at low cost. </p>
</li>
<li><p>Findings: </p>
<ul>
<li>While qualities like performance, efficiency, fault-tolerance, and consistency are important at all scales, at specific sizes some qualities require more effort to achieve than others. </li>
<li>The importance of finding an optimal communication schedule increases as the number of servers increase and networking becomes the bottleneck. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h3 id="How-does-queries-carry-out-with-memcache"><a href="#How-does-queries-carry-out-with-memcache" class="headerlink" title="How does queries carry out with memcache?"></a>How does queries carry out with memcache?</h3><ol>
<li>When a web server needs data, it first requests the value from memcache by providing a string key. </li>
<li>If the item addressed by that key is not cached, the web server retrieves the data from the database or other backend service and populates the cache with the key-value pair. </li>
</ol>
<p><img src="/imgs/Distributed/Memcache/read.png" width="30%"></p>
<h3 id="How-does-writes-carry-out-with-memcache"><a href="#How-does-writes-carry-out-with-memcache" class="headerlink" title="How does writes carry out with memcache?"></a>How does writes carry out with memcache?</h3><ol>
<li>For write requests, the web server issues SQL statements to the database and then sends a delete request to memcache that invalidates any stale data. </li>
<li>We choose to delete cached data instead of updating it because deletes are idempotent. <ul>
<li>Consider two concurrent writes, there is no guarantee that the update operation is in the same order as database updates. </li>
<li>If database updates write A first, then write B while memcache set write B first then write A, the memcache would be inconsistent with database. </li>
</ul>
</li>
</ol>
<p><img src="/imgs/Distributed/Memcache/write.png" width="30%"></p>
<h3 id="What-are-the-stages-of-problems-when-user-increasing"><a href="#What-are-the-stages-of-problems-when-user-increasing" class="headerlink" title="What are the stages of problems when user increasing?"></a>What are the stages of problems when user increasing?</h3><ol>
<li>At the beginning, there is no much users, the service can be provided with a single server running both scripts and database. </li>
<li>As the number of users growing, the first problem is usually that the server run out of CPU to execute scripts. <ul>
<li>Hence the solution is to use multiple servers to run scripts, and another server to run database. </li>
</ul>
</li>
<li>If the number of users keeps growing, the next problem is that the database run out of steam. <ul>
<li>The solution is usually using a distributed sharding database system. This is when all those consistent problem comes. </li>
</ul>
</li>
<li>However, the MySQL cannot process reads and writes fast. And if there are some keys being the hot zone, no matter how delicate we sharding, there is only one group for that key. <ul>
<li>The FaceBook uses memcache to solve this situation when most read requests are absorbed by memcache servers, only few are exposed to database servers. </li>
</ul>
</li>
</ol>
<h3 id="What-are-the-consistent-requirements"><a href="#What-are-the-consistent-requirements" class="headerlink" title="What are the consistent requirements?"></a>What are the consistent requirements?</h3><ol>
<li>When user only reads data, they can barely notice the stale data for a few seconds, but not too long, like data from yesterday. </li>
<li>When users changed something, if they immediately try to read it, they should see the data they changed. There should not have any stale in this case. </li>
</ol>
<h2 id="In-a-cluster-latency-and-load"><a href="#In-a-cluster-latency-and-load" class="headerlink" title="In a cluster: latency and load"></a>In a cluster: latency and load</h2><h3 id="How-does-clients-communicate-with-memcached-servers"><a href="#How-does-clients-communicate-with-memcached-servers" class="headerlink" title="How does clients communicate with memcached servers?"></a>How does clients communicate with memcached servers?</h3><ol>
<li>Client logic is provided as two components: a library that can be embedded into applications or as a standalone proxy named <em>mcrouter</em>. <ul>
<li>This proxy presents a <em>memcached</em> server interface and routes the requests/replies to/from other servers.</li>
</ul>
</li>
<li><code>get</code> requests is relied on UDP to reduce latency and overhead. <ul>
<li>Each thread in the web server is allowed to directly communicate with <em>memcached</em> servers directly, bypassing <em>mcrouter</em>, without establishing and maintaining a connection thereby reducing the overhead. </li>
<li>The UDP implementation detects packets that are dropped or received out of order (using sequence numbers) and treats them as errors on the client side. It does not provide any mechanism to try to recover from them.</li>
</ul>
</li>
<li>For reliability, clients perform <code>set</code> and <code>delete</code> operations over TCP through an instance of <em>mcrouter</em> running on the same machine as the web server. <ul>
<li>For operations where we need to confirm a state change (<code>update</code>s and <code>delete</code>s) TCP alleviates the need to add a retry mechanism to our UDP implementation. </li>
</ul>
</li>
</ol>
<h3 id="How-to-handle-incast-congestion"><a href="#How-to-handle-incast-congestion" class="headerlink" title="How to handle incast congestion?"></a>How to handle incast congestion?</h3><ol>
<li>Web servers have to routinely communicate with many memcached servers to satisfy a user request. <ul>
<li>As a result, all web servers communicate with every memcached server in a short period of time. </li>
<li>This all-to-all communication pattern can cause incast congestion or allow a single server to become the bottleneck for many web servers. </li>
</ul>
</li>
<li>Memcache clients implement flowcontrol mechanisms to limit incast congestion. <ul>
<li>Clients use a sliding window mechanism to control the number of outstanding requests. </li>
<li>The size of this sliding window grows slowly upon a successful request and shrinks when a request goes unanswered. </li>
<li>With lower window sizes, the application will have to dispatch more groups of memcache requests serially, increasing the duration of the web request. </li>
<li>As the window size gets too large, the number of simultaneous memcache requests causes incast congestion. </li>
</ul>
</li>
</ol>
<h3 id="How-to-prevent-stale-sets-problem"><a href="#How-to-prevent-stale-sets-problem" class="headerlink" title="How to prevent stale sets problem?"></a>How to prevent stale sets problem?</h3><ol>
<li>A stale set occurs when a web server sets a value in <em>memcache</em> that does not reflect the latest value that should be cached. <ul>
<li>For example, when server A tried to read an entry <code>get(k)</code> and missed, it read a value <code>v1</code> from database. But before its <code>set(k, v1)</code> being executed, another server updated <code>k=v2</code> and executed <code>delete(k)</code>. </li>
<li>The problem is that after <code>delete(k)</code> is executed, there is no mechanism to prevent the stale <code>set(k, v1)</code> being executed. Hence a stale entry will be left in the <em>memcache</em> for indefinite long time. </li>
</ul>
</li>
<li>A memcached instance gives a lease to a client to set data back into the cache when that client experiences a cache miss. <ul>
<li>The lease is a 64-bit token bound to the specific key the client originally requested. </li>
<li>The client provides the lease token when setting the value in the cache. </li>
<li>Verification can fail if memcached has invalidated the lease token due to receiving a delete request for that item. </li>
</ul>
</li>
</ol>
<h3 id="How-to-prevent-thundering-herds"><a href="#How-to-prevent-thundering-herds" class="headerlink" title="How to prevent thundering herds?"></a>How to prevent thundering herds?</h3><ol>
<li>A thundering herd happens when a specific key undergoes heavy read and write activity. <ul>
<li>If one client updates database and deletes a key, lots of clients <code>get()</code> but miss causing that they all try to fetch from database and all <code>set()</code>. </li>
</ul>
</li>
<li>Each memcached server regulates the rate at which it returns lease tokens. <ul>
<li>Each memcached server only return a lease token every period of time. </li>
<li>Other cache misses during that period will receive a special notification telling the client to wait a shot amount of time. </li>
<li>Return a lease every period of time instead of only return to first cache miss is to prevent the first client failed to acquire data from database. </li>
</ul>
</li>
</ol>
<h3 id="How-to-prevent-hit-rates-decreasing-caused-by-different-client-access-patterns"><a href="#How-to-prevent-hit-rates-decreasing-caused-by-different-client-access-patterns" class="headerlink" title="How to prevent hit rates decreasing caused by different client access patterns?"></a>How to prevent hit rates decreasing caused by different client access patterns?</h3><ol>
<li>A cluster’s memcached servers are partitioned into separate pools. <ul>
<li>One pool (named <em>wildcard</em>) is designated as the default. </li>
<li>Separate pools are provided for keys whose residence in wildcard is problematic. </li>
</ul>
</li>
<li>Within some pools, replication can be used to improve the latency and efficiency. <ul>
<li>Choose to replicate a category of keys within a pool when <ul>
<li>The application routinely fetches many keys simultaneously</li>
<li>The entire data set fits in one or two memcached servers</li>
<li>The request rate is much higher than what a single server can manage</li>
</ul>
</li>
<li>Favor replication in this instance over further dividing the key space due to the following reasons: <ul>
<li>The difference in memcached overhead for retrieving 100 keys per request instead of 1 key is small. </li>
<li>Replicating can reduce the requests need to be processed by each server, while dividing the key space can only reduce the keys retrieved from each server and increase the number of requests. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="How-to-handle-small-number-memcache-server-inaccessible-due-to-network-or-server-failure"><a href="#How-to-handle-small-number-memcache-server-inaccessible-due-to-network-or-server-failure" class="headerlink" title="How to handle small number memcache server inaccessible due to network or server failure?"></a>How to handle small number memcache server inaccessible due to network or server failure?</h3><ol>
<li><p>For small outages, it relies on an automated remediation system. </p>
<ul>
<li>These actions are not instant and can take up to a few minutes. </li>
<li>This duration is long enough to cause the aforementioned cascading failures. </li>
</ul>
</li>
<li><p>A small set of machines, named <em>Gutter</em>, takes over the responsibilities of a few failed servers. </p>
<ul>
<li>Gutter accounts for approximately 1% of the memcached servers in a cluster. </li>
<li>When a memcached client receives no response to its get request, the client assumes the server has failed and issues the request again to a special Gutter pool. </li>
<li><p>If this second request misses, the client will insert the appropriate key-value pair into the Gutter machine after querying the database.</p>
</li>
<li><p>Entries in Gutter expire quickly to obviate Gutter invalidations. </p>
</li>
<li><p>Gutter limits the load on backend services at the cost of slightly stale data. </p>
</li>
</ul>
</li>
<li><p>This design differs from an approach in which a client rehashes keys among the remaining memcached servers. </p>
<ul>
<li>The server that becomes responsible for this hot key might also become overloaded. By shunting load to idle servers we limit that risk. </li>
</ul>
</li>
</ol>
<h2 id="In-a-region-replication"><a href="#In-a-region-replication" class="headerlink" title="In a region: replication"></a>In a region: replication</h2><h3 id="What-is-the-problem-of-scaling-memcache"><a href="#What-is-the-problem-of-scaling-memcache" class="headerlink" title="What is the problem of scaling memcache?"></a>What is the problem of scaling memcache?</h3><ol>
<li><p>In communication traffic:</p>
<ul>
<li><p>Highly requested items will only become more popular as more web servers are added to cope with increased user traffic. </p>
</li>
<li><p>Incast congestion also worsens as the number of memcached servers increases.</p>
</li>
<li><p>Split web and memcached servers into multiple <em>frontend</em> clusters. </p>
</li>
<li>These clusters, along with a storage cluster that contain the databases, define a region. </li>
</ul>
</li>
<li><p>The number of invalidations will increase</p>
<ul>
<li>The storage cluster is responsible for invalidating cached data to keep frontend clusters consistent with the authoritative versions. </li>
<li>As an optimization, a web server that modifies data also sends invalidations to its own cluster to provide read-after-write semantics for a single user request and reduce the amount of time stale data is present in its local cache. </li>
<li>Also batch invalidations can reduce packet rates. </li>
</ul>
</li>
<li><p>If users’ requests are randomly routed to all available frontend clusters then the cached data will be roughly the same across all the frontend clusters. </p>
<ul>
<li><p>Over-replicating the data can be memory inefficient, especially for large, rarely accessed items. </p>
</li>
<li><p>Reduce the number of replicas by having multiple frontend clusters share the same set of memcached servers which is call a regional pool. </p>
</li>
</ul>
</li>
</ol>
<h3 id="How-to-mitigate-the-poor-hit-rates-when-a-cold-cluster-starts-up"><a href="#How-to-mitigate-the-poor-hit-rates-when-a-cold-cluster-starts-up" class="headerlink" title="How to mitigate the poor hit rates when a cold cluster starts up?"></a>How to mitigate the poor hit rates when a cold cluster starts up?</h3><ol>
<li>When we bring a new cluster online, an existing one fails, or perform scheduled maintenance the caches will have very poor hit rates diminishing the ability to insulate backend services. </li>
<li>A system called Cold Cluster Warmup mitigates this by allowing clients in the “cold cluster” (i.e. the frontend cluster that has an empty cache) to retrieve data from the “warm cluster” (i.e. a cluster that has caches with normal hit rates) rather than the persistent storage. </li>
<li>With this system cold clusters can be brought back to full capacity in a few hours instead of a few days. </li>
<li>The cold cluster warmup is turned off once the cold cluster’s hit rate stabilizes and the benefits diminish. </li>
</ol>
<h3 id="How-to-solve-the-race-in-cold-cluster-warmup-caused-by-update-in-cold-cluster"><a href="#How-to-solve-the-race-in-cold-cluster-warmup-caused-by-update-in-cold-cluster" class="headerlink" title="How to solve the race in cold cluster warmup caused by update in cold cluster?"></a>How to solve the race in cold cluster warmup caused by update in cold cluster?</h3><ol>
<li>If a client in the cold cluster does a database update, and a subsequent request from another client retrieves the stale value from the warm cluster before the warm cluster has received the invalidation, that item will be indefinitely inconsistent in the cold cluster. </li>
<li>Memcached deletes support nonzero hold-off times that reject add operations for the specified hold-off time. <ul>
<li>By default, all deletes to the cold cluster are issued with a two second hold-off. </li>
<li>When a miss is detected in the cold cluster, the client re-requests the key from the warm cluster and adds it into the cold cluster. </li>
<li>The failure of the add indicates that newer data is available on the database and thus the client will refetch the value from the databases. </li>
</ul>
</li>
</ol>
<h2 id="Across-regions-consistency"><a href="#Across-regions-consistency" class="headerlink" title="Across regions: consistency"></a>Across regions: consistency</h2><h3 id="How-to-execute-a-write-operation"><a href="#How-to-execute-a-write-operation" class="headerlink" title="How to execute a write operation?"></a>How to execute a write operation?</h3><ol>
<li><p>Of many regions, one region is designated to hold the master databases and the other regions to contain read-only replicas. </p>
</li>
<li><p>For writes from a master region:</p>
<ul>
<li><p>The storage cluster of each region will send invalidations after they have replicated data. </p>
</li>
<li><p>It avoids a race condition in which an invalidation arrives before the data has been replicated from the master region. </p>
</li>
</ul>
</li>
<li><p>For writes from a non-master region: </p>
<ul>
<li>The user’s next request could result in confusion if his recent change is missing. </li>
<li>Employ a remote marker mechanism to minimize the probability of reading stale data. <ul>
<li>The presence of the marker indicates that data in the local replica database are potentially stale and the query should be redirected to the master region. </li>
</ul>
</li>
<li>When a web server wishes to update data that affects a key $k$, that server<ul>
<li>sets a remote marker $r_k$ in the region</li>
<li>performs the write to the master embedding $k$ and $r_k$ to be invalidated in the SQL statement</li>
<li>deletes $k$ in the local cluster</li>
</ul>
</li>
<li>On a subsequent request for $k$, <ul>
<li>A web server will be unable to find the cached data</li>
<li>Check whether $r_k$ exists</li>
<li>Direct its query to the master or local region depending on the presence of $r_k$. </li>
</ul>
</li>
<li>The remote markers are implemented by using a regional pool. </li>
<li>This mechanism may reveal stale information during concurrent modifications to the same key as one operation may delete a remote marker that should remain present for another in-flight operation. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Spark/" class="post-title-link" itemprop="url">Spark</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:50:52" itemprop="dateCreated datePublished" datetime="2023-09-26T13:50:52+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:31:31" itemprop="dateModified" datetime="2023-10-04T16:31:31+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/zaharia-spark.pdf">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li><p>Contribution:</p>
<ul>
<li>Present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. </li>
<li>RDDs provide an interface based on coarse-grained transformations (e.g., map, filter and join) that apply the same operation to many data items. </li>
</ul>
</li>
<li><p>Issues:</p>
<ul>
<li>Two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. </li>
<li><p>Current frameworks lack abstractions for leveraging distributed memory. </p>
<ul>
<li>In most current frameworks, the only way to reuse data between computations is to write it to an external stable storage system. </li>
<li>This makes them inefficient for those that reuse intermediate results across multiple computations. </li>
<li>This incurs substantial overheads due to data replication, disk I/O, and serialization. </li>
</ul>
</li>
<li><p>There are some specialized frameworks developed for some applications that require data reuse. But they do not provide abstractions for more general reuse. </p>
</li>
<li>Both replicatint the data across machines or logging updates across machines are expensive for data-intensive workloads. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="RDDs"><a href="#RDDs" class="headerlink" title="RDDs"></a>RDDs</h2><h3 id="What-is-RDD"><a href="#What-is-RDD" class="headerlink" title="What is RDD?"></a>What is RDD?</h3><ol>
<li>An RDD is a read-only, partitioned collection of records. <ul>
<li>Although individual RDDs are immutable, it is possible to implement mutable state by having multiple RDDs to represent multiple versions of a dataset. </li>
</ul>
</li>
<li>RDDs can only be created through deterministic operations on either data in stable storage or other RDDs. <ul>
<li>These operations are called transformations to differentiate them from other operations on RDDs. </li>
</ul>
</li>
<li>An RDD has enough information about how it was derived from other datasets (its lineage) to compute its partitions from data in stable storage. <ul>
<li>A program cannot reference an RDD that it cannot reconstruct after a failure. </li>
<li>RDDs do not need to be materialized at all times. </li>
</ul>
</li>
<li>Users can control two other aspects of RDDs: persistence and partitioning. <ul>
<li>Users can indicate which RDDs they will reuse and choose a storage strategy for them. </li>
<li>They can also ask that an RDD’s elements be partitioned across machines based on a key in each record. </li>
</ul>
</li>
</ol>
<h3 id="What-is-the-advantges-of-the-RDD-model-comparing-with-distributed-shared-memory-DSM"><a href="#What-is-the-advantges-of-the-RDD-model-comparing-with-distributed-shared-memory-DSM" class="headerlink" title="What is the advantges of the RDD model comparing with distributed shared memory (DSM)?"></a>What is the advantges of the RDD model comparing with distributed shared memory (DSM)?</h3><ol>
<li><p>The main difference between RDDs and DSM is that RDDs can only be created (“written”) through coarsegrained transformations, while DSM allows reads and writes to each memory location. </p>
<ul>
<li>This restricts RDDs to applications that perform bulk writes, but allows for more efficient fault tolerance.</li>
<li>RDDs do not need to incur the overhead of checkpointing, as they can be recovered using lineage. </li>
<li>Only the lost partitions of an RDD need to be recomputed upon failure, and they can be recomputed in parallel on different nodes, without having to roll back the whole program. </li>
</ul>
</li>
<li><p>RDDs’ immutable nature lets a system mitigate slow nodes (stragglers) by running backup copies of slow tasks as in MapReduce. </p>
<ul>
<li>Backup tasks would be hard to implement with DSM, as the two copies of a task would access the same memory locations and interfere with each other’s updates. </li>
</ul>
</li>
<li><p>In bulk operations on RDDs, a runtime can schedule tasks based on data locality to improve performance. </p>
</li>
<li><p>RDDs degrade gracefully when there is not enough memory to store them, as long as they are only being used in scan-based operations. </p>
<p>Partitions that do not fit in RAM can be stored on disk and will provide similar performance to current data-parallel systems. </p>
</li>
</ol>
<h3 id="What-kind-of-applications-are-suited-for-RDDs"><a href="#What-kind-of-applications-are-suited-for-RDDs" class="headerlink" title="What kind of applications are suited for RDDs?"></a>What kind of applications are suited for RDDs?</h3><ol>
<li>RDDs are best suited for batch applications that apply the same operation to all elements of a dataset. <ul>
<li>RDDs can efficiently remember each transformation as one step in a lineage graph and can recover lost partitions without having to log large amounts of data. </li>
</ul>
</li>
<li>RDDs would be less suitable for applications that make asynchronous finegrained updates to shared state. </li>
</ol>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="What-is-the-runtime-of-Spark"><a href="#What-is-the-runtime-of-Spark" class="headerlink" title="What is the runtime of Spark?"></a>What is the runtime of Spark?</h3><ol>
<li>Developers write a <strong>driver</strong> program that connects to a cluster of <strong>workers</strong>. <ul>
<li>The driver defines one or more RDDs and invokes actions on them. </li>
<li>Spark code on the driver also tracks the RDDs’ lineage. </li>
</ul>
</li>
<li>The workers are long-lived processes that can store RDD partitions in RAM across operations. </li>
</ol>
<p><img src="/imgs/Distributed/Spark/runtime.png" width="30%"></p>
<h3 id="What-is-Spark-programming-interface-of-RDDs"><a href="#What-is-Spark-programming-interface-of-RDDs" class="headerlink" title="What is Spark programming interface of RDDs?"></a>What is Spark programming interface of RDDs?</h3><ol>
<li>Each dataset is represented as an object and transformations are invoked using methods on these objects. </li>
<li>Programmers start by defining one or more RDDs through transformations on data in stable storage. <ul>
<li>They can then use these RDDs in actions, which are operations that return a value to the application or export data to a storage system. </li>
</ul>
</li>
<li>Programmers can call a <code>persist()</code> method to indicate which RDDs they want to reuse in future operations. <ul>
<li>Spark keeps persistent RDDs in memory by default, but it can spill them to disk if there is not enough RAM. </li>
<li>Users can set a persistence priority on each RDD to specify which in-memory data should spill to disk first. </li>
<li>The user can call <code>persist</code> with a <code>RELIABLE</code> flag to reliably replicate some of the versions of RDDs to reduce fault recovery times. </li>
</ul>
</li>
</ol>
<h3 id="What-RDD-operations-are-supported-in-Spark"><a href="#What-RDD-operations-are-supported-in-Spark" class="headerlink" title="What RDD operations are supported in Spark?"></a>What RDD operations are supported in Spark?</h3><ol>
<li>Users provide arguments to RDD operations by passing closures (function literals). </li>
<li>RDDs themselves are statically typed objects parametrized by an element type. </li>
<li>Transformations are <strong>lazy</strong> operations that define a new RDD. <ul>
<li>$map(f: T\Rightarrow U): RDD[T]\Rightarrow RDD[U]$</li>
<li>$filter(f: T\Rightarrow Bool): RDD[T]\Rightarrow RDD[T]$</li>
<li>$flatMap(f: T\Rightarrow Seq[U]): RDD[T]\Rightarrow RDD[U]$</li>
<li>$sample(fraction: Float): RDD[T]\Rightarrow RDD[T]$ (Deterministic sampling)</li>
<li>$groupByKey(): RDD[(K,V)]\Rightarrow RDD[(K,Seq[V])]$</li>
<li>$reduceByKey(f:(V,V)\Rightarrow V): RDD[(K,V)]\Rightarrow RDD[(K,V)]$</li>
<li>$union(): (RDD[T],RDD[T])\Rightarrow RDD[T]$</li>
<li>$join():(RDD[(K,V)],RDD[(K,W)])\Rightarrow RDD[(K,(V,W))]$</li>
<li>$cogroup():(RDD[(K,V)],RDD[(K,W)])\Rightarrow RDD[(K,(Seq[V],Seq[W]))]$</li>
<li>$crossProduct():(RDD[T],RDD[U])\Rightarrow RDD[(T,U)]$</li>
<li>$mapValues(f:V\Rightarrow W):RDD[(K,V)]\Rightarrow RDD[(K,W)]$ (Preserves partitioning)</li>
<li>$sort(c:Comparator[K]):RDD[(K,V)]\Rightarrow RDD[(K,V)]$</li>
<li>$partitionBy(p:Partitioner[K]):RDD[(K,V)]\Rightarrow RDD[(K,V)]$</li>
</ul>
</li>
<li>Actions launch a computation to return a value to the program or write data to external storage. <ul>
<li>$count(): RDD[T]\Rightarrow Long:$ Returns the number of elements in the dataset</li>
<li>$collect(): RDD[T]\Rightarrow Seq[T]:$ Returns the elements themselves</li>
<li>$reduce(f:(T,T)\Rightarrow T): RDD[T]\Rightarrow T$</li>
<li>$lookup(k:K):RDD[(K,V)]\Rightarrow Seq[V]$ (On hash/range partitioned RDDs)</li>
<li>$save(path:String):$ Outputs RDD to a storage system</li>
</ul>
</li>
</ol>
<h3 id="What-is-the-common-interface-of-each-RDD"><a href="#What-is-the-common-interface-of-each-RDD" class="headerlink" title="What is the common interface of each RDD?"></a>What is the common interface of each RDD?</h3><ol>
<li>A set of partitions, which are atomic pieces of the dataset<ul>
<li>$partitioner():$ Return metadata specifying whether the RDD is hash/range partitioned</li>
</ul>
</li>
<li>A set of dependencies on parent RDDs<ul>
<li>$dependencies():$ Return a list of dependencies</li>
</ul>
</li>
<li>A function for computing the dataset based on its parents<ul>
<li>$iterator(p, parentIters):$ Compute the elements of partition p given iterators for its parent partitions</li>
</ul>
</li>
<li>Metadata about its partitioning scheme and data placement<ul>
<li>$partitions():$ Return a list of Partition objects</li>
<li>$preferredLocations(p):$ List nodes where partition $p$ can be accessed faster due to data locality</li>
</ul>
</li>
</ol>
<h3 id="What-is-the-interface-of-representing-dependencies-between-RDDs"><a href="#What-is-the-interface-of-representing-dependencies-between-RDDs" class="headerlink" title="What is the interface of representing dependencies between RDDs?"></a>What is the interface of representing dependencies between RDDs?</h3><ol>
<li><p>Classify dependencies into two types: </p>
<ul>
<li>Narrow dependencies: each partition of the parent RDD is used by at most one partition of the child RDD. </li>
<li>Wide dependencies: multiple child partitions may depend on it. </li>
</ul>
</li>
<li><p>Narrow dependencies allow for pipelined execution on one cluster node, which can compute all the parent partitions. </p>
<p>Wide dependencies require data from all parent partitions to be available and to be shuffled across the nodes using a MapReducelike operation. </p>
</li>
<li><p>Recovery after a node failure is more efficient with a narrow dependency, as only the lost parent partitions need to be recomputed, and they can be recomputed in parallel on different nodes. </p>
<p>In a lineage graph with wide dependencies, a single failed node might cause the loss of some partition from all the ancestors of an RDD, requiring a complete re-execution. </p>
</li>
</ol>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="How-does-Spark-schedule-computations"><a href="#How-does-Spark-schedule-computations" class="headerlink" title="How does Spark schedule computations?"></a>How does Spark schedule computations?</h3><ol>
<li><p>Whenever a user runs an action on an RDD, the scheduler examines that RDD’s lineage graph to build a DAG of stages to execute. </p>
<ul>
<li><p>Each stage contains as many pipelined transformations with narrow dependencies as possible. </p>
</li>
<li><p>The boundaries of the stages are the shuffle operations required for wide dependencies, or any already computed partitions that can short-circuit the computation of a parent RDD. </p>
</li>
</ul>
</li>
<li><p>The scheduler then launches tasks to compute missing partitions from each stage until it has computed the target RDD. </p>
<ul>
<li>The scheduler assigns tasks to machines based on data locality using delay scheduling. </li>
<li>If a task needs to process a partition that is available in memory on a node, we send it to that node. </li>
<li>Otherwise, if a task processes a partition for which the containing RDD provides preferred locations (e.g., an HDFS file), we send it to those. </li>
</ul>
</li>
</ol>
<h3 id="How-does-Spark-handle-task-failures"><a href="#How-does-Spark-handle-task-failures" class="headerlink" title="How does Spark handle task failures?"></a>How does Spark handle task failures?</h3><ol>
<li><p>For wide dependencies, it is possible that those alive workers have long passed the wide-dependent points and already discard intermediate results causing the stage become unavailable. In that case, all the dependencies need to be re-calculated, which is costly. </p>
</li>
<li><p>Hence, intermediate records are materialized on the nodes holding parent partitions to simplify fault recovery. </p>
<ul>
<li><p>If a task fails, it will be re-run on another node as long as its stage’s parents are still available. </p>
</li>
<li><p>If some stages have become unavailable, limited number of tasks to compute the missing partitions are resubmitted in parallel. </p>
</li>
</ul>
</li>
</ol>
<h3 id="How-does-Spark-manage-storage-of-persistent-RDDs"><a href="#How-does-Spark-manage-storage-of-persistent-RDDs" class="headerlink" title="How does Spark manage storage of persistent RDDs?"></a>How does Spark manage storage of persistent RDDs?</h3><ol>
<li>In-memory storage as deserialized Java objects<ul>
<li>It provides the fastest performance, because the Java VM can access each RDD element natively. </li>
</ul>
</li>
<li>In-memory storage as serialized data<ul>
<li>It lets users choose a more memory-efficient representation than Java object graphs when space is limited, at the cost of lower performance. </li>
</ul>
</li>
<li>On-disk storage<ul>
<li>It is useful for RDDs that are too large to keep in RAM but costly to recompute on each use. </li>
</ul>
</li>
</ol>
<h3 id="How-does-Spark-evict-RDDs-when-run-out-of-memory"><a href="#How-does-Spark-evict-RDDs-when-run-out-of-memory" class="headerlink" title="How does Spark evict RDDs when run out of memory?"></a>How does Spark evict RDDs when run out of memory?</h3><ol>
<li>LRU eviction policy is used at the level of RDDs. </li>
<li>Unless the LRU RDD is the same RDD as the one with the new partition. <ul>
<li>The old partition is kept in memory to prevent cycling partitions from the same RDD in and out. </li>
<li>This is important because most operations will run tasks over an entire RDD, so it is quite likely that the partition already in memory will be needed in the future. </li>
</ul>
</li>
</ol>
<h3 id="When-will-checkpointing-be-useful"><a href="#When-will-checkpointing-be-useful" class="headerlink" title="When will checkpointing be useful?"></a>When will checkpointing be useful?</h3><ol>
<li>Checkpointing is useful for RDDs with long lineage graphs containing wide dependencies. <ul>
<li>A node failure in the cluster may result in the loss of some slice of data from each parent RDD, requiring a full recomputation. </li>
</ul>
</li>
<li>For RDDs with narrow dependencies on data in stable storage, checkpointing may never be worthwhile. <ul>
<li>If a node fails, lost partitions from these RDDs can be recomputed in parallel on other nodes, at a fraction of the cost of replicating the whole RDD. </li>
</ul>
</li>
<li>The read-only nature of RDDs makes  them simpler to checkpoint than general shared memory. <ul>
<li>Consistency is not a concern, RDDs can be written out in the background without requiring program pauses or distributed snapshot schemes. </li>
</ul>
</li>
</ol>
<h1 id="Experiement"><a href="#Experiement" class="headerlink" title="Experiement"></a>Experiement</h1><ol>
<li><p>The main contribution of this paper is improving the performance of iterative applications, hence the author measured the speedup of Spark against Hadoop. </p>
<ul>
<li><p>One scene of iterative applications is machine learning applications. The author choosed k-means and logistic regression to measure performance. </p>
<ul>
<li>The iteration time of k-means is dominated by computation. </li>
<li>Logistic regression is less compute-intensive and thus more sensitive to time spent in deserialization and I/O. </li>
<li>The time consumed should be reported for the first iteration and subsequent iterations separately as following: </li>
</ul>
<p><img src="/imgs/Distributed/Spark/ml-base.png" width="50%"></p>
</li>
<li><p>Another scene is PageRank algorithm. </p>
</li>
</ul>
</li>
<li><p>Hadoop ran slower due to several factors</p>
<ul>
<li>Minimum overhead of the Hadoop software stack<ul>
<li>It can be measured by running no-op Hadoop jobs. </li>
</ul>
</li>
<li>Overhead of HDFS while serving data<ul>
<li>HDFS performed multiple memory copies and a checksum to serve each block. </li>
<li>It can be seen by comparing the time of accessing in-memory HDFS and local file. </li>
</ul>
</li>
<li>Deserialization cost to convert binary records to usable in-memory Java objects<ul>
<li>It can be seen by comparing the time of access text input and binary input. </li>
</ul>
</li>
</ul>
<p><img src="/imgs/Distributed/Spark/hadoop.png" width=50%></p>
</li>
<li><p>The scalability is another concerned metrics. The result of machine learning algorithm is as shown below:</p>
<p><img src="/imgs/Distributed/Spark/ml-scale.png" width="50%"></p>
</li>
<li><p>Other important metric of the systems is failure fault recovery. </p>
<ul>
<li>The author measured the time consumed in each iteration while a node failed at the start of 6th iteration. </li>
</ul>
<p><img src="/imgs/Distributed/Spark/fault-recovery.png" width="50%"></p>
</li>
<li><p>Given that Spark makes the assumption that most RDDs will be stored in memory, the author also measured the performance when memory is limited. </p>
<ul>
<li>The performance degrades gracefully with less space.</li>
</ul>
<p><img src="/imgs/Distributed/Spark/mem-lim.png" width=50%></p>
</li>
<li><p>Given that Spark provides an interactive interpreter, the author also measured the response time thes of those queries. </p>
</li>
<li><p>To prove that RDD can support a wide class of applications, the author showed how to use RDDs to express exists programming models (e.g. MapReduce, SQL, Pregel)</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Spanner/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Spanner/" class="post-title-link" itemprop="url">Spanner</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:47:51" itemprop="dateCreated datePublished" datetime="2023-09-26T13:47:51+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:31:17" itemprop="dateModified" datetime="2023-10-04T16:31:17+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/spanner.pdf">Spanner: Google’s Globally-Distributed Database</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li><p>Spanner’s main focus is managing cross-datacenter replicated data. </p>
</li>
<li><p>Problems of Bigtable:</p>
<ul>
<li><p>Bigtable can be difficult to use for those that have complex, evolving schemas, or those that want strong consistency in the presence of wide-area replication. </p>
</li>
<li><p>Many applications at Google have chosen to use Megastore because of its semi-relational data model and support for synchronous replication, despite its relatively poor write throughput. </p>
</li>
</ul>
</li>
<li><p>Spanner new features:</p>
<ul>
<li><p>Spanner has evolved from a Bigtable-like versioned key-value store into a temporal multi-version database. </p>
</li>
<li><p>Data is stored in schematized semi-relational tables</p>
<ul>
<li>Data is versioned, and each version is automatically timestamped with its commit time</li>
<li>Old versions of data are subject to configurable garbage-collection policies</li>
<li>Applications can read data at old timestamps. </li>
</ul>
</li>
</ul>
</li>
<li><p>Spanner assigns globally-meaningful commit timestamps to transactions. </p>
<ul>
<li>If a transaction $T_1$ commits before another transaction $T_2$ starts, then $T_1$’s commit timestamp is smaller than $T_2$’s. </li>
<li>Spanner provides externally consistent reads and writes, and globally-consistent reads across the database at a timestamp.</li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Structure-of-implementation"><a href="#Structure-of-implementation" class="headerlink" title="Structure of implementation"></a>Structure of implementation</h2><h3 id="What-is-the-overview-of-Spanner-server-organization"><a href="#What-is-the-overview-of-Spanner-server-organization" class="headerlink" title="What is the overview of Spanner server organization?"></a>What is the overview of Spanner server organization?</h3><ol>
<li>A Spanner deployment is called a universe. </li>
<li>Spanner is organized as a set of zones. <ul>
<li>A zone has one zonemaster and between one hundred and several thousand spanservers. </li>
<li>The zonemaster assigns data to spanservers; the spanservers serve data to clients. </li>
<li>The per-zone location proxies are used by clients to locate the spanservers assigned to serve their data. </li>
<li>The per-zone location proxies are used by clients to locate the spanservers assigned to serve their data. </li>
</ul>
</li>
<li>The universe master is primarily a console that displays status information about all the zones for interactive debugging. </li>
<li>The placement driver handles automated movement of data across zones on the timescale of minutes. <ul>
<li>The placement driver periodically communicates with the spanservers to find data that needs to be moved, either to meet updated replication constraints or to balance load. </li>
</ul>
</li>
<li>The universe master and the placement driver are currently singletons. </li>
</ol>
<p><img src="/imgs/Distributed/Spanner/structure.png" width="50%"></p>
<h3 id="What-is-the-zones-in-Spanner"><a href="#What-is-the-zones-in-Spanner" class="headerlink" title="What is the zones in Spanner?"></a>What is the zones in Spanner?</h3><ol>
<li>Each zone is the rough analog of a deployment of Bigtable servers. </li>
<li>Zones are the unit of administrative deployment. The set of zones is also the set of locations across which data can be replicated. </li>
<li>Zones can be added to or removed from a running system as new datacenters are brought into service and old ones are turned off, respectively. </li>
<li>Zones are also the unit of physical isolation. There may be one or more zones in a datacenter. </li>
</ol>
<h3 id="Software-stack"><a href="#Software-stack" class="headerlink" title="Software stack"></a>Software stack</h3><p><img src="/imgs/Distributed/Spanner/software.png" width=50%></p>
<h4 id="How-does-Spanner-manage-key-value-pairs"><a href="#How-does-Spanner-manage-key-value-pairs" class="headerlink" title="How does Spanner manage key-value pairs?"></a>How does Spanner manage key-value pairs?</h4><ol>
<li>At the bottom, each spanserver is responsible for between 100 and 1000 instances of a data structure called a tablet. </li>
<li>A tablet is similar to Bigtable’s tablet abstraction, in that it implements a bag of the following mappings: <code>(key: string, timestamp: int64) → string</code></li>
<li>Unlike Bigtable, Spanner assigns timestamps to data, which is more like a multi-version database than a key-value store. </li>
<li>A tablet’s state is stored in set of B-tree-like files and a write-ahead log, all on a distributed file system called Colossus. </li>
</ol>
<h4 id="How-does-Spanner-support-replication"><a href="#How-does-Spanner-support-replication" class="headerlink" title="How does Spanner support replication?"></a>How does Spanner support replication?</h4><ol>
<li>Each spanserver implements a single Paxos state machine on top of each tablet. <ul>
<li>Its Paxos implementation supports long-lived leaders with time-based leader leases, whose length defaults to 10 seconds. </li>
<li>The implementation logs every Paxos write twice: once in the tablet’s log, and once in the Paxos log. (made out of expediency, to be remedied eventually)</li>
<li>The implementation of Paxos is pipelined, so as to improve Spanner’s throughput in the presence of WAN latencies; but writes are applied by Paxos in order. </li>
</ul>
</li>
<li>The Paxos state machines are used to implement a consistently replicated bag of mappings. <ul>
<li>Each state machine stores its metadata and log in its corresponding tablet. </li>
<li>Writes must initiate the Paxos protocol at the leader; reads access state directly from the underlying tablet at any replica that is sufficiently up-to-date. </li>
</ul>
</li>
</ol>
<h4 id="How-does-Spanner-manage-lock-table"><a href="#How-does-Spanner-manage-lock-table" class="headerlink" title="How does Spanner manage lock table?"></a>How does Spanner manage lock table?</h4><ol>
<li>At every replica that is a leader, each spanserver implements a lock table to implement concurrency control. </li>
<li>The lock table contains the state for two-phase locking: it maps ranges of keys to lock states. </li>
<li>Having a long-lived Paxos leader is critical to efficiently managing the lock table.</li>
</ol>
<h4 id="How-does-Spanner-manage-transactions"><a href="#How-does-Spanner-manage-transactions" class="headerlink" title="How does Spanner manage transactions?"></a>How does Spanner manage transactions?</h4><ol>
<li><p>At every replica that is a leader, each spanserver also implements a transaction manager to support distributed transactions. </p>
<ul>
<li>The transaction manager is used to implement a <strong>participant leader</strong>; the other replicas in the group will be referred to as <strong>participant slaves</strong>. </li>
</ul>
</li>
<li><p>If a transaction involves only one Paxos group, it can bypass the transaction manager, since the lock table and Paxos together provide transactionality. </p>
</li>
<li><p>If a transaction involves more than one Paxos group, those groups’ leaders coordinate to perform twophase commit. </p>
<ul>
<li><p>One of the participant groups is chosen as the coordinator: the participant leader of that group will be referred to as the <strong>coordinator leader</strong>, and the slaves of that group as <strong>coordinator slaves</strong>. </p>
</li>
<li><p>The state of each transaction manager is stored in the underlying Paxos group. </p>
</li>
</ul>
</li>
</ol>
<h3 id="Data-management"><a href="#Data-management" class="headerlink" title="Data management"></a>Data management</h3><h4 id="How-does-Spanner-manage-data-placement"><a href="#How-does-Spanner-manage-data-placement" class="headerlink" title="How does Spanner manage data placement?"></a>How does Spanner manage data placement?</h4><ol>
<li>A <strong>directory</strong> is a set of contiguous keys that share a common prefix. <ul>
<li>A directory is the unit of data placement. All data in a directory has the same replication configuration. </li>
<li>Directories can be moved while client operations are ongoing. </li>
<li>A Paxos group may contain multiple directories, i.e. a Spanner tablet is a container that may encapsulate multiple partitions of the row space. </li>
</ul>
</li>
<li>Spanner will shard a directory into multiple fragments if it grows too large. <ul>
<li>Fragments may be served from different Paxos groups. </li>
<li>Movedir actually moves fragments, and not whole directories, between groups. </li>
</ul>
</li>
</ol>
<h4 id="When-would-Spanner-move-a-directory"><a href="#When-would-Spanner-move-a-directory" class="headerlink" title="When would Spanner move a directory?"></a>When would Spanner move a directory?</h4><ol>
<li>To shed load from a Paxos group. </li>
<li>To put directories that are frequently accessed together into the same group. </li>
<li>To move a directory into a group that is closer to its accessors. </li>
</ol>
<h4 id="How-does-Spanner-move-a-directory"><a href="#How-does-Spanner-move-a-directory" class="headerlink" title="How does Spanner move a directory?"></a>How does Spanner move a directory?</h4><ol>
<li>Movedir is not implemented as a single transaction, so as to avoid blocking ongoing reads and writes on a bulky data move. </li>
<li>Instead, movedir registers the fact that it is starting to move data and moves the data in the background. </li>
<li>When it has moved all but a nominal amount of the data, it uses a transaction to atomically move that nominal amount and update the metadata for the two Paxos groups. </li>
</ol>
<h4 id="How-does-applications-specify-placement-of-data"><a href="#How-does-applications-specify-placement-of-data" class="headerlink" title="How does applications specify placement of data?"></a>How does applications specify placement of data?</h4><ol>
<li>A directory is also the smallest unit whose geographicreplication properties can be specified by an application. </li>
<li>Administrators control two dimensions: the number and types of replicas, and the geographic placement of those replicas. <ul>
<li>They create a menu of named options in these two dimensions. </li>
</ul>
</li>
<li>An application controls how data is replicated, by tagging each database and/or individual directories with a combination of those options. </li>
</ol>
<h4 id="What-is-the-data-model-of-Spanner"><a href="#What-is-the-data-model-of-Spanner" class="headerlink" title="What is the data model of Spanner?"></a>What is the data model of Spanner?</h4><ol>
<li><p>Spanner exposes the following set of data features to applications</p>
<ul>
<li>A data model based on schematized semi-relational tables</li>
<li>A query language</li>
<li>Generalpurpose transactions</li>
</ul>
</li>
<li><p>The application data model is layered on top of the directory-bucketed key-value mappings supported by the implementation. </p>
<ul>
<li><p>An application creates one or more databases in a universe. </p>
</li>
<li><p>Each database can contain an unlimited number of schematized tables. </p>
</li>
<li><p>Tables look like relational-database tables, with rows, columns, and versioned values. </p>
</li>
</ul>
</li>
<li><p>Every table is required to have an ordered set of one or more primary-key columns. </p>
<ul>
<li>The primary keys form the name for a row, and each table defines a mapping from the primary-key columns to the non-primary-key columns. </li>
<li>A row has existence only if some value (even if it is NULL) is defined for the row’s keys. </li>
</ul>
</li>
<li><p>Every Spanner database must be partitioned by clients into one or more hierarchies of tables. </p>
<ul>
<li>Each row in a directory table with key K, together with all of the rows in descendant tables that start with K in lexicographic order, forms a directory. </li>
</ul>
</li>
</ol>
<h2 id="TrueTime-API"><a href="#TrueTime-API" class="headerlink" title="TrueTime API"></a>TrueTime API</h2><h3 id="How-does-TrueTime-API-represents-time"><a href="#How-does-TrueTime-API-represents-time" class="headerlink" title="How does TrueTime API represents time?"></a>How does TrueTime API represents time?</h3><ol>
<li><p>TrueTime explicitly represents time as a <code>TTinterval</code>, which is an interval with bounded time uncertainty. The endpoints of a TTinterval are of type <code>TTstamp</code>. </p>
</li>
<li><p>The <code>TT.now()</code> method returns a <code>TTinterval</code>: <code>[earliest, latest]</code> that is guaranteed to contain the absolute time during which <code>TT.now()</code> was invoked. </p>
<ul>
<li>Denote the absolute time of an event e by the function $t_{abs}(e)$. </li>
<li>TrueTime guarantees that for an invocation <code>tt = TT.now()</code>, $tt.earliest ≤ t_{abs}(e_{now}) ≤ tt.latest$, where $e_{now}$ is the invocation event. </li>
</ul>
</li>
<li><p><code>TT.after(t)</code> returns <code>true</code> if t has definitely passed. </p>
<p><code>TT.before(t)</code> returns <code>true</code> if t has definitely not arrived. </p>
</li>
</ol>
<h3 id="How-is-TrueTime-API-implemented"><a href="#How-is-TrueTime-API-implemented" class="headerlink" title="How is TrueTime API implemented?"></a>How is TrueTime API implemented?</h3><ol>
<li>TrueTime is implemented by a set of <strong>time master</strong> machines per datacenter and a <strong>timeslave daemon</strong> per machine. </li>
<li>The majority of masters have GPS receivers with dedicated antennas. <ul>
<li>These masters are separated physically to reduce the effects of antenna failures, radio interference, and spoofing. </li>
</ul>
</li>
<li>The remaining masters (which we refer to as <strong>Armageddon masters</strong>) are equipped with atomic clocks. </li>
</ol>
<h3 id="What-does-time-masters-need-to-do"><a href="#What-does-time-masters-need-to-do" class="headerlink" title="What does time masters need to do?"></a>What does time masters need to do?</h3><ol>
<li><p>All masters’ time references are regularly compared against each other. </p>
</li>
<li><p>Each master also cross-checks the rate at which its reference advances time against its own local clock, and evicts itself if there is substantial divergence. </p>
</li>
<li><p>Between synchronizations,</p>
<ul>
<li><p>Armageddon masters advertise a slowly increasing time uncertainty that is derived from conservatively applied worst-case clock drift.</p>
</li>
<li><p>GPS masters advertise uncertainty that is typically close to zero.</p>
</li>
</ul>
</li>
</ol>
<h3 id="What-does-daemons-need-to-do"><a href="#What-does-daemons-need-to-do" class="headerlink" title="What does daemons need to do?"></a>What does daemons need to do?</h3><ol>
<li>Every daemon polls a variety of masters to reduce vulnerability to errors from any one master. <ul>
<li>Some are GPS masters chosen from nearby datacenters</li>
<li>The rest are GPS masters from farther datacenters, as well as some Armageddon masters.</li>
</ul>
</li>
<li>Daemons apply a variant of Marzullo’s algorithm to detect and reject liars, and synchronize the local machine clocks to the non-liars. </li>
<li>To protect against broken local clocks, machines that exhibit frequency excursions larger than the worstcase bound derived from component specifications and operating environment are evicted. </li>
<li>Between synchronizations, <ul>
<li>A daemon advertises a slowly increasing time uncertainty. $\epsilon$ is derived from conservatively applied worst-case local clock drift. </li>
<li>$\epsilon$ also depends on time-master uncertainty and communication delay to the time masters. </li>
</ul>
</li>
</ol>
<h2 id="Concurrency-control"><a href="#Concurrency-control" class="headerlink" title="Concurrency control"></a>Concurrency control</h2><h3 id="How-does-Spanner-manage-Paxos-leader-leases"><a href="#How-does-Spanner-manage-Paxos-leader-leases" class="headerlink" title="How does Spanner manage Paxos leader leases?"></a>How does Spanner manage Paxos leader leases?</h3><ol>
<li>Spanner depends on the <strong>disjointness invariant</strong>: for each Paxos group, each Paxos leader’s lease interval is disjoint from every other leader’s. </li>
<li>The Spanner implementation permits a Paxos leader to abdicate by releasing its slaves from their lease votes. <ul>
<li>To preserve the disjointness invariant, Spanner constrains when abdication is permissible. </li>
<li>Define $s_{max}$ to be the maximum timestamp used by a leader. Before abdicating, a leader must wait until $TT.after(s_{max})$ is true. </li>
</ul>
</li>
</ol>
<h3 id="Read-write-transactions"><a href="#Read-write-transactions" class="headerlink" title="Read-write transactions"></a>Read-write transactions</h3><h4 id="How-does-Spanner-assign-timestamps-to-RW-transactions"><a href="#How-does-Spanner-assign-timestamps-to-RW-transactions" class="headerlink" title="How does Spanner assign timestamps to RW transactions?"></a>How does Spanner assign timestamps to RW transactions?</h4><ol>
<li>For a given transaction, Spanner assigns it the timestamp that Paxos assigns to the Paxos write that represents the transaction commit. </li>
<li><strong>Monotonicity invariant</strong>: within each Paxos group, Spanner assigns timestamps to Paxos writes in monotonically increasing order, even across leaders. <ul>
<li>This invariant is enforced across leaders by making use of the disjointness invariant: a leader must only assign timestamps within the interval of its leader lease. </li>
<li>Whenever a timestamp $s$ is assigned, $s_{max}$ is advanced to s to preserve disjointness. </li>
</ul>
</li>
</ol>
<h4 id="How-does-Spanner-enfore-external-consistency-invariant"><a href="#How-does-Spanner-enfore-external-consistency-invariant" class="headerlink" title="How does Spanner enfore external-consistency invariant?"></a>How does Spanner enfore external-consistency invariant?</h4><ol>
<li><p><strong>External-consistency invariant</strong>: if the start of a transaction $T_2$ occurs after the commit of a transaction $T_1$, then the commit timestamp of $T_2$ must be greater than the commit timestamp of $T_1$. </p>
</li>
<li><p>Define the start and commit events for a transaction $T_i$ by $e^{start}_i$ and $e^{commit}_i$ ; and the commit timestamp of a transaction $T_i$ by $s_i$. The invariant becomes $t_{abs}(e^{commit}_1 ) &lt; t_{abs}(e^{start}_2 ) \Rightarrow s_1 &lt; s_2$. </p>
</li>
<li><p>Define the arrival event of the commit request at the coordinator leader for a write $T_i$ to be $e^{server}_i$. </p>
</li>
<li><p>The protocol for executing transactions and assigning timestamps obeys two rules</p>
<ul>
<li><strong>Start</strong>: The coordinator leader for a write $T_i$ assigns a commit timestamp $s_i$ no less than the value of $TT.now().latest$, computed after $e^{server}_i$ . </li>
<li><strong>Commit Wait</strong>: The coordinator leader ensures that clients cannot see any data committed by $T_i$ until $TT.after(s_i)$ is true. <ul>
<li>Commit wait ensures that $s_i$ is less than the absolute commit time of $T_i$, or $s_i &lt; t_{abs}(e^{commit}_i )$. </li>
</ul>
</li>
</ul>
</li>
<li><p>Proof:</p>
<p><img src="/imgs/Distributed/Spanner/ec_invariant.png" width="50%"></p>
</li>
</ol>
<h4 id="How-does-Spanner-handle-reads-within-RW-transactions"><a href="#How-does-Spanner-handle-reads-within-RW-transactions" class="headerlink" title="How does Spanner handle reads within RW transactions?"></a>How does Spanner handle reads within RW transactions?</h4><ol>
<li>Reads in a transaction do not see the effects of the transaction’s writes. <ul>
<li>Writes that occur in a transaction are buffered at the client until commit. </li>
<li>A read returns the timestamps of any data read, and uncommitted writes have not yet been assigned timestamps. </li>
</ul>
</li>
<li>Reads within read-write transactions use wound-wait to avoid deadlocks. <ul>
<li>When Transaction $T_n$ requests a data item currently held by $T_k$, $T_n$ is allowed to wait only if it has a timestamp larger than that of $T_k$, otherwise $T_k$ is killed (i.e. $T_k$ is wounded by $T_n$). </li>
<li>This scheme allows the younger transaction requesting a lock to “wait” if the older transaction already holds a lock, but forces the younger one to be suspended (“wound”) if the older transaction requests a lock on an item already held by the younger one.</li>
</ul>
</li>
</ol>
<h4 id="What-is-the-client-drive-two-phase-commit"><a href="#What-is-the-client-drive-two-phase-commit" class="headerlink" title="What is the client drive two-phase commit?"></a>What is the client drive two-phase commit?</h4><ol>
<li>The client issues reads to the leader replica of the appropriate group, which acquires read locks and then reads the most recent data. </li>
<li>While a client transaction remains open, it sends keepalive messages to prevent participant leaders from timing out its transaction. </li>
<li>When a client has completed all reads and buffered all writes, it begins two-phase commit. <ul>
<li>The client chooses a coordinator group. </li>
<li>Then it sends a commit message to each participant’s leader with the identity of the coordinator and any buffered writes. </li>
</ul>
</li>
<li>Having the client drive two-phase commit avoids sending data twice across wide-area links. </li>
</ol>
<h4 id="How-to-perform-the-two-phase-commit"><a href="#How-to-perform-the-two-phase-commit" class="headerlink" title="How to perform the two-phase commit?"></a>How to perform the two-phase commit?</h4><ol>
<li>A non-coordinator-participant leader, <ul>
<li>It first acquires write locks. </li>
<li>It then chooses a prepare timestamp that must be larger than any timestamps it has assigned to previous transactions, and logs a prepare record through Paxos. </li>
<li>Each participant then notifies the coordinator of its prepare timestamp. </li>
</ul>
</li>
<li>The coordinator leader, <ul>
<li>It also first acquires write locks, but skips the prepare phase. </li>
<li>It chooses a timestamp for the entire transaction after hearing from all other participant leaders. </li>
<li>The commit timestamp s must be <ul>
<li>Greater or equal to all prepare timestamps, </li>
<li>Greater than $TT.now().latest$ at the time the coordinator received its commit message, </li>
<li>Greater than any timestamps the leader has assigned to previous transactions. </li>
<li>The coordinator leader logs a commit record through Paxos, or an abort if it timed out while waiting on the other participants. </li>
</ul>
</li>
</ul>
</li>
<li>Before allowing any coordinator replica to apply the commit record, the coordinator leader waits until $TT.after(s)$. <ul>
<li>This wait is typically overlapped with Paxos communication. </li>
</ul>
</li>
<li>After commit wait, the coordinator sends the commit timestamp to the client and all other participant leaders. </li>
<li>Each participant leader logs the transaction’s outcome through Paxos. All participants apply at the same timestamp and then release locks. </li>
</ol>
<h3 id="Read-only-transactions"><a href="#Read-only-transactions" class="headerlink" title="Read-only transactions"></a>Read-only transactions</h3><h4 id="What-kinds-of-RO-transactions-does-Spanner-provide"><a href="#What-kinds-of-RO-transactions-does-Spanner-provide" class="headerlink" title="What kinds of RO transactions does Spanner provide?"></a>What kinds of RO transactions does Spanner provide?</h4><ol>
<li>A read-only transaction is a kind of transaction that has the performance benefits of snapshot isolation. <ul>
<li>Reads in a read-only transaction execute at a system-chosen timestamp without locking, so that incoming writes are not blocked. </li>
</ul>
</li>
<li>A snapshot read is a read in the past that executes without locking. <ul>
<li>A client can specify a timestamp for a snapshot read. </li>
<li>Or they can provide an upper bound on the desired timestamp’s staleness and let Spanner choose a timestamp. </li>
</ul>
</li>
<li>For both read-only transactions and snapshot reads<ul>
<li>The execution of the reads in a read-only transaction can proceed on any replica that is sufficiently up-to-date. </li>
<li>Commit is inevitable once a timestamp has been chosen, unless the data at that timestamp has been garbagecollected. <ul>
<li>Clients can avoid buffering results inside a retry loop. </li>
<li>When a server fails, clients can internally continue the query on a different server by repeating the timestamp and the current read position.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="How-does-Spanner-server-reads-given-its-timestamp"><a href="#How-does-Spanner-server-reads-given-its-timestamp" class="headerlink" title="How does Spanner server reads given its timestamp?"></a>How does Spanner server reads given its timestamp?</h4><ol>
<li>Every replica tracks a value called safe time $t_{safe}$ which is the maximum timestamp at which a replica is up-to-date. </li>
<li>Each Paxos state machine has a safe time $t^{Paxos}_{safe}$. <ul>
<li>$t^{Paxos}_{safe}$ is the timestamp of the highest-applied Paxos write. </li>
<li>Because timestamps increase monotonically and writes are applied in order, writes will no longer occur at or below $t^{Paxos}_{safe}$ with respect to Paxos. </li>
</ul>
</li>
<li>Each transaction manager has a safe time $t^{TM}_{safe}$. <ul>
<li>$t^{TM}_{safe}$ is $\infty$ at a replica if there are zero prepared (but not committed) transactions, i.e. transactions in between the two phases of two-phase commit. </li>
<li>If there are any such transactions, then the state affected by those transactions is indeterminate. <ul>
<li>A participant replica does not know yet whether such transactions will commit. </li>
<li>The commit protocol ensures that every participant knows a lower bound on a prepared transaction’s timestamp. </li>
<li>Every participant leader (for a group $g$) for a transaction $T_i$ assigns a prepare timestamp $s^{prepare}_{i,g}$ to its prepare record. </li>
<li>The coordinator leader ensures that the transaction’s commit timestamp $s_i ≥ s^{prepare}_{i,g}$ over all participant groups $g$. </li>
<li>For every replica in a group $g$, over all transactions $T_i$ prepared at $g$, $t^{TM}_{safe} = min_i(s^{prepare}_{i,g}) − 1$ over all transactions prepared at $g$.</li>
</ul>
</li>
</ul>
</li>
<li>Define $t_{safe} = min(t^{Paxos}_{safe} , t^{TM}_{safe})$. A replica can satisfy a read at a timestamp $t$ if $t ≤ t_{safe}$. </li>
</ol>
<h4 id="How-to-optimize-for-the-safe-read-timestamp"><a href="#How-to-optimize-for-the-safe-read-timestamp" class="headerlink" title="How to optimize for the safe read timestamp?"></a>How to optimize for the safe read timestamp?</h4><ol>
<li><p>No reads can occur at timestamps later than $t^{TM}_{safe}$, even if the reads do not conflict with the transaction. </p>
<ul>
<li><p>Can be removed by augmenting $t^{TM}_{safe}$ with a fine-grained mapping from key ranges to preparedtransaction timestamps. </p>
</li>
<li><p>This information can be stored in the lock table, which already maps key ranges to lock metadata. </p>
</li>
<li><p>When a read arrives, it only needs to be checked against the fine-grained safe time for key ranges with which the read conflicts. </p>
</li>
</ul>
</li>
<li><p>$t^{Paxos}_{safe}$ cannot advance in the absence of Paxos writes. A snapshot read at $t$ cannot execute at Paxos groups whose last write happened before $t$. </p>
<ul>
<li>Spanner addresses this problem by taking advantage of the disjointness of leader-lease intervals. </li>
<li>Each Paxos leader advances $t^{Paxos}_{safe}$ by keeping a threshold above which future writes’ timestamps will occur. <ul>
<li>It maintains a mapping $MinNextTS(n)$ from Paxos sequence number $n$ to the minimum timestamp that may be assigned to Paxos sequence number $n + 1$. </li>
<li>A replica can advance $t^{Paxos}_{safe}$ to $MinNextTS(n) − 1$ when it has applied through $n$.  </li>
</ul>
</li>
<li>A single leader can enforce its $MinNextTS()$ promises easily. <ul>
<li>The timestamps promised by $MinNextTS()$ lie within a leader’s lease, the disjointness invariant enforces $MinNextTS()$ promises across leaders. </li>
<li>If a leader wishes to advance MinNextTS() beyond the end of its leader lease, it must first extend its lease. </li>
</ul>
</li>
<li>$s_{max}$ is always advanced to the highest value in $MinNextTS()$ to preserve disjointness. </li>
</ul>
</li>
</ol>
<h4 id="How-to-perform-RO-transactions"><a href="#How-to-perform-RO-transactions" class="headerlink" title="How to perform RO transactions?"></a>How to perform RO transactions?</h4><ol>
<li>A read-only transaction executes in two phases<ul>
<li>Assign a timestamp $s_{read}$</li>
<li>Then execute the transaction’s reads as snapshot reads at sread. </li>
<li>The snapshot reads can execute at any replicas that are sufficiently up-to-date.</li>
</ul>
</li>
<li>The simple assignment of $s_{read} = TT.now().latest$<ul>
<li>At any time after a transaction starts, it preserves external consistency by an argument analogous to that presented for writes. </li>
<li>Such a timestamp may require the execution of the data reads at $s_{read}$ to block if $t_{safe}$ has not advanced sufficiently. </li>
</ul>
</li>
<li>To reduce the chances of blocking, Spanner should assign the oldest timestamp that preserves external consistency. </li>
</ol>
<h4 id="How-does-Spanner-assign-timestamps-to-RO-transactions"><a href="#How-does-Spanner-assign-timestamps-to-RO-transactions" class="headerlink" title="How does Spanner assign timestamps to RO transactions?"></a>How does Spanner assign timestamps to RO transactions?</h4><ol>
<li>Assigning a timestamp requires a negotiation phase between all of the Paxos groups that are involved in the reads. </li>
<li>Spanner requires a scope expression for every read-only transaction, which is an expression that summarizes the keys that will be read by the entire transaction. </li>
<li>If the scope’s values are served by a single Paxos group<ul>
<li>The client issues the read-only transaction to that group’s leader. That leader assigns $s_{read}$ and executes the read. </li>
<li>Define $LastTS()$ to be the timestamp of the last committed write at a Paxos group. </li>
<li>If there are no prepared transactions, the assignment $s_{read} = LastTS()$ trivially satisfies external consistency: the transaction will see the result of the last write, and therefore be ordered after it. </li>
</ul>
</li>
<li>If the scope’s values are served by multiple Paxos groups<ul>
<li>The most complicated option is to do a round of communication with all of the groups’s leaders to negotiate $s_{read}$ based on $LastTS()$. </li>
<li>A simpler choice is that the client avoids a negotiation round, and just has its reads execute at $s_{read} = TT.now().latest$. </li>
</ul>
</li>
<li>If a transaction has just committed, a non-conflicting read-only transaction must still be assigned $s_{read}$ so as to follow that transaction. <ul>
<li>This weakness can be remedied by augmenting $LastTS()$ with a fine-grained mapping from key ranges to commit timestamps in the lock table. </li>
<li>When a read-only transaction arrives, its timestamp can be assigned by taking the maximum value of $LastTS()$ for the key ranges with which the transaction conflicts, unless there is a conflicting prepared transaction. </li>
</ul>
</li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ol>
<li><p>For the performance experiment, the author tested for the two common benchmarks: latency and throughput. </p>
<ul>
<li><p>For the latency experiments, clients issued sufficiently few operations so as to avoid queuing at the servers. </p>
</li>
<li><p>For the throughput experiments, clients issued sufficiently many operations so as to saturate the servers’ CPUs. </p>
</li>
<li><p>The result is as shown below. 1D means one replica with commit wait disabled. </p>
</li>
<li><p>As the number of replicas increases, </p>
<ul>
<li>The latency stays roughly constant with less standard deviation because Paxos executes in parallel at a group’s replicas, hence the latency to achieve a quorum becomes less sensitive to slowness at one slave replica. </li>
</ul>
<p><img src="/imgs/Distributed/Spanner/performance.png" width="75%"></p>
</li>
</ul>
</li>
<li><p>The author also tested the scalability of two-phase commit. </p>
<p><img src="/imgs/Distributed/Spanner/2pc.png" width=50%></p>
</li>
<li><p>The availability after server failure is another important metric. </p>
<ul>
<li><p>Non-leader killing has no effect on read throughput. </p>
</li>
<li><p>Killing leaders while giving time to handoff leadership to a different zone has a minor effect aroung $3-4\%$. </p>
</li>
<li><p>Killing leaders with no warning has a severe effect: the rate of completion drops almost to $0$. </p>
<p><img src="/imgs/Distributed/Spanner/availability.png" width="50%"></p>
</li>
</ul>
</li>
<li><p>The uncertainty of TrueTime would affect the reliability of Spanner. </p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/2-Phase-Commit/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/2-Phase-Commit/" class="post-title-link" itemprop="url">2-Phase Commit</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2023-09-26 13:45:59 / Modified: 13:47:09" itemprop="dateCreated datePublished" datetime="2023-09-26T13:45:59+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#before-or-after-atomicity">Before-or-After Atomicity</a>
<ul>
<li><a href="#what-is-before-or-after-atomicity">What is before-or-after atomicity?</a></li>
<li><a href="#what-is-the-problem-of-before-or-after-atomicity">What is the problem of before-or-after atomicity?</a></li>
<li><a href="#what-kind-of-coordination-is-correct">What kind of coordination is correct?</a></li>
</ul>
</li>
<li><a href="#locking-disciplines">Locking disciplines</a>
<ul>
<li><a href="#simple-locking">Simple locking</a>
<ul>
<li><a href="#what-are-the-rules-of-simple-locking">What are the rules of simple locking?</a></li>
<li><a href="#what-are-lock-point-and-lock-set-of-a-transaction">What are lock point and lock set of a transaction?</a></li>
<li><a href="#how-can-we-implement-lock-manager">How can we implement lock manager?</a></li>
<li><a href="#how-to-argue-the-correctness-of-simple-locking">How to argue the correctness of simple locking?</a></li>
</ul>
</li>
<li><a href="#two-phase-locking">Two-phase locking</a>
<ul>
<li><a href="#what-are-the-rules-of-two-phase-locking">What are the rules of two-phase locking?</a></li>
<li><a href="#how-can-we-implement-lock-manager-2">How can we implement lock manager?</a></li>
<li><a href="#how-does-locks-interact-with-log-based-recovery">How does locks interact with log-based recovery?</a></li>
</ul>
</li>
<li><a href="#distributed-two-phase-commit">Distributed two-phase commit</a>
<ul>
<li><a href="#what-is-correct-in-multi-site-transaction">What is correct in multi-site transaction?</a></li>
<li><a href="#what-is-the-problem-of-multi-site-transaction">What is the problem of multi-site transaction?</a></li>
<li><a href="#what-is-the-steps-of-distributed-two-phase-commit-protocol">What is the steps of distributed two-phase commit protocol?</a></li>
<li><a href="#how-to-handle-the-lost-delayed-or-duplicated-message-problem">How to handle the lost, delayed, or duplicated message problem?</a></li>
<li><a href="#how-does-a-worker-site-recover-from-crashes">How does a worker site recover from crashes?</a></li>
<li><a href="#how-can-we-optimize-the-protocol">How can we optimize the protocol?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</p>
<h1 id="before-or-after-atomicity"><a class="markdownIt-Anchor" href="#before-or-after-atomicity"></a> Before-or-After Atomicity</h1>
<h2 id="what-is-before-or-after-atomicity"><a class="markdownIt-Anchor" href="#what-is-before-or-after-atomicity"></a> What is before-or-after atomicity?</h2>
<ol>
<li>
<p>Before-or-after property states that several actions that concurrently operate on the same data should not interfere with one another.</p>
</li>
<li>
<p>Concurrent actions have the before-or-after property if their effect from the point of view of their invokers is the same as if the actions occurred either completely before or completely after one another.</p>
</li>
</ol>
<h2 id="what-is-the-problem-of-before-or-after-atomicity"><a class="markdownIt-Anchor" href="#what-is-the-problem-of-before-or-after-atomicity"></a> What is the problem of before-or-after atomicity?</h2>
<ol>
<li>The programmer does not necessarily know the identities of all the other actions that might touch the shared vari­ able.</li>
<li>This lack of knowledge can make it problematic to coordinate actions by explicit program steps.</li>
<li>Instead, what the programmer needs is an automatic, implicit mechanism that ensures proper handling of every shared variable.</li>
</ol>
<h2 id="what-kind-of-coordination-is-correct"><a class="markdownIt-Anchor" href="#what-kind-of-coordination-is-correct"></a> What kind of coordination is correct?</h2>
<ol>
<li>Coordination among concurrent actions can be considered to be correct if every result is guaranteed to be one that could have been obtained by some purely serial application of those same actions.</li>
<li>As long as the intermediate states are not visible above the implementing layer, and the system is guaranteed to end up in one of the acceptable final states, we can declare the coordination to be correct.</li>
</ol>
<h1 id="locking-disciplines"><a class="markdownIt-Anchor" href="#locking-disciplines"></a> Locking disciplines</h1>
<h2 id="simple-locking"><a class="markdownIt-Anchor" href="#simple-locking"></a> Simple locking</h2>
<h3 id="what-are-the-rules-of-simple-locking"><a class="markdownIt-Anchor" href="#what-are-the-rules-of-simple-locking"></a> What are the rules of simple locking?</h3>
<ol>
<li>Each transaction must acquire a lock for every shared data object it intends to read or write before doing any actual reading and writing.</li>
<li>It may release its locks only after the transaction installs its last update and commits or completely restores the data and aborts.</li>
<li>Applications that discover which objects need to be read by reading other shared data objects have no alter­ native but to lock every object that they might need to read.</li>
</ol>
<h3 id="what-are-lock-point-and-lock-set-of-a-transaction"><a class="markdownIt-Anchor" href="#what-are-lock-point-and-lock-set-of-a-transaction"></a> What are lock point and lock set of a transaction?</h3>
<ol>
<li><em>Lock point</em>: the first instant at which it has acquired all of its locks.</li>
<li><em>Lock set</em>: The collection of locks it has acquired when it reaches its lock point.</li>
</ol>
<h3 id="how-can-we-implement-lock-manager"><a class="markdownIt-Anchor" href="#how-can-we-implement-lock-manager"></a> How can we implement lock manager?</h3>
<ol>
<li>
<p>Acquire locks</p>
<ul>
<li>
<p>Each transaction supply its intended lock set as an argu­ment to the <strong>begin_transaction</strong> operation, which acquires all of the locks of the lock set, if necessary waiting for them to become available.</p>
<ul>
<li>Interpose itself on all calls to read data and to log changes, to verify that they refer to variables that are in the lock set.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Release locks</p>
<ul>
<li>Intercept the call to <em>commit</em> or <em>abort</em> (or, if the application uses roll-forward recovery, to log an <em>END</em> record) at which time it auto­matically releases all of the locks of the lock set.</li>
</ul>
</li>
</ol>
<h3 id="how-to-argue-the-correctness-of-simple-locking"><a class="markdownIt-Anchor" href="#how-to-argue-the-correctness-of-simple-locking"></a> How to argue the correctness of simple locking?</h3>
<ol>
<li>Imagine that an all-seeing outside observer maintains an ordered list to which it adds each transaction identifier as soon as the transaction reaches its lock point and removes it from the list when it begins to release its locks.</li>
<li>Each transaction has agreed not to read or write anything until that transaction has been added to the observer’s list.</li>
<li>Since no data object can appear in the lock sets of two transactions, no data object in any transaction’s lock set appears in the lock set of the transaction preceding it in the list, and by induction to any transaction earlier in the list.</li>
<li>Thus all of this transaction’s input values are the same as they will be when the preceding transaction in the list commits or aborts.</li>
<li>The same argument applies to the transaction before the preceding one, so all inputs to any trans­ action are identical to the inputs that would be available if all the transactions ahead of it in the list ran serially, in the order of the list.</li>
<li>Thus the simple locking discipline ensures that this transaction runs completely after the preceding one and completely before the next one.</li>
<li>Concurrent transactions will produce results as if they had been serialized in the order that they reached their lock points.</li>
</ol>
<h2 id="two-phase-locking"><a class="markdownIt-Anchor" href="#two-phase-locking"></a> Two-phase locking</h2>
<h3 id="what-are-the-rules-of-two-phase-locking"><a class="markdownIt-Anchor" href="#what-are-the-rules-of-two-phase-locking"></a> What are the rules of two-phase locking?</h3>
<ol>
<li>It avoids the requirement that a transaction must know in advance which locks to acquire.</li>
<li>The twophase locking discipline allows a transaction to acquire locks as it proceeds, and the trans­ action may read or write a data object as soon as it acquires a lock on that object.</li>
<li>The primary constraint is that the transaction may not release any locks until it passes its lock point.</li>
<li>The transaction can release a lock on an object that it only reads any time after it reaches its lock point if it will never need to read that object again, even to abort.</li>
</ol>
<h3 id="how-can-we-implement-lock-manager-2"><a class="markdownIt-Anchor" href="#how-can-we-implement-lock-manager-2"></a> How can we implement lock manager?</h3>
<ol>
<li>Intercept all calls to read and write data; it acquires a lock (perhaps having to wait) on the first use of each shared variable.</li>
<li>As with simple locking, it then holds the locks until it intercepts the call to <em>commit</em>, <em>abort</em>, or log the <em>END</em> record of the transaction, at which time it releases them all at once.</li>
</ol>
<h3 id="how-does-locks-interact-with-log-based-recovery"><a class="markdownIt-Anchor" href="#how-does-locks-interact-with-log-based-recovery"></a> How does locks interact with log-based recovery?</h3>
<ol>
<li>
<p>Whether locks themselves are data objects for which changes should be logged?</p>
<ul>
<li>
<p>At the completion of crash recovery there should be no pending transactions because any transactions that were pending at the time of the crash should have been rolled back by the recovery procedure, and recov­ ery does not allow any new transactions to begin until it completes.</p>
</li>
<li>
<p>Since locks exist only to coordinate pending transactions, it would clearly be an error if there were locks still set when crash recovery is complete.</p>
</li>
<li>
<p>Locks belong in vol­atile storage, where they will automatically disappear on a crash, rather than in non­-volatile storage, where the recovery procedure would have to hunt them down to release them.</p>
</li>
</ul>
</li>
<li>
<p>whether or not the log-based recovery algorithm will construct a correct system state?</p>
<ul>
<li>The transactions that were not complete at the instant of the crash had nonoverlapping lock sets at the moment that the lock values vanished.</li>
<li>Those particular actions can safely be redone or undone without con­ cern for before-or-after atomicity during recovery.</li>
<li>The locks created a particular serialization of the transactions and the log has captured that serialization.</li>
<li>Since <em>RECOVER</em> performs <em>UNDO</em> actions in reverse order as specified in the log, and it per­ forms <em>REDO</em> actions in forward order, again as specified in the log, <em>RECOVER</em> reconstructs exactly that same serialization.</li>
</ul>
</li>
</ol>
<h2 id="distributed-two-phase-commit"><a class="markdownIt-Anchor" href="#distributed-two-phase-commit"></a> Distributed two-phase commit</h2>
<h3 id="what-is-correct-in-multi-site-transaction"><a class="markdownIt-Anchor" href="#what-is-correct-in-multi-site-transaction"></a> What is correct in multi-site transaction?</h3>
<ol>
<li>Correctness of the multiple-site ato­ micity protocol will be achieved if all the sites commit or if all the sites abort.</li>
<li>It is failed if some sites commit their part of a multiple-site transaction while others abort their part of that same transaction.</li>
</ol>
<h3 id="what-is-the-problem-of-multi-site-transaction"><a class="markdownIt-Anchor" href="#what-is-the-problem-of-multi-site-transaction"></a> What is the problem of multi-site transaction?</h3>
<ol>
<li>The coordinator has created a higher-layer transaction and each of the workers is to perform a transaction that is nested in the higher-layer transaction.</li>
<li>The complication is that the coordinator and workers cannot reliably communicate.</li>
<li>The problem thus reduces to constructing a reliable distributed version of the two-phase commit protocol. We can do that by applying persistent senders and duplicate suppression.</li>
</ol>
<h3 id="what-is-the-steps-of-distributed-two-phase-commit-protocol"><a class="markdownIt-Anchor" href="#what-is-the-steps-of-distributed-two-phase-commit-protocol"></a> What is the steps of distributed two-phase commit protocol?</h3>
<ol>
<li>
<p>Beginning</p>
<ul>
<li>
<p>The coordinator creates a top-layer outcome record for the overall transaction.</p>
</li>
<li>
<p>It persistently sends the content of sub-transactions to each site, referring to the same transaction, respectively.</p>
</li>
<li>
<p>A worker site, upon receiving a request, checks for duplicates and then creates a transaction of its own, but it makes the transaction a nested one, with its superior being the coordinator’s original transaction.</p>
</li>
<li>
<p>Then the worker site goes about doing the pre-commit part of the requested action, reporting back to coordinator that this much has gone well.</p>
</li>
</ul>
</li>
<li>
<p>Two-phase commit</p>
<ul>
<li>Phase one:
<ul>
<li>The coordinatior , upon collecting a complete set of such responses then moves to the two-phase commit part of the transaction, by sending <code>PREPARE</code> messages to each worker site.</li>
<li>Each worker site, upon receiving this message, commits—but only tentatively—or aborts.</li>
<li>Having created durable tentative versions (or logged to journal storage its planned updates) and having recorded an outcome record saying that it is <code>PREPARED</code> either to commit or abort, worker sites persistently sends a response to coorinator of commit or abort.</li>
<li>If all workers send PREPARED messages, phase one of the two-phase commit is complete.</li>
<li>If any worker responds with an abort message, or doesn’t respond at all, the coordinator has the usual choice of aborting the entire transaction or perhaps trying a different worker site to carry out that component transaction.</li>
</ul>
</li>
<li>Phase two:
<ul>
<li>The coordinator commits the entire transaction by marking its own outcome record <code>COMMITTED</code>.</li>
<li>It sends a completion message back to each worker site.</li>
<li>Each worker site, upon receiving such a message, changes its state from <code>PREPARED</code> to <code>COM­MITTED</code>, performs any needed post-commit actions, and exits.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="how-to-handle-the-lost-delayed-or-duplicated-message-problem"><a class="markdownIt-Anchor" href="#how-to-handle-the-lost-delayed-or-duplicated-message-problem"></a> How to handle the lost, delayed, or duplicated message problem?</h3>
<ol>
<li>If coordinator doesn’t receive a response of ready message of some sub-transaction request from one or more of the workers in a reasonable time she resends the message to the non-responding workers as many times as necessary to elicit a response.</li>
<li>If a worker site receives a duplicate request of <code>PREPARE</code> from coordinator, its persistent sender sends back a duplicate of the <code>PREPARED</code> or <code>ABORTED</code> response.</li>
<li>If the coordinator goes down before the coordinator sends final <code>COMMITTED</code> message in phase-two, all of the workers must wait until it recovers; in this protocol, the coordinator is a single point of failure.</li>
<li>The coordinator must remem­ ber, reliably and for an indefinite time, the outcome of this transaction.
<ul>
<li>If a completion message does not arrive in a reasonable period of time, the persistent sender at the worker site will resend its <code>PREPARED</code> message.</li>
<li>Whenever the coordinator receives a duplicate <code>PREPARED</code> message, it simply sends back the current state of the outcome record for the named transaction.</li>
</ul>
</li>
</ol>
<h3 id="how-does-a-worker-site-recover-from-crashes"><a class="markdownIt-Anchor" href="#how-does-a-worker-site-recover-from-crashes"></a> How does a worker site recover from crashes?</h3>
<ol>
<li>It must classify any <code>PREPARED</code> transaction as a tentative win­ner that it should restore to the <code>PREPARED</code> state.</li>
<li>If the worker is using locks for before-or-after atomicity, the recovery procedure must reacquire any locks the PREPARED transaction was holding at the time of the failure.</li>
<li>The recovery procedure must restart the persistent sender, to learn the current status of the higher-layer transaction.</li>
<li>If the worker site uses version histories, only the last step, restarting the persistent sender, is required.</li>
<li>If worker site crashes after sending <code>PREPARED</code> before receiving <code>COMMIT</code>, other servers will actually commit. So we want server recovery as <code>PREPARED</code>. So before sending <code>PREPARED</code>, servers need to make their log durable.</li>
</ol>
<h3 id="how-can-we-optimize-the-protocol"><a class="markdownIt-Anchor" href="#how-can-we-optimize-the-protocol"></a> How can we optimize the protocol?</h3>
<ol>
<li>The initial RPC request and response could also carry the PREPARE and PREPARED messages, respectively.
<ul>
<li>Once a worker sends a <code>PREPARED</code> message, it loses the ability to unilaterally abort, and it must remain on the knife edge awaiting instructions from the coordinator.</li>
<li>To minimize this wait, it is usually preferable to delay the <code>PREPARE</code>/<code>PREPARED</code> message pair until the coordinator knows that the other workers seem to be in a position to do their parts.</li>
</ul>
</li>
<li>Have a fourth acknowl­ edgment message from the worker sites to the coordinator.
<ul>
<li>Once all acknowledgments are in, the coordinator can then safely discard its outcome record, since every worker site is known to have gotten the word.</li>
</ul>
</li>
<li>Presumed commit: The coordi­nator answers any inquiry about a non-existent outcome record by sending a <code>COMMITTED</code> response.
<ul>
<li>The coordinator commits by destroying the out­ come record, so a fourth acknowledgment message from every worker is unnecessary.</li>
<li>The coor­dinator can persistently ask for acknowledgment of aborted transactions, and discard the outcome record after all these acknowledgments are in.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Aurora/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Aurora/" class="post-title-link" itemprop="url">Aurora</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:28:51" itemprop="dateCreated datePublished" datetime="2023-09-26T13:28:51+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:30:48" itemprop="dateModified" datetime="2023-10-04T16:30:48+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/aurora.pdf">Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Aurora is a relational database service for OLTP workloads. </li>
<li>Issue: <ul>
<li>In modern distributed cloud services, resilience and scalability are increasingly achieved by decoupling compute from storage and by replicating storage across multiple nodes. </li>
<li>The central constraint in high throughput data processing has moved from compute and storage to the network. </li>
</ul>
</li>
<li>Contribution: <ul>
<li>Build storage as an independent fault-tolerant and self-healing service across multiple data-centers. <ul>
<li>Protect the database from performance variance and transient or permanent failures at either the networking or storage tiers. </li>
</ul>
</li>
<li>Only write redo log records to storage<ul>
<li>Reduce network IOPS by an order of magnitude. </li>
<li>When bottleneck removed, the scalability of the system is greatly improved. And they can aggressively optimize numerous other points of contention. </li>
</ul>
</li>
<li>Move some of the most complex and critical functions (backup and redo recovery) from one-time expensive operations in the database engine to continuous asynchronous operations amortized across a large distributed fleet. <ul>
<li>This yields near-instant crash recovery without checkpointing as well as inexpensive backups that do not interfere with foreground processing. </li>
</ul>
</li>
<li>Also, it achieves consensus on durable state across numerous storage nodes using an efficient asynchronous scheme, avoiding expensive and chatty recovery protocols. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Background-conceptions-of-AWS"><a href="#Background-conceptions-of-AWS" class="headerlink" title="Background conceptions of AWS"></a>Background conceptions of AWS</h2><h2 id="Durability-at-scale"><a href="#Durability-at-scale" class="headerlink" title="Durability at scale"></a>Durability at scale</h2><h3 id="How-does-Aurora-tolerate-failures"><a href="#How-does-Aurora-tolerate-failures" class="headerlink" title="How does Aurora tolerate failures?"></a>How does Aurora tolerate failures?</h3><ol>
<li>It uses a quorum-based voting protocol. The oridinary quorum voting protocol is as followed:<ul>
<li>If each of the $V$ copies of a replicated data item is assigned a vote, a read must obtain quorum of $V_r$ votes while a write must obtain quorum of $V_w$ votes. </li>
<li>Each read must be aware of the most recent write, formulated as $V_r + V_w &gt; V$. </li>
<li>Each write must be aware of the most recent write to avoid conflicting writes, formulated as $V_w &gt; V/2$. </li>
</ul>
</li>
<li>The failure of each Availability Zone (AZ) is independent. <ul>
<li>For a common setting $V=3$, $V_r=2$, $V_w=2$, we can set $3$ replicas in different AZs to be tolerant to large-scale events in addition to the smaller individual failures. </li>
<li>However, the failure of an AZ will break quorum for any of the replicas that concurrently have failures in another AZ. </li>
<li>While the individual failures of replicas in each of the AZs are uncorrelated, the failure of an AZ is a correlated failure of all disks and nodes in that AZ. </li>
</ul>
</li>
<li>Quorum protocol of Aurora:<ul>
<li>Replicate each data item $6$ ways across $3$ AZs with $2$ copies of each item in each AZ. </li>
<li>Use a quorum model with $6$ votes ($V = 6$), a write quorum of $4/6$ ($V_w = 4$), and a read quorum of $3/6$ ($V_r = 3$). </li>
<li>It can lose a single AZ and one additional node ($AZ+1$) without losing read availability, and lose any two nodes, including a single AZ failure and maintain write availability. </li>
</ul>
</li>
</ol>
<h3 id="How-does-Aurora-shrink-the-window-of-vulnerability"><a href="#How-does-Aurora-shrink-the-window-of-vulnerability" class="headerlink" title="How does Aurora shrink the window of vulnerability?"></a>How does Aurora shrink the window of vulnerability?</h3><ol>
<li>To provide sufficient durability in this model, one must ensure the probability of a double fault on uncorrelated failures, represented by Mean Time to Failure (MTTF), is sufficiently low over the time it takes to repair one of these failures, Mean Time to Repair (MTTR). </li>
<li>It is difficult, past a point, to reduce the probability of MTTF on independent failures. </li>
<li>Focus on reducing MTTR<ul>
<li>Partition the database volume into small fixed size segments, currently $10$GB in size. <ul>
<li>These are each replicated $6$ ways into Protection Groups (PGs) so that each PG consists of six  segments, organized across $3$ AZs, with two segments in each AZ. </li>
<li>A storage volume is a concatenated set of PGs, physically implemented using a large fleet of storage nodes that are provisioned as virtual hosts with attached SSDs using Amazon Elastic Compute Cloud (EC2). </li>
<li>The PGs that constitute a volume are allocated as the volume grows. </li>
</ul>
</li>
<li>Segments are now unit of independent background noise failure and repair. The system monitor and automatically repair faults as part of service. </li>
</ul>
</li>
</ol>
<h3 id="What-are-the-problems-of-I-O-volume-of-mirrored-MySQL"><a href="#What-are-the-problems-of-I-O-volume-of-mirrored-MySQL" class="headerlink" title="What are the problems of I/O volume of mirrored MySQL?"></a>What are the problems of I/O volume of mirrored MySQL?</h3><p><img src="/imgs/Distributed/Aurora/MySQL.png" width="50%"></p>
<ol>
<li>The engine needs to write various types of data: <ul>
<li>The redo log, the binary (statement) log that is archived to Amazon Simple Storage Service (S3) in order to support point-in-time restores</li>
<li>The modified data pages, a second temporary write of the data page (double-write) to prevent torn pages, and finally the metadata (FRM) files. </li>
</ul>
</li>
<li>Steps 1, 3, and 5 are sequential and synchronous. <ul>
<li>Latency is additive because many writes are sequential. </li>
<li>Even on asynchronous writes, one must wait for the slowest operation, leaving the system at the mercy of outliers. </li>
<li>From a distributed system perspective, this model can be viewed as having a 4/4 write quorum, and is vulnerable to failures and outlier performance. </li>
</ul>
</li>
<li>User operations that are a result of OLTP applications cause many different types of writes often representing the same information in multiple ways. </li>
</ol>
<h3 id="How-does-Aurora-reduce-network-traffic"><a href="#How-does-Aurora-reduce-network-traffic" class="headerlink" title="How does Aurora reduce network traffic?"></a>How does Aurora reduce network traffic?</h3><ol>
<li><p>The only writes that cross the network are redo log records. The log applicator is pushed to the storage tier where it can be used to generate database pages in background or on demand. </p>
</li>
<li><p>Generating each page from the complete chain of its modifications from the beginning of time is prohibitively expensive. </p>
<ul>
<li>Continually materialize database pages in the background to avoid regenerating them from scratch on demand every time. </li>
<li>As far as the engine is concerned, the log is the database, and any pages that the storage system materializes are simply a cache of log applications. </li>
</ul>
</li>
<li><p>The primary only writes log records to the storage service and streams those log records as well as metadata updates to the replica instances. </p>
<p><img src="/imgs/Distributed/Aurora/NetworkIO.png" width=50%></p>
</li>
<li><p>The storage node involves the following steps: </p>
<ul>
<li>(1) receive log record and add to an in-memory queue</li>
<li>(2) persist record on disk and acknowledge</li>
<li>(3) organize records and identify gaps in the log since some batches may be lost</li>
<li>(4) gossip with peers to fill in gaps</li>
<li>(5) coalesce log records into new data pages</li>
<li>(6) periodically stage log and new pages to S3</li>
<li>(7) periodically garbage collect old versions</li>
<li>(8) periodically validate CRC codes on pages</li>
<li>Each of the steps above asynchronous, and only steps (1) and (2) are in the foreground path potentially impacting latency. </li>
</ul>
<p><img src="/imgs/Distributed/Aurora/StorageNode.png" width="50%"></p>
</li>
</ol>
<h3 id="Logs"><a href="#Logs" class="headerlink" title="Logs"></a>Logs</h3><h3 id="How-does-Aurora-decide-what-is-completed-and-what-is-durable"><a href="#How-does-Aurora-decide-what-is-completed-and-what-is-durable" class="headerlink" title="How does Aurora decide what is completed and what is durable?"></a>How does Aurora decide what is completed and what is durable?</h3><ol>
<li>At a high level, The system maintains points of consistency and durability, and continually advances these points as it receives acknowledgements for outstanding storage requests. </li>
<li>The logic for tracking partially completed transactions and undoing them is kept in the database engine, just as if it were writing to simple disks.<br>Upon restart, before the database is allowed to access the storage volume, the storage service does its own recovery which is focused not on user-level transactions, but on making sure that the database sees a uniform view of storage despite its distributed nature. </li>
<li>Completeness: <ul>
<li><em>Log Sequence Number (LSN)</em>: Each log record has an associated LSN that is a monotonically increasing value generated by the database. </li>
<li><em>Volume Complete LSN (VCL)</em>: The storage service determines the highest LSN for which it can guarantee availability of all prior log records. </li>
<li>During storage recovery, every log record with an LSN larger than the VCL must be truncated. </li>
</ul>
</li>
<li>Durability:<ul>
<li><em>Consistency Point LSNs (CPL)</em>: The database can further constrain a subset of points that are allowable for truncation by tagging log records</li>
<li><em>Volume Durable LSN (VDL)</em>: the highest CPL that is smaller than or equal to VCL and truncate all log records with LSN greater than the VDL.</li>
<li>A CPL can be thought of as delineating some limited form of storage system transaction that must be accepted in order. </li>
</ul>
</li>
</ol>
<h3 id="How-does-the-database-and-storage-interact"><a href="#How-does-the-database-and-storage-interact" class="headerlink" title="How does the database and storage interact?"></a>How does the database and storage interact?</h3><ol>
<li>Each database-level transaction is broken up into multiple mini-transactions (MTRs) that are ordered and must be performed atomically. </li>
<li>Each mini-transaction is composed of multiple contiguous log records (as many as needed). </li>
<li>The final log record in a mini-transaction is a CPL. </li>
<li>On recovery, the database talks to the storage service to establish the durable point of each PG and uses that to establish the VDL and then issue commands to truncate the log records above VDL.</li>
</ol>
<h3 id="How-does-the-database-write-data"><a href="#How-does-the-database-write-data" class="headerlink" title="How does the database write data?"></a>How does the database write data?</h3><ol>
<li>Database view:<ul>
<li>As the database receives acknowledgements to establish the write quorum for each batch of log records, it advances the current VDL. </li>
<li>The database allocates a unique ordered LSN for each redo log record of each transaction that is no greater than the sum of the current VDL and the LSN Allocation Limit (LAL). </li>
<li>This limit ensures that the database does not get too far ahead of the storage system and introduces back-pressure that can throttle the incoming writes if the storage or network cannot keep up.</li>
</ul>
</li>
<li>PG view:<ul>
<li>Each segment of each PG only sees a subset of log records in the volume that affect the pages residing on that segment. </li>
<li>Each log record contains a backlink that identifies the previous log record for that PG to track the point of completeness of the log records that have reached each segment. </li>
<li><em>Segment Complete LSN (SCL)</em>: Identifies the greatest LSN below which all log records of the PG have been received, established through backlinks. </li>
<li>The SCL is used by the storage nodes when they gossip with each other in order to find and exchange log records that they are missing. </li>
</ul>
</li>
</ol>
<h3 id="How-does-the-database-commit-transactions"><a href="#How-does-the-database-commit-transactions" class="headerlink" title="How does the database commit transactions?"></a>How does the database commit transactions?</h3><ol>
<li>When a client commits a transaction, the thread handling the commit request sets the transaction aside by recording its “commit LSN” as part of a separate list of transactions waiting on commit and moves on to perform other work. </li>
<li>Completing a commit, if and only if, the latest VDL is greater than or equal to the transaction’s commit LSN. </li>
<li>As the VDL advances, the database identifies qualifying transactions that are waiting to be committed and uses a dedicated thread to send commit acknowledgements to waiting clients.</li>
</ol>
<h3 id="Where-does-the-database-read"><a href="#Where-does-the-database-read" class="headerlink" title="Where does the database read?"></a>Where does the database read?</h3><ol>
<li>Pages are served from the buffer cache and only result in a storage IO request if the page in question is not present in the cache. </li>
<li>While the Aurora database does not write out pages on cache eviction (or anywhere else), it enforces a similar guarantee: a page in the buffer cache must always be of the latest version. <ul>
<li><em>Page LSN</em>: identify the log record associated with the latest change to the page</li>
<li>Evict a page from the cache only if its page LSN is greater than or equal to the VDL. </li>
<li>Hence, all changes in the page have been hardened in the log, and on a cache miss, it is sufficient to request a version of the page as of the current VDL to get its latest durable version. </li>
</ul>
</li>
</ol>
<h3 id="How-does-the-database-read"><a href="#How-does-the-database-read" class="headerlink" title="How does the database read?"></a>How does the database read?</h3><ol>
<li>When reading a page from disk, the database establishes a read-point, representing the VDL at the time the request was issued. </li>
<li>The database can then select a storage node that is complete with respect to the read point, knowing that it will therefore receive an up to date version. </li>
<li>Protection Group Min Read Point LSN (PGMRPL): represents that all the log records of the PG below it are unnecessary. <ul>
<li>If there are read replicas, the writer gossips with them to establish the per-PG Minimum Read Point LSN across all nodes. </li>
<li>A storage node segment is guaranteed that there will be no read page requests with a read-point that is lower than the PGMRPL. </li>
<li>Each storage node is aware of the PGMRPL from the database and can, therefore, advance the materialized pages on disk by coalescing the older log records and then safely garbage collecting them. </li>
</ul>
</li>
</ol>
<h3 id="How-does-the-database-replicate"><a href="#How-does-the-database-replicate" class="headerlink" title="How does the database replicate?"></a>How does the database replicate?</h3><ol>
<li>A single writer and up to 15 read replicas can all mount a single shared storage volume. </li>
<li>To minimize lag, the log stream generated by the writer and sent to the storage nodes is also sent to all read replicas. </li>
<li>In the reader, the database consumes this log stream by considering each log record in turn. <ul>
<li>If the log record refers to a page in the reader’s buffer cache, it uses the log applicator to apply the specified redo operation to the page in the cache. </li>
<li>Otherwise it simply discards the log record. </li>
<li>The replicas consume log records asynchronously from the perspective of the writer, which acknowledges user commits independent of the replica. </li>
<li>The only log records that will be applied are those whose LSN is less than or equal to the VDL.<br>The log records that are part of a single mini-transaction are applied atomically in the replica’s cache to ensure that the replica sees a consistent view of all database objects. </li>
</ul>
</li>
</ol>
<h3 id="How-does-the-database-perform-undo-and-redo"><a href="#How-does-the-database-perform-undo-and-redo" class="headerlink" title="How does the database perform undo and redo?"></a>How does the database perform undo and redo?</h3><ol>
<li>The same redo log applicator is used in the forward processing path as well as on recovery where it operates synchronously and in the foreground while the database is offline. </li>
<li>The redo log applicator is decoupled from the database and operates on storage nodes, in parallel, and all the time in the background. Once the database starts up it performs volume recovery in collaboration with the storage service. </li>
<li>Undo recovery can happen when the database is online after the system builds the list of these in-flight transactions from the undo segments. </li>
</ol>
<h3 id="How-does-the-database-reestablish-runtime-state"><a href="#How-does-the-database-reestablish-runtime-state" class="headerlink" title="How does the database reestablish runtime state?"></a>How does the database reestablish runtime state?</h3><ol>
<li>It contacts for each PG, a read quorum of segments which is sufficient to guarantee discovery of any data that could have reached a write quorum. </li>
<li>Once the database has established a read quorum for every PG it can recalculate the VDL above which data is truncated by generating a truncation range that annuls every log record after the new VDL, up to and including an end LSN which the database can prove is at least as high as the highest possible outstanding log record that could ever have been seen. </li>
<li>The database infers this upper bound because it allocates LSNs, and limits how far allocation can occur above VDL (the 10 million limit described earlier). </li>
<li>The truncation ranges are versioned with epoch numbers, and written durably to the storage service so that there is no confusion over the durability of truncations in case recovery is interrupted and restarted.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/CRAQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/CRAQ/" class="post-title-link" itemprop="url">CRAQ</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:20:23" itemprop="dateCreated datePublished" datetime="2023-09-26T13:20:23+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:30:18" itemprop="dateModified" datetime="2023-10-04T16:30:18+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/craq.pdf">Object Storage on CRAQ High-throughput chain replication for read-mostly workloads</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Issue: many commercially-deployed systems sacrifice stronger consistency properties in the desire for greater availability and higher throughput.</li>
<li>Contribution:<ul>
<li>This system is an improvement on Chain Replication, maintains strong consistency while greatly improving read throughput by enabling any chain node to handle read operations. <ul>
<li>By distributing load across all object replicas, CRAQ scales linearly with chain size without increasing consistency coordination. </li>
</ul>
</li>
<li>CRAQ’s design naturally supports eventual-consistency among read operations for lower-latency reads during write contention and degradation to read-only behavior during transient partitions. <ul>
<li>CRAQ allows applications to specify the maximum staleness acceptable for read operations.</li>
</ul>
</li>
<li>Leveraging these load-balancing properties, we describe a wide-area system design for building CRAQ chains across geographically-diverse clusters that preserves strong locality properties. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h2><h3 id="What-is-object-based-storage"><a href="#What-is-object-based-storage" class="headerlink" title="What is object-based storage?"></a>What is object-based storage?</h3><ol>
<li>Object-based storage: Supported by key-value databases, data is presented to applications as entire units. </li>
<li>Object stores support two basic primitives: read (or query) operations return the data block stored under an object name, and write (or update) operations change the state of a single object.  </li>
<li>Object stores are better suited for flat namespaces, such as in key-value databases, as opposed to hierarchical directory structures where file systems are better. </li>
<li>Object stores simplify the process of supporting whole-object modifications.<br>Typically only need to reason about the ordering of modifications to a specific object, as opposed to the entire storage system.<br>Significantly cheaper to provide consistency guarantees per object instead of across all operations and/or objects. </li>
</ol>
<h3 id="What-is-strong-consistency-and-eventual-consistency"><a href="#What-is-strong-consistency-and-eventual-consistency" class="headerlink" title="What is strong consistency and eventual consistency?"></a>What is strong consistency and eventual consistency?</h3><ol>
<li><p>Strong consistency guarantees that all read and write operations to an object are executed in some sequential order, and that a read to an object always sees the latest written value. </p>
</li>
<li><p>Eventual consistency implies that </p>
<ul>
<li><p>Writes to an object are still applied in a sequential order on all nodes, but eventually-consistent reads to different nodes can return stale data for some period of inconsistency. </p>
</li>
<li><p>However, read operations will never return an older version than the latest committed write. </p>
</li>
<li>A client will also see monotonic read consistency if it maintains a session with a particular node. </li>
</ul>
</li>
<li><p>The Eventual consistency is still different from the guarantees from ZooKeeper or Raft. In ZooKeeper or Raft, clients only see monotonic read consistency even if they cross sessions with different servers. </p>
</li>
</ol>
<h3 id="Chain-replication"><a href="#Chain-replication" class="headerlink" title="Chain replication"></a>Chain replication</h3><h4 id="What-is-chain-replication"><a href="#What-is-chain-replication" class="headerlink" title="What is chain replication?"></a>What is chain replication?</h4><ol>
<li>It organizes all nodes storing an object in a chain, where the chain tail handles all read requests, and the chain head handles all write requests,  as shown below:<br><img src="/imgs/Distributed/CRAQ/basicCR.png" alt=""></li>
<li>Writes propagate down the chain before the client is acknowledged, thus providing a simple ordering of all object operations, and hence strong consistency, at the tail. </li>
<li>The lack of any complex or multi-round protocols yields simplicity, good throughput, and easy recovery. </li>
</ol>
<h4 id="What-is-the-problem-of-basic-chain-replication"><a href="#What-is-the-problem-of-basic-chain-replication" class="headerlink" title="What is the problem of basic chain replication?"></a>What is the problem of basic chain replication?</h4><ol>
<li>All reads for an object must go to the same node, leading to potential hotspots.</li>
<li>Multiple chains can be constructed across a cluster of nodes for better load balancing via consistent hashing or a more centralized directory approach. </li>
<li>But these algorithms might still find load imbalances if particular objects are disproportionally popular.<br>All reads to a chain may then be handled by a potentially-distant node, namely the chain’s tail. </li>
</ol>
<h4 id="How-does-CR-handle-client-requests"><a href="#How-does-CR-handle-client-requests" class="headerlink" title="How does CR handle client requests?"></a>How does CR handle client requests?</h4><ol>
<li>The <em>head</em> of the chain handles all write operations from clients. <ul>
<li>When a write operation is received by a node, it is propagated to the next node in the chain. </li>
<li>Once the write reaches the tail node, it has been applied to all replicas in the chain, and it is considered committed. </li>
<li>When the tail commits the write, a reply is sent to the client. The CR paper describes the tail sending a message directly back to the client. </li>
</ul>
</li>
<li>The tail node handles all read operations, so only values which are committed can be returned by a read. </li>
<li>The simple topology of CR makes write operations cheaper than in other protocols offering strong consistency.<br>Multiple concurrent writes can be pipelined down the chain, with transmission costs equally spread over all nodes. </li>
</ol>
<h4 id="Why-CR-cannot-provide-read-from-intermediate-nodes"><a href="#Why-CR-cannot-provide-read-from-intermediate-nodes" class="headerlink" title="Why CR cannot provide read from intermediate nodes?"></a>Why CR cannot provide read from intermediate nodes?</h4><ol>
<li>The reading result can violate the strong consistency. </li>
<li>Concurrent reads to different nodes could see different writes as they are in the process of propagating down the chain. </li>
</ol>
<h2 id="CRAQ-System-Model"><a href="#CRAQ-System-Model" class="headerlink" title="CRAQ System Model"></a>CRAQ System Model</h2><h3 id="How-does-CRAQ-handle-write-requests"><a href="#How-does-CRAQ-handle-write-requests" class="headerlink" title="How does CRAQ handle write requests?"></a>How does CRAQ handle write requests?</h3><ol>
<li>A node in CRAQ can store multiple versions of an object, each including a monotonically-increasing version number and an additional attribute whether the version is clean or dirty. All versions are initially marked as clean. </li>
<li>When a node receives a new version of an object via a write being propagated down the chain, the node appends this latest version to its list for the object. <ul>
<li>If the node is not the tail, it marks the version as dirty, and propagates the write to its successor. </li>
<li>Otherwise, if the node is the tail, it marks the version as clean, at which time we call the object version (write) as committed. </li>
<li>The tail node can then notify all other nodes of the commit by sending an acknowledgement backwards through the chain. </li>
</ul>
</li>
<li>When an acknowledgement message for an object version arrives at a node, the node marks the object version as clean. The node can then delete all prior versions of the object. </li>
<li>If the node has exactly one version for an object, the object is implicitly in the clean state; otherwise, the object is dirty and the properlyordered version must be retrieved from the chain tail.</li>
</ol>
<h3 id="How-does-CRAQ-handle-read-requests-to-guarantee-strong-consistency"><a href="#How-does-CRAQ-handle-read-requests-to-guarantee-strong-consistency" class="headerlink" title="How does CRAQ handle read requests to guarantee strong consistency?"></a>How does CRAQ handle read requests to guarantee strong consistency?</h3><ol>
<li>When a node receives a read request for an object, if the latest known version of the requested object is clean, the node returns this value. </li>
<li>If the latest version number of the object requested is dirty, the node contacts the tail and asks for the tail’s last committed version number (a version query). The node then returns that version of the object. </li>
<li>The tail could commit a new version between when it replied to the version request and when the intermediate node sends a reply to the client.<br>This does not violate strong consistency, as read operations are serialized with respect to the tail. </li>
</ol>
<p><img src="/imgs/Distributed/CRAQ/read.png" alt=""></p>
<h3 id="In-what-scenarios-does-CRAQ-out-performs-basic-CR"><a href="#In-what-scenarios-does-CRAQ-out-performs-basic-CR" class="headerlink" title="In what scenarios does CRAQ out-performs basic CR?"></a>In what scenarios does CRAQ out-performs basic CR?</h3><ol>
<li>In read-mostly workloads, most of the read requests handled solely by the C − 1 non-tail nodes (as clean reads), and thus throughput in these scenarios scales linearly with chain size C. </li>
<li>In write-heavy workloads, most read requests to non-tail nodes as dirty. But these version queries are lighter-weight than full reads, allowing the tail to process them at a much higher rate before it becomes saturated. </li>
</ol>
<h3 id="How-does-CRAQ-support-eventual-consistency"><a href="#How-does-CRAQ-support-eventual-consistency" class="headerlink" title="How does CRAQ support eventual consistency?"></a>How does CRAQ support eventual consistency?</h3><ol>
<li><p>It allows read operations to a chain node to return the newest object version known to it. </p>
</li>
<li><p>It can also support eventual consistency with maximum-bounded inconsistency. </p>
<ul>
<li>The limit imposed can be based on time (relative to a node’s local clock) or on absolute version numbers. </li>
</ul>
</li>
<li><p>If the chain is still available, this inconsistency is actually in terms of the returned version being newer than the last committed one. </p>
<p>If the system is partitioned and the node cannot participate in writes, the version may be older than the current committed one.</p>
</li>
</ol>
<h3 id="How-does-CRAQ-recover-from-failure"><a href="#How-does-CRAQ-recover-from-failure" class="headerlink" title="How does CRAQ recover from failure?"></a>How does CRAQ recover from failure?</h3><ol>
<li>Each chain node needs to know its predecessor and successor, as well as the chain head and tail. </li>
<li>When a head fails, its immediate successor takes over as the new chain head; likewise, the tail’s predecessor takes over when the tail fails. </li>
<li>If intermediate node fails, drop it from chain, predecessor may need to re-send recent writes since write request pipeline may have been broken from the failure server. </li>
</ol>
<h3 id="How-does-CRAQ-manage-configuration"><a href="#How-does-CRAQ-manage-configuration" class="headerlink" title="How does CRAQ manage configuration?"></a>How does CRAQ manage configuration?</h3><ol>
<li>An object’s identifier consists of both a chain identifier and a key identifier. </li>
<li>Applications can specify their requirements in multiple ways:<ul>
<li>Implicit Datacenters &amp; Global Chain Size $\{num_datacenters, chain_size\}$. <ul>
<li>To determine exactly which datacenters store the chain, consistent hashing is used with unique datacenter identifiers.</li>
</ul>
</li>
<li>Explicit Datacenters &amp; Global Chain Size $\{chain_size, dc_1, dc_2, …, dc_N\}$<ul>
<li>The order of the chain is the same as specified with head within $dc_1$ and tail within $dc_N$. </li>
<li>To determine which nodes within a datacenter store objects assigned to the chain, consistent hashing is used on the chain identifier. </li>
<li>Each datacenter $dc_i$ has a node which connects to the tail of datacenter $dc_{i−1}$ and a node which connects to the head of datacenter $dc_{i+1}$. </li>
<li><code>chain_size</code> being $0$ indicates that the chain should use all nodes within each datacenter. </li>
</ul>
</li>
<li>Explicit Datacenter Chain Sizes $\{dc_1, chain_size_1, …, dc_N, chain_size_N\}$<ul>
<li>This allows for non-uniformity in chain load balancing. </li>
<li>The chain nodes within each datacenter are chosen in the same manner as the previous method, and $chain_size_i$ can also be set to $0$. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="How-does-CRAQ-handle-transient-failure-e-g-partition-failure"><a href="#How-does-CRAQ-handle-transient-failure-e-g-partition-failure" class="headerlink" title="How does CRAQ handle transient failure (e.g. partition failure)?"></a>How does CRAQ handle transient failure (e.g. partition failure)?</h3><ol>
<li>In methods 2 and 3 above, $dc_1$ can be set as a master datacenter. <ul>
<li>If a datacenter is the master for a chain, writes to the chain will only be accepted by that datacenter during transient failures. </li>
</ul>
</li>
<li>When a master is not defined, <ul>
<li>Writes will only continue in a partition if the partition contains a majority of the nodes in the global chain. </li>
<li>The minority partition will become read-only for maximumbounded inconsistent read operations. </li>
</ul>
</li>
</ol>
<h3 id="How-should-CRAQ-choose-nodes-within-a-Datacenter"><a href="#How-should-CRAQ-choose-nodes-within-a-Datacenter" class="headerlink" title="How should CRAQ choose nodes within a Datacenter?"></a>How should CRAQ choose nodes within a Datacenter?</h3><ol>
<li>In CRAQ’s current implementation, we place chains within a datacenter using consistent hashing, mapping potentially many chain identifiers to a single head node. </li>
<li>Another approach is to use the membership management service as a directory service in assigning and storing randomized chain membership, i.e., each chain can include some random set of server nodes. <ul>
<li>This approach improves the potential for parallel system recovery. </li>
<li>But it would increase centralization and state, and require storing more metadata information in the coordination service.</li>
</ul>
</li>
</ol>
<h3 id="How-does-CRAQ-support-mini-transaction-of-single-key-operations"><a href="#How-does-CRAQ-support-mini-transaction-of-single-key-operations" class="headerlink" title="How does CRAQ support mini-transaction of single-key operations?"></a>How does CRAQ support mini-transaction of single-key operations?</h3><ol>
<li>For Prepend/Append and Increment/Decrement operations, <ul>
<li>The head of the chain storing the key’s object can simply apply the operation to the latest version of the object, even if the latest version is dirty, and then propagate a full replacement write down the chain. </li>
<li>If these operations are frequent, the head can buffer the requests and batch the updates. These enhancements would be much more expensive using a traditional two-phase-commit protocol. </li>
</ul>
</li>
<li>For the test-and-set operation, <ul>
<li>The head of the chain checks if its most recent committed version number equals the version number specified in the operation. </li>
<li>If there are no outstanding uncommitted versions of the object, the head accepts the operation and propagates an update down the chain. </li>
<li>If there are outstanding writes, we simply reject the test-and-set operation, and clients are careful to back off their request rate if continuously rejected. </li>
</ul>
</li>
<li>The optimistic two-phase protocol need only be implemented with the chain heads, not all involved nodes. <ul>
<li>The chain heads can lock any keys involved in the minitransaction until it is fully committed. </li>
<li>It reduces the write throughput of CRAQ as writes to the same object can no longer be pipelined. </li>
</ul>
</li>
</ol>
<h3 id="How-does-CRAQ-lower-write-latency-with-multicast"><a href="#How-does-CRAQ-lower-write-latency-with-multicast" class="headerlink" title="How does CRAQ lower write latency with multicast?"></a>How does CRAQ lower write latency with multicast?</h3><ol>
<li>Within a datacenter, this would take the form of a network-layer multicast protocol, while application-layer multicast protocols may be better-suited for wide-area chains. </li>
<li>No ordering or reliability guarantees are required from these multicast protocols. </li>
<li>The actual value can be multicast to the entire chain. Then, only a small metadata message needs to be propagated down the chain to ensure that all replicas have received a write before the tail. </li>
<li>If a node does not receive the multicast for any reason, the node can fetch the object from its predecessor after receiving the write commit message and before further propagating the commit message. </li>
<li>When the tail receives a propagated write request, a multicast acknowledgment message can be sent to the multicast group instead of propagating it backwards along the chain. </li>
</ol>
<h3 id="How-does-CRAQ-use-ZooKeeper-to-manage-configuration"><a href="#How-does-CRAQ-use-ZooKeeper-to-manage-configuration" class="headerlink" title="How does CRAQ use ZooKeeper to manage configuration?"></a>How does CRAQ use ZooKeeper to manage configuration?</h3><ol>
<li><p>During initialization, a CRAQ node creates an ephemeral file in <code>/nodes/dc_name/node_id</code>. </p>
<ul>
<li><p>The content of the file contains the node’s IP address and port number. </p>
</li>
<li><p>CRAQ nodes can query <code>/nodes/dc_name</code> to determine the membership list for its datacenter. They creates a watch on the children list of <code>/nodes/dc_name</code>. </p>
</li>
</ul>
</li>
<li><p>When a CRAQ node receives a request to create a new chain, a file is created in <code>/chains/chain_id</code>. </p>
<ul>
<li>The chain’s placement strategy determines the contents of the file, but it only includes this chain configuration information, not the list of a chain’s current nodes. </li>
<li>Instead of letting nodes register their membership for each chain they belong to (<em>i.e.</em>, chain metadata explicitly names the chain’s current members), any node participating in the chain will query the chain file and place a watch on it. </li>
<li>This is based on the assumption that the number of chains will generally be at least an order of magnitude larger than the number of nodes in the system, or that chain dynamism may be significantly greater than nodes joining or leaving the system. </li>
</ul>
</li>
</ol>
<h3 id="How-does-nodes-communicate-with-each-other"><a href="#How-does-nodes-communicate-with-each-other" class="headerlink" title="How does nodes communicate with each other?"></a>How does nodes communicate with each other?</h3><ol>
<li>The nodes within each datacenter organize themselves into a one-hop DHT using the identifiers generated when joining the system. <ul>
<li>A node’s chain predecessor and successor, the head node and the tail node are defined as its predecessor and successor in the DHT ring. </li>
</ul>
</li>
<li>All RPC-based communication between nodes, or between nodes and clients, is over TCP connections. <ul>
<li>Each node maintains a pool of connected TCP connections with its chain’s predecessor, successor, and tail. </li>
<li>For chains that span across multiple datacenters, the last node of one datacenter maintains a connection to the first node of its successor datacenter. </li>
<li>Any node that maintains a connection to a node outside of its datacenter must also place a watch on the node list of the external datacenter. </li>
</ul>
</li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ol>
<li><p>The main contribution is that it support read from intermediate nodes. So the author measured the read throughput of CRAQ and compared it with basic CR. </p>
<p><img src="/imgs/Distributed/CRAQ/4.png" alt=""><img src="/imgs/Distributed/CRAQ/6.png" alt=""></p>
</li>
<li><p>The author also tested the factors affecting read throughput, i.e. number of clients, number of nodes, and writes. </p>
<p><img src="/imgs/Distributed/CRAQ/5.png" alt=""><img src="/imgs/Distributed/CRAQ/7.png" alt=""></p>
</li>
<li><p>Another important test in distributed system is that how long does the system take to recover, and how do they perform during the failure. </p>
<p><img src="/imgs/Distributed/CRAQ/10-13.png" alt=""></p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Zookeeper/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Zookeeper/" class="post-title-link" itemprop="url">Zookeeper</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:16:26" itemprop="dateCreated datePublished" datetime="2023-09-26T13:16:26+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:29:33" itemprop="dateModified" datetime="2023-10-04T16:29:33+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/zookeeper.pdf">ZooKeeper: Wait-free coordination for Internet-scale systems</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Contribution<ul>
<li>Moved away from implementing specific primitives on the server side<ul>
<li>Opted for exposing an API that enables application developers to implement their own primitives. </li>
<li>It enables new primitives without requiring changes to the service core. </li>
</ul>
</li>
<li>Moved away from blocking primitives. <ul>
<li>Manipulate simple wait-free data objects organized hierarchically as in file systems. </li>
</ul>
</li>
<li>Provide a per client guarantee of FIFO execution of requests and linearizability for all writes. <ul>
<li>Read requests are satisfied by local servers. </li>
</ul>
</li>
</ul>
</li>
<li>Features<ul>
<li>Provide a simple and high performance kernel for building more complex coordination primitives at the client. </li>
<li>Incorporates elements from group messaging, shared registers, and distributed lock services in a replicated, centralized service. </li>
<li>It has the wait-free aspects of shared registers with an event-driven mechanism. </li>
<li>Using a simple pipelined architecture that allows us to have hundreds or thousands of requests outstanding while still achieving low latency. </li>
<li>With asynchronous operations, a client is able to have multiple outstanding operations at a time. </li>
<li>Enables caching data on the client side with ZooKeeper watches to avoid the problem of delayed update caused by slow or faulty client. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="ZooKeeper-service-overview"><a href="#ZooKeeper-service-overview" class="headerlink" title="ZooKeeper service overview"></a>ZooKeeper service overview</h2><p>Znode: an in-memory data node in the ZooKeeper data<br>Data tree: a hierarchical namespace to organize znodes</p>
<h3 id="How-does-client-interact-with-ZooKeeper-server"><a href="#How-does-client-interact-with-ZooKeeper-server" class="headerlink" title="How does client interact with ZooKeeper server?"></a>How does client interact with ZooKeeper server?</h3><ol>
<li><p>Clients submit requests to ZooKeeper through a client API using a ZooKeeper client library. </p>
<ul>
<li><code>create(path, data, flags)</code>: returns the name of the new znode</li>
<li><code>delete(path, version)</code>: Deletes the znode path if that znode is at the expected version. </li>
<li><code>exists(path, watch)</code>: The watch flag enables a client to set a watch on the znode. </li>
<li><code>getData(path, watch)</code>: Returns the data and meta-data, such as version information, associated with the znode. ZooKeeper does not set the watch if the znode does not exist. </li>
<li><code>setData(path, data, version)</code>: Writes data[] to znode path if the version number is the current version of the znode. </li>
<li><code>getChildren(path, watch)</code></li>
<li><code>sync(path)</code>: Waits for all updates pending at the start of the operation to propagate to the server that the client is connected to. The path is currently ignored. </li>
</ul>
</li>
<li><p>All methods have both a synchronous and an asynchronous version available through the API. </p>
</li>
<li><p>If the actual version number of the znode does not match the expected version number the update fails with an unexpected version error.<br> If the version number is −1, it does not perform version checking.</p>
</li>
<li><p>The client library also manages the network connections between the client and ZooKeeper servers.</p>
</li>
</ol>
<h3 id="What-are-the-flags-of-znodes"><a href="#What-are-the-flags-of-znodes" class="headerlink" title="What are the flags of znodes?"></a>What are the flags of znodes?</h3><ol>
<li><em>Regular</em>: Clients manipulate regular znodes by creating and deleting them explicitly</li>
<li><em>Ephemeral</em>: Clients create such znodes, and they either delete them explicitly, or let the system remove them automatically when the session that creates them terminates</li>
<li><em>Sequential</em>: Those znodes in the same parent znode have a monotonically increasing counter appended to its name. The newer znode has larger sequence value. </li>
<li><em>Watch</em><ul>
<li>A read operation with a watch flag set completes as normal except that the server promises to notify the client when the information returned has changed. </li>
<li>Watches are one-time triggers associated with a session; they are unregistered once triggered or the session closes. </li>
<li>Watches indicate that a change has happened, but do not provide the change. </li>
<li>Session events, such as connection loss events, are also sent to watch callbacks so that clients know that watch events may be delayed. </li>
</ul>
</li>
</ol>
<h3 id="What-are-the-differences-between-znodes-and-files-in-file-system"><a href="#What-are-the-differences-between-znodes-and-files-in-file-system" class="headerlink" title="What are the differences between znodes and files in file system?"></a>What are the differences between znodes and files in file system?</h3><ol>
<li><p>Znodes map to abstractions of the client application, typically corresponding to meta-data used for coordination purposes. </p>
</li>
<li><p>Znodes allow clients to store some information that can be used for meta-data or configuration in a distributed computation, like the leadership of a replica group can be stored to a known location. </p>
</li>
<li><p>ZooKeeper does not use handles to access znodes. Each request instead includes the full path of the znode being operated on. </p>
<ul>
<li><p>It simplifies the API (no <code>open()</code> or <code>close()</code> methods)</p>
</li>
<li><p>It also eliminates extra state that the server would need to maintain. </p>
</li>
</ul>
</li>
</ol>
<h3 id="What-does-ZooKeeper-guarantee"><a href="#What-does-ZooKeeper-guarantee" class="headerlink" title="What does ZooKeeper guarantee?"></a>What does ZooKeeper guarantee?</h3><ol>
<li><p>This definition of its linearizability is called A-linearizability (asynchronous linearizability) that allows a client to have multiple outstanding operations. </p>
<ul>
<li><p><em>Linearizable writes</em>: all requests that update the state of ZooKeeper are serializable and respect precedence. </p>
</li>
<li><p><em>FIFO client order</em>: all requests from a given client are executed in the order that they were sent by the client. </p>
</li>
</ul>
</li>
<li><p>A-linearizability can choose to guarantee no specific order for outstanding operations of the same client or to guarantee FIFO order. </p>
</li>
<li><p>A system that satisfies A-linearizability also satisfies linearizability. Because only update requests are A-linearizable, ZooKeeper processes read requests locally at each replica. </p>
</li>
</ol>
<h3 id="How-to-change-configuration"><a href="#How-to-change-configuration" class="headerlink" title="How to change configuration?"></a>How to change configuration?</h3><ol>
<li><p>Two requirements:</p>
<ul>
<li>As the new leader starts making changes, we do not want other processes to start using the configuration that is being changed. </li>
<li>If the new leader dies before the configuration has been fully updated, we do not want the processes to use this partial configuration. </li>
</ul>
</li>
<li><p>The new leader can designate a path as the ready znode; other processes will only use the configuration when that znode exists. </p>
<ul>
<li>The new leader makes the configuration change by deleting ready, updating the various configuration znodes, and creating ready. </li>
<li>All of these changes can be pipelined and issued asynchronously to quickly update the configuration state given the FIFO client order guarantee. </li>
</ul>
</li>
</ol>
<h3 id="How-should-a-client-read-configurations"><a href="#How-should-a-client-read-configurations" class="headerlink" title="How should a client read configurations?"></a>How should a client read configurations?</h3><ol>
<li><p>If a client sees the ready exists before the new leader starts to make a change, it could read the partial configuration in progress and cannot notice anything. </p>
<ul>
<li>The client need to set the watch flag when they check the existence of the ready znode. </li>
<li>Then it will see a notification informing the client of the change before it can read any of the new configuration. </li>
</ul>
</li>
<li><p>If A changes the shared configuration in ZooKeeper and tells B of the change through the shared communication channel, B would expect to see the change when it re-reads the configuration.</p>
<ul>
<li>If B’s ZooKeeper replica is slightly behind A’s, it may not see the new configuration. </li>
<li>B can make sure that it sees the most up-to-date information by issuing a write before re-reading the configuration. </li>
<li><code>sync</code> causes a server to apply all pending write requests before processing the read without the overhead of a full write. </li>
</ul>
</li>
</ol>
<h2 id="Implement-primitives"><a href="#Implement-primitives" class="headerlink" title="Implement primitives"></a>Implement primitives</h2><h3 id="How-does-ZooKeeper-manage-configuration"><a href="#How-does-ZooKeeper-manage-configuration" class="headerlink" title="How does ZooKeeper manage configuration?"></a>How does ZooKeeper manage configuration?</h3><ol>
<li>Configuration is stored in a znode, $z_c$. </li>
<li>Processes start up with the full pathname of $z_c$. Starting processes obtain their configuration by reading zc with the watch flag set to true. </li>
<li>If the configuration in $z_c$ is ever updated, the processes are notified and read the new configuration, again setting the watch flag to true. </li>
</ol>
<h3 id="How-does-ZooKeeper-manage-rendezvous"><a href="#How-does-ZooKeeper-manage-rendezvous" class="headerlink" title="How does ZooKeeper manage rendezvous?"></a>How does ZooKeeper manage rendezvous?</h3><ol>
<li>Use a rendezvous znode, $z_r$, which is an node created by the client. </li>
<li>The client passes the full pathname of $z_r$ as a startup parameter of the master and worker processes. </li>
<li>When the master starts, it fills in $z_r$ with information about addresses and ports it is using. </li>
<li>When workers start, they read $z_r$ with watch set to true. <ul>
<li>If $z_r$ has not been filled in yet, the worker waits to be notified when $z_r$ is updated. </li>
<li>If $z_r$ is an ephemeral node, master and worker processes can watch for $z_r$ to be deleted and clean themselves up when the client ends. </li>
</ul>
</li>
</ol>
<h3 id="How-does-ZooKeeper-manage-group-mambership"><a href="#How-does-ZooKeeper-manage-group-mambership" class="headerlink" title="How does ZooKeeper manage group mambership?"></a>How does ZooKeeper manage group mambership?</h3><ol>
<li><p>Designate a znode, $z_g$ to represent the group. </p>
</li>
<li><p>When a process member of the group starts, it creates an <code>ephemeral</code> child znode under $z_g$. Processes may put process information in the data of the child znode. </p>
</li>
<li><p>If each process has a unique name or identifier, then that name is used as the name of the child znode; otherwise, the process creates the znode with the <code>SEQUENTIAL</code> flag to obtain a unique name assignment. </p>
</li>
</ol>
<h3 id="How-does-ZooKeeper-manage-Mini-transaction"><a href="#How-does-ZooKeeper-manage-Mini-transaction" class="headerlink" title="How does ZooKeeper manage Mini-transaction?"></a>How does ZooKeeper manage Mini-transaction?</h3><ol>
<li>In a mini-transaction, we want the <code>getData</code> and <code>setData</code> to be atomic. </li>
<li>ZooKeeper can support mini-transaction using version number. </li>
</ol>
<figure class="highlight rust"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">true</span>:</span><br><span class="line">	X, v = <span class="title function_ invoke__">getData</span>(K)</span><br><span class="line">  <span class="keyword">if</span> <span class="title function_ invoke__">setData</span>(K, X+<span class="number">1</span>, v):</span><br><span class="line">		<span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h3 id="How-does-ZooKeeper-implement-simple-locks"><a href="#How-does-ZooKeeper-implement-simple-locks" class="headerlink" title="How does ZooKeeper implement simple locks?"></a>How does ZooKeeper implement simple locks?</h3><ol>
<li><p>The lock is represented by a znode. To acquire a lock, a client tries to create the designated znode with the <code>EPHEMERAL</code> flag. </p>
</li>
<li><p>If the create succeeds, the client holds the lock. Otherwise, the client can read the znode with the watch flag set to be notified if the current leader dies. </p>
</li>
<li><p>A client releases the lock when it dies or explicitly deletes the znode. </p>
</li>
<li><p>Other clients that are waiting for a lock try again to acquire a lock once they observe the znode being deleted.</p>
</li>
</ol>
<h3 id="How-does-ZooKeeper-implement-simple-locks-without-herd-effect"><a href="#How-does-ZooKeeper-implement-simple-locks-without-herd-effect" class="headerlink" title="How does ZooKeeper implement simple locks without herd effect?"></a>How does ZooKeeper implement simple locks without herd effect?</h3><ol>
<li><p>Herd effect of simple locks: If there are many clients waiting to acquire a lock, they will all vie for the lock when it is released even though only one client can acquire the lock. </p>
</li>
<li><p>Define a lock znode l to implement such locks. Line up all the clients requesting the lock and each client obtains the lock in order of request arrival. </p>
<ul>
<li>Use the <code>SEQUENTIAL</code> flag to order the client’s attempt to acquire the lock with respect to all other attempts. </li>
<li>If the client’s znode has the lowest sequence number, the client holds the lock. </li>
<li>Otherwise, the client waits for deletion of the znode that either has the lock or will receive the lock before this client’s znode. </li>
</ul>
</li>
<li>By only watching the znode that precedes the client’s znode, the herd effect can be avoided by only waking up one process when a lock is released or a lock request is abandoned. </li>
<li>Once the znode being watched by the client goes away, the client must check if it now holds the lock. <ul>
<li>The previous lock request may have been abandoned and there is a znode with a lower sequence number still waiting for or holding the lock. </li>
</ul>
</li>
<li>Releasing a lock is as simple as deleting the znode n that represents the lock request. </li>
<li><p>We can see by browsing the ZooKeeper data the amount of lock contention, break locks, and debug locking problems. </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Lock</span></span><br><span class="line">n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">C = getChildren(l, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">if</span> n is lowest znode in C, <span class="built_in">exit</span></span><br><span class="line">p = znode in C ordered just before n</span><br><span class="line"><span class="keyword">if</span> exists(p, <span class="literal">true</span>) wait <span class="keyword">for</span> watch event</span><br><span class="line"><span class="keyword">goto</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//Unlock</span></span><br><span class="line">delete(n)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="How-does-ZooKeeper-implement-Read-Write-locks"><a href="#How-does-ZooKeeper-implement-Read-Write-locks" class="headerlink" title="How does ZooKeeper implement Read/Write locks?"></a>How does ZooKeeper implement Read/Write locks?</h3><ol>
<li>The write locks is similar to the simple locks since they are all exclusive. </li>
<li><p>Since read locks may be shared, and only earlier write lock znodes prevent the client from obtaining a read lock, read locks only need to check that no lower write znode. </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Write lock</span></span><br><span class="line">n = create(l + “/write-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">C = getChildren(l, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">if</span> n is lowest znode in C, <span class="built_in">exit</span></span><br><span class="line">p = znode in C ordered just before n</span><br><span class="line"><span class="keyword">if</span> exists(p, <span class="literal">true</span>) wait <span class="keyword">for</span> event</span><br><span class="line"><span class="keyword">goto</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Read Lock</span></span><br><span class="line">n = create(l + “/read-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">C = getChildren(l, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">if</span> no write znodes lower than n in C, <span class="built_in">exit</span></span><br><span class="line">p = write znode in C ordered just before n</span><br><span class="line"><span class="keyword">if</span> exists(p, <span class="literal">true</span>) wait <span class="keyword">for</span> event</span><br><span class="line"><span class="keyword">goto</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="What-is-the-difference-between-ZooKeeper-locks-and-thread-mutex-locks"><a href="#What-is-the-difference-between-ZooKeeper-locks-and-thread-mutex-locks" class="headerlink" title="What is the difference between ZooKeeper locks and thread mutex locks?"></a>What is the difference between ZooKeeper locks and thread mutex locks?</h3><ol>
<li>In ZooKeeper lock, if lock holder fails, system automatically releases locks. So locks are not really enforcing atomicity of other activities. </li>
<li>To make writes atomic, use “ready” trick or mini-transactions. </li>
<li>For master/leader election, new leader must inspect state and clean up. </li>
<li>Or use soft locks like MapReduce, for performance but not correctness. Only one worker will do each task, and each task is OK to be done twice. </li>
</ol>
<h3 id="How-does-ZooKeeper-implementa-double-barrier"><a href="#How-does-ZooKeeper-implementa-double-barrier" class="headerlink" title="How does ZooKeeper implementa double barrier?"></a>How does ZooKeeper implementa double barrier?</h3><ol>
<li>Double barriers enable clients to synchronize the beginning and the end of a computation. <ul>
<li>Enter barrier: At the beginning, a client need to wait until the number of waiting clients exceeds the threshold before it can execute the computation. </li>
<li>Leave barrier: At the end, a client need to wait until the number of finished clients exceeds the threshold before it can exit. </li>
</ul>
</li>
<li>Represent a barrier in ZooKeeper with a znode, referred to as $b$. </li>
<li>Every process $p$ registers with $b$ on entry by creating a znode as a child of $b$, and unregisters when it is ready to leave by removing the child. <ul>
<li>Processes can enter the barrier when the number of child znodes of $b$ exceeds the barrier threshold. </li>
<li>Processes can leave the barrier when all of the processes have removed their children. </li>
</ul>
</li>
<li>We use watches to efficiently wait for enter and exit conditions to be satisfied. <ul>
<li>To enter, processes watch for the existence of a ready child of b that will be created by the process that causes the number of children to exceed the barrier threshold. </li>
<li>To leave, processes watch for a particular child to disappear and only check the exit condition once that znode has been removed. </li>
</ul>
</li>
</ol>
<h2 id="Implementation-of-ZooKeeper"><a href="#Implementation-of-ZooKeeper" class="headerlink" title="Implementation of ZooKeeper"></a>Implementation of ZooKeeper</h2><h3 id="How-does-ZooKeeper-serve-requests"><a href="#How-does-ZooKeeper-serve-requests" class="headerlink" title="How does ZooKeeper serve requests?"></a>How does ZooKeeper serve requests?</h3><ol>
<li>Upon receiving a request, a server prepares it for execution in request processor. </li>
<li>If a write request requires coordination among the servers, they use atomic broadcast to reach an agreement , and finally servers commit changes to the ZooKeeper database fully replicated across all servers of the ensemble. <ul>
<li>When a server processes a write request, it also sends out and clears notifications relative to any watch that corresponds to that update. </li>
<li>Only the server that a client is connected to tracks and triggers notifications for that client. </li>
<li>Servers process writes in order and do not process other writes or reads concurrently. This ensures strict succession of notifications. </li>
</ul>
</li>
<li>In the case of read requests, a server simply reads the state of the local database and generates a response to the request. <ul>
<li>Each read request is processed and tagged with a <em>zxid</em> that corresponds to the last transaction seen by the server. </li>
<li><em>Zxid</em> defines the partial order of the read requests with respect to the write requests. </li>
<li>For applications that require that ZooKeeper do not serve stale data, they can call <code>sync()</code> followed by the read operation. </li>
</ul>
</li>
</ol>
<h3 id="How-does-ZooKeeper-manage-database"><a href="#How-does-ZooKeeper-manage-database" class="headerlink" title="How does ZooKeeper manage database?"></a>How does ZooKeeper manage database?</h3><ol>
<li>The replicated database is an in-memory database containing the entire data tree. </li>
<li>Each znode in the tree stores a maximum of 1MB of data by default, but this maximum value is a configuration parameter that can be changed in specific cases. </li>
<li>For recoverability,<ul>
<li>Efficiently log updates to disk, and we force writes to be on the disk media before they are applied to the in-memory database. </li>
<li>A replay log (a write-ahead log) of committed operations is kept and periodically generate snapshots of the in-memory database. </li>
</ul>
</li>
</ol>
<h3 id="How-does-request-processor-handle-write-requests"><a href="#How-does-request-processor-handle-write-requests" class="headerlink" title="How does request processor handle write requests?"></a>How does request processor handle write requests?</h3><ol>
<li>When the leader receives a write request, it calculates what the state of the system will be when the write is applied and transforms it into a transaction that captures this new state. </li>
<li>The future state must be calculated because there may be outstanding transactions that have not yet been applied to the database. </li>
</ol>
<h3 id="How-does-servers-reach-agreement"><a href="#How-does-servers-reach-agreement" class="headerlink" title="How does servers reach agreement?"></a>How does servers reach agreement?</h3><ol>
<li>All requests that update ZooKeeper state are forwarded to the leader. </li>
<li>The leader executes the request and broadcasts the change to the ZooKeeper state through Zab. <ul>
<li>Zab uses by default simple majority quorums to decide on a proposal, so Zab and thus ZooKeeper can only work if a majority of servers are correct. </li>
<li>Zab guarantees that changes broadcast by a leader are delivered in the order they were sent and all changes from previous leaders are delivered to an established leader before it broadcasts its own changes. </li>
<li>Because idempotent transactions is used, multiple delivery is acceptable as long as they are delivered in order. </li>
<li>ZooKeeper requires Zab to redeliver at least all messages that were delivered after the start of the last snapshot. </li>
</ul>
</li>
<li>The server that receives the client request responds to the client when it delivers the corresponding state change. </li>
<li>Use TCP for transport so message order is maintained by the network. </li>
</ol>
<h3 id="How-does-ZooKeeper-take-snapshot"><a href="#How-does-ZooKeeper-take-snapshot" class="headerlink" title="How does ZooKeeper take snapshot?"></a>How does ZooKeeper take snapshot?</h3><ol>
<li>ZooKeeper snapshots is called fuzzy snapshots since we do not lock the ZooKeeper state to take the snapshot.<br>Instead, we do a depth first scan of the tree atomically reading each znode’s data and meta-data and writing them to disk. </li>
<li>Since the resulting fuzzy snapshot may have applied some subset of the state changes delivered during the generation of the snapshot, the result may not correspond to the state of ZooKeeper at any point in time. </li>
<li>However, since state changes are idempotent, we can apply them twice as long as we apply the state changes in order. </li>
</ol>
<h3 id="How-does-ZooKeeper-handle-sync"><a href="#How-does-ZooKeeper-handle-sync" class="headerlink" title="How does ZooKeeper handle sync()"></a>How does ZooKeeper handle sync()</h3><ol>
<li>It does not need to atomically broadcast sync as using a leader-based algorithm, and it simply places the <code>sync</code> operation at the end of the queue of requests between the leader and the server executing the call to sync. </li>
<li>The follower must be sure that the leader is still the leader.<ul>
<li>If there are pending transactions that commit, then the server does not suspect the leader. </li>
<li>If the pending queue is empty, the leader needs to issue a null transaction to commit and orders the <code>sync</code> after that transaction. </li>
<li>Hence, when the leader is under load, no extra broadcast traffic is generated. </li>
<li>Timeouts are set such that leaders realize they are not leaders before followers abandon them, so it does not issue the null transaction. </li>
</ul>
</li>
</ol>
<h3 id="How-does-ZooKeeper-ensure-to-serve-data-at-least-as-update-as-last-server-served-that-data"><a href="#How-does-ZooKeeper-ensure-to-serve-data-at-least-as-update-as-last-server-served-that-data" class="headerlink" title="How does ZooKeeper ensure to serve data at least as update as last server served that data?"></a>How does ZooKeeper ensure to serve data at least as update as last server served that data?</h3><ol>
<li>Check the last zxid of the client against its last zxid. </li>
<li>If the client has a more recent view than the server, the server does not reestablish the session with the client until the server has caught up. </li>
</ol>
<h3 id="How-to-detect-client-session-failures"><a href="#How-to-detect-client-session-failures" class="headerlink" title="How to detect client session failures?"></a>How to detect client session failures?</h3><ol>
<li>The leader determines that there has been a failure if no other server receives anything from a client session within the session timeout. </li>
<li>If the client sends requests frequently enough, then there is no need to send any other message.<br>Otherwise, the client sends heartbeat messages during periods of low activity. </li>
<li>If the client cannot communicate with a server to send a request or heartbeat, it connects to a different ZooKeeper server to re-establish its session. </li>
<li>To prevent the session from timing out, the ZooKeeper client library sends a heartbeat after the session has been idle for $s/3$ ms and switch to a new server if it has not heard from a server for $2s/3$ ms, where $s$ is the session timeout in milliseconds. </li>
</ol>
<h1 id="Evaluation-and-results"><a href="#Evaluation-and-results" class="headerlink" title="Evaluation and results"></a>Evaluation and results</h1><ol>
<li><p>To show the scalability of the system, the author varied the number of servers that make up the ZooKeeper service, but always kept the number of clients the same. </p>
<p>The throughput performance of a saturated system as the ratio of reads to writes vary is as shown below. </p>
<p><img src="/imgs/Distributed/ZooKeeper/Throughput.png"></p>
</li>
<li><p>There are two reasons for write requests taking longer than read requests. </p>
<ul>
<li>First, write requests must go through atomic broadcast, which requires some extra processing and adds latency to requests. </li>
<li>The other reason for longer processing of write requests is that servers must ensure that transactions are logged to non-volatile store before sending acknowledgments back to the leader. </li>
</ul>
</li>
<li><p>The atomic broadcast protocol does most of the work of the system and thus limits the performance of ZooKeeper more than any other component. </p>
<p>To benchmark its performance the author simulates clients by generating the transactions directly at the leader, so there is no client connections or client requests and replies. </p>
<p>The result is as shown below:</p>
<p><img src="/imgs/Distributed/ZooKeeper/AtomicBroadcast.png"></p>
</li>
<li><p>The author also tested the throughput of the system when occurring different failure events. </p>
<p>The events are: ① Failure and recovery of a follower; ② Failure and recovery of a different follower; ③ Failure of the leader; ④Failure of two followers (a, b) in the first two marks, and recovery at the third mark (c); ⑤ Failure of the leader; ⑥Recovery of the leader. </p>
<p><img src="/imgs/Distributed/ZooKeeper/Failures.png"></p>
</li>
<li><p>To assess the latency of requests, the author creates a worker process that simply sends a create, waits for it to finish, sends an asynchronous delete of the new node, and then starts the next create. </p>
<p>Then the throughput can be calculated by dividing the number of create requests completed by the total time it took for all the workers to complete. </p>
<p><img src="/imgs/Distributed/ZooKeeper/Latency.png"></p>
</li>
<li><p>The author also measured the performance of primitives implemented with ZooKeeper. They measured the performance of barriers. </p>
<p>The time to process all barriers increase roughly linearly with the number of barriers, showing that concurrent access to the same part of the data tree did not produce any unexpected delay</p>
<p>Latency increases proportionally to the number of clients. This is a consequence of not saturating the ZooKeeper service due to clients waiting on other clients. </p>
<p><img src="/imgs/Distributed/ZooKeeper/Barrier.png"></p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
