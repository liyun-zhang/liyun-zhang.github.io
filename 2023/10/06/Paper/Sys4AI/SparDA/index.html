<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Paper: SparDA: Accelerating Dynamic Sparse Deep Neural Networks via Sparse-Dense Transformation @[toc] Purpose Sparsity has become the most important and efficient approach to accelerate neural networ">
<meta property="og:type" content="article">
<meta property="og:title" content="SparDA">
<meta property="og:url" content="http://liyun-zhang.github.io/2023/10/06/Paper/Sys4AI/SparDA/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:description" content="Paper: SparDA: Accelerating Dynamic Sparse Deep Neural Networks via Sparse-Dense Transformation @[toc] Purpose Sparsity has become the most important and efficient approach to accelerate neural networ">
<meta property="og:locale">
<meta property="og:image" content="http://liyun-zhang.github.io/imgs/Sys4ai/SparDA/tile.png">
<meta property="article:published_time" content="2023-10-06T08:32:20.000Z">
<meta property="article:modified_time" content="2024-03-16T03:24:36.494Z">
<meta property="article:author" content="LiyunZhang">
<meta property="article:tag" content="Sys4AI">
<meta property="article:tag" content="Sparsity">
<meta property="article:tag" content="Accelerator">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://liyun-zhang.github.io/imgs/Sys4ai/SparDA/tile.png">


<link rel="canonical" href="http://liyun-zhang.github.io/2023/10/06/Paper/Sys4AI/SparDA/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://liyun-zhang.github.io/2023/10/06/Paper/Sys4AI/SparDA/","path":"2023/10/06/Paper/Sys4AI/SparDA/","title":"SparDA"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>SparDA | LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">LiyunZhang</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Purpose"><span class="nav-number">1.</span> <span class="nav-text">Purpose</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Model"><span class="nav-number">2.</span> <span class="nav-text">Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-the-pipeline-of-SparDA"><span class="nav-number">2.1.</span> <span class="nav-text">What is the pipeline of SparDA?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-permutation-invariant"><span class="nav-number">2.2.</span> <span class="nav-text">What is permutation invariant?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-are-the-rules-of-applying-permutation-invariant"><span class="nav-number">2.3.</span> <span class="nav-text">What are the rules of applying permutation invariant?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-STile"><span class="nav-number">2.4.</span> <span class="nav-text">What is STile?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-transform-input-and-eliminate-overhead"><span class="nav-number">2.5.</span> <span class="nav-text">How to transform input and eliminate overhead?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-choose-an-efficient-STile"><span class="nav-number">2.6.</span> <span class="nav-text">How to choose an efficient STile?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#How-to-represent-dynamic-sparsity"><span class="nav-number">2.7.</span> <span class="nav-text">How to represent dynamic sparsity?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-are-the-APIs-of-SparDA-What-is-its-working-pipeline"><span class="nav-number">2.8.</span> <span class="nav-text">What are the APIs of SparDA? What is its working pipeline?</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Evaluation"><span class="nav-number">3.</span> <span class="nav-text">Evaluation</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">

  
  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/10/06/Paper/Sys4AI/SparDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="SparDA | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SparDA
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-06 16:32:20" itemprop="dateCreated datePublished" datetime="2023-10-06T16:32:20+08:00">2023-10-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 11:24:36" itemprop="dateModified" datetime="2024-03-16T11:24:36+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Sys4AI/" itemprop="url" rel="index"><span itemprop="name">Sys4AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.10936">SparDA: Accelerating Dynamic Sparse Deep Neural Networks via Sparse-Dense Transformation</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Sparsity has become the most important and efficient approach to accelerate neural networks by erasing a large portion of computation without unraveling model accuracy. </li>
<li>Issues:<ul>
<li>Most commodity accelerators (e.g., GPUs and TPUs) are mainly designed for efficient dense computations. </li>
<li>Previous research proposes sparsity optimizations to fit sparse computation, which only work for fixed granularities and perform poorly when the granularity mismatches. <ul>
<li>Fine-grained computation kernels cannot well saturate hardware due to the random memory access caused by fine-grained data. </li>
</ul>
</li>
<li>Existing solutions have to use time-consuming compiling to improve the efficiency of sparse kernels in an ahead-of-time manner and thus are limited to static sparsity. </li>
<li>An efficient general index construction mechanism for all data granularities is still missing. <ul>
<li>The performance of the previous sparse index construction methodology is also poor due to the constraint of sparse computation kernels. </li>
</ul>
</li>
</ul>
</li>
<li>Challenges:<ul>
<li>The dynamic sparsity pattern in different scenarios, even with the same scenario, is quite diverse and complex. </li>
<li>The key to optimizing such dynamic sparsity is simultaneously performing the calculations with an efficient kernel without computation waste. <ul>
<li>Therefore, optimizing such complex dynamic sparsity patterns requires breaking the binding between the data and computation granularity. </li>
</ul>
</li>
</ul>
</li>
<li>Contribution:<ul>
<li>Identified an important property called <em>permutation invariant</em>. <ul>
<li>It enables SparDA to extract dynamic sparsity patterns of tensors only known at runtime with negligible overhead. </li>
<li>It can transform dynamic sparse computation into equivalent dense computation that has been highly optimized on commodity accelerators. </li>
</ul>
</li>
<li>By combining permutation invariant with computation tiling, SparDA exploits the effect of permutation invariant to allow permutation on finer-grained granularity instead of the whole row or column of a tensor. <ul>
<li>It implies that the sparsity could be finer-grained and irregular if the non-zero values can be compacted into multiple dense computation tiles. </li>
<li>Define the sparsity pattern of the non-zero values and the compacted computation tile as a sparse tile, i.e., STile. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="What-is-the-pipeline-of-SparDA"><a href="#What-is-the-pipeline-of-SparDA" class="headerlink" title="What is the pipeline of SparDA?"></a>What is the pipeline of SparDA?</h2><ol>
<li>The design of STile naturally splits sparse computation into two decoupled stages: data permutation and dense computation. <ul>
<li>Decoupling frees the computation stage from handling the intricate encoding and decoding of sparse tensors. Thus, the computation can more efficiently utilize the accelerators. </li>
<li>With the decoupled stages, SparDA can leverage a wide range of well-optimized implementations of dense computation, including hardware instructions, manually optimized kernels, and automatically tuned kernels. </li>
<li>The data permutation stage transforms sparse data into a dense format with a new primitive <code>SLoad</code>. After the computation, the produced dense data is transformed back to the required format (e.g., sparse format) with the other primitive <code>SWrite</code>. </li>
</ul>
</li>
<li>The first stage is to learn the sparsity distribution from only a few samples. <ul>
<li>The STile optimizer analyzes the sparsity of each operator. It selects the most suitable STile from a set of pre-constructed STiles, each connected to a well-optimized dense computation tile. </li>
<li>SparDA generates a sparse kernel for the operator based on the selected STile. </li>
<li>This stage can be executed during the initialization and periodically to deal with possible shifting of sparsity distribution. </li>
</ul>
</li>
<li>The second stage is applying the generated sparse kernel at runtime. <ul>
<li>To deal with the dynamically changed sparsity, SparDA detects the sparsity online and builds the index of the sparse data following the requirement of the STile. </li>
</ul>
</li>
<li>There are two components in the sparse kernel. <ul>
<li>The first one rearranges the sparse data into dense format when loading data across different memory hierarchies. </li>
<li>The second one applies dense computation to condensed data without knowing their indices. </li>
</ul>
</li>
</ol>
<h2 id="What-is-permutation-invariant"><a href="#What-is-permutation-invariant" class="headerlink" title="What is permutation invariant?"></a>What is permutation invariant?</h2><ol>
<li>Tensor Expression (TE) describes deep learning computation in existing deep learning compilers. <ul>
<li>ReduceSum: $C[p]+=A[p,l]$</li>
<li>Addition: $C[p]=A[p]+B[p]$</li>
<li>MatMul: $C[m,n]+=A[m,k]\times B[k,n]$</li>
<li>BatchMatMul: $C[b,m,n]+=A[b,m,k]\times B[b,k,n]$</li>
<li>Convolution: $C[n,f,x,y]+=A[n,m,x+i,y+j]\times B[f,m,i,j]$</li>
</ul>
</li>
<li>Definition of permutation invariant dimension:<ul>
<li>In a tensor expression $Y \leftarrow f (X_1,\dots , X_n)$, where $f$ is an operator, $X_i$ and $Y$ are its input and output tensors, respectively. </li>
<li>A dimension $k$ in the operator is <strong>permutation invariant</strong> if it satisfies: $\forall P \in \Phi_k, ∃ P’ ∈ \Phi_k \text{s.t. }P’(f(P(X_1),\dots , P(X_n))=Y$</li>
<li>$\Phi_k$ is the set of all permutation functions on $k$ dimension. $P (X)$ means a permutation function $P$ is applied to the $k$ dimension of the tensor $X$, to shuffle the elements on $k$ dimension to a new order. If $k$ dimension does not exist in $X$, $P (X) = X$. </li>
</ul>
</li>
<li>For a permutation invariant dimension, when permutation is applied to this dimension of the input tensors, a reverse permutation exists on the output tensor to make the result the same as the original computation. </li>
</ol>
<h2 id="What-are-the-rules-of-applying-permutation-invariant"><a href="#What-are-the-rules-of-applying-permutation-invariant" class="headerlink" title="What are the rules of applying permutation invariant?"></a>What are the rules of applying permutation invariant?</h2><ol>
<li>Permutation invariant of tensor dimensions can be classified into three categories:<ul>
<li><strong>Sporadic dimension</strong> exists in one or more tensors of a tensor expression but does not span all tensors. For example, $m$, $n$, $k$, $f$, $l$ of the tensor expressions. </li>
<li><strong>Prevalent dimension</strong> is the dimension that exists in all the tensors (i.e., input and output tensors) of a tensor expression. Examples of prevalent dimensions are $p$ and $b$. </li>
<li><strong>Compound dimension</strong> is the dimension that is involved in an arithmetic expression. E.g. $x$, $y$ and $i$, $j$ in Convolution. </li>
</ul>
</li>
<li>When permutation invariant is applied to only one dimension of a tensor expression, the dimension can be sporadic or prevalent but not a compound dimension. <ul>
<li>Because permuting a compound dimension violates its corresponding arithmetic expression. </li>
</ul>
</li>
<li>When permutation invariant is applied on multiple dimensions of a tensor expression: <ul>
<li>When the permuted dimensions are sporadic, each dimension can only have a single permutation function. </li>
<li>When the permuted dimensions include a prevalent dimension, the permutation function on each element of the prevalent dimension could be different. </li>
</ul>
</li>
</ol>
<h2 id="What-is-STile"><a href="#What-is-STile" class="headerlink" title="What is STile?"></a>What is STile?</h2><ol>
<li>A tile is a sliced piece of an operator’s computation. <ul>
<li>Computation tiling slices the computation into many small homogeneous pieces to parallelize the computation and increase data reuse. </li>
</ul>
</li>
<li>Permutation invariance can be applied to each tile independently; that is, the permutation functions on each tile can be different, leading to more diverse and fine-grained sparsity granularity. </li>
<li>An STile is a group of non-redundant elements following a specific type of layout associated with a dense computation tile. <ul>
<li>The non-redundant element is called the data tile, representing the sparsity granularity. The scattered data tiles can be condensed into dense tiles. </li>
<li>In reverse, a dense tile can correspond to different STiles with different permutation functions. </li>
</ul>
</li>
</ol>
<p><img src="/imgs/Sys4ai/SparDA/tile.png" width="50%"></p>
<h2 id="How-to-transform-input-and-eliminate-overhead"><a href="#How-to-transform-input-and-eliminate-overhead" class="headerlink" title="How to transform input and eliminate overhead?"></a>How to transform input and eliminate overhead?</h2><ol>
<li><strong>Sparse-Dense Transform</strong>: With permutation invariant, we can construct a permutation function $P$ to move all the redundant elements to the end while all the non-redundant elements to the front. The redundant elements can be safely removed to build a shorter $k$ dimension.  </li>
<li><code>SLoad</code> maps sparse data tiles in input tensors to dense data blocks, while <code>SWrite</code> writes the output dense data block to the specified output data format, which could be sparse or dense. <ul>
<li><code>SLoad</code> and <code>SWrite</code> work on data rearrangement when the data moves from global memory to shared memory and in reverse. </li>
<li>As long as the data tile could saturate the read/write transaction of the memory (e.g., $32$ bytes in CUDA GPUs), the data rearrangement would introduce little overhead because loading sparse data tiles does not waste memory bandwidth. </li>
<li>This property further enables zero-copy of sparse data in online dynamic sparsity scenarios because the effective data tiles can be directly selected from their original data format and written to the higher-level memory with the desired format.</li>
</ul>
</li>
</ol>
<h2 id="How-to-choose-an-efficient-STile"><a href="#How-to-choose-an-efficient-STile" class="headerlink" title="How to choose an efficient STile?"></a>How to choose an efficient STile?</h2><ol>
<li>The most efficient STile for a sparse operator is determined mainly by two factors, i.e., the efficiency of its associated dense computation tile and the operator’s dynamic sparsity. </li>
<li>Though different-sized computation tiles are all dense for the first factor, they have different computation efficiency. <ul>
<li>Usually, the smaller the computation tile is, the less efficient it is. Because it is harder to saturate all the available cores. </li>
<li>Some carefully designed coordination of threads could significantly improve the computation efficiency of small computation tiles, leading to many efficient small computation tiles. </li>
<li>These well-optimized computation tiles are stored in a tile database of SparDA and serve as the base of STiles. </li>
</ul>
</li>
<li>All the STiles can be applied to a given sparsity for the second factor but lead to varied computation efficiency. <ul>
<li>If the data tile of an STile is larger than the granularity of the given sparsity, a proportion of the computation is wasted. </li>
<li>If the data tile is much smaller than the sparsity granularity, the computation tile is inefficient. </li>
</ul>
</li>
<li>In online dynamic sparsity, the most suitable STile is chosen based on several representative samples. <ul>
<li>It traverses all the STiles in the tile database to compute their cost on the given dynamically sparse operator and picks the best. </li>
<li><code>CoverAlgo</code> outputs the number of STiles needed to cover all the non-zero values of a given sparsity sample. The cost is the sum of the n sparsity samples. </li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">@param  OP       : a dynamically sparse operator,</span></span><br><span class="line"><span class="string">        D_sparse : a list of n sparsity samples of Op</span></span><br><span class="line"><span class="string">@return Best_tile: the best STile for Op</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ChooseStile</span>(<span class="params">D_sparse, Op</span>):</span><br><span class="line">  Best_stile, Cost_opt = null, inf</span><br><span class="line">  <span class="keyword">for</span> S <span class="keyword">in</span> GetStileFromTileDB(Op):</span><br><span class="line">    Cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> D <span class="keyword">in</span> D_sparse:</span><br><span class="line">      num_stiles = CoverAlgo(D, S.data_tile)</span><br><span class="line">      Cost += Num_stiles * S.tile_cost</span><br><span class="line">    <span class="keyword">if</span> Cost &lt; Cost_opt:</span><br><span class="line">      Best_stile = S</span><br><span class="line">      Cost_opt = Cost</span><br><span class="line">  <span class="keyword">return</span> Best_stile</span><br></pre></td></tr></table></figure>
<h2 id="How-to-represent-dynamic-sparsity"><a href="#How-to-represent-dynamic-sparsity" class="headerlink" title="How to represent dynamic sparsity?"></a>How to represent dynamic sparsity?</h2><ol>
<li>The representation is a sparsity attribute that can be efficiently constructed and parsed while consuming less memory. <ul>
<li>The sparsity attribute combines a $0-1$ attribute matrix along with a sparsity granularity. Each value in the attribute matrix represents a data block’s existence, which is the sparsity granularity’s size. <ul>
<li>One type it can represent is that the location of sparse values keeps changing while the granularity is the same. Another type allows the granularity to change. </li>
</ul>
</li>
<li>The sparsity granularity is in the form of $(S_{dim1},\dots , S_{dimN} )$, where $S_{dim}$ is the size of the granularity on dimension dim. </li>
</ul>
</li>
<li>During online model execution, SparDA detects the annotated sparsity and builds the index of the non-zero blocks in every sparse tensor. <ul>
<li>The non-zero blocks are in the granularity of the data tile of the chosen STile. The blocks are translated into a bunch of STiles online. </li>
<li>SparDA constructs the sparsity index in an out-of-order manner because the permutation invariant property relaxes the order of the indices in a sparse data format. </li>
</ul>
</li>
<li>Unlike traditional sparse data format, which has data in it, SparDA only constructs an index while leaving the data as is. The index directly references the data blocks in their original tensor. <ul>
<li>STile uses the index to load the data blocks across memory hierarchies and rearranges the data blocks on-the-fly into the dense format. </li>
</ul>
</li>
</ol>
<h2 id="What-are-the-APIs-of-SparDA-What-is-its-working-pipeline"><a href="#What-are-the-APIs-of-SparDA-What-is-its-working-pipeline" class="headerlink" title="What are the APIs of SparDA? What is its working pipeline?"></a>What are the APIs of SparDA? What is its working pipeline?</h2><ol>
<li>Its working pipeline is as follows:  <ul>
<li>To make PyTorch sparsity-aware, we first integrated the representation of dynamic sparsity into PyTorch with a class called <code>DSparsity</code>. </li>
<li>Users can annotate the arbitrary dynamic sparsity pattern with a unified interface called <code>SetDSparsity</code>. </li>
<li>After annotation, SparDA builds the sparse indexes through the fast index constructor with negligible overhead. </li>
<li>After index construction, the STile optimization policy will choose an appropriate STile from the STile database according to the offline profiled performance table. </li>
<li>The just-in-time code generator emits and compiles the corresponding STile for sparse computation. </li>
</ul>
</li>
<li>SparDA has already constructed around $1500$ STiles from the dense computation kernels and profiles the performance of these STiles under different sparsity ratios. </li>
<li>Users can easily customize the online STile optimization policy through the interface <code>RegisterOptPolicy</code> for different scenarios. <ul>
<li>Users can also easily expand more STiles by adding corresponding dense computation kernels and their tensor expressions into the database. </li>
</ul>
</li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ol>
<li>The authors evaluate SparDA on four representative dynamic sparse scenarios: MoE models, dynamic sparsity caused by different sequence lengths, dynamic sparse algorithms, and sparse training. </li>
<li>They compared SparDA with the state-of-art dense and sparse baselines: PyTorch (v1.11.0) and PyTorch with state-of-art sparse kernels (PyTorch-S). <ul>
<li>We integrate the state-of-the-art sparse libraries to construct PyTorch-S, including cuSPARSE (v11.6), Sputnik, and Triton. We select the best performance of all the sparse libraries as the final results of PyTorch-S. </li>
</ul>
</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Sys4AI/" rel="tag"><i class="fa fa-tag"></i> Sys4AI</a>
              <a href="/tags/Sparsity/" rel="tag"><i class="fa fa-tag"></i> Sparsity</a>
              <a href="/tags/Accelerator/" rel="tag"><i class="fa fa-tag"></i> Accelerator</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/10/04/Paper/Sys4AI/ELF/" rel="prev" title="Elf">
                  <i class="fa fa-chevron-left"></i> Elf
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/10/09/Paper/Sys4AI/Parameter-Server/" rel="next" title="Parameter Server">
                  Parameter Server <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
