<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/about/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/about/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"about/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2024/03/13/OpenSource/TinyKV/Multi-Raft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/13/OpenSource/TinyKV/Multi-Raft/" class="post-title-link" itemprop="url">Placement Driver and Multi-Raft</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-03-13 23:26:08" itemprop="dateCreated datePublished" datetime="2024-03-13T23:26:08+08:00">2024-03-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-17 11:48:36" itemprop="dateModified" datetime="2024-03-17T11:48:36+08:00">2024-03-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/TiKV-TinyKV/" itemprop="url" rel="index"><span itemprop="name">TiKV/TinyKV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Placement-Driver"><a href="#Placement-Driver" class="headerlink" title="Placement Driver"></a>Placement Driver</h1><h2 id="How-does-PD-know-the-information-of-regions"><a href="#How-does-PD-know-the-information-of-regions" class="headerlink" title="How does PD know the information of regions?"></a>How does PD know the information of regions?</h2><ol>
<li>The PD needs the necessary information to decide on placement and split. This information is sent to the PD by the heartbeat from the region or the store to the PD. </li>
<li>The store will send a heartbeat to the PD periodically when the onSchedulerStoreHeartbeatTick is triggered by the ticker. <ul>
<li>This heartbeat mainly tells the PD about the space information, e.g., disk capacity, used size, and available space. </li>
<li>The PD will update its knowledge about the given store according to the heartbeat. </li>
</ul>
</li>
<li>The region will send a heartbeat to the PD when something about the region is changed, e.g., peer joined or left, split. <ul>
<li>This heartbeat mainly tells the PD about the region information, e.g., region metadata, peer identity, lagged peers, and the approximate size of this region. </li>
<li>The PD will justify the validation of this heartbeat by the RegionEpoch of it. The region information will updated if it is valid. </li>
</ul>
</li>
</ol>
<h2 id="How-does-PD-schedule-placement"><a href="#How-does-PD-schedule-placement" class="headerlink" title="How does PD schedule placement?"></a>How does PD schedule placement?</h2><ol>
<li>The schedule decisions are made periodically. However, they are not sent to region leaders immediately. The operators are returned when the PD receives a heartbeat from the region leader. </li>
<li>Every PD server has a <code>RaftCluster</code> that stores the information of the entire cluster. It handles the heartbeats sent to PD. </li>
<li>Each <code>RaftCluster</code> has a <code>coordinator</code> to run schedulers in parallel and store generated operators in its <code>schedule.OperatorController</code>. </li>
<li>When a region heartbeat is processed, the <code>RaftCluster</code> would ask for the <code>schedule.OperatorController</code> to <code>Dispatch</code> operators to the region. </li>
<li>These operators are sent to leaders as Raft commands. Similar to commands from clients, they may not be committed. </li>
</ol>
<h2 id="What-are-the-schedulers"><a href="#What-are-the-schedulers" class="headerlink" title="What are the schedulers?"></a>What are the schedulers?</h2><ol>
<li>They all implemented the <code>Scheduler</code> control that provides the <code>Schedule(opt.Cluster) *operator.Operator</code> interface to generate operators. </li>
<li>A new goroutine is created to perform the schedule when a new scheduler is added to the <code>coordinator</code>. If new operators are created, they will be appended to the <code>schedule.OperatorController</code> waiting for the next heartbeat from the target region. </li>
<li>One kind of scheduler is to transfer a leader when the current leader is no longer considered available. </li>
<li>Another scheduler is to move regions between stores to balance the storage usage of each store. </li>
</ol>
<h1 id="Multi-Raft"><a href="#Multi-Raft" class="headerlink" title="Multi-Raft"></a>Multi-Raft</h1><h2 id="How-to-split-regions"><a href="#How-to-split-regions" class="headerlink" title="How to split regions?"></a>How to split regions?</h2><ol>
<li>Every peer periodically checks whether their region needs to be split by sending a <code>SplitCheckTask</code> to the <code>splitCheckHandler</code> of its store. <ul>
<li>The checker will calculate the total size of the key-value pairs in this region. A split is initiated if the size is larger than <code>splitSize</code>. </li>
<li>The checker does not read through Raft. Instead, it directly reads from the DB engine file. </li>
<li>The checker will also notify the peer of the current size of its region it just calculated for the peer to update its information and tell the PD later. </li>
</ul>
</li>
<li>When a split key is generated, the checker will notify the peer to be prepared for splitting. <ul>
<li>The peer would ask the PD to assign globally unique IDs to identify the new region and peers. </li>
<li>The PD will send the leader an <code>AdminRequest</code> that contains the <code>NewRegionId</code> and <code>NewPeerIds</code> to suggest a split. </li>
</ul>
</li>
<li>If the <code>AdminRequest</code> is committed, all peers of the split region will create a new peer of the new region in their store. <ul>
<li>Notably, all data in the different regions of the same store are written into the same DB engine file. </li>
<li>There is no need to copy or move files when creating new peers. The only thing need to do is to set the new peers in the correct state. </li>
<li>The information on new peers must be updated in both the store and the PD. </li>
</ul>
</li>
</ol>
<h2 id="How-to-move-regions"><a href="#How-to-move-regions" class="headerlink" title="How to move regions?"></a>How to move regions?</h2><ol>
<li>When we split the regions, no data is deleted from the store, which seems useless. However, splitting regions allows us to have finer-grained control. We can move regions to new stores later. </li>
<li>A move consists of adding a new peer and removing an existing peer. <ul>
<li>When adding a new peer, it is in a lagged state. The leader will send a snapshot to bring it up to date. </li>
<li>When removing an existing peer, the data of its region will be removed from the store engine file. </li>
</ul>
</li>
<li>Similar to split, peer changing needs to sync with the store and the PD. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2024/03/12/OpenSource/TinyKV/MVCC-Transaction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/12/OpenSource/TinyKV/MVCC-Transaction/" class="post-title-link" itemprop="url">MVCC Transaction</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-03-12 15:44:46" itemprop="dateCreated datePublished" datetime="2024-03-12T15:44:46+08:00">2024-03-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-17 11:58:37" itemprop="dateModified" datetime="2024-03-17T11:58:37+08:00">2024-03-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/TiKV-TinyKV/" itemprop="url" rel="index"><span itemprop="name">TiKV/TinyKV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<p><img src="/imgs/TiKV/mvcc.png"></p>
<h1 id="How-does-the-MVCC-layer-record-operations"><a href="#How-does-the-MVCC-layer-record-operations" class="headerlink" title="How does the MVCC layer record operations?"></a>How does the MVCC layer record operations?</h1><ol>
<li>There are three column families: <code>default</code> to hold user values, <code>lock</code> to store locks and <code>write</code> to record changes. <ul>
<li>The <code>lock</code> CF is accessed using the user key; it stores a serialized <code>Lock</code> data structure. </li>
<li>The <code>default</code> CF is accessed using the user key and the start timestamp of the transaction in which it was written; it stores the user value only. </li>
<li>The <code>write</code> CF is accessed using the user key and the commit timestamp of the transaction where it was written; it stores a <code>write</code> data structure. </li>
</ul>
</li>
<li>The key idea is that <code>default</code> CF stores all the written key-value pairs, including those uncommitted ones. </li>
<li>Whether a key-value pair is committed is determined by whether a record in the <code>write</code> CF contains the <code>StartTS</code> of the <code>default</code> CF. <ul>
<li>All readings are done by first iterating over the <code>write</code> CF to find the visible entry according to the commit timestamps integrated into the key of <code>write</code> CF. </li>
<li>We want the <code>write</code> CF to be sorted by the time from the most recent to the earliest and grouped by keys. Hence, keys are encoded so that the ascending order of encoded keys is ordered first by user key (ascending), then by timestamp (descending). </li>
</ul>
</li>
</ol>
<h1 id="How-do-clients-manage-transactions"><a href="#How-do-clients-manage-transactions" class="headerlink" title="How do clients manage transactions?"></a>How do clients manage transactions?</h1><ol>
<li>Each transaction is represented by a unique start timestamp provided by clients. All APIs provided to clients are executed through a <code>MvccTxn</code> structure to perform the MVCC operations. </li>
<li>Clients can issue as much <code>KvGet</code>, <code>KvPreWrite</code>, or <code>KvScan</code> as they want during a transaction. </li>
<li>After the operations they want have been executed, the transaction can be committed by <code>KvCommit</code>. If the commit fails, it is rollbacked by <code>KvBatchRollback</code>. </li>
<li>Clients can also use <code>KvCheckTxnStatus</code> to check the status of their transactions. If the time-to-live of any lock is expired, the transaction must be rollbacked.</li>
</ol>
<h1 id="How-to-perform-Get-Prewrite-and-Scan"><a href="#How-to-perform-Get-Prewrite-and-Scan" class="headerlink" title="How to perform Get, Prewrite, and Scan?"></a>How to perform Get, Prewrite, and Scan?</h1><ol>
<li>In a <code>Get(key)</code>, <ul>
<li>If the <code>key</code> is locked by an older transaction, we should not read it now because the modification of the older transaction may be visible in this current transaction. </li>
<li>If the <code>key</code> is locked by a younger transaction, it does not matter because the modification is not visible to us. </li>
<li>The MVCC layer gets values by checking the <code>write</code> CF to find the most recently committed write. Then, access the value of the key with the <code>StartTS</code> information provided by the write record from the <code>default</code> CF. </li>
</ul>
</li>
<li>In a <code>PreWrite(key, value)</code>, <ul>
<li>This transaction must hold an exclusive lock, i.e., it must wait if either an older or a younger transaction held the lock. </li>
<li>Also, the most recent committed write must be before the start time of this transaction, i.e., there is no write committed to this key after this transaction is initiated. </li>
<li>During the <code>PreWrite</code>, value modifications are written into the <code>default</code> CF in the MVCC layer. However, due to missing records in the <code>write</code> CF, they are not visible in the <code>Get</code> operations. </li>
</ul>
</li>
<li>The <code>Scan</code> is similar to <code>Get</code>. It is done through a <code>Scanner</code> that uses an <code>engine_util.DBIterator</code> to iterator over the <code>write</code> CF. </li>
</ol>
<h1 id="How-to-perform-commit-and-rollback"><a href="#How-to-perform-commit-and-rollback" class="headerlink" title="How to perform commit and rollback?"></a>How to perform commit and rollback?</h1><ol>
<li>In a <code>KvCommit</code>, <ul>
<li>Each modification must be assured that this transaction holds the exclusive lock. </li>
<li>Notably, this transaction may have already been committed or rollbacked due to the unreliable network. </li>
<li>During the commit, the modification records of <code>WriteKindPut</code> are written into the <code>write</code> CF so that the latter transactions can see the results. </li>
<li>Finally, all locks held by this transaction will be released when the commit succeeds. </li>
</ul>
</li>
<li>If the commit fails or the clients detect that the transaction is out of TTL for the lock of the primary key, <code>KvBatchRollback</code> will be issued. <ul>
<li>The records of the kind <code>WriteKindRollback</code> of the modified keys are written into the <code>write</code> CF and the value of the <code>default</code> CF is required to be deleted. </li>
<li>Similar to <code>KvCommit</code>, there is a possibility that this transaction is already committed or rollbacked. </li>
</ul>
</li>
<li>When the client encounters a lock failure, it will issue a <code>KvResolveLock</code> to commit or rollback. <ul>
<li><code>Lock</code> CF is iterated to find all keys locked by this transaction. Those are all keys to be resolved and will either be committed or rollbacked. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2024/02/22/Courses/15445/00-Trade-offs-summary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/02/22/Courses/15445/00-Trade-offs-summary/" class="post-title-link" itemprop="url">00. Trade-offs summary</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-02-22 18:39:09" itemprop="dateCreated datePublished" datetime="2024-02-22T18:39:09+08:00">2024-02-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-15 15:07:34" itemprop="dateModified" datetime="2024-03-15T15:07:34+08:00">2024-03-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<ol>
<li><strong>Database</strong> v.s. <strong>CSV files</strong>: <ul>
<li>Database provided a better encapsulation representation that provides ensurence of data integrety, necessary operations and durability. Only SQL is exposed to the users. </li>
<li>CSV files requires used to write all operations, e.g. searching and writing, by themselves. It is enough for small amount of data. </li>
<li>Database can be easily used to manage complex or enormously large data which can be difficult for CSV files. </li>
</ul>
</li>
<li><strong>Disk-oriented DBMS</strong> v.s. <strong>OS memory mapping</strong>:<ul>
<li>The basic idea is that DBMS always knows better than OS. </li>
<li>Transaction safety: OS can flush dirty pages at any time causing dirty data corrupt database. </li>
<li>Error handling: DBMS may isolate the error and handle it only in the storage layer. </li>
</ul>
</li>
<li></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/12/09/OpenSource/TinyKV/TinyKV-vs-Spanner/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/12/09/OpenSource/TinyKV/TinyKV-vs-Spanner/" class="post-title-link" itemprop="url">TinyKV v.s. Spanner</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-12-09 17:38:00" itemprop="dateCreated datePublished" datetime="2023-12-09T17:38:00+08:00">2023-12-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-17 12:29:58" itemprop="dateModified" datetime="2024-03-17T12:29:58+08:00">2024-03-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/TiKV-TinyKV/" itemprop="url" rel="index"><span itemprop="name">TiKV/TinyKV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Organization"><a href="#Organization" class="headerlink" title="Organization"></a>Organization</h1><h2 id="What-is-the-structure-of-TinyKV"><a href="#What-is-the-structure-of-TinyKV" class="headerlink" title="What is the structure of TinyKV?"></a>What is the structure of TinyKV?</h2><ol>
<li>Each machine has a <code>Server</code> with a certain object that implements a <code>Storage</code> interface to store Key/Value pairs. <ul>
<li>The <code>Storage</code> interface only requires the provision of <code>Read</code> and <code>Write</code> APIs. The way they implement these methods decided this <code>Server</code> can provide the service. </li>
<li>Suppose that <code>Storage</code> is simply using <code>badger.DB</code> that directly writes data, the <code>Server</code> only provides a standalone database service. </li>
<li>If that <code>Storage</code> is <code>RaftStorage</code> that completes these methods by interacting with Raft groups, it can provide a distributed fault-tolerance database. </li>
</ul>
</li>
<li>Inside <code>RaftStorage</code>, there is a <code>RaftStore</code> that controls all <code>peer</code>s. Each peer is a peer in a raft group. <ul>
<li>When multi-raft is enabled, several peers in the same <code>RaftStore</code> may manage different shards. </li>
</ul>
</li>
<li>Overall, a <code>RaftStorage</code> is responsible for an entire key space. All peers in the same key space, though may be in charge of incontinuous shards, must communicate with clients through this <code>RaftStorage</code>. <ul>
<li>If another key space exists, e.g., another database, a separate <code>RaftStorage</code> must be created. </li>
</ul>
</li>
</ol>
<h2 id="How-to-execute-a-command-from-a-client"><a href="#How-to-execute-a-command-from-a-client" class="headerlink" title="How to execute a command from a client?"></a>How to execute a command from a client?</h2><ol>
<li>The following is a diagram of the execution flow. </li>
<li>Clients give commands through the APIs provided by the <code>Server</code>. Those APIs will execute <code>Read</code> or <code>Write</code> provided by <code>Storage</code>. </li>
<li><code>RaftStorage</code> will send a message of <code>raft_cmdpb.Request</code> to <code>RaftStore</code> through <code>RaftRouter</code>. The request contains <code>regionId</code> and <code>peerId</code> information for the router to locate the destination. </li>
<li>The <code>RaftRouter</code> will send the request to the <code>RaftWorker</code> of <code>RaftStore</code>. When the <code>RaftWorker</code> receives messages, it will initiate a <code>peerMsgHandler</code> for each message individually. <ul>
<li><code>RaftWorker</code> will start the next <code>peerMsgHandler</code> until the last one is finished instead of creating goroutines to process them in parallel. </li>
<li>Raft peers will only receive commands and messages from <code>RaftWorker</code>. Hence, only one Raft thread in the entire <code>RaftStore</code> processes messages rather than handling them in parallel. </li>
</ul>
</li>
<li><code>peerMsgHandle</code> will ask <code>RaftGroup</code> to handle the message by <code>Step</code>. </li>
<li>After all read messages are handled, all peers that have received messages are asked to handle <code>raft.Ready</code> one be one. Applied commands will be executed; persisted states will be written into <code>PeerStorage</code>. </li>
</ol>
<p><img src="/imgs/TiKV/exec.png"></p>
<h1 id="Time-Representation"><a href="#Time-Representation" class="headerlink" title="Time Representation"></a>Time Representation</h1><h2 id="How-does-Raft-represent-time"><a href="#How-does-Raft-represent-time" class="headerlink" title="How does Raft represent time?"></a>How does Raft represent time?</h2><ol>
<li>Instead of using a TrueTime API to represent time like Spanner, TinyKV uses a logical time to control workflows. </li>
<li></li>
<li>All peers in the same store should register on <code>tickDriver</code> so that it will know which peers it needs to send messages to when events are triggered. </li>
<li>Each store has a store ticker, while each peer has a peer ticker. The store ticker is the driver of the entire store; it represents the logical time of this machine. The peer tickers are simply for scheduling peer events. </li>
<li>When the store ticker received from <code>time.Tick(baseTickInterval)</code>, <ul>
<li>A <code>MsgTypeTick</code> will be sent to all known peers. Each peer will check whether their schedule is up. </li>
<li>Then, the <code>tick</code> is increased by $1$ and <code>tickDriver</code> checks the store’s schedule. If a schedule is up, the driver will send a message of <code>MsgTypeStoreTick</code> to the store. </li>
</ul>
</li>
</ol>
<h2 id="What-events-are-scheduled-in-peers"><a href="#What-events-are-scheduled-in-peers" class="headerlink" title="What events are scheduled in peers?"></a>What events are scheduled in peers?</h2><ol>
<li>When the <code>tickDrive</code> of <code>RaftStore</code> sends a <code>MsgTypeTick</code> to <code>RaftWorker</code>, the <code>peerMsgHandler</code> will check several schedules. </li>
<li>When <code>PeerTickRaft</code> is up, the Raft node ticker is moved forward. </li>
<li>When <code>PeerTickRaftLogGC</code> is up, and existing applied entries are more than the limit, it will propose a Raft command with an administrator request to compact the Raft log to the applied index when this request is made. <ul>
<li>The compaction is only executed when this request is committed. </li>
<li>The Raft node and peer storage modify their state according to the compact index and term. Then, a garbage collection task is sent to the <code>raftLogGCWorker</code>, which will remove compacted entries from <code>RaftDB</code>. </li>
</ul>
</li>
<li>When <code>PeerTickSchedulerHeartbeat</code> is up, a <code>SchedulerRegionHeartbeatTask</code> is sent to <code>schedulerWorker</code>. </li>
</ol>
<h2 id="How-do-transactions-represent-timestamps"><a href="#How-do-transactions-represent-timestamps" class="headerlink" title="How do transactions represent timestamps?"></a>How do transactions represent timestamps?</h2><ol>
<li>The placement driver is also responsible for assigning client timestamps for transaction events. <ul>
<li>The placement driver is another Raft group powered by etcd. Only the leader can assign timestamps. When the physical timestamp is changed, the leader will save a timestamp seconds later than the new one. </li>
<li>When a new leader is elected, it will read from the etcd files to sync its timestamp with the last saved timestamp. The new leader needs to wait until the time of the read timestamp comes before it can assign any new timestamp. </li>
<li>We can induct that the timestamp assigned by the PD group is unique and monotonously increasing. Therefore, the TiKV did not take the TrueTime API in Spanner. </li>
</ul>
</li>
<li>A transaction timestamp is an <code>uint64</code> that consists of two parts: <ul>
<li>The high bits are the physical time with units of milliseconds. The lower bits are the logical time to distinguish the events in the same millisecond. </li>
<li>It is reset to zero every time the physical time is updated to prevent logical time from becoming too large. </li>
</ul>
</li>
<li>The server will check whether need to update the physical timestamp every $50\ ms$, instead of every millisecond. <ul>
<li>The physical timestamp will sync with the current time when it has not been updated for over $150\ ms$. </li>
<li>When used over half of the logical timestamp, the physical timestamp will add $1\ ms$ and reset the logical timestamp. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/11/24/OpenSource/TinyKV/Storage-Write-and-Read/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/11/24/OpenSource/TinyKV/Storage-Write-and-Read/" class="post-title-link" itemprop="url">Storage: Write and Read</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-11-24 11:18:35" itemprop="dateCreated datePublished" datetime="2023-11-24T11:18:35+08:00">2023-11-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-17 12:21:20" itemprop="dateModified" datetime="2024-03-17T12:21:20+08:00">2024-03-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/TiKV-TinyKV/" itemprop="url" rel="index"><span itemprop="name">TiKV/TinyKV</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Key-Value-Storage-storage-Storage"><a href="#Key-Value-Storage-storage-Storage" class="headerlink" title="Key-Value Storage (storage.Storage)"></a>Key-Value Storage (storage.Storage)</h1><ol>
<li>Each <code>Server</code> uses the KV storage interfaces <code>Write</code> and <code>Reader</code> to implement RPC interfaces like <code>RawGet</code>, <code>RawPut</code>, <code>RawDelete</code>, and <code>RawScan</code> (in <code>kv/storage/server/raw_api.go</code>) provided to clients for corresponding functions. </li>
<li>This higher-level abstract storage interface hides lower-level storage implementation from the servers and clients. How do we implement the interface changes in different scenarios? <ul>
<li>In <code>kv/storage/mem_storage.go</code>, it is implemented as an in-memory standalone storage. </li>
<li>In <code>kv/storage/standalone_storage/standalone_storage.go</code>, it is implemented as BadgerDB. </li>
<li>In <code>kv/storage/raft_storage/raft_server.go</code>, it is implemented as a distributed storage based on Raft. </li>
</ul>
</li>
</ol>
<h2 id="Interface"><a href="#Interface" class="headerlink" title="Interface"></a>Interface</h2><ol>
<li><p>The storage interface must implement <code>Write</code> and <code>Reader</code>. </p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// kv/storage/storage.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Storage <span class="keyword">interface</span> &#123;</span><br><span class="line">	Write(ctx *kvrpcpb.Context, batch []Modify) <span class="type">error</span></span><br><span class="line">	Reader(ctx *kvrpcpb.Context) (StorageReader, <span class="type">error</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>Write</code> receives a batch of modification requests. Each request can be a put request with a key-value pair or a delete request with only a key field. </p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// kv/storage/modify.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Modify <span class="keyword">struct</span> &#123;</span><br><span class="line">	Data <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Put <span class="keyword">struct</span> &#123;</span><br><span class="line">	Key   []<span class="type">byte</span></span><br><span class="line">	Value []<span class="type">byte</span></span><br><span class="line">	Cf    <span class="type">string</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Delete <span class="keyword">struct</span> &#123;</span><br><span class="line">	Key []<span class="type">byte</span></span><br><span class="line">	Cf  <span class="type">string</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>The <code>Reader</code> returns an API to get the value of a certain key or iterate over a column family. </p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// kv/storage/storage.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> StorageReader <span class="keyword">interface</span> &#123;</span><br><span class="line">	<span class="comment">// When the key doesn&#x27;t exist, return nil for the value</span></span><br><span class="line">	GetCF(cf <span class="type">string</span>, key []<span class="type">byte</span>) ([]<span class="type">byte</span>, <span class="type">error</span>)</span><br><span class="line">	IterCF(cf <span class="type">string</span>) engine_util.DBIterator</span><br><span class="line">	Close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>The <code>DBIterator</code> is another interface that provides APIs to iterate over the database. </p>
<ul>
<li><code>Item()</code> returns a pointer to the current key-value pair. </li>
<li><code>Valid()</code> returns false when iteration is done. </li>
<li><code>Next()</code> would advance the iterator by one. </li>
<li><code>Seek([]byte)</code> would seek the provided key if present. If absent, it would seek the next smallest key greater than provided. </li>
<li><code>Close()</code> would close the iterator. </li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// kv/util/engine_util/cf_iterator.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> DBIterator <span class="keyword">interface</span> &#123;</span><br><span class="line">	Item() DBItem</span><br><span class="line">	Valid() <span class="type">bool</span></span><br><span class="line">	Next()</span><br><span class="line">	Seek([]<span class="type">byte</span>)</span><br><span class="line">	Close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>The access to <code>DBIterm</code> is an interface that provides methods to get or copy the keys and values. </p>
<ul>
<li><code>KeyCopy(dst []byte) byte</code> and <code>ValueCopy(dst []byte) ([byte, error])</code> returns a copy of the key or value of the item, writing it to the <code>dst</code> slice. If <code>nil</code> is passed, or the capacity of <code>dst</code> isn’t sufficient, a new slice would be allocated and returned. </li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> DBItem <span class="keyword">interface</span> &#123;</span><br><span class="line">	Key() []<span class="type">byte</span></span><br><span class="line">	KeyCopy(dst []<span class="type">byte</span>) []<span class="type">byte</span></span><br><span class="line">	Value() ([]<span class="type">byte</span>, <span class="type">error</span>)</span><br><span class="line">	ValueSize() <span class="type">int</span></span><br><span class="line">	ValueCopy(dst []<span class="type">byte</span>) ([]<span class="type">byte</span>, <span class="type">error</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>A <code>BadgerIterator</code> and <code>CFItem</code> is implemented that supports <code>DBIterator</code> and <code>DBItem</code>. </p>
<ul>
<li><code>func NewCFIterator(cf string, txn *badger.Txn) *BadgerIterator</code> can be used to create an iterator. </li>
<li>They are wrappers of <code>badger.Iterator</code> and <code>badger.Item</code> to support column family in <code>Valid</code> and <code>Seek</code> easier. </li>
<li>The column family prefix is stored inside and will be checked in <code>BadgerIterator.Valid</code> or removed in <code>CFItem.Key</code> and <code>CFItem.KeyCopy</code>. </li>
</ul>
</li>
</ol>
<h2 id="Simple-example-Project1-StandaloneKV"><a href="#Simple-example-Project1-StandaloneKV" class="headerlink" title="Simple example: Project1 StandaloneKV"></a>Simple example: Project1 StandaloneKV</h2><ol>
<li>This project will help us learn how to write and read data with <code>BadgerDB</code> and how these interfaces are used. </li>
<li>Both writing and reading are handled through a badger transaction. <ul>
<li>The BadgerDB transaction can use <code>Set</code> and <code>Delete</code> to process write requests, and it will be committed after all write requests are finished. </li>
<li>The <code>BadgerIterator</code> uses the <code>NewIterator</code> method of the transaction to create a <code>badger.Iterator</code>. </li>
<li>When the iterator is no longer needed, <code>badger.Iterator</code> will be closed and the transaction discarded until the reader is finished. </li>
</ul>
</li>
</ol>
<h2 id="RaftStorage"><a href="#RaftStorage" class="headerlink" title="RaftStorage"></a>RaftStorage</h2><p>The code is in <code>kv/storage/raft_storage/raft_server.go</code>. </p>
<ol>
<li>In a multi-Raft scenario, a <code>Store</code> stands for an instance of tinykv-server, a <code>Peer</code> stands for a Raft node running on a Store, while a <code>Region</code> is a collection of Peers, also called a Raft group. </li>
<li>When starting a <code>RaftStorage</code>, it first starts a <code>rs.resolveWorker</code> and a <code>rs.snapWorker</code>. Then, a <code>Node</code> is started. <ul>
<li>After a series of self-examinations, <code>Node</code> will start the core <code>Raftstore</code>. </li>
<li><code>Raftstore</code> first loads peers in this store. It scans the db engine, loads all regions and their peers from it, and registers them. </li>
<li>Then, it begins to start workers to get jobs done. <ul>
<li>A <code>raftWorker</code> is used to execute Raft commands. Messages for all peers are received here, and it needs to transmit them to their destination. </li>
<li>A <code>storeWorker</code> is used to handle store commands. <ul>
<li>One kind is <code>MsgTypeStoreStart</code>, which will be sent immediately after this worker is running. </li>
<li>Another kind is <code>MsgTypeStoreTick</code> to control the <code>tick()</code> on all peers in the same store. Another <code>tikerDriver</code> is used to generate this command. </li>
<li><code>MsgTypeStoreRaftMessage</code> can redirect a misplaced message. </li>
</ul>
</li>
<li>Other background threads, e.g., garbage collection or scheduler, are started as workers with different handlers. </li>
</ul>
</li>
</ul>
</li>
<li>A <code>RaftstoreRouter</code> is created together with <code>Raftstore</code>. <ul>
<li>It has a <code>peerSender chan message.Msg</code> that will be used to communicate with <code>raftWorker</code> and another <code>storeSender chan&lt;- message.Msg</code> that will be read by the <code>storeWorker</code>. </li>
<li>It provides <code>Send</code>, <code>SendRaftMessage</code>, and <code>SendRaftCommand</code> for communication inside this store. </li>
</ul>
</li>
<li><code>Write</code> and <code>Reader</code> will generate a corresponding request of type <code>raft_cmdpb.RaftCmdRequest</code>. <ul>
<li>The request is a proposal to the Raft group, and the result will only be returned when its request entry is applied. </li>
<li>It is sent through a <code>router</code>, i.e., ignoring <code>RawXXX</code>, <code>Write</code> and <code>Reader</code> receive commands from other servers or clients and forward them to peers. </li>
<li>Then, the command will be wrapped with a <code>Callback</code> that can be used to track the execution process of this command. When the <code>Callback</code> receives the done signal, it can return the response to the <code>Write</code> or <code>Reader</code>. </li>
</ul>
</li>
</ol>
<h1 id="Raft-storage-raft-Storage"><a href="#Raft-storage-raft-Storage" class="headerlink" title="Raft storage (raft.Storage)"></a>Raft storage (raft.Storage)</h1><p>This is the storage interface for Raft nodes to persist in their internal states. Unlike the KV storage interface above, this is only used when implementing a Raft node. </p>
<h2 id="Interface-1"><a href="#Interface-1" class="headerlink" title="Interface"></a>Interface</h2><ol>
<li>The interfaces required are all read-only functions. The writings are performed by upper applications. </li>
<li><code>InitialState()</code> returns the saved <code>HardState</code> and <code>ConfState</code> information. </li>
<li><p><code>Entries(lo, hi uint64)</code> returns a slice of log entries in the range <code>[lo,hi)</code>. </p>
<ul>
<li><code>MaxSize</code> limits the total size of the log entries returned, but it returns at least one entry, if any. </li>
</ul>
</li>
<li><code>Term(i uint64)</code> returns the term of entry <code>i</code>, which must be in the range <code>[FirstIndex()-1, LastIndex()]</code>. <ul>
<li>The term of the entry before <code>FirstIndex</code> is retained for matching purposes, even though the rest of that entry may not be available. </li>
</ul>
</li>
<li><code>LastIndex()</code> returns the index of the last entry in the log. </li>
<li><code>FirstIndex()</code> returns the index of the first log entry that is possibly available via <code>Entries</code> (older entries have been incorporated into the latest Snapshot; if storage only contains the dummy entry, the first log entry is unavailable). </li>
<li><p><code>Snapshot()</code> returns the most recent snapshot. </p>
<ul>
<li>If the snapshot is temporarily unavailable, it should return <code>ErrSnapshotTemporarilyUnavailable</code>, so the raft state machine knows Storage needs some time to prepare the snapshot and call Snapshot later. </li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Storage <span class="keyword">interface</span> &#123;</span><br><span class="line">	InitialState() (pb.HardState, pb.ConfState, <span class="type">error</span>)</span><br><span class="line">	Entries(lo, hi <span class="type">uint64</span>) ([]pb.Entry, <span class="type">error</span>)</span><br><span class="line">	Term(i <span class="type">uint64</span>) (<span class="type">uint64</span>, <span class="type">error</span>)</span><br><span class="line">	LastIndex() (<span class="type">uint64</span>, <span class="type">error</span>)</span><br><span class="line">	FirstIndex() (<span class="type">uint64</span>, <span class="type">error</span>)</span><br><span class="line">	Snapshot() (pb.Snapshot, <span class="type">error</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Simple-example-MemStorage"><a href="#Simple-example-MemStorage" class="headerlink" title="Simple example: MemStorage"></a>Simple example: MemStorage</h2><ol>
<li>This is defined in <code>raft/storage.go</code>. Besides those read-only interfaces, <code>MemStorage</code> provides write methods for upper applications to change states. </li>
<li><code>SetHardState(st pb.HardState) error</code> changes the current <code>HardState</code> to the given one. </li>
<li><code>ApplySnapshot(snap pb.Snapshot) error</code> overwrites the contents of this Storage object with those of the given snapshot. <ul>
<li>In this case, it overwrites the current snapshot and truncates <code>ents</code> with only a dummy entry. </li>
</ul>
</li>
<li><code>CreateSnapshot(i uint64, cs *pb.ConfState, data []byte) (pb.Snapshot, error)</code> makes a snapshot which can be used to reconstruct the state. <ul>
<li>If any configuration changes have been made since the last compaction, the result of the last <code>ApplyConfChange</code> must be passed in. </li>
<li>The state machine’s data is acquired by the upper application, not this method. </li>
</ul>
</li>
<li><code>Compact(compactIndex uint64) error</code> discards all log entries prior to <code>compactIndex</code>. <ul>
<li>The application is responsible for not attempting to compact an index greater than <code>raftLog.applied</code>. </li>
</ul>
</li>
<li><code>Append(entries []pb.Entry) error</code> append the new entries to storage, i.e., persist Raft entries when this storage is writing to disk. </li>
</ol>
<h2 id="Project-2B-PeerStorage"><a href="#Project-2B-PeerStorage" class="headerlink" title="Project 2B: PeerStorage"></a>Project 2B: PeerStorage</h2><h3 id="Basic-duties"><a href="#Basic-duties" class="headerlink" title="Basic duties"></a>Basic duties</h3><ol>
<li>A peer storage must maintain the persistence of both Key-Value pairs in this peer and its Raft logs. <ul>
<li>The entries of Raft logs are stored with the key <code>LocalPrefix_RegionRaftPrefix_regionID_RaftLogSuffix_entry index</code>. </li>
</ul>
</li>
<li>After acquiring a value from <code>*badger.DB</code>, it can be transferred to the correct type with the <code>Unmarshal</code> provided by the <code>struct</code> generated from protobuf. </li>
<li><code>SaveReadyState</code> will be used by <code>peerMsgHandler.HandleRaftReady</code> to persist according to <code>Ready</code>. It needs to install a snapshot and update persisted new Raft log entries, including deleting the entries the new leader removed. </li>
<li>The Raft commands that manipulate key-value storages are executed through the engines provided by <code>PeerStorage</code>. </li>
</ol>
<h3 id="Snapshot-generation-and-application"><a href="#Snapshot-generation-and-application" class="headerlink" title="Snapshot generation and application"></a>Snapshot generation and application</h3><ol>
<li><p>The snapshot generation is an asynchronous process. </p>
<ul>
<li><p>When the Raft node requests a snapshot, <code>PeerStorage</code> sends a message of <code>RegionTaskGen</code> to <code>regionWorker</code> and sets its <code>snapState</code> to <code>SnapState_Generating</code> and the channel to receive a snapshot. </p>
</li>
<li><p>The snapshots are sent by a <code>snapRunner</code>, different from the ordinary entries. The snapshot will be sliced into chunks of size <code>snapChunkLen</code> and sent to the same <code>grpc.ClientStream</code>. </p>
</li>
</ul>
</li>
<li><p>The receiver will collect all data from the <code>grpc.ClientStream</code> and write them into the same snapshot file. </p>
<ul>
<li>The snapshot is first sent to Raft to check its validation. If valid, <code>PeerStorage</code> will apply it in the <code>HandleRaftReady</code> of the <code>peerMsgHandler</code>. </li>
<li>The <code>PeerStorage</code> sends a <code>RegionTaskApply</code> to <code>regionWorker</code>. The worker will clean up the range of this region and directly ingest the content of the snapshot file into the DB with <code>IngestExternalFiles</code>. </li>
</ul>
</li>
</ol>
<p><img src="/imgs/TiKV/Snapshot.png"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/10/30/OpenSource/BusTub/BusTub-Overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/30/OpenSource/BusTub/BusTub-Overview/" class="post-title-link" itemprop="url">BusTub Overview</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-30 20:04:58" itemprop="dateCreated datePublished" datetime="2023-10-30T20:04:58+08:00">2023-10-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 23:05:22" itemprop="dateModified" datetime="2024-03-16T23:05:22+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/BusTub/" itemprop="url" rel="index"><span itemprop="name">BusTub</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Shell-Execution"><a href="#Shell-Execution" class="headerlink" title="Shell Execution"></a>Shell Execution</h1><ol>
<li><p>The shell (in the <a target="_blank" rel="noopener" href="https://github.com/cmu-db/bustub/blob/master/tools/shell/shell.cpp">shell.cpp</a>) uses <code>linenoise</code> to read input lines until a line ending with <code>;</code> or beginning with <code>\</code>. </p>
<ul>
<li><code>;</code> means the end of a query while <code>\</code> leads an internal meta-command. </li>
<li>The backbone of the DBMS is <code>bustub::BustubInstance</code> (in <a target="_blank" rel="noopener" href="https://github.com/cmu-db/bustub/blob/master/src/common/bustub_instance.cpp">bustub_instance.cpp</a>). It is initialized at the beginning of the shell. </li>
<li>After reading the complete query, execute it using <code>BustubInstance.ExecuteSql</code>. The result is writen in <code>bustub::FortTableWriter</code>. </li>
</ul>
</li>
<li><p>In <code>BustubInstance.ExecuteSqlTxn</code>:</p>
<ul>
<li>First, determine whether this is an internal meta-command. If so, only execute the corresponding command and exit. </li>
<li>If this is a query, use <strong><code>bustub::Binder</code></strong> (in <a target="_blank" rel="noopener" href="https://github.com/cmu-db/bustub/blob/master/src/binder/binder.cpp#L45">binder.cpp</a>) to parse the query into an AST. Handle the output format according to the statement type using <code>HandlexxxStatement</code>. </li>
<li>Then <strong><code>bustub::Planner</code></strong> will plan the query execution according to the generated AST. And <strong><code>bustub::Optimizer</code></strong> will optimize the plan tree with certain rules. </li>
<li>Then, the plan will be executed by <strong><code>ExecutionEngine.Execute</code></strong> (in <a target="_blank" rel="noopener" href="https://github.com/cmu-db/bustub/blob/master/src/include/execution/execution_engine.h#L54">execution_engine.h</a>). </li>
<li>Finally, the result will be written in the shell with a writer. </li>
</ul>
</li>
<li>To provide easier format control, the <code>bustub::ResultWriter</code> provides interfaces for standard output. <ul>
<li><code>BeginHeader</code>, <code>EndHeader</code>, <code>BeginRow</code>, <code>EndRow</code>, and <code>BeginTable</code>, <code>EndTable</code> to output pre-defined messages. </li>
<li><code>WriteCell</code> and <code>WriteHeaderCell</code> are used to output data and pre-defined separators. </li>
<li>Its sub-classes need to override these methods to perform as they want. <ul>
<li><code>NoopWriter</code> does nothing in all these methods. </li>
<li><code>SimpleStreamWriter</code> will output simple, plain text. </li>
<li><code>HtmlWriter</code> will output the results in HTML code that can be shown in the browser. </li>
<li><code>FortTableWriter</code> uses <code>fort</code> to output table. </li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/10/12/OpenSource/BusTub/Project-4-Concurrency-Control/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/12/OpenSource/BusTub/Project-4-Concurrency-Control/" class="post-title-link" itemprop="url">Project #4: Concurrency Control</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-12 13:26:44" itemprop="dateCreated datePublished" datetime="2023-10-12T13:26:44+08:00">2023-10-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 23:46:29" itemprop="dateModified" datetime="2024-03-16T23:46:29+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/BusTub/" itemprop="url" rel="index"><span itemprop="name">BusTub</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="LockManager"><a href="#LockManager" class="headerlink" title="LockManager"></a>LockManager</h1><h2 id="Locks"><a href="#Locks" class="headerlink" title="Locks"></a>Locks</h2><ol>
<li><code>LockManager</code> is used to protect the data in the database. According to the lock granularities, the lock information is stored in <code>table_lock_map_</code> and <code>row_lock_map_</code>. </li>
<li>The interface of <code>LockManager</code> may be executed concurrently by each transaction. Hence <code>table_lock_map_latch_</code> and <code>row_lock_map_latch_</code> are necessary to protect the data of <code>LockManager</code>. </li>
<li>The information of each object is stored in <code>LockRequestQueue</code>, where all requests are stored, including both granted and waiting requests. Each queue has a <code>latch_</code> to protect its data and as the lock of the condition variable. </li>
<li>The <code>table_lock_map_latch_</code> or <code>row_lock_map_latch_</code> can be released once the <code>latch_</code> of the corresponding queue is acquired to support better throughput of processing lock requests. </li>
</ol>
<h2 id="Lock"><a href="#Lock" class="headerlink" title="Lock"></a>Lock</h2><h3 id="Before-granting-locks"><a href="#Before-granting-locks" class="headerlink" title="Before granting locks"></a>Before granting locks</h3><ol>
<li>Check whether this transaction allows the acquisition of this lock based on its isolation level and current state. </li>
<li>Check whether this transaction is upgrading its lock. <ul>
<li>If so, check whether there is another transaction that is upgrading. </li>
<li>If not, check whether it can upgrade to this new lock mode. </li>
<li>If can, set the <code>upgrading_</code> of this queue, remove the recording of the currently holding lock from the transaction, update the lock_mode_ and granted_ of the request in the queue, and use this request as the request to be granted. </li>
</ul>
</li>
<li>If this is not an upgrade, create a new request and insert it into the queue. </li>
<li>When locking a row, we must also ensure that appropriate locks are held in the table. </li>
</ol>
<h3 id="Grant-locks"><a href="#Grant-locks" class="headerlink" title="Grant locks"></a>Grant locks</h3><ol>
<li>If the transaction is not aborted, try to grant the lock. If failed, sleep the process with the condition variable. </li>
<li>The rules for granting locks are as follows:<ul>
<li>If there are granted locks and they are compatible with all granted locks, grant the lock. </li>
<li>If there is no granted lock,<ul>
<li>If there is an upgrading request, when this is the upgrading request, or this is compatible with the upgrading request, grant the lock. </li>
<li>If there is no upgrading request, grant the lock when this is the first request on the waiting list or this is compatible with the first request. </li>
</ul>
</li>
</ul>
</li>
<li>If the loop of trying to grant the lock due to the transaction is aborted, its request needs to be deleted from the queue. Otherwise, add the record of the holding the lock to the transaction. </li>
</ol>
<h2 id="UnlockTable"><a href="#UnlockTable" class="headerlink" title="UnlockTable"></a>UnlockTable</h2><ol>
<li>The transaction should not have any record of holding locks of rows of this table. </li>
<li>Find the request for this transaction, remove it from the queue, update the transaction state according to the isolation level, unlock the lock, and remove the recording from the transaction. </li>
<li>When unlocking a table, we need to ensure that no lock of rows in that table is held. </li>
</ol>
<h1 id="Concurrent-query-execution"><a href="#Concurrent-query-execution" class="headerlink" title="Concurrent query execution"></a>Concurrent query execution</h1><h2 id="SeqScan-Executor"><a href="#SeqScan-Executor" class="headerlink" title="SeqScan Executor"></a>SeqScan Executor</h2><ol>
<li>The concurrency control of deletion depends on the <code>SeqScan</code> executor. In <code>Init()</code>, we need to determine the lock type according to <code>exec_ctx_-&gt;IsDelete()</code>. <ul>
<li>If this scan is for future deletion, we must acquire an exclusive lock on rows and an intension-exclusive lock on the table. </li>
<li>Otherwise, we lock the table with an intension-shared lock if the isolation level is not <code>READ_UNCOMMITED</code>. </li>
</ul>
</li>
<li>In <code>Next</code>, if shared locks are on the previous row and the isolation level is <code>READ_COMMITTED</code>, we must first release the shared lock. Hence, we can only execute <code>++(*iter_)</code> at the beginning instead of right after fetching the row. <ul>
<li>Also, when reading tuples, we always acquire a lock first. If, after reading, the tuple does not match the predicate, we should force unlock the lock despite the lock mode. </li>
</ul>
</li>
<li>A corner case for this is that some transactions may execute a deletion before another sequential scan. Then, the later sequential scan should only acquire a shared lock while the first deletion has already acquired an exclusive lock. </li>
<li>When there is no more tuple to emit, we should unlock the shared lock this executor locks on the table. </li>
</ol>
<h2 id="Insert-Delete-and-Update-Executors"><a href="#Insert-Delete-and-Update-Executors" class="headerlink" title="Insert, Delete, and Update Executors"></a>Insert, Delete, and Update Executors</h2><ol>
<li>As aforementioned, the concurrency control of deletion depends on <code>SeqScan</code>. Therefore, we do not need to do anything extra in the <code>Delete</code> executor. </li>
<li>For insert and update, we need to acquire an intension-exclusive lock on the table in the <code>Init</code>. </li>
<li>Insert needs to acquire an exclusive lock on the inserted row, but only after the row is generated. </li>
<li>The update can use an in-place update and needs an exclusive lock before that. </li>
</ol>
<h2 id="Transaction-manager"><a href="#Transaction-manager" class="headerlink" title="Transaction manager"></a>Transaction manager</h2><ol>
<li>On abort, the transaction manager needs to restore the changes made by this transaction. The first is to restore the changes made to tables directly. <ul>
<li>The reversion must be performed in the opposite order as the modification. </li>
<li>If the transaction inserted or deleted some tuples, we must reverse the <code>is_deleted_</code> flag in metadata. </li>
<li>If the transaction updated some tuples without in-place update optimization, we need to insert a delete and an insert record. </li>
<li>If the transaction uses in-place update optimization, we must keep the old tuples and their meta in the record to restore when aborted. </li>
<li>Modification to indexes needs to be reverted similarly. </li>
</ul>
</li>
<li>After reverted modifications in abort or entered commit, all locks held by the transaction must be released. We only need to check the lock sets in a transaction. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/10/09/Paper/Sys4AI/Parameter-Server/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/09/Paper/Sys4AI/Parameter-Server/" class="post-title-link" itemprop="url">Parameter Server</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-09 19:09:31" itemprop="dateCreated datePublished" datetime="2023-10-09T19:09:31+08:00">2023-10-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-15 23:31:40" itemprop="dateModified" datetime="2024-03-15T23:31:40+08:00">2024-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Sys4AI/" itemprop="url" rel="index"><span itemprop="name">Sys4AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf">Scaling Distributed Machine Learning with the Parameter Server</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Issues:<ul>
<li>Many research settings run jobs exclusively on a cluster without contention. </li>
</ul>
</li>
<li>Challenges:<ul>
<li>Sharing imposes three challenges: <ul>
<li>Accessing the parameters requires an enormous amount of network bandwidth. </li>
<li>Many machine learning algorithms are sequential. </li>
<li>At scale, fault tolerance is critical. </li>
</ul>
</li>
<li>When solving distributed data analysis problems, the issue of reading and updating parameters shared between different worker nodes is ubiquitous. <ul>
<li>Using key-value pair abstraction to update parameters naively is inefficient: values are typically small (floats or integers), and the overhead of sending each update as a key value operation is high. </li>
</ul>
</li>
<li>For efficient operation, fault tolerance must not require a full restart of a long-running computation. </li>
</ul>
</li>
<li>Contributions:<ul>
<li>Factoring out commonly required components of machine learning systems enables application-specific code to remain concise. </li>
<li>As a shared platform to target systems-level optimizations, it provides a robust, versatile, and high-performance implementation capable of handling a diverse array of algorithms, from sparse logistic regression to topic models and distributed sketching. </li>
<li>Five key features:<ul>
<li><strong>Efficient communication</strong> is achieved through an asynchronous communication model. </li>
<li><strong>Flexible consistency models</strong> to balance algorithmic convergence rate and system efficiency. </li>
<li><strong>Elastic scalability</strong>, where new nodes can be added without restarting the running framework. </li>
<li><strong>Fault tolerance and durability</strong> provide fast recovery and well-defined behavior. </li>
<li><strong>Ease of use</strong>. </li>
</ul>
</li>
<li>Achieved synergy by picking the right systems techniques, adapting them to the machine learning algorithms, and modifying the machine learning algorithms to be more systems-friendly. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="What-is-a-conventional-distributed-ML-system"><a href="#What-is-a-conventional-distributed-ML-system" class="headerlink" title="What is a conventional distributed ML system?"></a>What is a conventional distributed ML system?</h2><ol>
<li>Training typically consists of three components: feature extraction, the objective function, and learning. <ul>
<li>Feature extraction processes the raw training data to obtain feature vectors, where each feature captures an attribute of the training data. </li>
<li>Preprocessing can be executed efficiently by existing frameworks such as MapReduce. </li>
</ul>
</li>
<li>There are three components: task scheduler, worker, and server. <ul>
<li>The parameters are stored in servers while the training data is partitioned among the workers. </li>
<li>The task scheduler first notifies each worker to execute <code>LoadData</code>. Then, in each iteration, it issues each worker to execute <code>WorkerIterate</code>. </li>
<li>Each worker has two functions:<ul>
<li><code>LoadData</code> will load and cache a part of training data, which will not change in all iterations. It will also pull the initial working set of parameters from the server.  </li>
<li><code>WorkIterate</code> will calculate the gradient based on the current parameter and send this gradient to the server before pulling new parameters. </li>
</ul>
</li>
<li>Servers only need to aggregate gradients from workers and update parameters in each iteration in <code>ServerIterate</code>. </li>
</ul>
</li>
<li>In each iteration, every worker independently uses its training data to determine what changes should be made to the weights to get closer to an optimal value. <ul>
<li>Because each worker’s updates reflect only its training data, the system needs a mechanism to allow these updates to mix. It does so by expressing the updates as a subgradient. </li>
</ul>
</li>
<li>The most expensive step in the algorithm is computing the subgradient to update $w$. This task is divided among all the workers, each executing <code>WorkerIterate</code>.  </li>
<li>The total size of <code>w</code> may exceed the capacity of a single machine. <ul>
<li>A worker needs to know a coordinate of w if and only if some of its training data references that entry. </li>
<li>A particular worker’s working set of entries can be trivially cached locally. </li>
</ul>
</li>
</ol>
<p><img src="/imgs/Sys4ai/PS/subgradient.png" width="25%"></p>
<h2 id="What-is-the-structure-of-this-new-parameter-server"><a href="#What-is-the-structure-of-this-new-parameter-server" class="headerlink" title="What is the structure of this new parameter server?"></a>What is the structure of this new parameter server?</h2><ol>
<li>Parameter server nodes are grouped into a server group and several worker groups. <ul>
<li>A server node in the server group maintains a partition of the globally shared parameters. </li>
<li>Workers communicate only with the server nodes (not among themselves), updating and retrieving the shared parameters. </li>
</ul>
</li>
<li>There is a scheduler node for each worker group. It assigns tasks to workers and monitors their progress. If workers are added or removed, unfinished tasks are rescheduled. </li>
<li>The parameter server supports independent parameter namespaces. <ul>
<li>This allows a worker group to isolate its shared parameters from others. </li>
<li>Several worker groups may also share the same namespace: we may use more than one worker group to solve the same deep learning application to increase parallelization. </li>
</ul>
</li>
</ol>
<p><img src="/imgs/Sys4ai/PS/arch.png" width="25%"></p>
<h2 id="How-to-optimize-the-cost-of-communication-of-pulling-parameters"><a href="#How-to-optimize-the-cost-of-communication-of-pulling-parameters" class="headerlink" title="How to optimize the cost of communication of pulling parameters?"></a>How to optimize the cost of communication of pulling parameters?</h2><ol>
<li>Many learning algorithms represent parameters as structured mathematical objects. <ul>
<li>Workers usually send a segment of a vector or an entire row of the matrix. </li>
</ul>
</li>
<li>Batch both the communication of updates and their processing on the parameter server and allow the consistency tracking to be implemented efficiently. <ul>
<li>This lets us treat the parameters as (key, value) pairs while endowing them with vector and matrix semantics, where non-existing keys are associated with zeros. </li>
</ul>
</li>
<li>Data is sent between nodes using push and pull operations. <ul>
<li>If $R$ is a key range, then <code>w.push(R, dest)</code> sends all existing entries of <code>w</code> in key range $R$ to the destination, a particular node, or a node group such as the server group. </li>
<li>Similarly, <code>w.pull(R, dest)</code> reads all existing entries of $w$ in key range $R$ from the destination. </li>
<li>Gradients share the keys of the worker’s working set $w$. Hence, the programmer can use w.push(R, g, dest) to save memory for local gradients. </li>
</ul>
</li>
<li>The system also supports user-defined filters to selectively synchronize individual (key, value) pairs, allowing fine-grained data consistency control within a task. <ul>
<li>The optimization algorithm usually contains information on which parameters are most valuable for synchronization. </li>
<li>One example is the significantly modified filter, which only pushes entries that have changed by more than a threshold since their last synchronization. </li>
</ul>
</li>
</ol>
<h2 id="How-to-support-flexible-consistency"><a href="#How-to-support-flexible-consistency" class="headerlink" title="How to support flexible consistency?"></a>How to support flexible consistency?</h2><ol>
<li>A remote procedure call issues tasks. It can be a push or a pull that a worker issues to servers. Tasks may include any number of subtasks. </li>
<li>Tasks are executed asynchronously. <ul>
<li>The caller can perform further computation immediately after issuing a task. </li>
<li>The caller marks a task as finished only after receiving the callee’s reply. The callee marks a task as finished only if the call of the task is returned and all subtasks issued by this call are finished. </li>
<li>A caller that wishes to serialize task execution can place an execute-after-finished dependency between tasks. </li>
</ul>
</li>
<li>We can implement different models by task dependency. <ul>
<li><strong>Sequential</strong>: All tasks are executed one by one. </li>
<li><strong>Eventual</strong>: All tasks may be started simultaneously. This is only recommendable if the underlying algorithms are robust with regard to delays. </li>
<li><strong>Bounded Delay</strong>: When a maximal delay time $\tau$ is set, a new task will be blocked until all previous tasks $\tau$ times ago have been finished. <ul>
<li>$\tau=0$ is the sequential consistency model, and an infinite delay $\tau = \infty$ becomes the eventual consistency model. </li>
</ul>
</li>
</ul>
</li>
<li>The dependency graphs may be dynamic; the caller traverses the DAG. <ul>
<li>If the graph is static, the caller can send all tasks with the DAG to the callee to reduce synchronization costs. </li>
</ul>
</li>
<li>This inconsistency potentially slows down the convergence progress. Some algorithms may be less sensitive to this type of inconsistency. <ul>
<li>The best trade-off between system efficiency and algorithm convergence rate usually depends on various factors, including the algorithm’s sensitivity to data inconsistency, feature correlation in training data, and capacity difference of hardware components. </li>
</ul>
</li>
</ol>
<p><img src="/imgs/Sys4ai/PS/dag.png" width="50%"></p>
<h2 id="Well-defined-behavior-Background"><a href="#Well-defined-behavior-Background" class="headerlink" title="Well-defined behavior (Background)"></a>Well-defined behavior (Background)</h2><h3 id="How-to-provide-a-well-defined-behavior-after-failure"><a href="#How-to-provide-a-well-defined-behavior-after-failure" class="headerlink" title="How to provide a well-defined behavior after failure?"></a>How to provide a well-defined behavior after failure?</h3><ol>
<li>To provide a well-defined behavior, we only need to find a way to determine the logical order of concurrent operations. <ul>
<li>In a single-machine situation, suppose we perform a write to key $k$ with timestamp $t_1$ and then perform another write to $k$ with timestamp $t_2$. <ul>
<li>Since $t_2 &gt; t_1$, the second write must be newer than the first so the database can safely overwrite the original value. </li>
</ul>
</li>
<li>In a distributed system, this assumption does not hold. The problem is <strong>clock skew</strong>, such as, different clocks tend to run at different rates, so we cannot assume that time $t$ on node $a$ happened before time $t + 1$ on node $b$.</li>
</ul>
</li>
<li>Lamport clock is a logical clock that provides a partial order of events. If an event $A$ causally happens before another event $B$, i.e. $A\rightarrow B$, then $timestamp(A) &lt; timestamp(B)$. <ul>
<li>Causality means that if one event leads to another, then there is a path of events from the first event to the second event. </li>
<li>The algorithm provides only a partial order of events as we cannot relate all the events with the “happened before” relationship. </li>
</ul>
</li>
<li>In a distributed system, we can define mainly 3 types of events that each process can execute: a local event, a send event, and a receive event. Then the rules are defined as: <ul>
<li>For a local event, $a \rightarrow b$, if $timestamp(a) &lt; timestamp(b)$. </li>
<li>If process $P_1$ sends a message to process $P_2$, then, $send(message) \rightarrow receive(message)$. </li>
<li>If $a \rightarrow b$ and $b\rightarrow c$ then $a \rightarrow c$. </li>
</ul>
</li>
<li>The time stamps of each node are updated as follows rules:<ul>
<li>Before executing an event, the process increments the logical timestamp by $1$. </li>
<li>During a send event, the process increments the logical timestamp by $1$ and sends the time along with the message. </li>
<li>During a receive event, the recipient’s counter is updated to the max value of its time stamp and the timestamp in the received message. It then increments the timestamp by $1$. </li>
</ul>
</li>
</ol>
<h3 id="What-is-the-problem-with-the-Lamport-clock"><a href="#What-is-the-problem-with-the-Lamport-clock" class="headerlink" title="What is the problem with the Lamport clock?"></a>What is the problem with the Lamport clock?</h3><ol>
<li>One of the shortcomings of Lamport’s clock is that it cannot identify concurrent events that are causally related. <ul>
<li>When two nodes concurrently modify the same object and send the result to a third node, the third node cannot determine which modification happens first. </li>
</ul>
</li>
<li>Instead of using integer values for the timestamp, a vector clock uses a vector of integer values to represent the timestamp. <ul>
<li>If we have $N$ processes in the group, then each process will have a vector with $N$ elements. </li>
<li>Before executing an event, the process $i$ increments the $i$-th element of its vector clock by $1$. </li>
<li>During a send event, the process $i$ increments the $i$-th element of its vector clock by $1$ and sends the vector along with the message. </li>
<li>During a receive event, the process $i$ increments the $i$-th element of its vector clock by $1$. For all other processes, it takes the maximum of the corresponding component of the incoming message and its local vector. It sets it as the corresponding element in the local clock itself. </li>
</ul>
</li>
<li>The vector clocks are compared as follows: <ul>
<li>$V_1 = V_2\iff \forall i=1\to N,V_1[i]=V_2[i]$. </li>
<li>$V_1\le V_2\iff \forall i=1\to N,V_1[i]\le V_2[i]$. </li>
<li>$V_1&lt;V_2\iff V_1\le V_2\land\exist j, V_1[j]&lt;V_2[j]$. </li>
</ul>
</li>
<li>We can identify the concurrent events when $NOT(V_1\le V_2)\land NOT(V_2\le V_1)$. <ul>
<li>When concurrent events are detected, the system must resolve the conflicts. Vector clock only provides a method to find concurrent events. </li>
<li>One way to resolve conflicts is to leave it to the client, who knows the semantics, to decide which version is correct. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-problem-of-vector-clocks-in-the-parameter-server"><a href="#What-is-the-problem-of-vector-clocks-in-the-parameter-server" class="headerlink" title="What is the problem of vector clocks in the parameter server?"></a>What is the problem of vector clocks in the parameter server?</h2><ol>
<li>Each (key, value) pair is associated with a vector clock, which records the time of each node on this (key, value) pair. Hence, a naive vector clock implementation requires $O(nm)$ space to handle $n$ nodes and $m$ parameters. <ul>
<li>With thousands of nodes and billions of parameters, this is infeasible in terms of memory and bandwidth. </li>
</ul>
</li>
<li>Many parameters have the same timestamp due to the parameter server’s range-based communication pattern. <ul>
<li>If a node pushes the parameters in a range, then the timestamps of the parameters associated with the node are likely the same. </li>
<li>Assume that $V_i(k)$ is the time of key $k$ for node $i$. Given a key range $R$, the ranged vector clock $V_i(R)=t$ means $\forall k\in R, V_i(k)=t$. </li>
</ul>
</li>
<li>Initially, there is only one range vector clock for each node $i$. It covers the entire parameter key space as its range with $0$ as its initial timestamp. Each range set may split the range and create at most $3$ new vector clocks. </li>
</ol>
<h2 id="How-to-optimize-the-cost-of-communication"><a href="#How-to-optimize-the-cost-of-communication" class="headerlink" title="How to optimize the cost of communication?"></a>How to optimize the cost of communication?</h2><ol>
<li>A worker might send the same key lists again. Hence, it is desirable for the receiving node to cache the key lists. Later, the sender only needs to send a hash of the list rather than the list itself. </li>
<li>Values may contain many zero entries. Hence, we only need to send nonzero (key, value) pairs. We use the fast Snappy compression library to compress messages, effectively removing the zeros. </li>
<li>Naive replication potentially increases network traffic by $k$ times, where $k$ is the number of replications. <ul>
<li>The parameter server framework permits replication after aggregation for many algorithms. </li>
<li>With n workers, replication uses only k/n bandwidth. Often, k is a small constant, while n is hundreds to thousands. </li>
<li>While aggregation increases the delay of the task reply, it can be hidden by relaxed consistency conditions. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-difference-in-fault-tolerance-between-a-parameter-server-and-a-conventional-distributed-system"><a href="#What-is-the-difference-in-fault-tolerance-between-a-parameter-server-and-a-conventional-distributed-system" class="headerlink" title="What is the difference in fault tolerance between a parameter server and a conventional distributed system?"></a>What is the difference in fault tolerance between a parameter server and a conventional distributed system?</h2><ol>
<li>Servers replicate parameters with consistent hashing. </li>
<li>When a worker fails, the recovery depends on the algorithm designer. <ul>
<li>If the training data is huge, recovering a worker node may be more expensive than recovering a server node. </li>
<li>Losing a small amount of training data during optimization typically only affects the model. </li>
</ul>
</li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ol>
<li>The author evaluated the system on Sparse Logistic Regression and Latent Dirichlet Allocation. </li>
<li>They compare systems by running them to reach the same objective value. A better system achieves a lower objective in less time. They also compared the worker node utilization in different systems. </li>
<li>The reduction of network traffic by each system component is measured. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/10/06/Paper/Sys4AI/SparDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/06/Paper/Sys4AI/SparDA/" class="post-title-link" itemprop="url">SparDA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-06 16:32:20" itemprop="dateCreated datePublished" datetime="2023-10-06T16:32:20+08:00">2023-10-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 11:24:36" itemprop="dateModified" datetime="2024-03-16T11:24:36+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Sys4AI/" itemprop="url" rel="index"><span itemprop="name">Sys4AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.10936">SparDA: Accelerating Dynamic Sparse Deep Neural Networks via Sparse-Dense Transformation</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Sparsity has become the most important and efficient approach to accelerate neural networks by erasing a large portion of computation without unraveling model accuracy. </li>
<li>Issues:<ul>
<li>Most commodity accelerators (e.g., GPUs and TPUs) are mainly designed for efficient dense computations. </li>
<li>Previous research proposes sparsity optimizations to fit sparse computation, which only work for fixed granularities and perform poorly when the granularity mismatches. <ul>
<li>Fine-grained computation kernels cannot well saturate hardware due to the random memory access caused by fine-grained data. </li>
</ul>
</li>
<li>Existing solutions have to use time-consuming compiling to improve the efficiency of sparse kernels in an ahead-of-time manner and thus are limited to static sparsity. </li>
<li>An efficient general index construction mechanism for all data granularities is still missing. <ul>
<li>The performance of the previous sparse index construction methodology is also poor due to the constraint of sparse computation kernels. </li>
</ul>
</li>
</ul>
</li>
<li>Challenges:<ul>
<li>The dynamic sparsity pattern in different scenarios, even with the same scenario, is quite diverse and complex. </li>
<li>The key to optimizing such dynamic sparsity is simultaneously performing the calculations with an efficient kernel without computation waste. <ul>
<li>Therefore, optimizing such complex dynamic sparsity patterns requires breaking the binding between the data and computation granularity. </li>
</ul>
</li>
</ul>
</li>
<li>Contribution:<ul>
<li>Identified an important property called <em>permutation invariant</em>. <ul>
<li>It enables SparDA to extract dynamic sparsity patterns of tensors only known at runtime with negligible overhead. </li>
<li>It can transform dynamic sparse computation into equivalent dense computation that has been highly optimized on commodity accelerators. </li>
</ul>
</li>
<li>By combining permutation invariant with computation tiling, SparDA exploits the effect of permutation invariant to allow permutation on finer-grained granularity instead of the whole row or column of a tensor. <ul>
<li>It implies that the sparsity could be finer-grained and irregular if the non-zero values can be compacted into multiple dense computation tiles. </li>
<li>Define the sparsity pattern of the non-zero values and the compacted computation tile as a sparse tile, i.e., STile. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="What-is-the-pipeline-of-SparDA"><a href="#What-is-the-pipeline-of-SparDA" class="headerlink" title="What is the pipeline of SparDA?"></a>What is the pipeline of SparDA?</h2><ol>
<li>The design of STile naturally splits sparse computation into two decoupled stages: data permutation and dense computation. <ul>
<li>Decoupling frees the computation stage from handling the intricate encoding and decoding of sparse tensors. Thus, the computation can more efficiently utilize the accelerators. </li>
<li>With the decoupled stages, SparDA can leverage a wide range of well-optimized implementations of dense computation, including hardware instructions, manually optimized kernels, and automatically tuned kernels. </li>
<li>The data permutation stage transforms sparse data into a dense format with a new primitive <code>SLoad</code>. After the computation, the produced dense data is transformed back to the required format (e.g., sparse format) with the other primitive <code>SWrite</code>. </li>
</ul>
</li>
<li>The first stage is to learn the sparsity distribution from only a few samples. <ul>
<li>The STile optimizer analyzes the sparsity of each operator. It selects the most suitable STile from a set of pre-constructed STiles, each connected to a well-optimized dense computation tile. </li>
<li>SparDA generates a sparse kernel for the operator based on the selected STile. </li>
<li>This stage can be executed during the initialization and periodically to deal with possible shifting of sparsity distribution. </li>
</ul>
</li>
<li>The second stage is applying the generated sparse kernel at runtime. <ul>
<li>To deal with the dynamically changed sparsity, SparDA detects the sparsity online and builds the index of the sparse data following the requirement of the STile. </li>
</ul>
</li>
<li>There are two components in the sparse kernel. <ul>
<li>The first one rearranges the sparse data into dense format when loading data across different memory hierarchies. </li>
<li>The second one applies dense computation to condensed data without knowing their indices. </li>
</ul>
</li>
</ol>
<h2 id="What-is-permutation-invariant"><a href="#What-is-permutation-invariant" class="headerlink" title="What is permutation invariant?"></a>What is permutation invariant?</h2><ol>
<li>Tensor Expression (TE) describes deep learning computation in existing deep learning compilers. <ul>
<li>ReduceSum: $C[p]+=A[p,l]$</li>
<li>Addition: $C[p]=A[p]+B[p]$</li>
<li>MatMul: $C[m,n]+=A[m,k]\times B[k,n]$</li>
<li>BatchMatMul: $C[b,m,n]+=A[b,m,k]\times B[b,k,n]$</li>
<li>Convolution: $C[n,f,x,y]+=A[n,m,x+i,y+j]\times B[f,m,i,j]$</li>
</ul>
</li>
<li>Definition of permutation invariant dimension:<ul>
<li>In a tensor expression $Y \leftarrow f (X_1,\dots , X_n)$, where $f$ is an operator, $X_i$ and $Y$ are its input and output tensors, respectively. </li>
<li>A dimension $k$ in the operator is <strong>permutation invariant</strong> if it satisfies: $\forall P \in \Phi_k, ∃ P’ ∈ \Phi_k \text{s.t. }P’(f(P(X_1),\dots , P(X_n))=Y$</li>
<li>$\Phi_k$ is the set of all permutation functions on $k$ dimension. $P (X)$ means a permutation function $P$ is applied to the $k$ dimension of the tensor $X$, to shuffle the elements on $k$ dimension to a new order. If $k$ dimension does not exist in $X$, $P (X) = X$. </li>
</ul>
</li>
<li>For a permutation invariant dimension, when permutation is applied to this dimension of the input tensors, a reverse permutation exists on the output tensor to make the result the same as the original computation. </li>
</ol>
<h2 id="What-are-the-rules-of-applying-permutation-invariant"><a href="#What-are-the-rules-of-applying-permutation-invariant" class="headerlink" title="What are the rules of applying permutation invariant?"></a>What are the rules of applying permutation invariant?</h2><ol>
<li>Permutation invariant of tensor dimensions can be classified into three categories:<ul>
<li><strong>Sporadic dimension</strong> exists in one or more tensors of a tensor expression but does not span all tensors. For example, $m$, $n$, $k$, $f$, $l$ of the tensor expressions. </li>
<li><strong>Prevalent dimension</strong> is the dimension that exists in all the tensors (i.e., input and output tensors) of a tensor expression. Examples of prevalent dimensions are $p$ and $b$. </li>
<li><strong>Compound dimension</strong> is the dimension that is involved in an arithmetic expression. E.g. $x$, $y$ and $i$, $j$ in Convolution. </li>
</ul>
</li>
<li>When permutation invariant is applied to only one dimension of a tensor expression, the dimension can be sporadic or prevalent but not a compound dimension. <ul>
<li>Because permuting a compound dimension violates its corresponding arithmetic expression. </li>
</ul>
</li>
<li>When permutation invariant is applied on multiple dimensions of a tensor expression: <ul>
<li>When the permuted dimensions are sporadic, each dimension can only have a single permutation function. </li>
<li>When the permuted dimensions include a prevalent dimension, the permutation function on each element of the prevalent dimension could be different. </li>
</ul>
</li>
</ol>
<h2 id="What-is-STile"><a href="#What-is-STile" class="headerlink" title="What is STile?"></a>What is STile?</h2><ol>
<li>A tile is a sliced piece of an operator’s computation. <ul>
<li>Computation tiling slices the computation into many small homogeneous pieces to parallelize the computation and increase data reuse. </li>
</ul>
</li>
<li>Permutation invariance can be applied to each tile independently; that is, the permutation functions on each tile can be different, leading to more diverse and fine-grained sparsity granularity. </li>
<li>An STile is a group of non-redundant elements following a specific type of layout associated with a dense computation tile. <ul>
<li>The non-redundant element is called the data tile, representing the sparsity granularity. The scattered data tiles can be condensed into dense tiles. </li>
<li>In reverse, a dense tile can correspond to different STiles with different permutation functions. </li>
</ul>
</li>
</ol>
<p><img src="/imgs/Sys4ai/SparDA/tile.png" width="50%"></p>
<h2 id="How-to-transform-input-and-eliminate-overhead"><a href="#How-to-transform-input-and-eliminate-overhead" class="headerlink" title="How to transform input and eliminate overhead?"></a>How to transform input and eliminate overhead?</h2><ol>
<li><strong>Sparse-Dense Transform</strong>: With permutation invariant, we can construct a permutation function $P$ to move all the redundant elements to the end while all the non-redundant elements to the front. The redundant elements can be safely removed to build a shorter $k$ dimension.  </li>
<li><code>SLoad</code> maps sparse data tiles in input tensors to dense data blocks, while <code>SWrite</code> writes the output dense data block to the specified output data format, which could be sparse or dense. <ul>
<li><code>SLoad</code> and <code>SWrite</code> work on data rearrangement when the data moves from global memory to shared memory and in reverse. </li>
<li>As long as the data tile could saturate the read/write transaction of the memory (e.g., $32$ bytes in CUDA GPUs), the data rearrangement would introduce little overhead because loading sparse data tiles does not waste memory bandwidth. </li>
<li>This property further enables zero-copy of sparse data in online dynamic sparsity scenarios because the effective data tiles can be directly selected from their original data format and written to the higher-level memory with the desired format.</li>
</ul>
</li>
</ol>
<h2 id="How-to-choose-an-efficient-STile"><a href="#How-to-choose-an-efficient-STile" class="headerlink" title="How to choose an efficient STile?"></a>How to choose an efficient STile?</h2><ol>
<li>The most efficient STile for a sparse operator is determined mainly by two factors, i.e., the efficiency of its associated dense computation tile and the operator’s dynamic sparsity. </li>
<li>Though different-sized computation tiles are all dense for the first factor, they have different computation efficiency. <ul>
<li>Usually, the smaller the computation tile is, the less efficient it is. Because it is harder to saturate all the available cores. </li>
<li>Some carefully designed coordination of threads could significantly improve the computation efficiency of small computation tiles, leading to many efficient small computation tiles. </li>
<li>These well-optimized computation tiles are stored in a tile database of SparDA and serve as the base of STiles. </li>
</ul>
</li>
<li>All the STiles can be applied to a given sparsity for the second factor but lead to varied computation efficiency. <ul>
<li>If the data tile of an STile is larger than the granularity of the given sparsity, a proportion of the computation is wasted. </li>
<li>If the data tile is much smaller than the sparsity granularity, the computation tile is inefficient. </li>
</ul>
</li>
<li>In online dynamic sparsity, the most suitable STile is chosen based on several representative samples. <ul>
<li>It traverses all the STiles in the tile database to compute their cost on the given dynamically sparse operator and picks the best. </li>
<li><code>CoverAlgo</code> outputs the number of STiles needed to cover all the non-zero values of a given sparsity sample. The cost is the sum of the n sparsity samples. </li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">@param  OP       : a dynamically sparse operator,</span></span><br><span class="line"><span class="string">        D_sparse : a list of n sparsity samples of Op</span></span><br><span class="line"><span class="string">@return Best_tile: the best STile for Op</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ChooseStile</span>(<span class="params">D_sparse, Op</span>):</span><br><span class="line">  Best_stile, Cost_opt = null, inf</span><br><span class="line">  <span class="keyword">for</span> S <span class="keyword">in</span> GetStileFromTileDB(Op):</span><br><span class="line">    Cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> D <span class="keyword">in</span> D_sparse:</span><br><span class="line">      num_stiles = CoverAlgo(D, S.data_tile)</span><br><span class="line">      Cost += Num_stiles * S.tile_cost</span><br><span class="line">    <span class="keyword">if</span> Cost &lt; Cost_opt:</span><br><span class="line">      Best_stile = S</span><br><span class="line">      Cost_opt = Cost</span><br><span class="line">  <span class="keyword">return</span> Best_stile</span><br></pre></td></tr></table></figure>
<h2 id="How-to-represent-dynamic-sparsity"><a href="#How-to-represent-dynamic-sparsity" class="headerlink" title="How to represent dynamic sparsity?"></a>How to represent dynamic sparsity?</h2><ol>
<li>The representation is a sparsity attribute that can be efficiently constructed and parsed while consuming less memory. <ul>
<li>The sparsity attribute combines a $0-1$ attribute matrix along with a sparsity granularity. Each value in the attribute matrix represents a data block’s existence, which is the sparsity granularity’s size. <ul>
<li>One type it can represent is that the location of sparse values keeps changing while the granularity is the same. Another type allows the granularity to change. </li>
</ul>
</li>
<li>The sparsity granularity is in the form of $(S_{dim1},\dots , S_{dimN} )$, where $S_{dim}$ is the size of the granularity on dimension dim. </li>
</ul>
</li>
<li>During online model execution, SparDA detects the annotated sparsity and builds the index of the non-zero blocks in every sparse tensor. <ul>
<li>The non-zero blocks are in the granularity of the data tile of the chosen STile. The blocks are translated into a bunch of STiles online. </li>
<li>SparDA constructs the sparsity index in an out-of-order manner because the permutation invariant property relaxes the order of the indices in a sparse data format. </li>
</ul>
</li>
<li>Unlike traditional sparse data format, which has data in it, SparDA only constructs an index while leaving the data as is. The index directly references the data blocks in their original tensor. <ul>
<li>STile uses the index to load the data blocks across memory hierarchies and rearranges the data blocks on-the-fly into the dense format. </li>
</ul>
</li>
</ol>
<h2 id="What-are-the-APIs-of-SparDA-What-is-its-working-pipeline"><a href="#What-are-the-APIs-of-SparDA-What-is-its-working-pipeline" class="headerlink" title="What are the APIs of SparDA? What is its working pipeline?"></a>What are the APIs of SparDA? What is its working pipeline?</h2><ol>
<li>Its working pipeline is as follows:  <ul>
<li>To make PyTorch sparsity-aware, we first integrated the representation of dynamic sparsity into PyTorch with a class called <code>DSparsity</code>. </li>
<li>Users can annotate the arbitrary dynamic sparsity pattern with a unified interface called <code>SetDSparsity</code>. </li>
<li>After annotation, SparDA builds the sparse indexes through the fast index constructor with negligible overhead. </li>
<li>After index construction, the STile optimization policy will choose an appropriate STile from the STile database according to the offline profiled performance table. </li>
<li>The just-in-time code generator emits and compiles the corresponding STile for sparse computation. </li>
</ul>
</li>
<li>SparDA has already constructed around $1500$ STiles from the dense computation kernels and profiles the performance of these STiles under different sparsity ratios. </li>
<li>Users can easily customize the online STile optimization policy through the interface <code>RegisterOptPolicy</code> for different scenarios. <ul>
<li>Users can also easily expand more STiles by adding corresponding dense computation kernels and their tensor expressions into the database. </li>
</ul>
</li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ol>
<li>The authors evaluate SparDA on four representative dynamic sparse scenarios: MoE models, dynamic sparsity caused by different sequence lengths, dynamic sparse algorithms, and sparse training. </li>
<li>They compared SparDA with the state-of-art dense and sparse baselines: PyTorch (v1.11.0) and PyTorch with state-of-art sparse kernels (PyTorch-S). <ul>
<li>We integrate the state-of-the-art sparse libraries to construct PyTorch-S, including cuSPARSE (v11.6), Sputnik, and Triton. We select the best performance of all the sparse libraries as the final results of PyTorch-S. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/10/04/Paper/Sys4AI/ELF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/10/04/Paper/Sys4AI/ELF/" class="post-title-link" itemprop="url">Elf</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-04 17:07:43" itemprop="dateCreated datePublished" datetime="2023-10-04T17:07:43+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-15 23:18:33" itemprop="dateModified" datetime="2024-03-15T23:18:33+08:00">2024-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Sys4AI/" itemprop="url" rel="index"><span itemprop="name">Sys4AI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/abs/10.1145/3447993.3448628">Elf: Accelerate High-resolution Mobile Deep Vision with Content-aware Parallel Offloading</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>To support applications running deep neural networks on multimedia data in real-time, having mobile devices offload the computation, especially the neural network inference, to edge clouds has proved effective. </li>
<li>Issues: <ul>
<li>In reality, we may not be able to find a dedicated and powerful server but must make do with less powerful ones. </li>
<li>Various techniques to make DNN models smaller to reduce the computation load lead to compromised model accuracy due to the fundamental trade-off between model size and model accuracy. </li>
<li>The inference latency can be significantly reduced by offloading the intensive model inference to a powerful edge server and with the high bandwidth and low latency provided by the emerging 5G networks. <ul>
<li>Most existing solutions use low-resolution images throughout the entire pipeline. </li>
<li>Most existing methods only consider offloading tasks between a single pair of servers and clients, assuming that no competing clients or extra edge resources are available. </li>
<li>The heterogeneous resource demands of applications running on edge servers and highly dynamic workloads by mobile users lead to resource fragmentation. </li>
</ul>
</li>
</ul>
</li>
<li>Challenges:<ul>
<li>The client must effectively partition the inference job into multiple pieces while maintaining the inference accuracy. </li>
<li>The system needs to be aware of available computation resources on each server and dynamically develop the frame partitioning solution to ensure no server in the parallel offloading procedure becomes the bottleneck. </li>
<li>Such a system should have a general framework design independent of its host deep vision applications.</li>
</ul>
</li>
<li>Contribution: <ul>
<li>The idea is to partition the video frame and offload the partial inference tasks to multiple servers for parallel processing. </li>
<li>Elf is a framework to accelerate high-resolution mobile deep vision offloading in heterogeneous client and edge server environments, by adaptively distributing the computation to available edge servers.</li>
<li>It employs a recurrent region proposal prediction algorithm, a region proposal centric frame partitioning, and a resource-aware multi-offloading scheme. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="What-is-the-pipeline-of-Elf"><a href="#What-is-the-pipeline-of-Elf" class="headerlink" title="What is the pipeline of Elf?"></a>What is the pipeline of Elf?</h2><ol>
<li>The first phase is recurrent region proposal prediction. <ul>
<li>On the mobile end, whenever a new video frame arrives, Elf predicts its region proposals (RPs) based on the ones detected in historical frames. </li>
<li>The algorithm must be lightweight, effectively learn the motion model of the objects/RPs from history frames, and pay more attention to recent frames. </li>
<li>Efficiently utilizing the historical RP inference results converts the computing-intensive image regression problem to a lightweight time series prediction problem. </li>
<li>An RP indexing algorithm keeps track of the motion across frames. A Low-Resolution Compensation scheme is proposed to handle new objects when they first appear. </li>
</ul>
</li>
<li>The second phase is frame partitioning and offloading. <ul>
<li>Ideally, a well-designed frame partitioning scheme should show a negligible overhead and have heterogeneous edge servers to finish parallel inference tasks at the same time. </li>
<li>The partitioning algorithm should be inclusive and aware of the number of and locations of RPs in a frame. Also, Elf discards background pixels that are unlikely to contain any RPs. </li>
<li>Partitions have different computation costs depending on the objects in each partition. The algorithm should consider this cost heterogeneity to achieve load balancing among the servers. </li>
<li>Unlike central clouds, edge cloud servers exhibit heterogeneous computing/storage/networking resources due to their distributed nature and high user mobility. Poor offloading may result in job stragglers completing tasks much slower than their peers, thus significantly increasing the overall latency. </li>
</ul>
</li>
<li>The last phase is partial inference and result integration. <ul>
<li>Taking the offloaded partitions as input, the edge servers run the application-specific CNN models to yield partial inference results. </li>
<li>These partial results are finally integrated on the mobile side to render the final result. </li>
</ul>
</li>
</ol>
<h2 id="Region-proposals"><a href="#Region-proposals" class="headerlink" title="Region proposals"></a>Region proposals</h2><h3 id="How-to-predict-region-proposals"><a href="#How-to-predict-region-proposals" class="headerlink" title="How to predict region proposals?"></a>How to predict region proposals?</h3><ol>
<li>An attention-based LSTM network is used to predict region proposals. <ul>
<li>It takes only the RPs of the past $N$ frames without the image of the current frame as its input and outputs the RPs of the current frame. </li>
<li>It can only handle the objects that already occurred in the previous frame. </li>
</ul>
</li>
<li>To handle new objects and reduce the computation overhead, Elf runs LRC once per n frames. <ul>
<li>$n$ is a hyperparameter, indicating the trade-off between computation cost and at most $n$-frame delay to realize new objects. </li>
<li>First, LRC down-samples a high-resolution video frame using a max-pooling operation. </li>
<li>Then Elf offloads the resized video frame and the partitions from regular-sized partitions to edge servers to run application-specific models, which usually consist of an object detection component. </li>
<li>Based on the inference results, Elf can roughly locate the new objects in the frame. </li>
</ul>
</li>
<li>The predicted RP bounding box may not cover all the pixels of an object due to motion. <ul>
<li>The bounding box is expanded by $p\%$. The downside of this scheme is the increased data transmission and computation. </li>
<li>It consults the corresponding RP position shift and the prediction confidence level as the indicators to assign different weights on $p$. </li>
</ul>
</li>
</ol>
<h3 id="How-do-we-index-region-proposals"><a href="#How-do-we-index-region-proposals" class="headerlink" title="How do we index region proposals?"></a>How do we index region proposals?</h3><ol>
<li>Vision-based matching algorithms are not considered because they introduce significant overheads in hundreds of milliseconds. </li>
<li>From the first video frame, Elf assigns a unique index to each region proposal. <ul>
<li>Elf matches each RP with the corresponding index assigned earlier in each upcoming frame. </li>
<li>If an RP includes a new object not seen before, a new index will be automatically assigned. </li>
</ul>
</li>
<li>Match the RPs across frames with a combination of RP position shift and RP area shift. <ul>
<li>The RP position shift measures the change of the center point along the x-/y-axis between the current frame and the previous frame. A larger value indicates a bigger spatial shift and, thus, a lower matching probability. </li>
<li>The RP area shift measures the area change between the RPs in two adjacent frames. A lower value indicates a higher matching probability. </li>
<li>When the x and y RP position shift are both under $0.02$ and the area shift ratio is under $0.2$, we declare a match. </li>
</ul>
</li>
</ol>
<h2 id="Offloading"><a href="#Offloading" class="headerlink" title="Offloading"></a>Offloading</h2><h3 id="How-to-optimize-Elf’s-schedule-problem"><a href="#How-to-optimize-Elf’s-schedule-problem" class="headerlink" title="How to optimize Elf’s schedule problem?"></a>How to optimize Elf’s schedule problem?</h3><ol>
<li>Assuming $M$ is the total number of RPs, while $N$ is the total number of servers; Elf packs the $M$ RP processing tasks and one LRC task into $N’$ offloading tasks ($N’≤N$ ), and offloads each task onto an edge server. </li>
<li>The overall objective of the partitioning and the offloading process is to minimize the completion time of the offloading tasks that are distributed across $N’$ edge servers, i.e., minimizing the task completion time, which has the longest execution time among all the tasks. </li>
<li>The optimization objective can be written as <ul>
<li>$\min\max(\{T^t_k\}), k\in[1,\dots,N’]\text{,}\\ \text{s.t. }\ T^t_k=T^t_{res,k}+T^t_{lrc,k}\cdot1(t\mod n=0)\cdot1(\arg\max\{p^t\}=k)\text{, }T^t_{rps,k}\approx\frac{C^t_{rps,k}}{p^t_k}\text{, }T^t_{lrc,k}\approx\frac{C^t_{lrc,k}}{p^t_k}$</li>
<li>$T^t_k$ is the completion time on the $k$-th server at the time-$t$, which consists of two completion-time terms, $T^t_{rps,k}$ and $T^t_{lrc,k}$ for RPs and LRC, respectively. </li>
<li>$C^t_{rps,k}$ and $C^t_{lrc,k}$ are the computing costs of the RP box and LRC offloading to the server $k$ while $p^t_k$ is the available resource capacity of the $k$-th server. </li>
</ul>
</li>
</ol>
<h3 id="Why-send-RP-coordinates-and-the-original-image-instead-of-a-cropped-RP-task"><a href="#Why-send-RP-coordinates-and-the-original-image-instead-of-a-cropped-RP-task" class="headerlink" title="Why send RP coordinates and the original image instead of a cropped RP task?"></a>Why send RP coordinates and the original image instead of a cropped RP task?</h3><ol>
<li>The execution time of $r$ ($r$ is a small number, such as $2$ and $3$) small RP (e.g., $&lt;5\%$ size of the original image) tasks are not much less than $r$ times of the execution time of running a single $r$-fold RP task. </li>
<li>It is hard to determine a good cropping strategy. <ul>
<li>The precise cut-out of individual RPs will lead to poor detection inference accuracy due to the lack of necessary background pixels. </li>
<li>If we leave large padding around the RPs, the total offloaded data will be too large to be efficient. </li>
</ul>
</li>
<li>Too many cropping operations generate high memory copy overheads, which may become problematic for mobile devices. </li>
</ol>
<h3 id="How-to-generate-RP-boxes"><a href="#How-to-generate-RP-boxes" class="headerlink" title="How to generate RP boxes?"></a>How to generate RP boxes?</h3><ol>
<li>Compared to a single RP, an RP-box is larger and consists of one or more nearby RPs. <ul>
<li>Each offloading task consists of either an LRC task, or an RP-box processing task, or both. </li>
<li>By scheduling an RP box instead of individual RPs, we can avoid fragmentation problems. </li>
</ul>
</li>
<li>The number of available edge servers determines the number of offloading tasks. </li>
<li><strong>RP box initialization</strong>: Before partitioning a frame, Elf first crops the area with all the RPs and horizontally partitions it into $N$ segments ($N$ is the number of available servers), where each segment corresponds to an initial RP box. <ul>
<li>The size of each RP box is initialized to be proportional to the available resource of the corresponding server.</li>
<li>At the LRC round, we partition the cropped image into $N − 1$ segments and have $N − 1$ RP boxes accordingly. We reserve one server for the LRC task. </li>
</ul>
</li>
<li><strong>RP association</strong>: Elf evaluates its spatial relationship with all the RP boxes for each RP. Given a pair of RP $r$ and box $b$, <ul>
<li>If $r$ is completely included in $b$ and we conveniently associate them. </li>
<li>If $r$ and $b$ are not overlap, $r$ is not associated with $b$. </li>
<li>If $r$ and $b$ are partially overlapped, it also overlaps with at least one other box. We choose to associate with the RP box that has the most overlap with the RP. <ul>
<li>If there is a tie, we choose the RP box with a larger gap between the server resource capacity and the computation costs of the RPs that are already associated. </li>
</ul>
</li>
</ul>
</li>
<li><strong>RP box adjustment</strong>: Elf resizes each RP box so that it can fully cover all the RPs that are associated with it. <ul>
<li>The computation cost of some RP boxes may drastically increase compared to the initialization stage and thus break the intended load balancing. </li>
<li>We examine those RP boxes whose cost increase exceeds a pre-defined threshold. For these boxes, we re-associate the RP with the lowest cost to the neighboring box, which has enough computation capacity to hold this RP. </li>
<li>After each re-association, the two boxes must adjust their sizes accordingly and estimate the new computation cost. We stop this process if the re-association results in a higher load imbalance. </li>
</ul>
</li>
<li>The $C^t_{rps,k}=\sum_v\{C^t_{rp,v}\}$ and $C^t_{lrc}=\alpha\cdot(\sum^M_{k=1}C^t_{rps,k})$.<br><img src="/imgs/Sys4ai/Elf/partition.png" width="50%"></li>
</ol>
<h3 id="How-do-we-estimate-server-capacity-and-RP-computation-cost"><a href="#How-do-we-estimate-server-capacity-and-RP-computation-cost" class="headerlink" title="How do we estimate server capacity and RP computation cost?"></a>How do we estimate server capacity and RP computation cost?</h3><ol>
<li>There are two approaches to estimating server capacity. <ul>
<li>The first approach is through passive profiling. <ul>
<li>It calculates the server m’s average end-to-end latency $T_m$ over the last $n$ (default value of $7$) offloading requests that are served by $m$. Then, the resource capacity is defined as $1/T_m$. </li>
<li>This passive profiling can help evaluate the trade-off between computing and network resources. </li>
</ul>
</li>
<li>The second approach is through proactive profiling: Elf periodically queries the server for its GPU utilization. </li>
</ul>
</li>
<li>There are also two ways of estimating an RP’s computation cost. <ul>
<li>The first approach is based on the RP’s area, assuming the cost is linearly proportional to the RP’s area. </li>
<li>The second approach is through Spatially Adaptive Computation Time (SACT). Elf can accordingly estimate the cost of an RP at the pixel level. <ul>
<li>SACT is an optimization that early stops partial convolutional operations by evaluating the confidence of the outputs of intermediate layers. </li>
<li>SACT indicates how much computation has been applied with each pixel of a raw frame input. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><ol>
<li>The author measured the latency of Elf using different numbers of servers. <ul>
<li>The latency with different server numbers highly depends on the size of the RP boxes shipped to each edge server. </li>
<li>The inference time shows distinct sensitivity among different deep vision models. <ul>
<li>The models with more, even fully, convolutional operations present a stronger correlation between frame resolution and inference latency. </li>
<li>Two-stage models usually generate the same number of Regions of Interest (ROI) independent of the input resolution and then ship each down the pipeline. The second stage thus costs the same time. </li>
<li>Two-stage models can dynamically adjust the number of ROI based on the frame resolution as a higher resolution input potentially involves more objects.<br><img src="/imgs/Sys4ai/Elf/server_num.png" width="15%"></li>
</ul>
</li>
</ul>
</li>
<li>The cost of each processing part: <ul>
<li>At the server end, the GPU utilization v.s. GPU numbers are measured. <ul>
<li>On average, Elf-3 only consumes $1.7\times$ GPU utilization in total running with $3$ GPUs, then <em>SO</em> to finish a single request. </li>
<li>A lower per GPU utilization allows Elf to have more of a chance to efficiently utilize those resource fragmentations, thus improving the total GPU utilization of edge servers.<br><img src="/imgs/Sys4ai/Elf/GPUutilization.png" width="15%"></li>
</ul>
</li>
<li>On the communication side, they measured the latency under different network conditions. <ul>
<li>Elf is less sensitive to the network bandwidth because it offloads much less data than SO.<br><img src="/imgs/Sys4ai/Elf/network.png" width="15%"></li>
</ul>
</li>
<li>At the mobile end, they measured Elf’s overhead. <ul>
<li>RP prediction costs 70%+ of the total time as the attention LSTM model is implemented in Python and exported to C++ with TorchScript. It can be improved by rewriting the prediction model with TensorRT.<br><img src="/imgs/Sys4ai/Elf/overhead.png" width="15%"></li>
</ul>
</li>
</ul>
</li>
<li>The inference accuracy and offload ratio of attention-based LSTM against vanilla LSTM, the fast tracker, showed the effectiveness of attention-based LSTM. </li>
<li>The impact of hyperparameters, e.g., the LRC parameter and RP expansion ratio, is measured. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/about/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/about/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/about/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
