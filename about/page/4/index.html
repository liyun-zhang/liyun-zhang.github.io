<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/about/page/4/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/about/page/4/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"about/page/4/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/04/Courses/15445/10-Two-Phase-Locking/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/04/Courses/15445/10-Two-Phase-Locking/" class="post-title-link" itemprop="url">10 Two-Phase Locking</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-04 16:47:09" itemprop="dateCreated datePublished" datetime="2023-08-04T16:47:09+08:00">2023-08-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 15:37:10" itemprop="dateModified" datetime="2024-03-16T15:37:10+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Two-phase-Locking"><a href="#Two-phase-Locking" class="headerlink" title="Two-phase Locking"></a>Two-phase Locking</h1><h2 id="How-can-we-guarantee-serialization-without-knowing-the-entire-schedule-ahead-of-time"><a href="#How-can-we-guarantee-serialization-without-knowing-the-entire-schedule-ahead-of-time" class="headerlink" title="How can we guarantee serialization without knowing the entire schedule ahead of time?"></a>How can we guarantee serialization without knowing the entire schedule ahead of time?</h2><ol>
<li>We can use locks to protect database objects. <ul>
<li>When a transaction wants to access some objects, it needs to acquire locks of those objects from a centralized lock manager. </li>
<li>Locks are issued by applications and handled in the lock manager, while latches are issued and acquired locally. Hence, locks are more expensive than latches, even if they are free. </li>
</ul>
</li>
<li>There are <code>S-LOCK</code> and <code>X-LOCK</code>. <ul>
<li><code>S-LOCKs</code> are shared locks for reads, while <code>X-LOCKs</code> are exclusive locks for writes. </li>
<li>Their compatibility matrix is as follows:<br><img src="/imgs/15445/2pl/sx_comp_matrix.png" width="50%"></li>
</ul>
</li>
<li>The lock manager keeps track of what transactions hold, what locks are locked, and what transactions are waiting to acquire any locks. <ul>
<li>The lock manager grants or blocks requests when transactions request or upgrade locks. The lock manager updates its internal lock table when transactions release or downgrade locks. </li>
<li>The lock manager is responsible for detecting deadlock and choosing some transactions to kill. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-problem-with-releasing-locks-and-acquiring-them-later-again"><a href="#What-is-the-problem-with-releasing-locks-and-acquiring-them-later-again" class="headerlink" title="What is the problem with releasing locks and acquiring them later again?"></a>What is the problem with releasing locks and acquiring them later again?</h2><ol>
<li>This may cause inconsistent reads for a transaction when another transaction modified the object when the lock was available. </li>
<li>This problem can be solved by two-phase locking. <ul>
<li>The first phase is growing: <ul>
<li>Each transaction requests or upgrades the locks it needs from the DBMS’s lock manager. The lock manager grants/denies lock requests.</li>
</ul>
</li>
<li>The second phase is shrinking: <ul>
<li>The transaction can only release or downgrade the locks that it previously acquired. It cannot acquire new locks. </li>
</ul>
</li>
</ul>
</li>
<li>Two-phase locking is sufficient to guarantee conflict serializability because it generates schedules whose precedence graph is acyclic.</li>
</ol>
<h2 id="What-is-the-problem-of-two-phase-locking"><a href="#What-is-the-problem-of-two-phase-locking" class="headerlink" title="What is the problem of two-phase locking?"></a>What is the problem of two-phase locking?</h2><ol>
<li>It is subject to cascading aborts caused by dirty reads. <ul>
<li>When a transaction modifies an object and releases the lock before it is aborted, the modified object is exposed to other transactions. </li>
<li>When the modifier is aborted, all other transactions using the modified object need to abort. </li>
</ul>
</li>
<li>This can be solved by strong strict two-phase locking (rigorous two-phase locking). <ul>
<li>The transaction can only release locks after it ends, i.e., committed or aborted. </li>
</ul>
</li>
<li>A schedule is strict if a value written by a transaction is not read or overwritten by other transactions until that transaction finishes. <ul>
<li>Its advantages are that it does not incur cascading aborts, and aborted transactions can be undone by restoring the original values of modified tuples. </li>
<li>However, it allows only conflict serializable schedules, but it is often stronger than needed for some apps. Most DBMSs prefer correctness before performance. </li>
</ul>
</li>
</ol>
<h1 id="Deadlock"><a href="#Deadlock" class="headerlink" title="Deadlock"></a>Deadlock</h1><h2 id="How-to-detect-and-resolve-deadlocks"><a href="#How-to-detect-and-resolve-deadlocks" class="headerlink" title="How to detect and resolve deadlocks?"></a>How to detect and resolve deadlocks?</h2><ol>
<li>The two-phase locking may lead to deadlocks. </li>
<li>The DBMS creates a waits-for graph to keep track of what locks each transaction is waiting to acquire<ul>
<li>Nodes are transactions. Edge from $T_i$ to $T_j$ if $T_i$ is waiting for $T_j$ to release a lock. </li>
<li>The system periodically checks for cycles in the waits-for graph and then decides how to break it. </li>
</ul>
</li>
<li>When the DBMS detects a deadlock, it will select a “victim” transaction to rollback to break the cycle. <ul>
<li>Depending on how it was invoked, the victim transaction will either restart or abort (more common). </li>
<li>There is a trade-off between the frequency of checking for deadlocks and how long transactions wait before deadlocks are broken. </li>
</ul>
</li>
<li>Selecting the proper victim depends on a lot of different variables.<ul>
<li>By age (lowest timestamp)</li>
<li>By progress (least/most queries executed)</li>
<li>By the number of items already locked</li>
<li>By the number of transactions that we have to rollback with it</li>
<li>We also should consider the # of times a transaction has been restarted in the past to prevent starvation. </li>
</ul>
</li>
<li>After selecting a victim transaction to abort, the DBMS can also decide how far to rollback the transaction’s changes. <ul>
<li>The first approach is to rollback the entire transaction and tell the application that it was aborted. </li>
<li>The second approach is rolling back a portion of a transaction to break the deadlock and then attempting to re-execute the undone queries. </li>
</ul>
</li>
</ol>
<h2 id="How-to-prevent-deadlocks"><a href="#How-to-prevent-deadlocks" class="headerlink" title="How to prevent deadlocks?"></a>How to prevent deadlocks?</h2><ol>
<li>When a transaction tries to acquire a lock held by another transaction, the DBMS kills one to prevent a deadlock. </li>
<li>Assign priorities based on timestamps, e.g., older timestamp means higher priority. </li>
<li>The first kind of rule is Wait-Die (“Old Waits for Young”)<ul>
<li>If requesting a transaction has higher priority than holding a transaction, then requesting a transaction waits for the holding transaction. Otherwise, requesting transaction aborts. </li>
</ul>
</li>
<li>The second kind of rule is Wound-Wait (“Young Waits for Old”)<ul>
<li>If requesting a transaction has higher priority than holding a transaction, then holding the transaction aborts and releases the lock. Otherwise, requesting transaction waits. </li>
</ul>
</li>
<li>In this case, only one “type” of direction is allowed when waiting for a lock. </li>
<li>When a transaction restarts, its new priority remains its original timestamp to prevent it from getting starved for resources like an old man at a corrupt senior center. </li>
</ol>
<h1 id="Lock-granularity"><a href="#Lock-granularity" class="headerlink" title="Lock granularity"></a>Lock granularity</h1><h2 id="What-are-the-database-objects"><a href="#What-are-the-database-objects" class="headerlink" title="What are the database objects?"></a>What are the database objects?</h2><ol>
<li>Depending on the lock granularity, it can be attributes, tuples, pages, or tables. </li>
<li>The trade-off is between parallelism versus overhead of requesting and lock manager processing. </li>
<li>In a hierarchical lock scheme, the objects from top-layer to lower-layer are database, table, page, tuple, and attribute. </li>
</ol>
<h2 id="How-to-support-multiple-granularities"><a href="#How-to-support-multiple-granularities" class="headerlink" title="How to support multiple granularities?"></a>How to support multiple granularities?</h2><ol>
<li>With only <code>S-LOCK</code> and <code>X-LOCK</code>, we have to check the locks of all children when we try to lock a higher-level node. </li>
<li>An intention lock allows a higher-level node to be locked in a shared or exclusive mode without checking all descendent nodes. </li>
<li>If a node is locked in an intention mode, then some transaction explicitly locks at a lower level in the tree. <ul>
<li>Intention-Shared (<code>IS</code>) indicates explicit locking at a lower level with shared locks. </li>
<li>Intention-Exclusive (<code>IX</code>) indicates explicit locking at a lower level with exclusive locks. </li>
<li>Shared+Intention-Exclusive (<code>SIX</code>) indicates that the subtree rooted by that node is locked explicitly in shared mode, and explicit locking is being done at a lower level with exclusive-mode locks. </li>
<li>Their compatibility matrix is as follows:<br><img src="/imgs/15445/2pl/intention.png" width="50%"></li>
</ul>
</li>
<li>Each transaction obtains an appropriate lock at the highest level of the database hierarchy. <ul>
<li>The transaction must hold at least <code>IS</code> on the parent node to get an <code>S</code> or <code>IS</code> lock on a node. </li>
<li>It must hold at least <code>IX</code> on the parent node to get <code>X</code>, <code>IX</code>, or <code>SIX</code> on a node. </li>
</ul>
</li>
<li>Multiple lock granularities are shown in the <code>S</code>, <code>X</code>, and <code>SIX</code> locks on higher-level objects. <ul>
<li>Intention-Shared (<code>IS</code>): Intent to get <code>S</code> lock(s) at a finer granularity. </li>
<li>Intention-Exclusive (<code>IX</code>): Intent to get <code>X</code> lock(s) at a finer granularity. </li>
<li>Shared+Intention-Exclusive (<code>SIX</code>): Like <code>S</code> and <code>IX</code> at the same time. </li>
</ul>
</li>
</ol>
<h2 id="How-to-use-locks"><a href="#How-to-use-locks" class="headerlink" title="How to use locks?"></a>How to use locks?</h2><ol>
<li>Applications typically don’t acquire a transaction’s locks manually (i.e., explicit SQL commands). <ul>
<li>Sometimes, you need to provide the DBMS with hints to help it improve concurrency. </li>
<li>Explicit locks are also helpful when making major changes to the database. </li>
</ul>
</li>
<li>Lock escalation: The DBMS can automatically switch to coarser-grained locks when a transaction acquires too many low-level locks. This reduces the number of requests that the lock manager must process. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/02/OpenSource/BusTub/Project-3-Query-Execution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/02/OpenSource/BusTub/Project-3-Query-Execution/" class="post-title-link" itemprop="url">Project #3: Query Execution</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-02 00:52:07" itemprop="dateCreated datePublished" datetime="2023-08-02T00:52:07+08:00">2023-08-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 23:40:29" itemprop="dateModified" datetime="2024-03-16T23:40:29+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/BusTub/" itemprop="url" rel="index"><span itemprop="name">BusTub</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Executors"><a href="#Executors" class="headerlink" title="Executors"></a>Executors</h1><h2 id="Overall"><a href="#Overall" class="headerlink" title="Overall"></a>Overall</h2><ol>
<li>The planner for each operation stores the necessary information to calculate the correct output. </li>
<li>The executor class for each operation stores the corresponding plan and other information to determine what tuple will be returned. <ul>
<li>All executors must override the <code>Init()</code> and <code>Next(Tuple *tuple, RID *rid)</code>. </li>
<li><code>Init()</code> is used to initialize the information to determine what tuple will be returned. It is separated from the constructor because its parent executor may need to fetch the tuples of the current executor several times. </li>
</ul>
</li>
<li>The <code>ExecutorContext</code> in each executor stores the metadata of the database system, including catalog and buffer pool manager. <ul>
<li>Catalog maintains the tables and indexes in the current database. </li>
<li>We can register a new table or index through a catalog or acquire metadata of a table (TableInfo) or index (IndexInfo) from the catalog. </li>
</ul>
</li>
<li><code>TableInfo</code> stores the name, OID, schema of the table, and a pointer for <code>TableHeap</code>. <ul>
<li>We can manipulate a table through its <code>TableHeap</code>. </li>
<li>The <code>TableHeap</code> provides methods to insert or get tuples and update or get tuple metadata. <ul>
<li>We need to mark the <code>is_deleted_</code> flag in its metadata to delete a tuple as <code>true</code>. </li>
<li>To update a tuple, we need to delete it first and insert the newly updated tuple into the table again. </li>
<li><code>TableHeap</code> also provides <code>MakeIterator()</code> to iterate through the table. </li>
</ul>
</li>
</ul>
</li>
<li>The <code>IndexInfo</code> stores the name and OID of the index, the schema for the index key, the name of the table, and a pointer of the <code>Index</code>. <ul>
<li>We can manipulate an index through its <code>Index</code>. </li>
<li>The <code>Index</code> provides methods to insert or delete an entry from the index and get index metadata. <ul>
<li>The metadata includes the names of the index and table, the mapping relation between the key schema and tuple schema, and the schema of the indexed key. </li>
<li>The <code>Index</code> is the abstract class of all kinds of index implementations. To use a specific known index implementation, we can <code>dynamic_cast</code> it. </li>
</ul>
</li>
</ul>
</li>
<li>This system supports <code>ArithmeticExpression</code>, <code>ColumnValueExpression</code>, <code>ComparisonExpression</code>, <code>ConstantValueExpression</code>, <code>LogicExpression</code>, <code>StringExpression</code>. <ul>
<li>The <code>AbstractExpression</code> provides an <code>Evaluate</code> method to calculate the desired <code>Value</code> according to the provided one <code>Tuple</code> and <code>Schema</code>. <ul>
<li>The <code>ArithmeticExpression</code> only supports <code>PLUS</code> and <code>MINUS</code> operation of two <code>Value</code>s. </li>
<li>The <code>ColumnValueExpression</code> returns the <code>Value</code> of the designated column. </li>
<li>The <code>ComparisonExpression</code> supports the comparison result of two <code>Values</code>s of <code>equal, not equal, less, less or equal, greater, greater or equal</code>. </li>
<li>The <code>ConstantValueExpression</code> returns a single constant <code>Value</code>. </li>
<li>The <code>LogicExpression</code> supports the <code>AND/OR</code> of two <code>Value</code>s. </li>
<li>The <code>StringExpression</code> converts a string to lower-case or upper-case. </li>
</ul>
</li>
<li>The <code>AbstractExpression</code> also provides an <code>EvaluateJoin</code> method to calculate the join condition of two <code>Tuple</code>s. They support the same functions as <code>Evaluation</code>, except that they consider two tuples. </li>
<li><code>ArithmeticExpression</code>, <code>ComparisonExpression</code>, <code>LogicExpression</code>, and <code>LogicExpression</code> may have child-expressions. During evaluation, they will first perform child expressions recursively. </li>
</ul>
</li>
</ol>
<h2 id="Scan"><a href="#Scan" class="headerlink" title="Scan"></a>Scan</h2><ol>
<li>In sequential scans, we only need to know which table to scan. <ul>
<li>The object ID and name of the table are stored in the planner. </li>
<li>In the <code>Init()</code>, the table metadata (<code>TableInfo</code>) is fetched from the catalog, and the corresponding iterator of the table is created. </li>
<li>In the <code>Next(Tuple *tuple, RID *rid)</code>, if the iterator is not at the end, we need to find the first tuple with <code>is_deleted_</code> being <code>false</code> and matching with <code>filter_predicate_</code>. </li>
</ul>
</li>
<li>In the index scan, we need to know which index to scan and which table to fetch the tuple. <ul>
<li>The OID of the index is stored in the planner, while the table name is stored in <code>IndexInfo</code>. </li>
<li>To fetch the iterator of the index, we need to cast the <code>Index</code> into the specific implementation. </li>
<li>Each iterator points to <code>(key, RecordID)</code> pair. We can fetch the tuple with the <code>GetTuple</code> of the <code>TableHeap</code>. </li>
<li>As long as the iterator is not at the end, we must find the first tuple with <code>is_deleted_</code> being <code>false</code>. </li>
</ul>
</li>
</ol>
<h2 id="Modification"><a href="#Modification" class="headerlink" title="Modification"></a>Modification</h2><ol>
<li>We will modify the table and all the associated indexes in the modification executors. <ul>
<li>The table OIDs are stored in the corresponding planner, while we can fetch all indexes of that table with the name of the table in <code>TableInfo</code>. </li>
<li>The indexed keys can be acquired using the <code>KeyFromTuple</code> method of <code>Tuple</code>. The stored values are RecordIDs. </li>
</ul>
</li>
<li>All the modification executors only return one row representing the number of modified tuples. Hence, the executor should handle all the modifications in one <code>Next</code> call. <ul>
<li>When no modification is performed, we should also return one row of $0$. </li>
<li>To distinguish between no modification and modification already finished in the last call, we should have a flag representing whether or not we have already modified the table. </li>
</ul>
</li>
<li>The tuples to insert or delete are acquired from the child executor in the insert executor and delete executor. </li>
<li>The update planner has a target expression for each output column. <ul>
<li>After acquiring the original tuple from the child executor, we can evaluate each output column with target expressions. </li>
<li>The expressions can be any expressions here. </li>
<li>When updating indexes, the old ones must be deleted before insertion. </li>
</ul>
</li>
</ol>
<h2 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h2><ol>
<li>The aggregations are implemented with a hash table. <ul>
<li>The hash table hashes the aggregate keys to the aggregate result. </li>
<li>When inserting a tuple into the hash table, it updates the aggregate result according to the aggregate function and the aggregate values from the tuple. </li>
<li>To use <code>std::hash</code>, the key is <code>AggregateKey</code> containing a vector of <code>Value</code> describing the group-by keys, while the value is <code>AggregateValue</code> containing another vector of <code>Value</code> describing aggregate values. <ul>
<li>The <code>AggregateKey</code> needs to override the <code>==</code> operator and provide the <code>()</code> operator in <code>struct std::hash&lt;bustub::AggregateKey&gt;</code> to calculate the hash value of <code>AggregateKey</code>. </li>
</ul>
</li>
</ul>
</li>
<li>Aggregations are pipeline breakers. Hence, the build phase could be performed in the Init(), where all tuples from the child executor are read and inserted into the hash table to calculate the outputs. <ul>
<li>If no tuple is inserted, the aggregation executor must return the default values for each aggregate function. </li>
<li>The aggregation planner has two vectors of <code>AbstractExpressionRef</code> to fetch the aggregate keys and aggregate values from tuples received from the child executor for aggregate computation. They are all <code>ColumnExpression</code>. </li>
</ul>
</li>
<li>In the <code>Next(Tuple *tuple, RID *rid)</code>, we must iterate over the hash table and output a tuple combining the aggregate keys and results. </li>
</ol>
<h2 id="NestedLoopJoin"><a href="#NestedLoopJoin" class="headerlink" title="NestedLoopJoin"></a>NestedLoopJoin</h2><ol>
<li><p>The executor must iterate over the left child executor and the right child executor. </p>
<ul>
<li>When the right child executor has emitted all tuples, the join executor will try to fetch a new tuple from the left child executor and re-initialize the right child executor for the following comparison. </li>
<li>Since we do not always fetch from the left child executor when the <code>Next</code> is called, we need to record the current left tuple when it is fetched. </li>
<li>To distinguish between the status of no more left tuples and those that have not fetched any left tuples yet, we can fetch the first left tuple in <code>Init()</code>. </li>
<li>To mark the status of no more left tuples, i.e., no more tuples to emit, we need a flag to record the status of left tuples and the returned value of the <code>Next</code> left child executor. </li>
</ul>
</li>
<li><p>For the left join, if an outer tuple does not match any inner tuple, we still need to emit the concatenation of that outer tuple with an all-null inner tuple. </p>
</li>
<li><p>The predicate expression of the executor can be either a <code>LogicExpression</code> or <code>ComparisonExpression</code>. </p>
</li>
</ol>
<h2 id="HashJoin"><a href="#HashJoin" class="headerlink" title="HashJoin"></a>HashJoin</h2><ol>
<li>Similar to aggregations, we need a hash table for the outer table. We define <code>JoinKey</code> and <code>JoinBucket</code> to hash. <ul>
<li>We store all tuples with the same values in the attributes of the join condition. </li>
<li>We must also record whether one tuple is used to support left join. <ul>
<li>When a tuple is matched with some inner tuple, all tuples in the same bucket must also matched. Hence, we only need to record the usage of each <code>JoinBucket</code>. </li>
<li>For left join, when there are no more right tuples, the executor still needs to iterate through the hash table to see if any bucket is unused. </li>
</ul>
</li>
<li>Similar to aggregation, <code>HashJoin</code> must build the hash table based on the outer table in <code>Init()</code>. </li>
</ul>
</li>
<li>Unlike <code>NestedLoopJoin</code>, the plan of HashJoin only needs to know how to fetch columns from each tuple to determine whether those tuples are matched. <ul>
<li>The expressions of the <code>HashJoin</code> planner are only <code>ColumnValueExpression</code>. </li>
</ul>
</li>
</ol>
<h2 id="Sort-amp-Top-N"><a href="#Sort-amp-Top-N" class="headerlink" title="Sort &amp; Top-N"></a>Sort &amp; Top-N</h2><ol>
<li>The compare function needs specific expressions to fetch designated columns from each tuple. <ul>
<li>Hence, we cannot implement only a non-static member function or a function with expressions as one of the parameters. </li>
<li>We need to implement a structure with an override <code>()</code> operator. The expressions are passed to the object in initialization. </li>
<li>For <code>priority_queue</code>, two nodes will be swapped when the comparison function returns true. </li>
</ul>
</li>
<li>In <code>Init()</code>, we must fetch and sort all tuples from the child executor. <ul>
<li>For the Top-N executor, the heap size should not be larger than $N$. The heap after the <code>Init()</code> is the counter-order of the output order. Hence, we still need a stack to support output in <code>Next</code>. </li>
</ul>
</li>
</ol>
<h1 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h1><ol>
<li>When optimizing a plan, the root plan is passed to the optimizer, and the optimizer will perform a DFS of the plan tree. </li>
<li>Each optimizer tries to recognize the pattern they must handle and produce a new plan when matched. </li>
</ol>
<h2 id="Nested-loop-join"><a href="#Nested-loop-join" class="headerlink" title="Nested loop join"></a>Nested loop join</h2><ol>
<li>To optimize nested loop join with hash join, the optimizer must find a nested loop join and separate all the conditions from <code>LogicExpression</code> into <code>ComparisonExpression</code>. <ul>
<li>If all the <code>ComparisonExpression</code> are <code>ComparisonType::Equal</code>, we can create a new plan of <code>HashJoin</code> with <code>left_key_expressions</code> and <code>right_key_expressions</code> extracted from those <code>ComparisonExpression</code>. </li>
</ul>
</li>
<li>Push down predicates of nested loop join to reduce the output of children of join and the complexity of join. <ul>
<li>Only predicates that only involve one side of the input of the join operator can be pushed down. </li>
<li>Some predicates need to be pushed into the left child, some to the right child, and some remain in the join. </li>
<li>To push down the predicate, we can create a new FilterPlanNode as a child of join and father of the original child. </li>
</ul>
</li>
<li>Nested loop join can be replaced by hash join when the predicates are all equal conditions. </li>
<li>If the parent planner of a nested loop join is an aggregation planner, and that aggregation planner involves only data from one side, we can push down that aggregation as one of the children of join. <ul>
<li>The insight is that aggregation usually outputs fewer rows than input since aggregation groups rows before sending one row for each group. </li>
<li>The columns in the mathematical expressions need to be modified according to the side of aggregation. </li>
</ul>
</li>
</ol>
<h2 id="Order-by"><a href="#Order-by" class="headerlink" title="Order by"></a>Order by</h2><ol>
<li>To optimize the sorted limit into top-N, we need to check whether the child of a limit plan is a sort. </li>
<li>When sorting the table, instead of executing sort algorithms, we may use the existing index to traverse the table. <ul>
<li>It can only be used when the desired order is ascending, or default and the child of the sorting planner is the SeqScan planner. </li>
</ul>
</li>
</ol>
<h2 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h2><ol>
<li><code>SELECT *</code>, aggregations, or renaming the columns in the planner may cause multiple identical project planners. <ul>
<li>The child of the projection planner is not required to be a projection. </li>
<li>We can remove the projection planner if it has the same schema as its child except for the column name. </li>
<li>Then, the output schema of the child is replaced by the output schema of the projection. </li>
</ul>
</li>
<li>The child of the projection planner may be emitting unnecessary columns. <ul>
<li>Besides the naive projection, a projection planner could also perform arithmetical expressions. </li>
<li>If the child is another projection, we can merge them and express the new projection under the column schema of the child’s input schema. </li>
<li>If the child is an aggregation planner, it only needs to calculate those necessary aggregations. </li>
</ul>
</li>
</ol>
<h2 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h2><ol>
<li>Always true filters can be eliminated to avoid re-evaluation for every row. </li>
<li>The filter planner with a child of sequential scan can be merged to reduce the data transmission between executors. </li>
<li>When the predicate of sequential scan is all equal conditions, and a corresponding index exists, the sequential scan can be replaced by an index scan to avoid scanning the entire table. <ul>
<li>It needs to modify the <code>IndexScanPlanner</code> and the <code>IndexScanExecutor</code> to support scanning only the row designated by the equal conditions. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/07/16/Courses/15445/09-Concurrency-Control/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/16/Courses/15445/09-Concurrency-Control/" class="post-title-link" itemprop="url">09 Concurrency Control</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-16 16:35:24" itemprop="dateCreated datePublished" datetime="2023-07-16T16:35:24+08:00">2023-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 15:27:44" itemprop="dateModified" datetime="2024-03-16T15:27:44+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Concurrency-control-amp-recovery"><a href="#Concurrency-control-amp-recovery" class="headerlink" title="Concurrency control &amp; recovery"></a>Concurrency control &amp; recovery</h1><h2 id="What-do-we-want-from-concurrency-control-amp-recovery"><a href="#What-do-we-want-from-concurrency-control-amp-recovery" class="headerlink" title="What do we want from concurrency control &amp; recovery?"></a>What do we want from concurrency control &amp; recovery?</h2><ol>
<li>The concurrency control is responsible for the lost update problem. <ul>
<li>How can we avoid race conditions when updating records at the same time? </li>
<li>This involves the buffer pool manager layer, access methods layer, and operator execution layer. </li>
</ul>
</li>
<li>Recovery is responsible for durability problems. <ul>
<li>How can we ensure the correct state in case of a power failure? </li>
<li>This involves the disk manager layer and buffer pool manager layer. </li>
</ul>
</li>
</ol>
<h2 id="How-do-applications-issue-changes-to-a-DBMS"><a href="#How-do-applications-issue-changes-to-a-DBMS" class="headerlink" title="How do applications issue changes to a DBMS?"></a>How do applications issue changes to a DBMS?</h2><ol>
<li>A transaction is the execution of a sequence of one or more operations (e.g., SQL queries) on a database to perform some higher-level function. <ul>
<li>Transaction is the basic unit of change in a DBMS. Partial transactions are not allowed. </li>
<li>A new transaction starts with the <code>BEGIN</code> command. </li>
<li>The transaction stops with either <code>COMMIT</code> or <code>ABORT</code>:<ul>
<li>If committed, the DBMS either saves all the transaction’s changes <strong>or aborts</strong> it.</li>
<li>If aborted, all changes are undone so that it’s as if the transaction was never executed at all. </li>
<li>Abort can be either self-inflicted or caused by the DBMS.</li>
</ul>
</li>
</ul>
</li>
<li>In a strawman system, each transaction is executed one-by-one as they arrive at the DBMS. <ul>
<li>Before a transaction starts, copy the entire database to a new file and make all changes.  <ul>
<li>If the transaction completes successfully, overwrite the original file with the new one. </li>
<li>If the transaction fails, remove the dirty copy. </li>
</ul>
</li>
<li>The problem with this system is that there is no concurrency, and copying the entire database can be expensive if it is large. </li>
</ul>
</li>
<li>Besides correctness and fairness, we also want the DBMS to allow concurrent execution of independent transactions to provide better utilization/throughput and increase user response times. </li>
</ol>
<h2 id="What-does-the-DBMS-want-to-prevent-when-supporting-concurrency"><a href="#What-does-the-DBMS-want-to-prevent-when-supporting-concurrency" class="headerlink" title="What does the DBMS want to prevent when supporting concurrency?"></a>What does the DBMS want to prevent when supporting concurrency?</h2><ol>
<li>Arbitrary interleaving of operations can lead to temporary inconsistency and permanent inconsistency. <ul>
<li>Temporary inconsistency is unavoidable and fine as long as no other transactions can see it. </li>
<li>Permanent inconsistency is unacceptable. </li>
</ul>
</li>
<li>The DBMS is only concerned about what data is read/written from/to the database. Changes to the “outside world,” e.g., sending an email, are beyond the scope of the DBMS. </li>
</ol>
<h1 id="Correctness"><a href="#Correctness" class="headerlink" title="Correctness"></a>Correctness</h1><h2 id="What-are-the-correctness-criteria"><a href="#What-are-the-correctness-criteria" class="headerlink" title="What are the correctness criteria?"></a>What are the correctness criteria?</h2><ol>
<li><strong>Atomicity</strong>: All actions in the transaction happen, or none happen, i.e., “all or nothing.” </li>
<li><strong>Consistency</strong>: If each transaction is consistent and the DB starts consistent, it ends up consistent. </li>
<li><strong>Isolation</strong>: Execution of one transaction is isolated from that of other transactions. </li>
<li><strong>Durability</strong>: If a transaction commits, its effects persist. </li>
</ol>
<h2 id="How-to-ensure-atomicity"><a href="#How-to-ensure-atomicity" class="headerlink" title="How to ensure atomicity?"></a>How to ensure atomicity?</h2><ol>
<li>The first approach is logging (Write Ahead Log / WAL). <ul>
<li>DBMS logs all actions to undo the actions of aborted transactions. </li>
<li>Maintain undo records both in memory and on disk. </li>
<li>When the DBMS returns from a crash, partial transactions need to be undoed according to the undo records. </li>
</ul>
</li>
<li>Another approach is shadow paging. <ul>
<li>DBMS makes copies of pages, and transactions make changes to those copies. </li>
<li>Only when the transaction is committed is the page made visible to others by modifying the pointer in the directory. </li>
<li>It does not need extra operations when it comes back from a crash. </li>
</ul>
</li>
</ol>
<h2 id="What-is-consistency"><a href="#What-is-consistency" class="headerlink" title="What is consistency?"></a>What is consistency?</h2><ol>
<li>At a high level, consistency means the “world” represented by the database is logically correct. All questions (i.e., queries) the application asks about the data will return logically correct results. </li>
<li>There are two notions of consistency.<ul>
<li>Database consistency means the database accurately models the real world and follows integrity constraints. <ul>
<li>Transactions in the future see the effects of transactions committed in the past inside the database. </li>
<li>The DBMS designer should maintain this consistency. </li>
</ul>
</li>
<li>Transaction consistency means that if the database is consistent before the transaction starts, it will also be consistent after. <ul>
<li>The application programmer is responsible for this consistency. The DBMS does not know the semantics of correctness. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="Isolation"><a href="#Isolation" class="headerlink" title="Isolation"></a>Isolation</h2><h3 id="What-do-we-want-from-isolation"><a href="#What-do-we-want-from-isolation" class="headerlink" title="What do we want from isolation?"></a>What do we want from isolation?</h3><ol>
<li>Users submit transactions, and each transaction executes as if it were running by itself, i.e., it is executed in a strawman system where no other transaction is executing simultaneously. </li>
<li>Isolation provides an easier programming model to reason about. </li>
<li>However, the DBMS achieves concurrency by interleaving the actions (reads/writes of DB objects) of transactions. Hence, we need to schedule interleave transactions but still make it appear they ran one at a time. </li>
<li>There is no guarantee that $T_1$ will execute before $T_2$ or vice-versa if both are submitted together. The net effect must be equivalent to these two transactions running serially in some order.</li>
</ol>
<h3 id="How-to-decide-whether-a-schedule-matches-isolation"><a href="#How-to-decide-whether-a-schedule-matches-isolation" class="headerlink" title="How to decide whether a schedule matches isolation?"></a>How to decide whether a schedule matches isolation?</h3><ol>
<li>If the schedule is equivalent to some serial execution, we can consider it correct. </li>
<li>For any database state, if the effect of executing the first schedule is identical to the effect of executing the second schedule, we say these two schedules are equivalent. </li>
<li>A schedule is serializable if it is equivalent to some serial execution of the transactions. <ul>
<li>If each transaction preserves consistency, every serializable schedule preserves consistency. </li>
</ul>
</li>
<li>There are two levels of serializability: conflict serializability (most commonly used) and view serializability. </li>
</ol>
<h3 id="What-conflicts-do-we-want-to-prevent"><a href="#What-conflicts-do-we-want-to-prevent" class="headerlink" title="What conflicts do we want to prevent?"></a>What conflicts do we want to prevent?</h3><ol>
<li>Two operations conflict if They are by different transactions and are on the same object while one is a write. </li>
<li>A read-write conflict will cause unrepeatable reading. <ul>
<li>Transaction gets different values when reading the same object multiple times. </li>
<li>The conflict is between the write from one transaction and a repeated following read from another transaction, i.e., this is actually $read_1-write-read_2$ conflict where the conflict is between $write-read_2$. </li>
<li>If there is only one read, it is not a read-write conflict. The first read will never be a read-write conflict. </li>
</ul>
</li>
</ol>
<ul>
<li>A write-read conflict will cause a dirty read. <ul>
<li>One transaction reads data written by another transaction that has not been committed yet. </li>
<li>The problem will happen when the read transaction is committed before the write transaction aborts. </li>
<li>If the write transaction is successfully committed, there is no problem. But we cannot know that when we commit the read transaction first. </li>
</ul>
</li>
<li>A write-write conflict will cause a lost update. <ul>
<li>One transaction overwrites uncommitted data from another uncommitted transaction. </li>
<li>This may cause the result to become a combination of two partial transactions. </li>
<li>There is no problem when every data written by $T_2$ is the last write to that data, i.e., every data written by $T_2$ is not overwritten by $T_1$. The problem happens when $T_1$ overwrites some data of $T_2$ while $T_2$ overwrites some data of $T_1$. </li>
</ul>
</li>
</ul>
<h3 id="How-do-we-determine-whether-a-schedule-is-conflict-serializable"><a href="#How-do-we-determine-whether-a-schedule-is-conflict-serializable" class="headerlink" title="How do we determine whether a schedule is conflict serializable?"></a>How do we determine whether a schedule is conflict serializable?</h3><ol>
<li>When there are only two schedules: <ul>
<li>Two schedules are conflict equivalent if and only if they involve the same actions of the same transactions and every pair of conflicting actions is ordered the same way. </li>
<li>Schedule S is conflict serializable if S is conflict equivalent to some serial schedule. </li>
<li>We can transform S into a serial schedule by swapping consecutive non-conflicting operations of different transactions. </li>
</ul>
</li>
<li>For more schedules, we can use the dependency graphs. <ul>
<li>Create one node per transaction in the graph. </li>
<li>Create an edge from $T_i$ to $T_j$ if an operation $O_i$ of $T_i$ conflicts with an<br>operation $O_j$ of $T_j$ and $O_i$ appears earlier in the schedule than $O_j$. </li>
<li>A schedule is conflict serializable if and only if its dependency graph is acyclic. </li>
</ul>
</li>
</ol>
<h3 id="How-do-we-determine-whether-a-schedule-is-view-serializable"><a href="#How-do-we-determine-whether-a-schedule-is-view-serializable" class="headerlink" title="How do we determine whether a schedule is view serializable?"></a>How do we determine whether a schedule is view serializable?</h3><ol>
<li>Schedules $S_1$ and $S_2$ are view equivalent if:<ul>
<li>If $T_1$ reads the initial value of A in $S_1$, then $T_1$ also reads the initial value of A in $S_2$. </li>
<li>If $T_1$ reads the value of A written by $T_2$ in $S_1$, then $T_1$ also reads the value of A written by $T_2$ in $S_2$. </li>
<li>If $T_1$ writes the final value of A in $S_1$, then $T_1$ also writes the final value of A in $S_2$. </li>
</ul>
</li>
<li>In a word, each transaction is different schedules that read the same values written by the same transaction, and at the end of all transactions, all data are written by the same transaction in the same value. </li>
<li>View Serializability allows for (slightly) more schedules than Conflict Serializability does. Neither definition allows all serializable schedules. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/07/15/Courses/15445/08-Query-Planning-Optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/15/Courses/15445/08-Query-Planning-Optimization/" class="post-title-link" itemprop="url">08 Query Planning & Optimization</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-15 19:52:41" itemprop="dateCreated datePublished" datetime="2023-07-15T19:52:41+08:00">2023-07-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 15:14:47" itemprop="dateModified" datetime="2024-03-16T15:14:47+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Query-planning"><a href="#Query-planning" class="headerlink" title="Query planning"></a>Query planning</h1><h2 id="What-are-the-logical-plans-and-physical-plans"><a href="#What-are-the-logical-plans-and-physical-plans" class="headerlink" title="What are the logical plans and physical plans?"></a>What are the logical plans and physical plans?</h2><ol>
<li>The optimizer maps a logical algebra expression to the optimal equivalent physical algebra expression. </li>
<li>Physical operators define a specific execution strategy using an access path, i.e., a specific algorithm. <ul>
<li>They depend on the physical format of the data they process, i.e., sorting and compression. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-process-flow-of-query-execution"><a href="#What-is-the-process-flow-of-query-execution" class="headerlink" title="What is the process flow of query execution?"></a>What is the process flow of query execution?</h2><ol>
<li>The application is connected to the database system and sends a SQL query, which may be rewritten to a different format in SQL rewriter. </li>
<li>The SQL string is parsed into tokens that make up the syntax tree. </li>
<li>The binder converts named objects in the syntax tree to internal identifiers by consulting the system catalog. </li>
<li>The binder emits a logical plan, which may be fed to a tree rewriter for additional schema info. </li>
<li>The logical plan is given to the optimizer, which will select the most efficient procedure to execute the plan. </li>
</ol>
<p><img src="/imgs/15445/Optimizer/architecture.png" width="50%"></p>
<h2 id="How-does-the-optimizer-work"><a href="#How-does-the-optimizer-work" class="headerlink" title="How does the optimizer work?"></a>How does the optimizer work?</h2><ol>
<li>One way is heuristics/rules. <ul>
<li>Rewrite the query to remove stupid/inefficient things. </li>
<li>These techniques may need to examine the catalog, but they do not need to examine data. </li>
</ul>
</li>
<li>Another way is the cost-based search. <ul>
<li>We need a model to estimate the cost of executing a plan. </li>
<li>Enumerate multiple equivalent plans for a query and pick the one with the lowest cost. </li>
</ul>
</li>
</ol>
<h1 id="Heuristics-optimization"><a href="#Heuristics-optimization" class="headerlink" title="Heuristics optimization"></a>Heuristics optimization</h1><h2 id="How-should-we-optimize-logical-plans"><a href="#How-should-we-optimize-logical-plans" class="headerlink" title="How should we optimize logical plans?"></a>How should we optimize logical plans?</h2><ol>
<li>Split conjunctive predicates. <ul>
<li>Decompose predicates into their simplest forms to make it easier for the optimizer to move them around. </li>
</ul>
</li>
<li>Predicate pushdown<ul>
<li>Move the predicate to the lowest applicable point in the plan. </li>
</ul>
</li>
<li>Replace cartesian products with joins.<ul>
<li>Replace all Cartesian Products with inner joins using the join predicates. </li>
</ul>
</li>
<li>Projection pushdown<ul>
<li>This eliminates redundant attributes before pipeline breakers to reduce materialization costs and the data passed around. </li>
</ul>
</li>
</ol>
<h2 id="How-should-we-optimize-for-nested-sub-queries"><a href="#How-should-we-optimize-for-nested-sub-queries" class="headerlink" title="How should we optimize for nested sub-queries?"></a>How should we optimize for nested sub-queries?</h2><ol>
<li>Rewrite to de-correlate and flatten them.<ul>
<li>For example, an <code>EXISTS</code> sub-query in the <code>WHERE</code> clause may be rewritten as an inner-join. </li>
</ul>
</li>
<li>Decompose nested queries and store results in a temporary table. <ul>
<li>For those sub-queries uncorrelated with an outer query, the optimizer breaks up queries into blocks and then concentrates on one block at a time. </li>
<li>Sub-queries are written to a temporary table and discarded after the query finishes. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-rewrite-expression"><a href="#How-can-we-rewrite-expression" class="headerlink" title="How can we rewrite expression?"></a>How can we rewrite expression?</h2><ol>
<li>This is implemented using if/then/else clauses or a pattern-matching rule engine. <ul>
<li>Search for expressions that match a pattern. When a match is found, rewrite the expression. Halt if there are no more rules that match. </li>
</ul>
</li>
<li>One approach is replacing impossible or unnecessary predicates with false ones. </li>
<li>Another approach is merging predicates, e.g., numeric ranging predicates. </li>
</ol>
<h1 id="Cost-based-search"><a href="#Cost-based-search" class="headerlink" title="Cost-based search"></a>Cost-based search</h1><h2 id="Cost-estimation"><a href="#Cost-estimation" class="headerlink" title="Cost estimation"></a>Cost estimation</h2><h3 id="What-cost-do-we-care"><a href="#What-cost-do-we-care" class="headerlink" title="What cost do we care?"></a>What cost do we care?</h3><ol>
<li>Physical Costs<ul>
<li>Predict CPU cycles, I/O, cache misses, RAM consumption, and network messages. </li>
<li>This cost depends heavily on hardware. </li>
</ul>
</li>
<li>Logical Costs<ul>
<li>Estimate output size per operator. </li>
<li>This cost is independent of the operator algorithm since algorithms are physical. </li>
<li>It needs estimations for operator result sizes. </li>
</ul>
</li>
<li>Algorithmic Costs<ul>
<li>Mainly the complexity of the operator algorithm implementation. </li>
</ul>
</li>
<li>We may use a combination of multiple costs that are weighted by magic constant factors. <ul>
<li>Some assumptions are that processing a tuple in memory is $400\times$ faster than reading a tuple from a disk, and sequential I/O is $4\times$ faster than random I/O. </li>
<li>The most commonly used cost is the combination of physical costs and logical costs. </li>
</ul>
</li>
</ol>
<h3 id="How-does-DBMS-estimate-the-costs"><a href="#How-does-DBMS-estimate-the-costs" class="headerlink" title="How does DBMS estimate the costs?"></a>How does DBMS estimate the costs?</h3><ol>
<li>The DBMS stores internal statistics about tables, attributes, and indexes in its internal catalog. <ul>
<li>Different systems update them at different times. </li>
</ul>
</li>
<li>Then, DBMS derives a predicate’s selection cardinality (selectivity), the fraction of tuples that qualify. </li>
<li>We can make some assumptions to estimate selectivity.<ul>
<li>Uniform data: The distribution of values (except for the heavy hitters) is the same. May maintain a heavy hitter list that stores the most common values and assume that the occurrence of the rest of the data is the same. </li>
<li>Independent predicates: The predicates on attributes are independent, i.e., the conjunction of predicates can result in the multiplication or addition of probabilities. </li>
<li>Inclusion principle: The domain of join keys overlaps such that each key in the inner relation will also exist in the outer table.</li>
<li>These assumptions may not be true. </li>
</ul>
</li>
</ol>
<h3 id="What-statistics-does-the-DBMS-maintain"><a href="#What-statistics-does-the-DBMS-maintain" class="headerlink" title="What statistics does the DBMS maintain?"></a>What statistics does the DBMS maintain?</h3><ol>
<li>Histograms: <ul>
<li>The naive and most accurate way is to maintain an occurrence count per value in a column. </li>
<li>Equi-width histograms maintain counts for a group of values. All buckets have the same width, i.e., the same number of values. </li>
<li>Equi-depth histograms vary the width of buckets so that the total number of occurrences for each bucket is roughly the same. </li>
<li>Equi-width or equi-depth histograms use the total count of a bucket divided by the number of values in that bucket as the count of each value. </li>
</ul>
</li>
<li>Sketches: <ul>
<li>Probabilistic data structure that gives an approximate count for a given value. </li>
<li>The cost-model can replace histograms with sketches to improve its selectivity estimate accuracy. </li>
</ul>
</li>
<li>Sampling: <ul>
<li>DBMS maintains a small subset of each table that it then uses to evaluate expressions to compute selectivity. </li>
<li>The selectivity is estimated by running the same query on the sample table. </li>
<li>The sample table is updated when the underlying tables change significantly. </li>
</ul>
</li>
</ol>
<h2 id="Query-optimization"><a href="#Query-optimization" class="headerlink" title="Query optimization"></a>Query optimization</h2><h3 id="How-do-we-perform-cost-based-optimization"><a href="#How-do-we-perform-cost-based-optimization" class="headerlink" title="How do we perform cost-based optimization?"></a>How do we perform cost-based optimization?</h3><ol>
<li>After performing rule-based rewriting, the DBMS will enumerate different plans for the query and estimate their costs. <ul>
<li>After exhausting all plans or some timeout, it chooses the best plan it has seen for the query. </li>
<li>The time spent on the search should be significantly smaller than the time spent executing the query. DBMS can set a time threshold to end the search. </li>
</ul>
</li>
<li>DBMS mainly enumerates the access methods (sequential scan, binary search / clustered indexes, index scan) and evaluation order. </li>
<li>Query planning for OLTP queries is easy because they are <strong>sargable</strong> (Search Argument Able). <ul>
<li>It is usually just picking the best index. </li>
<li>Joins are almost always on foreign key relationships with a small cardinality. </li>
</ul>
</li>
<li>There are two choices for multi-relation query planning. <ul>
<li>Bottom-up optimization: Start with nothing and then build up the plan to get to the desired outcome. </li>
<li>Top-down optimization: Start with the outcome you want, then work down the tree to find the optimal plan that gets you to that goal. </li>
</ul>
</li>
</ol>
<h3 id="How-does-bottom-up-optimization-work"><a href="#How-does-bottom-up-optimization-work" class="headerlink" title="How does bottom-up optimization work?"></a>How does bottom-up optimization work?</h3><ol>
<li>Break the query into blocks and generate the logical operators for each block. For each logical operator, generate a set of physical operators that implement it. </li>
<li>Relations or temporary relations can layer the whole diagram, i.e., results of logical operators. </li>
<li>We can visualize the whole optimization diagram as a tree with different layers. <ul>
<li>The top layer is the query output, and the bottom contains all the relations. </li>
<li>The middle layers are the enumerations of different ordering. Each layer only performs one more operator than its last layer. </li>
<li>Hence, each pair of layers is connected to an undetermined physical operator. </li>
</ul>
</li>
<li>From the bottom layer up, we enumerate the possible physical algorithm of each logical operator. <ul>
<li>Then, estimate the cost of all possible physical algorithms. </li>
<li>Leave only the more efficient physical algorithm for each logical operator after comparing with only the possible physical algorithms of the same logical operator. </li>
</ul>
</li>
<li>When it reaches the top layer, we can determine the most efficient path of all possible paths. </li>
<li>Then, iteratively construct a “left-deep” join tree that minimizes the estimated amount of work to execute the plan. <ul>
<li>Generate a left-deep tree to take advantage of the pipeline. </li>
</ul>
</li>
</ol>
<h3 id="How-does-top-down-optimization-work"><a href="#How-does-top-down-optimization-work" class="headerlink" title="How does top-down optimization work?"></a>How does top-down optimization work?</h3><ol>
<li>Start with a logical plan of what we want the query to be. </li>
<li>Perform a branch-and-bound search to traverse the plan tree by converting logical operators into physical operators. <ul>
<li>Traversing from logical operator to logical operator enumerates different orderings. </li>
<li>Traversing from logical operators to physical operators enumerates different physical algorithms. </li>
<li>The layers are similar to bottom-up optimization. </li>
<li>When we meet a logical operator, we must estimate the cost of all possible physical algorithms. <ul>
<li>So, for each physical algorithm, we must go deeper until the bottom to calculate the estimation. </li>
<li>For the sub-logical operators in the physical algorithm, we will enumerate its optimal execution in the lower levels. </li>
</ul>
</li>
<li>During the search, we can cut off a branch if its cost is already more expensive than another branch we have already seen. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/07/12/Courses/15445/07-Query-Execution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/12/Courses/15445/07-Query-Execution/" class="post-title-link" itemprop="url">07 Query Execution</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-12 16:51:12" itemprop="dateCreated datePublished" datetime="2023-07-12T16:51:12+08:00">2023-07-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 15:06:34" itemprop="dateModified" datetime="2024-03-16T15:06:34+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Sequential-Execution"><a href="#Sequential-Execution" class="headerlink" title="Sequential Execution"></a>Sequential Execution</h1><h2 id="Processing-model"><a href="#Processing-model" class="headerlink" title="Processing model"></a>Processing model</h2><h3 id="What-does-the-processing-model-do"><a href="#What-does-the-processing-model-do" class="headerlink" title="What does the processing model do?"></a>What does the processing model do?</h3><ol>
<li>The processing model defines how the system executes a query plan, i.e., how to traverse the query plan tree. </li>
<li>There are two processing directions: <ul>
<li>Top-to-Bottom: Start with the root and “pull” data from its children. Tuples are always passed with function calls. </li>
<li>Bottom-to-Top: Start with leaf nodes and push data to their parents. Allows for tighter control of caches/registers in pipelines. </li>
</ul>
</li>
<li>There are three commonly used models: the iterator model (volcano/pipeline model), the materialization model, and the vectorized/batch model. </li>
</ol>
<h3 id="How-does-the-iterator-model-execute"><a href="#How-does-the-iterator-model-execute" class="headerlink" title="How does the iterator model execute?"></a>How does the iterator model execute?</h3><ol>
<li><p>In the iterator model, operators are executed top-to-down. </p>
</li>
<li><p>Each query plan operator implements a <code>Next()</code> function. On each invocation, the operator returns either a single tuple or a <code>null</code> marker if there are no more tuples. </p>
<ul>
<li><p>The operator implements a loop that calls <code>Next()</code> on its children to retrieve their tuples and then process them. </p>
</li>
<li><p><code>Next()</code> is first called on the root operator, and the nodes on the tree will call the <code>Next()</code> on their children recursively. </p>
</li>
</ul>
</li>
<li><p>This model allows for up pipelining. However, some operators must block until their children emit all their tuples. </p>
<ul>
<li>Joins must wait until all tuples in the leaf child are processed and build the hash table to process tuples from the right child further. Hence, the tuples from the left child must block the pipeline, while those from the right child can enable the pipeline. </li>
<li>Subqueries and ordering (<code>Order By</code>) clauses must also block the pipeline. These are called pipeline breakers. </li>
</ul>
</li>
<li><p>Output control works easily with this approach. We only need to add constraints on the root operator. </p>
</li>
</ol>
<h3 id="How-does-the-materialization-model-execute"><a href="#How-does-the-materialization-model-execute" class="headerlink" title="How does the materialization model execute?"></a>How does the materialization model execute?</h3><ol>
<li><p>In the materialization model, operators are called bottom-to-up. </p>
</li>
<li><p>Each query plan operator implements an <code>Output()</code> function. </p>
<ul>
<li><p>On each invocation, the operator processes its input all at once and then emits its output all at once. </p>
</li>
<li><p>The operator materializes its output as a single result. </p>
</li>
<li>The operators can send either a materialized row or a single column. </li>
</ul>
</li>
<li><p>The DBMS can push down hints (e.g., <code>LIMIT</code>) to avoid scanning too many tuples. </p>
</li>
<li><p>The output can be either whole tuples (NSM) or subsets of columns (DSM). </p>
</li>
<li><p>This model is better for OLTP workloads because queries only access a small number of tuples at a time, which means lower execution and coordination overhead and fewer function calls. </p>
<ul>
<li>It is not good for OLAP queries with large intermediate results. </li>
</ul>
</li>
</ol>
<h3 id="How-does-the-vectorization-model-execute"><a href="#How-does-the-vectorization-model-execute" class="headerlink" title="How does the vectorization model execute?"></a>How does the vectorization model execute?</h3><ol>
<li>The problem with the iterator model is that it can only process one tuple at a time when we can take multiple tuples and vectorize them to process in parallel (SIMD). </li>
<li>The vectorization model is similar to the iterator model except that each operator emits a batch of tuples instead of a single tuple. </li>
<li>This is ideal for OLAP queries because it greatly reduces the number of invocations per operator. <ul>
<li>It allows operators to use vectorized (SIMD) instructions more easily to process tuple batches. </li>
</ul>
</li>
</ol>
<h2 id="Access-methods"><a href="#Access-methods" class="headerlink" title="Access methods"></a>Access methods</h2><h3 id="How-can-we-optimize-sequential-scans-with-data-skipping"><a href="#How-can-we-optimize-sequential-scans-with-data-skipping" class="headerlink" title="How can we optimize sequential scans with data skipping?"></a>How can we optimize sequential scans with data skipping?</h3><ol>
<li><p>The first approach is approximate queries. </p>
<ul>
<li><p>This method is lossy and may return incorrect results, but it is OK. </p>
</li>
<li><p>Execute queries on a sampled subset of the entire table to produce approximate results. </p>
</li>
</ul>
</li>
<li><p>The second approach is zone maps. </p>
<ul>
<li>This method is lossless. </li>
<li>Pre-computed aggregates are used for the attribute values on a page. DBMS checks the zone map first to decide whether it wants to access the page. </li>
<li>The trade-off is between page size and filter efficacy. </li>
</ul>
</li>
</ol>
<h3 id="What-is-a-multi-index-scan"><a href="#What-is-a-multi-index-scan" class="headerlink" title="What is a multi-index scan?"></a>What is a multi-index scan?</h3><ol>
<li>If there are multiple indexes that the DBMS can use for a query, one method for DBMS to execute is to try to filter tuples with an index with the least number of tuples matches and filter other indexes based on the filtered tuples of previous indexes. <ul>
<li>This is ideal in the case that some indexes have little tuples that match. Filtering those indexes first can significantly reduce the number of tuples to process in the following indexes. </li>
</ul>
</li>
<li>If all indexes have a lot of matching tuples, we can use another method:<ul>
<li>Compute sets of Record IDs using each matching index. Combine these sets based on the query’s predicates (union or intersect). Retrieve the records and apply any remaining predicates. </li>
<li>In this way, we can reduce a log of I/O and memory space by only fetching Record IDs in the first phase instead of fetching the entire tuple. </li>
</ul>
</li>
</ol>
<h2 id="Modification-queries"><a href="#Modification-queries" class="headerlink" title="Modification queries"></a>Modification queries</h2><h3 id="How-should-we-execute-update-and-delete-queries"><a href="#How-should-we-execute-update-and-delete-queries" class="headerlink" title="How should we execute update and delete queries?"></a>How should we execute update and delete queries?</h3><ol>
<li>Operators that modify the database (<code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code>) are responsible for modifying the target table and its indexes. <ul>
<li>The output of these operators can either be Record IDs or tuple data (i.e., <code>RETURNING</code>). </li>
</ul>
</li>
<li>To update or delete, child operators pass Record IDs to target tuples. <ul>
<li>The operator should remove the corresponding item from the index. </li>
<li>Then, the operator can modify the tuple or remove the tuple. </li>
<li>The update should re-insert the modified tuple into the index again. </li>
</ul>
</li>
<li>Updates have a possible Halloween problem: <ul>
<li>When we re-insert the modified tuple, its new place may be ahead of the cursor, i.e., we will pass the new tuple again in the future. <ul>
<li>For example, when the index is sorted according to an attribute, the modification increases that attribute. </li>
</ul>
</li>
<li>An update operation changes the physical location of a tuple, which causes a scan operator to visit the tuple multiple times. It can occur on clustered tables or index scans. </li>
<li>The solution is to keep track of previously seen tuples (e.g., through Record IDs). </li>
</ul>
</li>
</ol>
<h3 id="How-should-we-execute-insert-queries"><a href="#How-should-we-execute-insert-queries" class="headerlink" title="How should we execute insert queries?"></a>How should we execute insert queries?</h3><ol>
<li>The first choice is to materialize tuples inside of the operator. <ul>
<li>The insert operator needs to implement its method of how to materialize tuples. </li>
</ul>
</li>
<li>The second choice is for the operator to insert any tuple passed in from child operators. <ul>
<li>The insert operator only needs to accept tuples from its children instead of implementing itself. </li>
</ul>
</li>
</ol>
<h2 id="How-to-evaluate-expressions"><a href="#How-to-evaluate-expressions" class="headerlink" title="How to evaluate expressions?"></a>How to evaluate expressions?</h2><ol>
<li>The DBMS represents a <code>WHERE</code> clause as an expression tree. </li>
<li>The nodes in the tree represent different expression types: Comparisons (<code>=, &lt;, &gt;, !=</code>), conjunction (<code>AND</code>), disjunction (<code>OR</code>), arithmetic operators (<code>+, -, *, /, %</code>), constant values, tuple attribute references. </li>
<li>When evaluating the expression, DFS through the expression tree from the root. <ul>
<li>The performance is poor because the DBMS traverses the tree, and for each node it visits, it must figure out what the operator needs to do. </li>
</ul>
</li>
<li>A better approach is to evaluate the expression directly. <ul>
<li>Compile a function of the express (e.g., JIT compilation). In evaluation, DBMS executes the compiled function instead of traversing the tree. </li>
</ul>
</li>
</ol>
<h1 id="Parallel-execution"><a href="#Parallel-execution" class="headerlink" title="Parallel execution"></a>Parallel execution</h1><h2 id="Parallel-and-distributed"><a href="#Parallel-and-distributed" class="headerlink" title="Parallel and distributed"></a>Parallel and distributed</h2><h3 id="Why-care-about-parallel-execution"><a href="#Why-care-about-parallel-execution" class="headerlink" title="Why care about parallel execution?"></a>Why care about parallel execution?</h3><ol>
<li>It increased performance for potentially the same hardware resources, i.e., to gain higher throughput and lower latency. </li>
<li>It increased the responsiveness of the system. </li>
<li>It potentially lowers the total cost of ownership (TCO). <ul>
<li>Fewer machines means less parts / physical footprint / energy consumption. </li>
</ul>
</li>
</ol>
<h3 id="What-are-the-similarities-and-differences-between-parallel-and-distributed-DBMS"><a href="#What-are-the-similarities-and-differences-between-parallel-and-distributed-DBMS" class="headerlink" title="What are the similarities and differences between parallel and distributed DBMS?"></a>What are the similarities and differences between parallel and distributed DBMS?</h3><ol>
<li>Similarities:<ul>
<li>The database is spread across multiple resources to improve different aspects of the DBMS. </li>
<li>They need to make the database appear as a single logical instance to the application, regardless of physical organization. </li>
<li>SQL query for a single-resource DBMS should generate the same result on a parallel or distributed DBMS. </li>
</ul>
</li>
<li>Differences:<ul>
<li>For parallel DBMSs, resources are physically close to each other. Hence, resources communicate over high-speed interconnect. And communication is assumed to be cheap and reliable. </li>
<li>For distributed DBMSs, resources can be far from each other. Resources communicate using slow(er) interconnect. Therefore, communication costs and problems cannot be ignored. And communication is considered unreliable. </li>
</ul>
</li>
</ol>
<h2 id="Process-model"><a href="#Process-model" class="headerlink" title="Process model"></a>Process model</h2><h3 id="What-does-the-process-model-of-parallel-DBMSs-need-to-do"><a href="#What-does-the-process-model-of-parallel-DBMSs-need-to-do" class="headerlink" title="What does the process model of parallel DBMSs need to do?"></a>What does the process model of parallel DBMSs need to do?</h3><ol>
<li>It defines how the system is architected to support concurrent requests from a multi-user application. </li>
<li>A worker is the DBMS component responsible for executing tasks on behalf of the client and returning the results. </li>
<li>There are three approaches: process per DBMS worker, thread per DBMS worker, and embedded DBMS.</li>
</ol>
<h3 id="What-is-the-process-per-worker-model"><a href="#What-is-the-process-per-worker-model" class="headerlink" title="What is the process per worker model?"></a>What is the process per worker model?</h3><ol>
<li>Each worker is a separate OS process. Hence, this model relies entirely on the OS scheduler. </li>
<li>When an application connects with DBMS, it connects with a dispatcher process. The dispatcher picks the application processes. Then, the application communicates with the process directly. </li>
<li>The processes can use shared-memory for global data structures. </li>
<li>The advantage of this model is that a process crash does not take down the entire system. </li>
</ol>
<h3 id="What-is-the-thread-per-worker-model"><a href="#What-is-the-thread-per-worker-model" class="headerlink" title="What is the thread per worker model?"></a>What is the thread per worker model?</h3><ol>
<li>The whole DBMS is a single process with multiple worker threads in this model. </li>
<li>DBMS (mostly) manages its scheduling by controlling what each thread is doing. <ul>
<li>This also means less overhead per context switch and that DBMS does not have to manage shared memory. </li>
</ul>
</li>
<li>There may or may not be a dispatcher thread in the front. <ul>
<li>Applications may connect to the dispatcher, and the dispatcher immediately forwards the request to another thread while the application does not know about it. </li>
<li>Or applications can use the same scheme as the process per worker model. </li>
</ul>
</li>
<li>In this model, a thread crash may kill the entire system. </li>
</ol>
<h3 id="What-is-considered-when-DBMS-scheduling-threads"><a href="#What-is-considered-when-DBMS-scheduling-threads" class="headerlink" title="What is considered when DBMS scheduling threads?"></a>What is considered when DBMS scheduling threads?</h3><p>The DBMS decides where, when, and how to execute each query plan. </p>
<ol>
<li>How many tasks should it use?</li>
<li>How many CPU cores should it use?</li>
<li>What CPU core should the tasks execute on? </li>
<li>Where should a task store its output?</li>
</ol>
<h3 id="What-is-the-embedded-DBMS-model"><a href="#What-is-the-embedded-DBMS-model" class="headerlink" title="What is the embedded DBMS model?"></a>What is the embedded DBMS model?</h3><ol>
<li>In the aforementioned and most common systems, applications are in separate machines. They are connected through TCP or socket. Even if the applications crashed, DBMS remains running. </li>
<li>In embedded DBMS, DBMS runs inside of the same address space as the application. The application is (mostly) responsible for threads and scheduling. </li>
<li>The application may support outside connections. </li>
</ol>
<h2 id="Query-level-parallelism"><a href="#Query-level-parallelism" class="headerlink" title="Query-level parallelism"></a>Query-level parallelism</h2><h3 id="What-are-the-query-level-parallelisms"><a href="#What-are-the-query-level-parallelisms" class="headerlink" title="What are the query-level parallelisms?"></a>What are the query-level parallelisms?</h3><ol>
<li>Inter-Query: Execute multiple disparate queries simultaneously. <ul>
<li>This parallelism increases throughput and reduces latency. It improves overall performance by allowing multiple queries to execute simultaneously. </li>
<li>If queries are read-only, almost no explicit coordination between queries is required. Buffer pool can handle most of the sharing if necessary. </li>
<li>If multiple queries update the database simultaneously, this is hard to do correctly. </li>
</ul>
</li>
<li>Intra-Query: Execute the operations of a single query in parallel. <ul>
<li>This parallelism decreases latency for long-running queries, especially for OLAP queries. It improves the performance of a single query by executing its operators in parallel. </li>
<li>Organize operators in terms of a producer/consumer paradigm. </li>
</ul>
</li>
</ol>
<h3 id="How-can-we-achieve-intra-query-parallelism"><a href="#How-can-we-achieve-intra-query-parallelism" class="headerlink" title="How can we achieve intra-query parallelism?"></a>How can we achieve intra-query parallelism?</h3><ol>
<li>The first approach is using intra-operator (horizontal) parallelism. <ul>
<li>Decompose operators into independent fragments that perform the same function on different subsets of data. </li>
<li>Those decomposed operators are copied for each thread in the generated query plan. </li>
<li>The DBMS inserts an exchange operator into the query plan to coalesce/split results from multiple children/parent operators. The exchange operators are similar with barriers stating that data cannot be sent to the parent until all results are received. </li>
<li>There are three kinds of exchange operators:<ul>
<li>Gather: Combine the results from multiple workers into a single output stream. </li>
<li>Distribute: Split a single input stream into multiple output streams. </li>
<li>Repartition: Shuffle multiple input streams across multiple output streams. </li>
</ul>
</li>
</ul>
</li>
<li>The second approach is using inter-operator (vertical / pipeline) parallelism. <ul>
<li>Operations are overlapped with pipeline data from one stage to the next without materialization. </li>
<li>Each operator is a worker. Workers execute operators from different segments of a query plan at the same time. </li>
</ul>
</li>
<li>We can also combine these two approaches, which is called bushy parallelism. </li>
</ol>
<h2 id="I-O-parallelism"><a href="#I-O-parallelism" class="headerlink" title="I/O parallelism"></a>I/O parallelism</h2><h3 id="What-is-the-problem-of-query-level-parallelism"><a href="#What-is-the-problem-of-query-level-parallelism" class="headerlink" title="What is the problem of query-level parallelism?"></a>What is the problem of query-level parallelism?</h3><ol>
<li>Using additional processes/threads to execute queries in parallel won’t help if the disk is always the main bottleneck. </li>
<li>It can sometimes worsen the DBMS’s performance if a worker is accessing different segments of the disk simultaneously. </li>
</ol>
<h3 id="How-can-we-parallel-I-Os-with-multi-disk"><a href="#How-can-we-parallel-I-Os-with-multi-disk" class="headerlink" title="How can we parallel I/Os with multi-disk?"></a>How can we parallel I/Os with multi-disk?</h3><ol>
<li><p>Split the DBMS across multiple storage devices to improve disk bandwidth latency. </p>
<ul>
<li>There are many options<ul>
<li>Multiple disks per database, one database per disk, one relation per disk, and split relation across multiple disks. </li>
<li>The main trade-off is the number of disks and I/O parallelism. </li>
</ul>
</li>
</ul>
</li>
<li><p>Configure OS/hardware to store the DBMS’s files across multiple storage<br>devices, e.g., storage appliances, RAID configuration. </p>
<ul>
<li><p>This is transparent to the DBMS. </p>
</li>
<li><p>RAID 0 strips data into different disks. Each disk stores different data. </p>
</li>
<li>RAID 1 mirrors data in different disks. Each disk stores the same data. </li>
</ul>
</li>
</ol>
<h3 id="How-can-we-partition-the-database"><a href="#How-can-we-partition-the-database" class="headerlink" title="How can we partition the database?"></a>How can we partition the database?</h3><ol>
<li>Some DBMSs allow you to specify the disk location of each individual database. <ul>
<li>The buffer pool manager maps a page to a disk location. </li>
<li>This is also easy to do at the filesystem level if the DBMS stores each database in a separate directory. </li>
<li>The DBMS recovery log file might still be shared if transactions can update multiple databases. </li>
</ul>
</li>
<li>Logical splitting splits a single logical table into disjoint physical segments stored/managed separately. <ul>
<li>Partitioning should (ideally) be transparent to the application. </li>
<li>The application should only access logical tables and not have to worry about how things are physically stored. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/07/07/OpenSource/BusTub/Project-2-B-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/07/07/OpenSource/BusTub/Project-2-B-Tree/" class="post-title-link" itemprop="url">Project #2: B+Tree</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-07-07 13:28:28" itemprop="dateCreated datePublished" datetime="2023-07-07T13:28:28+08:00">2023-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 23:24:56" itemprop="dateModified" datetime="2024-03-16T23:24:56+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/BusTub/" itemprop="url" rel="index"><span itemprop="name">BusTub</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@<a href="c">toc</a></p>
<h1 id="B-Tree-Page"><a href="#B-Tree-Page" class="headerlink" title="B+Tree Page"></a>B+Tree Page</h1><ol>
<li><p>The <code>size_</code> in each page means the number of stored values, not keys, i.e., in internal nodes, <code>size_</code> is $1$ larger than the number of keys. </p>
</li>
<li><p><code>KeyComparator</code> accepts two keys to compare, which will return $0$ when they are equal, $-1$ for the first key “smaller” than the second key, $1$ for the second key being larger. </p>
</li>
<li><p>We need to design the semantic of the binary search that will be used in search, insert, and delete a key inside a node. </p>
<ul>
<li>In the leaf node: <ul>
<li>For search and delete, we want this function to tell us the index of the key if it exists. </li>
<li>For insert, we want this function to indicate the index to which we need to place the key. </li>
</ul>
</li>
<li>In the internal node:<ul>
<li>For the search, we only want it to inform us of the child that might have the given key. </li>
<li>For the insert, we want it to return the page ID to find a proper leaf page to store the key in forward search and give the index we need to place the split key when the backward split is required. </li>
<li>For delete, we want it the same as for insert in the forward search and to provide the index to help merge two children when the backward merge is required. </li>
</ul>
</li>
<li>In the following implementation of binary search, it will return the index of the key if it exists, or it will return the largest index with a smaller key. <ul>
<li>Notably, if the given key is smaller than all keys in the node, it will return $-1$. </li>
<li>In the internal node, we would expect the smallest possible result is $0$ since the first key is <code>NULL,</code> which should be smaller than any other. </li>
</ul>
</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">BinarySearch</span><span class="params">(KeyType key, KeyComparator comparator)</span> <span class="type">const</span> -&gt; <span class="type">int</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> lo = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int</span> hi = <span class="built_in">GetSize</span>();</span><br><span class="line">  <span class="keyword">while</span> (lo &lt; hi) &#123;</span><br><span class="line">    <span class="type">int</span> mid = (lo + hi) / <span class="number">2</span>;</span><br><span class="line">    <span class="keyword">if</span> (<span class="number">0</span> &lt; mid &amp;&amp; <span class="built_in">comparator</span>(key, array_[mid].first) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      hi = mid;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      lo = mid + <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> lo - <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>FindNextPageID</code> will provide the proper child for the caller to access to find a certain key. </p>
<ul>
<li>For a leaf node, this will only be used in the B+Tree search. If the binary search result is $-1$, we can conclude that that key does not exist and should tell the caller that. </li>
<li>For an internal node, this will be used in the B+Tree search or the forward search in insert and delete. If the binary search result is $-1$, then we should return the first value to indicate the key is smaller than all keys. </li>
</ul>
</li>
<li><p><code>InsertKey</code> should </p>
</li>
</ol>
<h1 id="B-Tree-search"><a href="#B-Tree-search" class="headerlink" title="B+Tree search"></a>B+Tree search</h1><ol>
<li>The thought is simple: we go down from the root until hit a leaf node. If the key is in the leaf node, return the value. Otherwise, the key does not exist. </li>
<li>For concurrency control, we can only release a page’s latch after acquiring its child’s latch. <ul>
<li>This can be implemented with the move assignment operation of <code>PageGuard</code>. In the execution, the right expression will first acquire the latch of the next page, then destroy the original page guard of the left variable. </li>
</ul>
</li>
</ol>
<h1 id="B-Tree-insert"><a href="#B-Tree-insert" class="headerlink" title="B+Tree insert"></a>B+Tree insert</h1><ol>
<li><p>For concurrency control, I implemented both optimistic and pessimistic schemes. </p>
<ul>
<li><p>The program will first try with optimistic search with only a write latch on the leaf node. </p>
</li>
<li><p>If the leaf node may overflow, release all latches and start again trying to acquire a write latch for all nodes. </p>
</li>
</ul>
</li>
<li><p>In the optimistic search, we can use the queue in <code>Context</code>. </p>
<ul>
<li>When fetched a new page guard, we pushed it to the back of <code>ctx.read_set_</code> and popped the front element out of the queue. We need to access the last element of the queue every time to find the next page to read. </li>
<li>When we realize we have reached the leaf node, we hold a read latch for that page. Hence, we still need to release the read latch and re-acquire the write latch, i.e., we cannot release the latch of the last internal page when we first acquire the leaf page. </li>
<li>The solution is to release the latch before reading the “grandchild” of it. </li>
</ul>
</li>
<li><p>In the pessimistic search, we must acquire a write latch for all pages we want to access. </p>
<ul>
<li>We can check whether a node is safe after acquiring its write latch. If the node is safe, we can release all latches acquired before it, including the header page. </li>
</ul>
</li>
<li><p>In the insert function, there are three cases to handle: </p>
<ul>
<li>The first case is that this is an empty tree, i.e., the root_page_id_ in the header page is invalid. We need to create a new page for the node and update the header page. </li>
<li>The second case is that the leaf node won’t overflow where a simple insertion is enough. </li>
<li>The last case is that the leaf node might overflow. </li>
</ul>
</li>
<li><p>When the leaf node might overflow: </p>
<ul>
<li>After acquiring the write latches, we need to check whether there is an overflow again in case the other thread already handled overflow, causing an unexpected split. </li>
<li><code>SolveLeafOverflow</code><ul>
<li>When a leaf reaches max size after insertion, it will immediately split. So, this is used after the insertion. </li>
<li>It will create a new page to store the larger half of the nodes and return the first key in the new page to insert to its parent node to indicate to this page. </li>
<li>Also, it needs to take care of the sibling pointers between leaf nodes. The new page will point to what the original page points to. The original page will point to the new page. </li>
</ul>
</li>
<li><code>SolveInternalOverflow</code><ul>
<li>Internal nodes won’t split immediately when they reach the maximum size. This means that internal nodes must split before insertion; otherwise, the address will overflow. </li>
<li>The process is similar to the leaf case, except it will choose a proper page to insert the key after splitting. (Or it does not need to split if it is a safe node). </li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="B-Tree-delete"><a href="#B-Tree-delete" class="headerlink" title="B+Tree delete"></a>B+Tree delete</h1><ol>
<li>The concurrency control is similar to the insert operation, except that the condition of whether a node is safe differs. </li>
<li>Compare with insertion, <ul>
<li>if the tree is empty, there is nothing else to do; </li>
<li>If the leaf node won’t underflow, a simple deletion is enough; </li>
<li>If the leaf node might underflow, we need to handle it and possibly follow cascade underflow. </li>
</ul>
</li>
<li>When the leaf node might overflow:<ul>
<li>Similar to the insertion situation, we must re-acquire write latches and check whether another thread handles the underflow. </li>
<li>If the underflowed leaf is the root, i.e., the tree has only one node, we do not need to do anything further. </li>
<li><code>SolveLeafUnderflow</code><ul>
<li>There are three situations: the left sibling can borrow a key, the right sibling can borrow a key, or neither sibling can borrow a key. </li>
<li>When we can borrow a key, the underflow is solved easily, and there is no more cascading. We need to modify the key in the parent node that separates the two involved nodes to the new first key of the right node. </li>
<li>When merging with one of the siblings, we must also handle the pointers between leaf nodes. If we move the data of the left node to the right node and delete the left node, it is hard for us to modify the pointer of the left of the left node. Instead, if we delete the right node, we only need to modify the left node’s pointer to the right node’s original pointer. </li>
<li>If a leaf only has a left sibling or right sibling to merge with, then we do not have a choice. </li>
</ul>
</li>
<li><code>SolveInternalUnderflow</code><ul>
<li>The process is similar to SolveLeafUnderflow, except for how to borrow a key. </li>
<li>When a node borrows a key to its left sibling, it will borrow the first valid key and the fire value, i.e., <code>array_[1].first</code> and <code>array_[0].second</code>. </li>
<li>When a node borrows a key to its right sibling, it will borrow the last key-value pair. The node accpeting those keys will use the key as its first valid key and the value as its first valid value. </li>
<li>We must set the corresponding key in the parent node to the borrowed key. </li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/06/28/Courses/15445/06-Operator-Algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/28/Courses/15445/06-Operator-Algorithms/" class="post-title-link" itemprop="url">06 Operator Algorithms</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-28 13:16:33" itemprop="dateCreated datePublished" datetime="2023-06-28T13:16:33+08:00">2023-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 14:50:53" itemprop="dateModified" datetime="2024-03-16T14:50:53+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Execute-queries"><a href="#Execute-queries" class="headerlink" title="Execute queries"></a>Execute queries</h1><h2 id="How-are-queries-planned"><a href="#How-are-queries-planned" class="headerlink" title="How are queries planned?"></a>How are queries planned?</h2><ol>
<li>The operators are arranged in an abstract syntax tree. Data flows from the leaves of the tree up towards the root. </li>
<li>The leaf nodes are access methods, e.g., scanning index and scanning table, feeding data up along its path to its parent node to do the processing. </li>
<li>The output of the root node is the result of the query. </li>
<li>This tree only describes a logical plan, i.e., instead of telling what implementation to use, it is just the logical flow we want. SQL only declares a logical plan, while the database system’s job is figuring out the optimal execution. </li>
</ol>
<h2 id="What-is-the-assumption-of-algorithms-in-database-systems"><a href="#What-is-the-assumption-of-algorithms-in-database-systems" class="headerlink" title="What is the assumption of algorithms in database systems?"></a>What is the assumption of algorithms in database systems?</h2><ol>
<li>Just like it cannot assume that a table fits entirely in memory, a disk-oriented DBMS cannot assume that query results fit in memory. </li>
<li>We will use the buffer pool to implement algorithms that need to spill to disk. </li>
<li>We will also prefer algorithms that maximize the amount of sequential I/O.</li>
</ol>
<h1 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h1><h2 id="What-implementation-should-we-use-to-execute-a-query-containing-an-ORDER-BY-with-a-LIMIT"><a href="#What-implementation-should-we-use-to-execute-a-query-containing-an-ORDER-BY-with-a-LIMIT" class="headerlink" title="What implementation should we use to execute a query containing an ORDER BY with a LIMIT?"></a>What implementation should we use to execute a query containing an ORDER BY with a LIMIT?</h2><ol>
<li>The DBMS only needs to scan the data once to find the top-N elements. </li>
<li>The ideal scenario for heapsort is when the top-N elements fit in memory so that the DBMS only has to maintain an in-memory sorted priority queue while scanning the data. </li>
</ol>
<h2 id="What-if-the-data-is-too-large-to-fit-in-memory-for-any-clause-including-ORDER-BY-with-a-LIMIT"><a href="#What-if-the-data-is-too-large-to-fit-in-memory-for-any-clause-including-ORDER-BY-with-a-LIMIT" class="headerlink" title="What if the data is too large to fit in memory? (for any clause including ORDER BY with a LIMIT)"></a>What if the data is too large to fit in memory? (for any clause including ORDER BY with a LIMIT)</h2><ol>
<li>We do not want to use quick sort in this scenario since data spilling to disk will cause too many random access. </li>
<li>We can use external merge sort that splits data into separate runs, sorts them individually, and then combines them into longer sorted runs. </li>
<li>A run is a list of key/value pairs. <ul>
<li>Keys are the attribute(s) used to compare and compute the sort order. </li>
<li>Values have two choices: it can either be the actual tuple data (i.e., early materialization) or be the Record IDs (i.e., late materialization)<ul>
<li>The advantage of early materialization is that it can produce results faster, while the disadvantage is that it needs to copy more data during the procedure. </li>
<li>The advantage of late materialization is that it can only fetch wanted data while it needs to find the actual data elsewhere (probably involving another disk I/O). </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="How-to-perform-an-external-merge-sort"><a href="#How-to-perform-an-external-merge-sort" class="headerlink" title="How to perform an external merge sort?"></a>How to perform an external merge sort?</h2><ol>
<li>Data is broken up into $N$ pages. The DBMS has a finite number of $B$ buffer pool pages to hold input and output data.</li>
<li>In the first phase, sort chunks of data that fit in memory and then write back the sorted chunks to a file on disk. <ul>
<li>In the first pass, we can read all $B$ pages of the table into memory, sort pages into runs, and write them back to disk. </li>
<li>Do not sort in the buffer since we do not wish other threads to see some partially sorted data, which may cause errors. Copy the data to somewhere else and copy it back to the buffer after it is sorted. </li>
</ul>
</li>
<li>In the second phase, combine sorted runs into larger chunks. <ul>
<li>In the following passes, each pass use $B-1$ pages for input and $1$ page for output. </li>
<li>Recursively merge pairs of runs into runs $B-1$-times as long. </li>
</ul>
</li>
<li>In general, the first pass creates $\lceil N/B\rceil$ sorted runs of size $B$ and the following passes are $(B-1)$-way merge. <ul>
<li>The number of passes is $1+\lceil\log_{B-1}\lceil N/B\rceil\rceil$. </li>
<li>The total I/O cost is $2N\cdot (number\ of\ passes)$. </li>
</ul>
</li>
<li>In a 2-way external merge sort, instead of sorting the $B$ pages into a run in the first pass, each page is sorted into a run. <ul>
<li>So, the number of passes is $1+\lceil\log_2N\rceil$. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-hide-the-disk-I-O"><a href="#How-can-we-hide-the-disk-I-O" class="headerlink" title="How can we hide the disk I/O?"></a>How can we hide the disk I/O?</h2><ol>
<li>A typical setup is using $3$ pages ($2$ for input pages and $1$ for output page). Even if we have more buffer space available ($B&gt;3$), it does not effectively utilize them if the worker must block disk I/O. </li>
<li>We can prefetch the next run in the background and store it in a second buffer while the system processes the current run. </li>
<li>This method continuously utilizes the disk to reduce the wait time for I/O requests at each step. </li>
</ol>
<h2 id="How-can-we-optimize-comparison"><a href="#How-can-we-optimize-comparison" class="headerlink" title="How can we optimize comparison?"></a>How can we optimize comparison?</h2><ol>
<li>The first approach is code specialization. <ul>
<li>Instead of providing a comparison function as a pointer to the sorting algorithm, create a hardcoded version of the sort that is specific to a key type. </li>
</ul>
</li>
<li>For string keys, the second approach is suffix truncation. <ul>
<li>First, compare a binary prefix of keys instead of a slower string comparison. </li>
<li>Fallback to a slower version if prefixes are equal. </li>
</ul>
</li>
</ol>
<h2 id="How-to-use-B-Tree-for-sorting-since-it-is-sorted"><a href="#How-to-use-B-Tree-for-sorting-since-it-is-sorted" class="headerlink" title="How to use B+Tree for sorting since it is sorted?"></a>How to use B+Tree for sorting since it is sorted?</h2><ol>
<li>If the table that must be sorted already has a B+Tree index on the sort attribute(s), then we can use that to accelerate sorting. </li>
<li>Retrieve tuples in desired order by traversing the tree’s leaf pages. <ul>
<li>For clustered B+Tree, this is always better than external sorting because there is no computational cost, and all disk access is sequential. </li>
<li>For non-clustered B+Tree, this is almost always a bad idea. In general, one I/O per data record. </li>
</ul>
</li>
</ol>
<h1 id="Aggregations"><a href="#Aggregations" class="headerlink" title="Aggregations"></a>Aggregations</h1><h2 id="How-can-we-implement-aggregations"><a href="#How-can-we-implement-aggregations" class="headerlink" title="How can we implement aggregations?"></a>How can we implement aggregations?</h2><ol>
<li>The DBMS needs a way to find tuples with the same distinguishing attributes for grouping quickly. </li>
<li>We can use the aforementioned sorting algorithms for aggregations specified with an <code>ORDER BY</code> clause. </li>
<li>Hashing is a better alternative for queries that do not need the data to be ordered, e.g., forming groups in <code>GROUP BY</code> or removing duplicates in <code>DISTINCT</code>. Hashing can be computationally cheaper than sorting. <ul>
<li>For each record, check whether there is an entry in the hash table. For <code>DISTINCT</code>, discard the duplicate. For <code>GROUP BY</code>, perform aggregate computation. </li>
</ul>
</li>
</ol>
<h2 id="How-to-do-hashing-when-spilling-data-onto-a-disk"><a href="#How-to-do-hashing-when-spilling-data-onto-a-disk" class="headerlink" title="How to do hashing when spilling data onto a disk?"></a>How to do hashing when spilling data onto a disk?</h2><ol>
<li>The first phase is partition. Divide tuples into buckets based on the hash key. Write them out to disk when they get full. <ul>
<li>Use a hash function $h_1$ to split tuples into partitions on the disk. </li>
<li>A partition is one or more pages that contain a set of keys with the same hash value. </li>
<li>Assume that we have $B$ buffers. We will use $B-1$ buffers for the partitions and $1$ buffer for the input data. </li>
</ul>
</li>
<li>The second phase is ReHash. Build an in-memory hash table for each partition and compute the aggregation. <ul>
<li>For each partition on the disk, read it into memory and build an in-memory hash table based on a second hash function $h_2$. </li>
<li>Then, go through each bucket of this hash table to bring together matching tuples. </li>
<li>Each time, we can use $B-1$ pages as input pages and $1$ page as output page. <ul>
<li>In each round, we can read in $B-1$ partitions. </li>
<li>After each round is finished, we can clear the hash table since the next $B-1$ partitions definitely won’t have the same keys as in the last round. </li>
</ul>
</li>
</ul>
</li>
<li>During the ReHash phase, store pairs of the form $(GroupKey→RunningVal)$. <ul>
<li>When we want to insert a new tuple into the hash table if we find a matching $GroupKey$, update the $RunningVal$ appropriately. Else, insert a new $(GroupKey→RunningVal)$. </li>
<li>The running totals of different aggregation functions are as follows:<ul>
<li>$AVG(col)\rightarrow (COUNT,SUM)$</li>
<li>$SUM(col)\rightarrow(SUM)$</li>
<li>$COUNT(col)\rightarrow(COUNT)$</li>
<li>$MIN(col)\rightarrow(MIN)$</li>
<li>$MAX(col)\rightarrow(MAX)$</li>
</ul>
</li>
</ul>
</li>
</ol>
<h1 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h1><h2 id="Why-do-we-need-to-join"><a href="#Why-do-we-need-to-join" class="headerlink" title="Why do we need to join?"></a>Why do we need to join?</h2><ol>
<li>Tables are normalized in a relational database to avoid unnecessary repetition of information. </li>
<li>The join operator reconstructs the original tuples without any information loss. </li>
<li>Join is an important operator in both OLAP and OLTP systems. Especially for the OLAP system, joining could take up to $15\sim 50\%$ of time. </li>
</ol>
<h2 id="What-are-join-algorithms-doing"><a href="#What-are-join-algorithms-doing" class="headerlink" title="What are join algorithms doing?"></a>What are join algorithms doing?</h2><ol>
<li>The most important kind is binary joins using inner equijoin algorithms. <ul>
<li>Binary means that the operator takes two tables as input. </li>
<li>Inner means it matches a certain tuple in the left table with another in the right. </li>
<li>Equijoin means that the condition of matching two tuples is the equivalence of some attributes. </li>
</ul>
</li>
<li>There are also other joins. <ul>
<li>Multi-way joins take more than two tables as input, primarily in the research literature. </li>
<li>Besides equijoin, there could also be anti-join, non-equijoin, etc. </li>
</ul>
</li>
<li>Compared with cross-product, join is more efficient and can be carefully optimized. </li>
</ol>
<h2 id="What-are-the-outputs-of-join-algorithms"><a href="#What-are-the-outputs-of-join-algorithms" class="headerlink" title="What are the outputs of join algorithms?"></a>What are the outputs of join algorithms?</h2><ol>
<li>In <code>R JOIN S</code>, for tuple $r \in R$ and tuple $s \in S$ that match on join attributes, concatenate $r$ and $s$ together into a new tuple. </li>
<li>The output contents can vary depending on the query’s processing model, storage model, or data requirements. </li>
<li>Basically, there are two choices that are similar in terms of sort. <ul>
<li>Early materialization<ul>
<li>Copy the values for the attributes in outer and inner tuples into a new output tuple. </li>
<li>Subsequent query plan operators never need to return to the base tables to get more data.</li>
</ul>
</li>
<li>Late materialization<ul>
<li>Only copy the join keys and the matching tuples’ Record IDs. </li>
<li>This is ideal for column stores because the DBMS does not copy data not needed for the query. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="How-can-we-measure-the-cost-of-join"><a href="#How-can-we-measure-the-cost-of-join" class="headerlink" title="How can we measure the cost of join?"></a>How can we measure the cost of join?</h2><ol>
<li>We can measure the join cost by the number of I/Os to compute the join. </li>
<li>Output costs are ignored since they depend on the data. </li>
<li>In the following analysis, we assume there are $m$ tuples stored in $M$ pages in table $R$, $n$ tuples stored in $N$ pages in table $S$. </li>
</ol>
<h2 id="Join-algorithms"><a href="#Join-algorithms" class="headerlink" title="Join algorithms"></a>Join algorithms</h2><h3 id="Nested-loop-join"><a href="#Nested-loop-join" class="headerlink" title="Nested loop join"></a>Nested loop join</h3><h4 id="What-is-the-most-naive-algorithm"><a href="#What-is-the-most-naive-algorithm" class="headerlink" title="What is the most naive algorithm?"></a>What is the most naive algorithm?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> S:</span><br><span class="line">    emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure>
<ol>
<li>For $R\bowtie S$, he left the table $R$ in the outer loop is called the outer table, and $S$ is called the inner table. </li>
<li>For every tuple in $R$, it scans $S$ once. The cost is $M+(m\cdot N)$. </li>
<li>If we use the smaller table with fewer tuples as the outer table, we can perform better since the number of pages is significantly smaller than the number of tuples. </li>
</ol>
<h4 id="How-can-we-better-use-the-data-already-read-from-disk"><a href="#How-can-we-better-use-the-data-already-read-from-disk" class="headerlink" title="How can we better use the data already read from disk?"></a>How can we better use the data already read from disk?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> block B_R <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> block B_S <span class="keyword">in</span> S:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> B_R:</span><br><span class="line">      <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> B_S:</span><br><span class="line">        emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure>
<ol>
<li>For every read block $B_R$ and $B_S$, we try to compute as much as possible, i.e., pair all tuples in $B_R$ with all tuples in $B_S$. </li>
<li>Every block in R scans S once. The cost is $M+(M\cdot N)$. </li>
<li>$M\cdot N$ won’t be affected by the order of tables. However, the first term can be smaller if we let the smaller table with fewer pages be the outer table. </li>
</ol>
<h4 id="How-can-we-take-advantage-of-more-buffer-space"><a href="#How-can-we-take-advantage-of-more-buffer-space" class="headerlink" title="How can we take advantage of more buffer space?"></a>How can we take advantage of more buffer space?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> B-<span class="number">2</span> pages p_R <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> page p_S <span class="keyword">in</span> S:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> B_2 pages:</span><br><span class="line">      <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> p_S:</span><br><span class="line">        emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure>
<ol>
<li>Use $B-2$ buffers for scanning the outer table. Use one buffer for the inner table and one buffer for storing<br>output. </li>
<li>The cost is $M+(\lceil M/(B-2)\rceil\cdot N)$. </li>
<li>If the outer relation completely fits in memory, the cost is $M+N$. </li>
</ol>
<h4 id="Can-we-avoid-sequential-scans-by-using-an-index"><a href="#Can-we-avoid-sequential-scans-by-using-an-index" class="headerlink" title="Can we avoid sequential scans by using an index?"></a>Can we avoid sequential scans by using an index?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">tuple</span> r <span class="keyword">in</span> R:</span><br><span class="line">  <span class="keyword">for</span> <span class="built_in">tuple</span> s <span class="keyword">in</span> Index(r_i = s_j):</span><br><span class="line">    emit, <span class="keyword">if</span> r <span class="keyword">and</span> s <span class="keyword">match</span></span><br></pre></td></tr></table></figure>
<p>Assume the cost of each index probe is some constant $C$ per tuple. The cost is $M+(m\cdot C)$</p>
<h3 id="Sort-merge-join"><a href="#Sort-merge-join" class="headerlink" title="Sort-merge join"></a>Sort-merge join</h3><h4 id="What-is-the-process-of-sort-merge-join"><a href="#What-is-the-process-of-sort-merge-join" class="headerlink" title="What is the process of sort-merge join?"></a>What is the process of sort-merge join?</h4><ol>
<li>The first phase is to sort both tables using the join key(s). </li>
<li>In the second phase, we step through the two sorted tables with cursors and emit matching tuples. </li>
<li>The sort cost of the outer table is $2M\cdot(1+\lceil \log_{B-1}\lceil M/B\rceil\rceil)$ and the sort cost of the inner table is $2N\cdot (1+\lceil\log_{B-1}\lceil N/B \rceil \rceil)$. </li>
<li>The merge cost is $M+N$. <ul>
<li>The worst case for the merging phase is when the join attribute of all the tuples in both relations contains the same value.</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sort R,S on join keys</span><br><span class="line">cursor_R points to R_sorted, cursor_S points to S_sorted</span><br><span class="line"><span class="keyword">while</span> cursor_R <span class="keyword">and</span> cursor_S:</span><br><span class="line">  <span class="keyword">if</span> cursor_R &gt; cursor_S:</span><br><span class="line">    increment cursor_S</span><br><span class="line">  <span class="keyword">if</span> cursor_R &lt; cursor_S:</span><br><span class="line">    increment cursor_R</span><br><span class="line">    possible backtrack cursor_S</span><br><span class="line">  <span class="keyword">elif</span> cursor_R <span class="keyword">and</span> surcor_s <span class="keyword">match</span>:</span><br><span class="line">    emit</span><br><span class="line">    increment cursor_s</span><br></pre></td></tr></table></figure>
<h4 id="How-do-the-two-cursors-move"><a href="#How-do-the-two-cursors-move" class="headerlink" title="How do the two cursors move?"></a>How do the two cursors move?</h4><ol>
<li>The cursor of the outer table will only move forward. Specifically, it will only move forward when we can ensure that we have matched the current tuple with all possible tuples, i.e., when we have a larger inner tuple. </li>
<li>The cursor of the inner table may move both forward and backward. <ul>
<li>It moves forward when some later tuple in the inner table may match with the current tuple in an outer tuple, i.e., when an inner tuple is smaller than the outer tuple or when they match (there are possibly more matches in the following). </li>
<li>It moves backward when there might have been some missing matches in the past, i.e., when the outer cursor moves forward, the key is the same as the last one; we need to backtrack to the earliest tuple that matches the previous outer tuple. </li>
</ul>
</li>
</ol>
<h4 id="When-is-sort-merge-join-useful"><a href="#When-is-sort-merge-join-useful" class="headerlink" title="When is sort-merge join useful?"></a>When is sort-merge join useful?</h4><ol>
<li>We can save some cost when one or both tables are sorted using the join key. </li>
<li>When output must be sorted on the join key, if the join output is larger, we might use sort-merge join to produce sorted output directly. <ul>
<li>If the output has a small amount, hash join might still be the better way. </li>
</ul>
</li>
</ol>
<h3 id="Hash-join"><a href="#Hash-join" class="headerlink" title="Hash join"></a>Hash join</h3><h4 id="How-does-hash-join-work"><a href="#How-does-hash-join-work" class="headerlink" title="How does hash join work?"></a>How does hash join work?</h4><ol>
<li><p>The thought behind hash join is that: </p>
<ul>
<li>If tuple $r \in R$ and a tuple$s \in S$ satisfy the join condition; they have the same value for the join attributes. </li>
<li>If that value is hashed to some partition $i$, the $R$ tuple must be in $r_i$ and the $S$ tuple in $s_i$. </li>
<li>Therefore, $R$ tuples in $r_i$ need only to be compared with $S$ tuples in $s_i$. </li>
</ul>
</li>
<li><p>In the first phase (build), we scan the outer relation and populate a hash table using the hash function $h_1$ on the join attributes. </p>
<p>In the second phase (probe), scan the inner relation and use $h_1$ on each tuple, jump to a location in the hash table, and find a matching tuple. </p>
</li>
<li><p>The keys stored in the hash table are the attribute(s) on which the query is joining the tables. We always need the original key to verify that we have a correct match in case of hash collisions. </p>
<p>The values stored vary per implementation, which depends on what the operators above the join in the query plan expect as its input. </p>
</li>
<li><p>Assume that we have enough buffers, we need to read and write both tables with the cost of $2(M+N)$ in the partitioning phase, and read both tables with the cost of $M+N$ in the probing phase. </p>
<ul>
<li>We can see that there is no constraint on the size of the inner table. </li>
</ul>
</li>
</ol>
<h4 id="How-big-of-a-table-can-we-hash-using-this-approach"><a href="#How-big-of-a-table-can-we-hash-using-this-approach" class="headerlink" title="How big of a table can we hash using this approach?"></a>How big of a table can we hash using this approach?</h4><ol>
<li>In the first phase of building a hash table, we can use at most $B-1$ spill partitions, leaving one page as an input buffer. When one partition is full, we should write it out on the disk and clear it. </li>
<li>total number of both the outer and inner table of each partition should be no more than $B$ blocks big so that in the second phase, we can store all tuples in the same partition in memory. </li>
<li>The total number of pages used is $B\cdot (B-1)$. A hash table of $N$ pages need about $\sqrt{N}$ buffers if the hash distribution is even. </li>
<li>When including the fudge factor $f&gt;1$ when the hash distribution is skewed, we need $B\cdot\sqrt{f\cdot N}$. </li>
</ol>
<h4 id="Can-we-optimize-the-search-for-tuples-that-do-not-have-any-match"><a href="#Can-we-optimize-the-search-for-tuples-that-do-not-have-any-match" class="headerlink" title="Can we optimize the search for tuples that do not have any match?"></a>Can we optimize the search for tuples that do not have any match?</h4><ol>
<li>We can create a Bloom filter during the build phase when the key will likely not exist in the hash table. This method is called Bloom filter or sideways information passing. </li>
<li>The Bloom filter is a probabilistic data structure (bitmap) that answers set membership queries. <ul>
<li>False negatives will never occur, while false positives can sometimes occur. </li>
<li>To insert a key into the filter, we use $k$ hash functions to set all $k$ bits to $1$. </li>
<li>During the lookup of a key, the key may exist if all $k$ bits hashed by the same $k$ hash function are all $1$. The key definitely does not exist if one of the $k$ bits is $0$. </li>
</ul>
</li>
</ol>
<h4 id="What-if-we-lack-the-memory-to-fit-the-entire-hash-table"><a href="#What-if-we-lack-the-memory-to-fit-the-entire-hash-table" class="headerlink" title="What if we lack the memory to fit the entire hash table?"></a>What if we lack the memory to fit the entire hash table?</h4><ol>
<li>We can use the recursive hash join (GRACE hash join). </li>
<li>Similar to the aforementioned algorithm, both tables should be hashed into the same number of buckets with the same hash function. </li>
<li>Perform regular hash join on each pair of matching buckets at the same level between two tables. </li>
<li>If the buckets do not fit in memory, use recursive partitioning to split the tables into chunks that will fit. <ul>
<li>Build another hash table for $bucket_{R,i}$ using hash function $h_2$ (with $h_2≠h_1$). </li>
<li>Then, probe it for each tuple of the other table’s bucket at that level. </li>
</ul>
</li>
<li><strong>Hybrid hash join</strong>: If the keys are skewed, the DBMS keeps the hot partition in-memory and immediately performs the comparison instead of spilling it to disk. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/06/27/OpenSource/BusTub/Project-1-Buffer-Pool/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/27/OpenSource/BusTub/Project-1-Buffer-Pool/" class="post-title-link" itemprop="url">Project #1: Buffer Pool</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-27 15:29:31" itemprop="dateCreated datePublished" datetime="2023-06-27T15:29:31+08:00">2023-06-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 23:13:06" itemprop="dateModified" datetime="2024-03-16T23:13:06+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/" itemprop="url" rel="index"><span itemprop="name">Open Source Code</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Source-Code/BusTub/" itemprop="url" rel="index"><span itemprop="name">BusTub</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@<a href="c">toc</a></p>
<h1 id="Task-1-LRU-K-Replacement-Policy"><a href="#Task-1-LRU-K-Replacement-Policy" class="headerlink" title="Task #1 - LRU-K Replacement Policy"></a>Task #1 - LRU-K Replacement Policy</h1><ol>
<li><p>Evict rule: </p>
<ul>
<li>When all evictable frames have more than $K$ access records, evict the one whose backward k-distance is maximum. </li>
<li>When some frames only have less than $K$ access records, evict the one with the earliest first access record among those less than $K$ frames. </li>
</ul>
</li>
<li><p>Comparison: </p>
<ul>
<li>When each frame is created or stored with a new page, push an access record of $0$ to its history to represent $+\inf$. </li>
<li>Each comparison uses the first two records in the list. If the first record (the earliest backward most k-distance) is the same, it must be $0$ causing a comparison of the second record, representing the true first access of that frame. </li>
<li>Remember to update the second comparison timestamp when the first time, find $0$ in the first timestamp. </li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> k_timestamp = frame.second.<span class="built_in">GetKTimestamp</span>();</span><br><span class="line"><span class="type">size_t</span> sec_timestamp = frame.second.<span class="built_in">GetSecondTimestamp</span>();</span><br><span class="line"><span class="keyword">if</span> (k_timestamp &lt; cmp_k_timestamp) &#123;</span><br><span class="line">	victim = frame.first;</span><br><span class="line">	cmp_k_timestamp = k_timestamp;</span><br><span class="line">	<span class="keyword">if</span> (k_timestamp == <span class="number">0</span>) &#123;</span><br><span class="line">		cmp_sec_timestamp = sec_timestamp;</span><br><span class="line">	&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (k_timestamp == cmp_k_timestamp &amp;&amp; sec_timestamp &lt; cmp_sec_timestamp) &#123;</span><br><span class="line">	victim = frame.first;</span><br><span class="line">	cmp_sec_timestamp = sec_timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>After successfully choosing a victim, we need to clear its history (leaving the $0$ as sentinel), set it to non-evictable, and reduce the current size of the replacer. </p>
</li>
</ol>
<h1 id="Task-2-Buffer-Pool-Manager"><a href="#Task-2-Buffer-Pool-Manager" class="headerlink" title="Task #2 - Buffer Pool Manager"></a>Task #2 - Buffer Pool Manager</h1><h2 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h2><ol>
<li>When the page that is asked to fetch is already in the buffer pool, we still need to do several things:<ul>
<li>Set it to non-evictable. </li>
<li>Record its access in the replacer. </li>
<li>Increase its pin count. </li>
</ul>
</li>
<li><p>When the page is not in the buffer pool or creating a new page: </p>
<ul>
<li>First, we need to acquire a free frame. <ul>
<li>Erase it from the page table when we are evicting one. </li>
<li>Write the content to disk when the old page is dirty. </li>
</ul>
</li>
<li>Then, similar to the other situation, we need to: <ul>
<li>Set it to non-evictable</li>
<li>Record its access in the replacer.</li>
<li>Put it in the page table.</li>
<li>Set its pin count to $1$</li>
<li>Set <code>is_dirty_</code> to <code>false</code></li>
<li>Set its page ID to frame. </li>
</ul>
</li>
<li>Even in <code>NewPage</code>, the <code>is_dirty_</code> is false because it can directly “write” empty to file by increasing offset, which is done when a larger page ID is written. There is no need actually to write empty content. </li>
</ul>
</li>
<li><p>When setting <code>is_dirty_</code> in <code>Unpin</code>, if the page is already dirty, we should not set it to clean, no matter what we are told. </p>
</li>
</ol>
<h2 id="Leaderboard-optimization"><a href="#Leaderboard-optimization" class="headerlink" title="Leaderboard optimization"></a>Leaderboard optimization</h2><ol>
<li>Fine-grain lock<ul>
<li>A coarse-grain lock is easier to program while sacrificing performance. </li>
<li>In fine-grain lock, a <code>core_latch_</code> is used to protect the core data of the buffer pool manager, i.e., <code>page_table_</code>, <code>free_list_</code>, <code>next_page_id_</code>). Each frame has its latch to protect its data, i.e., <code>pin_count_</code>, <code>is_dirty_</code>, <code>page_id_</code>. </li>
<li>Each function thread will, at most, grab the core_latch_ and one of the frame latches. </li>
<li>To avoid deadlock, each procedure is designed so that a thread will try to acquire a page latch only if it already has a <code>core_latch_</code> or it won’t want a core_latech later. </li>
<li>To obtain atomic when switching from <code>core_latch_</code> to a page latch, we must acquire the page latch before releasing the <code>core_latch_</code>. <ul>
<li>If we release core_latch_ first, some other thread may change the frame we want to use (e.g., evict the frame, modify its metadata) before acquiring the frame latch. </li>
</ul>
</li>
</ul>
</li>
<li>Delay write-out: <ul>
<li>Disk I/Os consume a large amount of time. So, we must avoid holding a lock while communicating with the disk. </li>
<li>When we need to write data into disk, instead of immediately calling the disk_manager_-&gt;WritePage, we create a temporary buffer in memory and copy data into the buffer. After all locks are released, we write that content in the buffer into a disk. </li>
<li>A similar thought is to delay reading data until all locks are released. However, <code>ReadData</code> is called in <code>FetchPage</code>, where we can only release the frame latch after the whole frame is ready for others to visit. Still, we can <code>ReadData</code> after <code>core_latch_</code> is released since the <code>core_latch_</code> could be the bottleneck of the manager. </li>
<li>However, there is a problem: <ul>
<li>When a page is evicted, and only a short time later, that exact page is fetched again. Since all latches are released before writing dirty data to disk, there is a chance that FetchPage reads stale data from disk before the up-to-date data is written to disk. </li>
<li>My other thought is maintaining a list of page IDs in the writing buffer. If the page ID of <code>FetchPage</code> is in the list, copy data from the writing buffer and erase that page from the writing buffer. </li>
<li>The problem is that we need to guarantee the atomic between getting the writing content and writing it to disk. Otherwise, tricky concurrent operations may cause the written content to be wrong. </li>
<li>Hence, we cannot unlock the latches until we are sure the content is written, which also makes fine-grained latches meaningless, given that disk I/O is the key problem of performance. </li>
</ul>
</li>
</ul>
</li>
<li>Pre-fetch<ul>
<li>To better suit the scan case, the buffer pool manager can pre-fetch several following pages when accessing three consecutive page IDs in a row. </li>
<li>Different from fetching pages through <code>FetchPage</code>, pre-fetched frames need to set <code>pin_count_</code> to $0$ and evictable. </li>
<li>Pre-fetch should not stall <code>FetchPage</code> from returning, so it must be run in a separate thread. </li>
<li>The problem is that pre-fetch must be an independent background thread. It won’t know whether the buffer pool manager is destroyed, which will cause a heap-use-after-free error. </li>
</ul>
</li>
</ol>
<h1 id="Task-3-Read-Write-Page-Guards"><a href="#Task-3-Read-Write-Page-Guards" class="headerlink" title="Task #3 - Read/Write Page Guards"></a>Task #3 - Read/Write Page Guards</h1><ol>
<li>Any <code>PageGuard</code> will automatically unpin itself when it is deconstructed. But they don’t need to pin themselves since they are pinned before <code>PageGuard</code> is created when fetching or creating a page. </li>
<li>If a <code>Read/WritePageGuard</code> is acquired through <code>FetchPageRead</code> or <code>FetchPageWrite</code>, the latch is acquired inside these functions. However, the latch won’t be acquired automatically if a <code>PageGuard</code> is acquired through its construction function. </li>
<li>No matter how <code>Read/WritePageGuard</code> is acquired, the latch will always be released automatically when the <code>PageGuard</code> is deconstructed by a deconstructor or move operation. </li>
<li>In the move assignment, we must first drop the original page, copy from the right reference, and finally deconstruct the right reference. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/06/25/Courses/15445/05-Index-Concurrency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/25/Courses/15445/05-Index-Concurrency/" class="post-title-link" itemprop="url">05 Index Concurrency</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-25 00:40:22" itemprop="dateCreated datePublished" datetime="2023-06-25T00:40:22+08:00">2023-06-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 14:31:10" itemprop="dateModified" datetime="2024-03-16T14:31:10+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Concurrency-control"><a href="#Concurrency-control" class="headerlink" title="Concurrency control"></a>Concurrency control</h1><h2 id="What-are-the-correctness-criteria-of-concurrency-control"><a href="#What-are-the-correctness-criteria-of-concurrency-control" class="headerlink" title="What are the correctness criteria of concurrency control?"></a>What are the correctness criteria of concurrency control?</h2><ol>
<li>Logical Correctness: <ul>
<li>Can a thread see the data that it is supposed to see? For example, correctly control data race. </li>
</ul>
</li>
<li>Physical Correctness: <ul>
<li>Is the internal representation of the object sound? For example, fetching a ptr will point to the correct address, and it won’t be freed between fetching and accessing. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-more-specific-difference-between-locks-and-latches"><a href="#What-is-the-more-specific-difference-between-locks-and-latches" class="headerlink" title="What is the more specific difference between locks and latches?"></a>What is the more specific difference between locks and latches?</h2><ol>
<li>What are they used to separate? What are they protecting? How long will they be held?<ul>
<li>Locks separate user transactions accessing the same tuples in database contents. Locks are held in the entire transaction. </li>
<li>Latches are used to separate threads accessing the same in-memory data structures. Latches are held in the critical section. </li>
</ul>
</li>
<li>How many modes do they have?<ul>
<li>Locks have four modes: shared, exclusive, update, and intention. </li>
<li>Latches only have two modes: read and write. </li>
</ul>
</li>
<li>How do they solve deadlock?<ul>
<li>Locks use detection and resolution by waits-for, timeout, or abort. </li>
<li>Latches can only avoid deadlock by code discipline. </li>
</ul>
</li>
<li>Where are they kept?<ul>
<li>Locks are kept in Lock Manager, while latches are kept in the protected data structure. </li>
</ul>
</li>
</ol>
<h2 id="What-are-the-two-latch-modes"><a href="#What-are-the-two-latch-modes" class="headerlink" title="What are the two latch modes?"></a>What are the two latch modes?</h2><ol>
<li><p>Read mode means that Multiple threads can read the same object simultaneously. </p>
<ul>
<li>A thread can acquire the read latch if another thread has it in read mode. </li>
</ul>
</li>
<li><p>Write mode only allows one thread to access the object. </p>
<ul>
<li>A thread cannot acquire a write latch if another thread has it in any mode. </li>
</ul>
</li>
<li>In the compatibility matrix, only two read mode access is allowed. </li>
</ol>
<h2 id="What-are-the-different-latch-implementations"><a href="#What-are-the-different-latch-implementations" class="headerlink" title="What are the different latch implementations?"></a>What are the different latch implementations?</h2><ol>
<li>The first is blocking OS Mutex. <ul>
<li>It is simple to use. </li>
<li>It takes about $25\ ns$ per lock/unlock invocation, which means that it is non-scalable. <ul>
<li><code>std::mutex</code> is slower than <code>pthread_mutex,</code> and <code>futex</code> is faster than both. </li>
</ul>
</li>
<li><code>futex</code> stands for fast userspace mutex. <ul>
<li>It has a userspace spinlock and a heavy-weight OS latch. </li>
<li>Threads will first try to acquire the userspace lock. If success, then that is good. </li>
<li>But if failed, the thread will fall back to the OS latch. OS takes control of the thread and when to schedule it, and DBMS can do nothing with it. Also, <code>syscall</code> is expensive. </li>
</ul>
</li>
</ul>
</li>
<li>The second approach is reader-writer latches. <ul>
<li>It allows for concurrent readers. Must manage read/write queues to avoid starvation. </li>
<li>It can be implemented on top of spinlock. </li>
<li>Still, <code>std::shared_mutex</code> is slower than <code>pthread_rwlock</code>. </li>
</ul>
</li>
</ol>
<h1 id="Latching-scheme"><a href="#Latching-scheme" class="headerlink" title="Latching scheme"></a>Latching scheme</h1><h2 id="Hash-table-latching"><a href="#Hash-table-latching" class="headerlink" title="Hash table latching"></a>Hash table latching</h2><h3 id="Why-are-deadlocks-not-possible-in-the-hash-table"><a href="#Why-are-deadlocks-not-possible-in-the-hash-table" class="headerlink" title="Why are deadlocks not possible in the hash table?"></a>Why are deadlocks not possible in the hash table?</h3><ol>
<li>All threads move in the same direction and only access a single page/slot at a time. <ul>
<li>Hence, no loops are waiting in this scenario. </li>
</ul>
</li>
<li>To resize the table, take a global write latch on the entire table. </li>
</ol>
<h3 id="How-can-we-design-hash-table-latching"><a href="#How-can-we-design-hash-table-latching" class="headerlink" title="How can we design hash table latching?"></a>How can we design hash table latching?</h3><ol>
<li>The coarser-grain approach is to use page latches. <ul>
<li>Each page has a reader-writer latch that protects its entire contents. </li>
</ul>
</li>
<li>The finer-grain approach is to use slot latches. <ul>
<li>Each slot has its latch. </li>
<li>It can use a single-mode latch to reduce meta-data and computational overhead. </li>
</ul>
</li>
</ol>
<h2 id="B-Tree-latching"><a href="#B-Tree-latching" class="headerlink" title="B+Tree latching"></a>B+Tree latching</h2><h3 id="What-should-we-use-latching-to-prevent"><a href="#What-should-we-use-latching-to-prevent" class="headerlink" title="What should we use latching to prevent?"></a>What should we use latching to prevent?</h3><ol>
<li>Threads are trying to modify the contents of a node at the same time. (This is logical correctness, i.e., data race)</li>
<li>One thread traverses the tree while another thread splits/merges nodes. <ul>
<li>This is physical correctness. Splitting/merging will cause free pointers, nodes, or in-node entries. </li>
<li>This will also cause a problem with logical correctness, i.e., false negative. <ul>
<li>If a thread gets the pointer of the node that possesses the key it wants, then before it accesses the node, the key is borrowed by a sibling, causing the thread to think the key does not exist. </li>
</ul>
</li>
</ul>
</li>
<li>When introducing sibling pointers, we may have a deadlock situation. </li>
</ol>
<h3 id="How-should-we-achieve-physical-correctness"><a href="#How-should-we-achieve-physical-correctness" class="headerlink" title="How should we achieve physical correctness?"></a>How should we achieve physical correctness?</h3><ol>
<li>The most naive method is to hold all locks until the entire process is done. But its performance is a disaster. <ul>
<li>We must release some latches when we are sure they are safe. </li>
<li>According to our goal, a node is safe when we know that it won’t be changed (split, merged, or redistributed) when updated. </li>
<li>We can know a child is safe when its child is not full on insertion or more than half-full on deletion, i.e., any later operations can be isolated on or below its level. </li>
</ul>
</li>
<li>For find operation, there won’t be any updates. Hence, we can always unlatch the parent when we acquire an R latch on the child. </li>
<li>We need to obtain W latches as required for insert or delete operations. Once a child is latched, check if it is safe. <ul>
<li>If the child is safe, we can release all latches on ancestors. The latches should be released from top to bottom to perform better. </li>
</ul>
</li>
</ol>
<h3 id="What-is-the-problem-with-the-aforementioned-strategy"><a href="#What-is-the-problem-with-the-aforementioned-strategy" class="headerlink" title="What is the problem with the aforementioned strategy?"></a>What is the problem with the aforementioned strategy?</h3><ol>
<li>Every insert/delete operation will take a write latch on the root, which makes the root a bottleneck with higher concurrency. </li>
<li>We can assume that most modifications to a B+Tree will not require a split or merge. </li>
<li>Hence, optimistically traverse the tree using read latches instead of assuming that there will be a split/merge as aforementioned. If the guess is wrong, repeat traversal with the pessimistic algorithm. </li>
<li>For insert/delete operations, set latches as if for search, get to leaf, and set W latch on leaf. If the leaf is unsafe, release all latches and restart the thread using the previous insert/delete protocol with write latches. </li>
</ol>
<h3 id="When-will-a-deadlock-occur"><a href="#When-will-a-deadlock-occur" class="headerlink" title="When will a deadlock occur?"></a>When will a deadlock occur?</h3><ol>
<li>With sibling pointers, we may move from one leaf node to another where deadlock could occur. </li>
<li>Latches cannot detect deadlock, so the only solution is to kill one thread. </li>
<li>The leaf node sibling latch acquisition protocol must support a “no-wait” mode. The DBMS’s data structures must cope with failed latch acquisitions. </li>
<li>Though some scenarios do not have a deadlock, the waiting thread cannot know what the other thread is doing, which means it can only kill itself to avoid deadlock. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/06/24/Courses/15445/04-Data-Organization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/06/24/Courses/15445/04-Data-Organization/" class="post-title-link" itemprop="url">04 Data Organization</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-06-24 17:12:22" itemprop="dateCreated datePublished" datetime="2023-06-24T17:12:22+08:00">2023-06-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 14:17:41" itemprop="dateModified" datetime="2024-03-16T14:17:41+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="What-does-the-database-access-methods-layer-need-to-do"><a href="#What-does-the-database-access-methods-layer-need-to-do" class="headerlink" title="What does the database access methods layer need to do?"></a>What does the database access methods layer need to do?</h1><ol>
<li>Data Organization<ul>
<li>How do we layout data structure in memory/pages, and what information should be stored to support efficient access? </li>
<li>This layer will organize all kinds of data inside the database, e.g., internal meta-data, core data storage, temporary data structures, and table indexes. </li>
</ul>
</li>
<li><p>Concurrency</p>
<ul>
<li>How can multiple threads be enabled to access the data structure simultaneously without causing problems? </li>
</ul>
</li>
<li><p>There are two types of data structures: hash tables and trees. </p>
</li>
</ol>
<h1 id="Hash-table"><a href="#Hash-table" class="headerlink" title="Hash table"></a>Hash table</h1><h2 id="How-is-the-naive-static-hash-table"><a href="#How-is-the-naive-static-hash-table" class="headerlink" title="How is the naive static hash table?"></a>How is the naive static hash table?</h2><ol>
<li><p>A hash table implements an unordered associative array that maps keys to values. </p>
</li>
<li><p>For any input key, hash functions return an integer representation. </p>
</li>
<li><p>The space complexity is $O(n)$, the average time complexity is $O(1)$ and the worst time complexity is $O(n)$. </p>
<ul>
<li>Databases need to be about constants. For time complexity, smaller constants can save much time in billions of operations. </li>
<li>The space will be several times larger than the number of keys. </li>
</ul>
</li>
<li><p>The naive static hash table assumes: </p>
<ul>
<li>The number of elements is known ahead of time and fixed. </li>
<li>Each key is unique. </li>
<li>We have the perfect hash function where two keys must have different hash values. </li>
</ul>
</li>
<li><p>These assumptions are not always satisfied. </p>
<ul>
<li><p>We need a dynamic hashing scheme to suit when the first assumption fails. </p>
</li>
<li><p>To suit when the last two assumptions failed, we need to design a more delicate hash function to reduce the collision rate and hashing scheme to handle collisions after hashing. </p>
<ul>
<li><p>The trade-off of the hash function is between being fast and collision rate. </p>
</li>
<li><p>The trade-off of the hashing scheme is between allocating a larger hash table and additional instructions to get/put keys. </p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="What-are-hash-functions"><a href="#What-are-hash-functions" class="headerlink" title="What are hash functions?"></a>What are hash functions?</h2><ol>
<li><p>We do not want to use a cryptographic hash function for DBMS hash tables, e.g., SHA-2. </p>
<ul>
<li>Because this hashing is only used internally, we will never worry about leaking keys. </li>
<li>We want something that is fast and has a low collision rate.</li>
</ul>
</li>
<li><p>The commonly used hash functions are: </p>
<ul>
<li>CRC-64: Used in networking for error detection. </li>
<li>MurmurHash: Designed as a fast, general-purpose hash function. </li>
<li>Google CityHash: Designed to be faster for short keys (&lt;64 bytes). </li>
<li>Facebook XXHash: From the creator of zstd compression, the state-of-the-art. </li>
<li>Google Farmhash: A newer version of CityHash with better collision rates. </li>
</ul>
</li>
<li><p>Their speed comparison is as follows:</p>
<p><img src="/imgs/15445/Organization/hash_func.png" width="75%"></p>
</li>
</ol>
<h2 id="Static-hashing-schemes"><a href="#Static-hashing-schemes" class="headerlink" title="Static hashing schemes"></a>Static hashing schemes</h2><h3 id="How-is-linear-probe-hashing"><a href="#How-is-linear-probe-hashing" class="headerlink" title="How is linear probe hashing?"></a>How is linear probe hashing?</h3><ol>
<li>A key-value pair is stored in the hashing table for all the schemes. The key is determining whether this is the key we want to find. </li>
<li>It resolves collisions by linearly searching for the next free slot in the table. </li>
<li>To determine whether an element is present, hash to a location in the index and scan for it. If an empty slot is found before the key, the element does not exist. </li>
<li>To delete an entry, there are two approaches: <ul>
<li><strong>Movement</strong>: Rehash the remaining keys until find the first empty slot. Nobody does this. </li>
<li><strong>Tombstone</strong>: Set a marker to indicate that the entry in the slot is logically deleted. The slot can be reused for new keys. This may still need periodic garbage collection. </li>
</ul>
</li>
</ol>
<h3 id="How-to-solve-the-non-unique-keys-problem"><a href="#How-to-solve-the-non-unique-keys-problem" class="headerlink" title="How to solve the non-unique keys problem?"></a>How to solve the non-unique keys problem?</h3><ol>
<li>The first choice is to store values in separate storage areas for each key, and the value of the hash table points to the area. </li>
<li>The second choice is storing duplicate key entries in the hash table. <ul>
<li>Read would return the first key they found. </li>
<li>Deletes would remove all the keys or a specific key-value pair. </li>
</ul>
</li>
</ol>
<h3 id="How-is-the-Robin-Hood-hashing"><a href="#How-is-the-Robin-Hood-hashing" class="headerlink" title="How is the Robin Hood hashing?"></a>How is the Robin Hood hashing?</h3><ol>
<li>This variant of linear probe hashing steals slots from “rich” keys and gives them to “poor” keys. <ul>
<li>Each key tracks the number of positions they are from where its optimal position (original hash value) is in the table. </li>
<li>The keys farther away from their optimal position are poorer, while the keys closer are richer. </li>
<li>On insert, a key takes the slot of another key if the first key is “richer” than the second key. And the second key will keep searching linearly until it finds another “richer” key. </li>
</ul>
</li>
<li>Stealing increases the number of writing operations compared with the linear probing scheme while reducing the time of the worst case. </li>
<li>This could have cascading/flooding problems where one inserts causes multiple “stealing.” Also, it does not consider the possibility of having hotkeys. </li>
</ol>
<h3 id="How-is-the-cuckoo-hashing"><a href="#How-is-the-cuckoo-hashing" class="headerlink" title="How is the cuckoo hashing?"></a>How is the cuckoo hashing?</h3><ol>
<li>Use multiple hash tables with different hash function seeds to ensure they won’t hash to the same value. <ul>
<li>On insert, check every table and pick anyone with a free slot. </li>
<li>If no table has a free slot, choose one victim, evict it, and then re-hash it to find a new location. </li>
</ul>
</li>
<li>Look-ups and deletions are always $O(1)$ because only one location per hash table is checked. </li>
<li>There is a possibility of cascading. The worst case is an infinite cascading loop. <ul>
<li>We can use extra code to detect if replacing is going in a loop. If so, we must double the hash table size with new hash functions and re-insert all keys. </li>
</ul>
</li>
</ol>
<h2 id="Dynamic-hashing-schemes"><a href="#Dynamic-hashing-schemes" class="headerlink" title="Dynamic hashing schemes"></a>Dynamic hashing schemes</h2><h3 id="How-is-chained-hashing"><a href="#How-is-chained-hashing" class="headerlink" title="How is chained hashing?"></a>How is chained hashing?</h3><ol>
<li>It maintains a linked list of buckets for each slot in the hash table. </li>
<li>Resolve collisions by placing all elements with the same hash key into the same bucket. <ul>
<li>To determine whether an element is present, hash to its bucket and scan for it. </li>
</ul>
</li>
<li>The problem is that the linked list can grow forever, causing the time spent on search to increase as the system runs. </li>
</ol>
<h3 id="How-is-extendible-hashing"><a href="#How-is-extendible-hashing" class="headerlink" title="How is extendible hashing?"></a>How is extendible hashing?</h3><ol>
<li>Multiple slot locations can point to the same bucket chain. </li>
<li>For the number of bits that need to be examined, there is a global and a local one. </li>
<li>It reshuffles bucket entries on split and increases the number of bits to examine when a list is full. <ul>
<li>If there is only one slot pointing to this list, i.e., the global examining bits are the same as the local one, <ul>
<li>The DBMS increases the global number, causing the size of the hashing table doubled. </li>
<li>Those lists with smaller local numbers will still set the pointers of corresponding new slots (with only the last global examining bit different) to themselves. </li>
<li>DBMS also increases the number of local examining bits on the fulled list. The keys in it will re-hash to their two new lists. </li>
</ul>
</li>
<li>If more than one slot is pointing to this list, i.e., the global examining bits are larger than the local one, <ul>
<li>The DBMS only increases the local examining bits of the full list and does the re-hashing. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="How-is-linear-hashing"><a href="#How-is-linear-hashing" class="headerlink" title="How is linear hashing?"></a>How is linear hashing?</h3><ol>
<li>It is similar to extensible hashing. But the hash table maintains a pointer that tracks the next bucket to split. </li>
<li>When any bucket overflows, split the bucket at the pointer location. <ul>
<li>When the split causes the number of slots in the hash table to double, it introduces a new hash function, taking modulo with the new hash table size. </li>
</ul>
</li>
<li>It uses multiple hashes to find the right bucket for a given key. <ul>
<li>It always first uses the old hash function with a smaller modulus. </li>
<li>If this hash value is smaller than the split pointer, which means that this slot has already been split, DBMS needs to use the new hash function to find the true hash value. </li>
<li>Otherwise, this slot is not split, and the old hash value works fine. </li>
</ul>
</li>
<li>Splitting buckets based on the split pointer will eventually get to all overflowed buckets. <ul>
<li>When the pointer reaches the last slot, delete the first hash function and return to the beginning. </li>
</ul>
</li>
<li>Deleting may cause the size of the hash table to shrink when it deletes the only entry in the second half of the hash table. <ul>
<li>DMBS shrinks the size of the hash table and deletes the new hash function. </li>
</ul>
</li>
</ol>
<h1 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h1><h2 id="Table-indexes"><a href="#Table-indexes" class="headerlink" title="Table indexes"></a>Table indexes</h2><h3 id="What-do-table-indexes-do"><a href="#What-do-table-indexes-do" class="headerlink" title="What do table indexes do?"></a>What do table indexes do?</h3><ol>
<li>A table index is a replica of a subset of a table’s attributes organized and sorted for efficient access using those attributes. </li>
<li>It is used in queries to find tuples with attributes that match certain values. </li>
<li>The DBMS ensures that the contents of the table and the index are logically synchronized. </li>
<li>The DBMS’s job is to figure out the best index(es) to execute each query. </li>
<li>There is a trade-off regarding the number of indexes to create per database, i.e., the lookup speed and synchronization overhead. <ul>
<li>We need extra storage overhead to store the data structure and maintenance overhead to keep synchronization. </li>
</ul>
</li>
</ol>
<h3 id="What-are-clustered-indexes"><a href="#What-are-clustered-indexes" class="headerlink" title="What are clustered indexes?"></a>What are clustered indexes?</h3><ol>
<li>The table is stored in the sort order specified by the primary key. <ul>
<li>It can be either heap- or index-organized storage. </li>
</ul>
</li>
<li>Some DBMSs always use a clustered index. If a table does not contain a primary key, the DBMS will<br>automatically make a hidden primary key. </li>
<li>Other DBMSs cannot use them at all. </li>
</ol>
<h2 id="Basic-B-Tree"><a href="#Basic-B-Tree" class="headerlink" title="Basic B+ Tree"></a>Basic B+ Tree</h2><h3 id="What-is-a-B-Tree"><a href="#What-is-a-B-Tree" class="headerlink" title="What is a B+ Tree?"></a>What is a B+ Tree?</h3><ol>
<li>A B+Tree is a self-balancing tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions always in $O(\log n)$. </li>
<li>A B+ tree is an M-way search tree. <ul>
<li>It is perfectly balanced, i.e., every leaf node is at the same depth in the tree. </li>
<li>Every node other than the root is at least half-full, i.e., $M/2-1 ≤ number\ of\ keys ≤ M-1$. </li>
<li>Every inner node with $k$ keys has $k+1$ non-null children. </li>
</ul>
</li>
</ol>
<h3 id="How-are-key-value-pairs-stored-in-each-node"><a href="#How-are-key-value-pairs-stored-in-each-node" class="headerlink" title="How are key/value pairs stored in each node?"></a>How are key/value pairs stored in each node?</h3><ol>
<li>Every B+Tree node is comprised of an array of key/value pairs. </li>
<li>The keys are derived from the attribute(s) on which the index is based. </li>
<li>The value stored in an inner node is a pointer to its corresponding children, while the value stored in a leaf node is its specific value. </li>
<li>There are two storage methods for the key-value pair. <ul>
<li>One is to store them consecutively, i.e., each value is stored immediately after its key. </li>
<li>The other is to store them in the same place in two arrays, i.e., each value has the same index as its key. </li>
</ul>
</li>
<li>The arrays are (usually) kept in sorted key order.</li>
</ol>
<h3 id="What-is-stored-in-the-leaf-node"><a href="#What-is-stored-in-the-leaf-node" class="headerlink" title="What is stored in the leaf node?"></a>What is stored in the leaf node?</h3><ol>
<li><p>Each leaf node also has pointers that point to its siblings. </p>
</li>
<li><p>There are two approaches to store the values: </p>
<ul>
<li>The first approach is to store them with record IDs, a pointer to the page ID, and an offset of the tuple to which the index entry corresponds. </li>
<li>The second approach is to store them with tuple data, specifically primary keys. </li>
</ul>
</li>
<li><p>The pros of the second approach are that we do not need to access another address to fetch the contents when primary keys are all we want. </p>
<p>However, secondary indexes must store the Record ID as their value. Hence, if we want secondary indexes in the second approach, we must look up other tables to find the Record ID with the same primary keys. </p>
</li>
</ol>
<h3 id="What-are-the-pros-and-cons-of-B-Tree-compared-with-B-Tree"><a href="#What-are-the-pros-and-cons-of-B-Tree-compared-with-B-Tree" class="headerlink" title="What are the pros and cons of B-Tree compared with B+Tree?"></a>What are the pros and cons of B-Tree compared with B+Tree?</h3><ol>
<li>The main difference is that  B-Tree stores keys and values in all nodes in the tree, while B+Tree only stores values in leaf nodes. Inner nodes only guide the search process. </li>
<li>B-Tree is more space-efficient since each key only appears once in the tree. </li>
<li>However, B-Tree needs to jump between pages when we want sequential access keys, causing much more I/O. </li>
</ol>
<h3 id="How-does-B-Tree-insert-a-node"><a href="#How-does-B-Tree-insert-a-node" class="headerlink" title="How does B+Tree insert a node?"></a>How does B+Tree insert a node?</h3><ol>
<li>Find correct leaf node L. Insert data entry into L in sorted order. </li>
<li>If L has enough space, then it is done. </li>
<li>Otherwise, split L keys into L and a new node L2. Insert the index entry pointing to L2 into the parent of L. <ul>
<li>The parent of L may need to be rebalanced after this insertion. </li>
</ul>
</li>
</ol>
<h3 id="How-does-B-Tree-delete-a-node"><a href="#How-does-B-Tree-delete-a-node" class="headerlink" title="How does B+Tree delete a node?"></a>How does B+Tree delete a node?</h3><ol>
<li>Start at the root and find leaf L, where the entry belongs. Remove the entry. </li>
<li>If L is at least half-full, then it is done. </li>
<li>If L has only M/2-1 entries, <ul>
<li>First, try to re-distribute, borrowing from siblings. </li>
<li>If re-distribution fails, merge L and sibling and delete the entry (pointing to L or sibling) from the parent of L. <ul>
<li>The parent of L may need a rebalance. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="B-Tree-usage-and-design"><a href="#B-Tree-usage-and-design" class="headerlink" title="B+Tree usage and design"></a>B+Tree usage and design</h2><h3 id="How-does-DBMS-use-B-Tree-in-the-selection-query"><a href="#How-does-DBMS-use-B-Tree-in-the-selection-query" class="headerlink" title="How does DBMS use B+Tree in the selection query?"></a>How does DBMS use B+Tree in the selection query?</h3><ol>
<li><p>When creating an index, a certain attribute order is specified to sort the tuple. </p>
<ul>
<li><p>When $A_1, \dots,A_n$ are specified, the B+Tree will store the corresponding $n$ values in as keys in each node. </p>
</li>
<li><p>When $a_1, \dots,a_n$ are stored in one node, all nodes in its left sub-tree have $A_1≤a_1,\dots,A_n≤a_n$ while all nodes in its right sub-tree are only guaranteed with $A_1 &gt; a_1$, i.e., the B+Tree is maintained with $A_1$ as its primary sorting key. </p>
</li>
</ul>
</li>
<li><p>In a selection condition, we can easily select with certain conditions on $A_1,\dots,A_k\ (k≤n)$. Some DBMS also support conditions specified only on $A_k, \dots,A_n$, which requires DBMS to access all leaf nodes sequentially. </p>
</li>
<li><p>Compared with the hash table, B+Tree can better support selection without knowing the attributes to look for. </p>
</li>
</ol>
<h3 id="How-to-handle-duplicate-keys"><a href="#How-to-handle-duplicate-keys" class="headerlink" title="How to handle duplicate keys?"></a>How to handle duplicate keys?</h3><ol>
<li>The first approach is to add the tuple’s unique Record ID as part of the key to ensure all keys are unique. <ul>
<li>The DBMS can still use partial keys to find tuples.</li>
</ul>
</li>
<li>The second approach is allowing leaf nodes to spill into overflow nodes containing duplicate keys. <ul>
<li>Only duplicate keys of existing keys can overflow. </li>
<li>Overflow nodes also need to split or merge when splitting or merging nodes. </li>
<li>This is more complex to maintain and modify. </li>
</ul>
</li>
</ol>
<h3 id="How-to-solve-redundant-page-jumps-when-sequential-access-leaf-nodes"><a href="#How-to-solve-redundant-page-jumps-when-sequential-access-leaf-nodes" class="headerlink" title="How to solve redundant page jumps when sequential access leaf nodes?"></a>How to solve redundant page jumps when sequential access leaf nodes?</h3><ol>
<li>In a clustered B+Tree, nodes on the same page are in consecutive order. <ul>
<li>We can traverse the left-most leaf page and retrieve tuples from all leaf pages. </li>
<li>This will always be better than sorting data for each query. </li>
</ul>
</li>
<li>In a non-clustered B+Tree, the DBMS can first figure out all the tuples it needs and then sort them based on their Page ID. </li>
</ol>
<h3 id="How-to-choose-node-size"><a href="#How-to-choose-node-size" class="headerlink" title="How to choose node size?"></a>How to choose node size?</h3><ol>
<li>The slower the storage device, the larger the optimal node size for a B+Tree. <ul>
<li>HDD takes $1\ MB$, SSD usually takes $10\ KB$, in-memory nodes have $512\ B$. </li>
</ul>
</li>
<li>Optimal sizes can vary depending on the workload. The trade-off is between leaf node scans and root-to-leaf traversals. <ul>
<li>With a larger size, we can have more sequential reads in leaf node scans, but we need to read more data in root-to-leaf traversals. </li>
</ul>
</li>
</ol>
<h3 id="How-to-choose-a-merge-threshold"><a href="#How-to-choose-a-merge-threshold" class="headerlink" title="How to choose a merge threshold?"></a>How to choose a merge threshold?</h3><ol>
<li>Some DBMSs do not always merge nodes when they are half full. </li>
<li>Delaying a merge operation may reduce the amount of reorganization. They assume that the missing part will be filled soon. </li>
<li>Having smaller nodes exist and periodically rebuilding the entire tree may also be better. </li>
</ol>
<h3 id="How-to-handle-variable-length-keys"><a href="#How-to-handle-variable-length-keys" class="headerlink" title="How to handle variable length keys?"></a>How to handle variable length keys?</h3><ol>
<li>The first approach is to store the keys as pointers to the tuple’s attribute. </li>
<li>The second approach is to allow variable-length nodes. It requires careful memory management. </li>
<li>The third approach is always to pad the key to the max length of the key type. </li>
<li>The last approach is a key map or indirection. It is similar to the in-node dictionary. </li>
</ol>
<h3 id="How-can-we-do-the-intra-node-search"><a href="#How-can-we-do-the-intra-node-search" class="headerlink" title="How can we do the intra-node search?"></a>How can we do the intra-node search?</h3><ol>
<li>The naive method is linear search. We can use SIMD to vectorize the process. </li>
<li>The second method is binary search, given that keys in a node are already sorted. </li>
<li>The third method is interpolation search. <ul>
<li>This requires a known distribution of keys. </li>
<li>It jumps to the approximate location of the desired key based on known distribution. </li>
</ul>
</li>
</ol>
<h3 id="How-can-we-optimize-space-usage"><a href="#How-can-we-optimize-space-usage" class="headerlink" title="How can we optimize space usage?"></a>How can we optimize space usage?</h3><ol>
<li>Prefix compression<ul>
<li>Sorted keys in the same leaf node will likely have the same prefix. </li>
<li>Extract common prefixes and store only a unique suffix for each key. </li>
</ul>
</li>
<li>Deduplication<ul>
<li>Non-unique indexes can end up storing multiple copies of the same key in leaf nodes. </li>
<li>Store the key once and then maintain a list of tuples with that key. </li>
</ul>
</li>
<li>Suffix truncation<ul>
<li>The keys in the inner nodes are only used to “direct traffic.” We don’t need the entire key. </li>
<li>Store a minimum prefix needed to route probes into the index correctly. </li>
</ul>
</li>
</ol>
<h3 id="How-can-we-optimize-consumed-time"><a href="#How-can-we-optimize-consumed-time" class="headerlink" title="How can we optimize consumed time?"></a>How can we optimize consumed time?</h3><ol>
<li>Pointer swizzling<ul>
<li>Nodes use page IDs to reference other nodes in the index. The DBMS must get the memory location from the page table during traversal. </li>
<li>If a page is pinned in the buffer pool, we can store raw pointers instead of page IDs. This avoids address lookups from the page table. </li>
</ul>
</li>
<li>Bulk insert<ul>
<li>The fastest way to build a new B+Tree for an existing table is first to sort the keys and then build the index from the bottom up. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/about/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/about/">1</a><span class="space">&hellip;</span><a class="page-number" href="/about/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/about/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/about/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/about/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
