<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/about/page/3/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/about/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"about/page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Raft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Raft/" class="post-title-link" itemprop="url">Raft</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:14:13" itemprop="dateCreated datePublished" datetime="2023-09-26T13:14:13+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-27 01:06:59" itemprop="dateModified" datetime="2024-02-27T01:06:59+08:00">2024-02-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/raft-extended.pdf">In Search of an Understandable Consensus Algorithm</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#abstract">Abstract</a></li>
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#basics">Basics</a>
<ul>
<li><a href="#how-does-raft-implements-consensus-overall">How does Raft implements consensus overall?</a></li>
<li><a href="#what-are-the-states-of-each-server">What are the states of each server?</a></li>
<li><a href="#how-to-divide-the-terms">How to divide the terms?</a></li>
<li><a href="#how-does-terms-change">How does terms change?</a></li>
<li><a href="#how-does-raft-handle-follower-and-candidate-crashes">How does Raft handle follower and candidate crashes?</a></li>
<li><a href="#states-stored-on-servers">States stored on servers</a></li>
</ul>
</li>
<li><a href="#leader-election">Leader election</a>
<ul>
<li><a href="#how-does-the-servers-states-transit">How does the servers states transit?</a></li>
<li><a href="#how-to-elect-a-leader">How to elect a leader?</a></li>
<li><a href="#how-is-the-election-held">How is the election held?</a></li>
<li><a href="#how-to-determine-that-a-server-loses-the-election">How to determine that a server loses the election?</a></li>
<li><a href="#how-to-handle-a-split-vote">How to handle a split vote?</a></li>
<li><a href="#how-to-ensure-that-the-leader-of-any-given-term-contains-all-of-the-entries-committed-in-previous-terms">How to ensure that the leader of any given term contains all of the entries committed in previous terms?</a></li>
<li><a href="#what-is-the-limitation-of-broadcast-time-and-election-timeout">What is the limitation of broadcast time and election timeout?</a></li>
<li><a href="#requestvote-rpc">RequestVote RPC</a></li>
</ul>
</li>
<li><a href="#log-replication">Log replication</a>
<ul>
<li><a href="#how-are-client-requests-handled">How are client requests handled?</a></li>
<li><a href="#what-is-stored-in-a-log-entry">What is stored in a log entry?</a></li>
<li><a href="#how-to-apply-a-log-entry-to-the-state-machines">How to apply a log entry to the state machines?</a></li>
<li><a href="#how-to-determine-the-consistency-between-logs">How to determine the consistency between logs?</a></li>
<li><a href="#how-to-check-the-consistency-in-appendentries-rpcs">How to check the consistency in AppendEntries RPCs?</a></li>
<li><a href="#what-kinds-of-inconsistency-may-incur">What kinds of inconsistency may incur?</a></li>
<li><a href="#how-does-leader-handle-follower-inconsistencies">How does leader handle follower inconsistencies?</a></li>
<li><a href="#how-does-appendentries-rpc-perform-consistency-check">How does AppendEntries RPC perform consistency check?</a></li>
<li><a href="#how-to-handle-uncommited-entries-from-previous-leaders">How to handle uncommited entries from previous leaders?</a></li>
<li><a href="#appendentries-rpc">AppendEntries RPC</a></li>
</ul>
</li>
<li><a href="#log-compaction">Log compaction</a>
<ul>
<li><a href="#how-does-raft-compact-logs">How does Raft compact logs?</a></li>
<li><a href="#how-to-handle-the-appendentries-that-requires-compated-entries">How to handle the AppendEntries that requires compated entries?</a></li>
<li><a href="#how-to-install-the-snapshot-from-leader">How to install the snapshot from leader?</a></li>
<li><a href="#when-should-a-server-to-snapshot">When should a server to snapshot?</a></li>
<li><a href="#how-to-reduce-the-delays-of-normal-operations-caused-by-a-snapshot">How to reduce the delays of normal operations caused by a snapshot?</a></li>
<li><a href="#installsnapshot-rpc">InstallSnapshot RPC</a></li>
</ul>
</li>
<li><a href="#client-interaction">Client interaction</a>
<ul>
<li><a href="#how-does-client-find-cluster-leader">How does client find cluster leader?</a></li>
<li><a href="#how-to-prevent-raft-execute-a-command-multiple-times">How to prevent Raft execute a command multiple times?</a></li>
<li><a href="#how-to-prevent-returning-stale-date-to-a-read-only-operation">How to prevent returning stale date to a read-only operation?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiements-and-results">Experiements and results</a></li>
<li><a href="#reproduce-and-unmentioned-parts">Reproduce and unmentioned parts</a></li>
</ul>
</p>
<h1 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h1>
<ol>
<li><strong>Main idea</strong>: Understandability also matters for an algorithm to be implemented and deployed. Separate leader election and log replication parts in consensus to increase understandability. Linearizability are provided by restricting entries accepted by followers and client commands are executed only once.</li>
<li><strong>Key findings</strong>: The key of the correctness of the system is that committed entries are durable in all nodes. Based on Log Matching property and Leader Append-Only property, we can easily induct it. Another corner case to be noticed is that leaders commit entries of previous terms only by committing entries from its own term.</li>
<li><strong>The system</strong>: The elected leaders are restricted to be at least as up-to-date as majority nodes, and only leaders can append entries from clients.</li>
<li><strong>Evaluation</strong>: The authors evaluated the understandability againsted Paxos and the performance of electing leaders are measured.</li>
</ol>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Enhance the understandability of Paxos
<ul>
<li>Raft separates the key elements of consensus, such as leader election, log replication, and safety</li>
<li>It enforces a stronger degree of coherency to reduce the number of states that must be considered</li>
</ul>
</li>
<li>Novel features: strong leader, leader election, membership changes.</li>
<li><strong>What is the common properties of consensus algorithms?</strong>
<ul>
<li>Safety: never returning an incorrect result under all non-Byzantine conditions, including network delays, partitions, and packet loss, duplication, and reordering.</li>
<li>Available as long as any majority of the servers are operational and can communicate with each other and with clients.</li>
<li>They do not depend on timing to ensure the consistency of the logs: faulty clocks and extreme message delays can, at worst, cause availability problems.</li>
<li>A command can complete as soon as a majority of the cluster has responded to a single round of remote procedure calls; a minority of slow servers need not impact overall system performance.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="basics"><a class="markdownIt-Anchor" href="#basics"></a> Basics</h2>
<h3 id="how-does-raft-implements-consensus-overall"><a class="markdownIt-Anchor" href="#how-does-raft-implements-consensus-overall"></a> How does Raft implements consensus overall?</h3>
<ol>
<li>First electing a distinguished leader, then giving the leader complete responsibility for managing the replicated log.</li>
<li>The leader accepts log entries from clients, replicates them on other servers, and tells servers when it is safe to apply log entries to their state machines.</li>
<li>A leader can fail or become disconnected from the other servers, in which case a new leader is elected.</li>
</ol>
<h3 id="what-are-the-states-of-each-server"><a class="markdownIt-Anchor" href="#what-are-the-states-of-each-server"></a> What are the states of each server?</h3>
<ol>
<li>At any given time each server is in one of three states: leader, follower, or candidate.</li>
<li>In normal operation there is exactly one leader and all of the other servers are followers.</li>
<li>Followers are passive: they issue no requests on their own but simply respond to requests from leaders and candidates.</li>
<li>The leader handles all client requests. If a client contacts a follower, the follower redirects it to the leader.</li>
<li>The candidate is used to elect a new leader.</li>
</ol>
<h3 id="how-to-divide-the-terms"><a class="markdownIt-Anchor" href="#how-to-divide-the-terms"></a> How to divide the terms?</h3>
<ol>
<li>Terms are numbered with consecutive integers. Each election begins a new term.</li>
<li>If an election results in a split vote, the term will end with no leader; a new term with a new election will begin shortly.</li>
<li>Terms act as a logical clock in Raft, and they allow servers to detect obsolete information such as stale leaders.</li>
</ol>
<h3 id="how-does-terms-change"><a class="markdownIt-Anchor" href="#how-does-terms-change"></a> How does terms change?</h3>
<ol>
<li>Each server stores a current term number, which increases monotonically over time.</li>
<li>Current terms are exchanged whenever servers communicate; if one server’s current term is smaller than the other’s, then it updates its current term to the larger value.</li>
<li>If a candidate or leader discovers that its term is out of date, it immediately reverts to follower state.</li>
<li>If a server receives a request with a stale term number, it rejects the request.</li>
</ol>
<h3 id="how-does-raft-handle-follower-and-candidate-crashes"><a class="markdownIt-Anchor" href="#how-does-raft-handle-follower-and-candidate-crashes"></a> How does Raft handle follower and candidate crashes?</h3>
<ol>
<li>If a follower or candidate crashes, then future <code>RequestVote</code> and <code>AppendEntries</code> RPCs sent to it will fail. Raft handles these failures by retrying indefinitely.</li>
<li>If a server crashes after completing an RPC but before responding, then it will receive the same RPC again after it restarts. Raft RPCs are idempotent, i.e. servers will ignore the RPCs that is already handled, so this causes no harm.</li>
</ol>
<h3 id="states-stored-on-servers"><a class="markdownIt-Anchor" href="#states-stored-on-servers"></a> States stored on servers</h3>
<ol>
<li>Persistent state on all servers: These states need to be updated on stable storage before responding to RPCs, i.e. communicating with outside.
<ul>
<li><code>currentTerm</code>: latest term server has seen (initialized to 0 on first boot, increases monotonically)</li>
<li><code>votedFor</code>: candidateId that received vote in current term (or <code>null</code> if none)</li>
<li><code>log[]</code>: log entries</li>
</ul>
</li>
<li>Volatile state on all servers:
<ul>
<li><code>commitIndex</code>: index of highest log entry known to be committed (initialized to 0, increases monotonically)</li>
<li><code>lastApplied</code>: index of highest log entry applied to state machine (initialized to 0, increases monotonically)</li>
</ul>
</li>
<li>Volatile state on leaders: These states need to be reinitialized after election
<ul>
<li><code>nextIndex[]</code>: for each server, index of the next log entry to send to that server (initialized to leader last log index + 1)</li>
<li><code>matchIndex[]</code>: for each server, index of highest log entry known to be replicated on server (initialized to 0, increases monotonically)</li>
</ul>
</li>
</ol>
<h2 id="leader-election"><a class="markdownIt-Anchor" href="#leader-election"></a> Leader election</h2>
<h3 id="how-does-the-servers-states-transit"><a class="markdownIt-Anchor" href="#how-does-the-servers-states-transit"></a> How does the servers states transit?</h3>
<ol>
<li>When servers start up, they begin as followers. A server remains in follower state as long as it receives valid RPCs from a leader or candidate.</li>
<li>Leaders send periodic heartbeats (<code>AppendEntries</code> RPCs that carry no log entries) to all followers in order to maintain their authority.</li>
<li>If a follower receives no communication over a period of time called the election timeout, then it assumes there is no viable leader and become a candidate to initiate a new election.</li>
<li>A candidate that receives votes from a majority of the full cluster becomes the new leader.<br />
<img src="/imgs/Distributed/Raft/01.png" alt="" /></li>
</ol>
<h3 id="how-to-elect-a-leader"><a class="markdownIt-Anchor" href="#how-to-elect-a-leader"></a> How to elect a leader?</h3>
<ol>
<li>To begin an election, a follower increments its current term and transitions to candidate state.</li>
<li>It then votes for itself and issues <code>RequestVote</code> RPCs in parallel to each of the other servers in the cluster.</li>
<li>Candidate continues in this state until one of three things happens
<ul>
<li>It wins the election</li>
<li>Another server establishes itself as leader</li>
<li>A period of time goes by with no winner.</li>
</ul>
</li>
</ol>
<h3 id="how-is-the-election-held"><a class="markdownIt-Anchor" href="#how-is-the-election-held"></a> How is the election held?</h3>
<ol>
<li>Each server will vote for at most one candidate in a given term, on a first-come-first-served basis to ensure that at most one candidate can win the election for a particular term.</li>
<li>A candidate wins an election if it receives votes from a majority of the servers in the full cluster for the same term.</li>
<li>Once a candidate wins an election, it becomes leader. It then sends heartbeat messages to all of the other servers to establish its authority and prevent new elections.</li>
</ol>
<h3 id="how-to-determine-that-a-server-loses-the-election"><a class="markdownIt-Anchor" href="#how-to-determine-that-a-server-loses-the-election"></a> How to determine that a server loses the election?</h3>
<ol>
<li>A candidate may receive an <code>AppendEntries</code> RPC from another server claiming to be leader.</li>
<li>If the leader’s term is at least as large as the candidate’s current term, then the candidate recognizes the leader as legitimate and returns to follower state.</li>
<li>If the term in the RPC is smaller than the candidate’s current term, then the candidate rejects the RPC and continues in candidate state.</li>
</ol>
<h3 id="how-to-handle-a-split-vote"><a class="markdownIt-Anchor" href="#how-to-handle-a-split-vote"></a> How to handle a split vote?</h3>
<ol>
<li>If many followers become candidates at the same time, votes could be split so that no candidate obtains a majority.</li>
<li>Each candidate will time out and start a new election by incrementing its term and initiating another round of RequestVote RPCs.</li>
<li>Raft uses randomized election timeouts to ensure that split votes are rare and that they are resolved quickly.
<ul>
<li>Election timeouts are chosen randomly from a fixed interval at the start of an election, and it waits for that timeout to elapse before starting the next election.</li>
<li>In most cases only a single server will time out; it wins the election and sends heartbeats before any other servers time out.</li>
</ul>
</li>
</ol>
<h3 id="how-to-ensure-that-the-leader-of-any-given-term-contains-all-of-the-entries-committed-in-previous-terms"><a class="markdownIt-Anchor" href="#how-to-ensure-that-the-leader-of-any-given-term-contains-all-of-the-entries-committed-in-previous-terms"></a> How to ensure that the leader of any given term contains all of the entries committed in previous terms?</h3>
<ol>
<li>A candidate must contact a majority of the cluster in order to be elected, which means that every committed entry must be present in at least one of those servers.</li>
<li>If the candidate’s log is at least as up-to-date as any other log in that majority, then it will hold all the committed entries.</li>
<li>In the <code>RequestVote</code> RPC, the voter denies its vote if its own log is more up-to-date than that of the candidate.</li>
<li>Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs.
<ul>
<li>If the logs have last entries with different terms, then the log with the later term is more up-to-date.</li>
<li>If the logs end with the same term, then whichever log is longer is more up-to-date.</li>
</ul>
</li>
</ol>
<h3 id="what-is-the-limitation-of-broadcast-time-and-election-timeout"><a class="markdownIt-Anchor" href="#what-is-the-limitation-of-broadcast-time-and-election-timeout"></a> What is the limitation of broadcast time and election timeout?</h3>
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mi>r</mi><mi>o</mi><mi>a</mi><mi>d</mi><mi>c</mi><mi>a</mi><mi>s</mi><mi>t</mi><mi>T</mi><mi>i</mi><mi>m</mi><mi>e</mi><mo>≪</mo><mi>e</mi><mi>l</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>T</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>o</mi><mi>u</mi><mi>t</mi><mo>≪</mo><mi>M</mi><mi>T</mi><mi>B</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">broadcastTime\ll electionTimeout\ll MTBF</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">b</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">i</span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal">c</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal">i</span><span class="mord mathnormal">m</span><span class="mord mathnormal">e</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≪</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span></span></span></span></li>
<li>BbroadcastTime is the average time it takes a server to send RPCs in parallel to every server in the cluster and receive their responses. electionTimeout is the election timeout. MTBF is the average time between failures for a single server.</li>
<li>The broadcast time should be an order of magnitude less than the election timeout so that leaders can reliably send the heartbeat messages required to keep followers from starting elections.</li>
<li>The election timeout should be a few orders of magnitude less than MTBF so that the system makes steady progress.</li>
</ol>
<h3 id="requestvote-rpc"><a class="markdownIt-Anchor" href="#requestvote-rpc"></a> RequestVote RPC</h3>
<ol>
<li>This is invoked by candidates to gather votes.</li>
<li>Arguments:
<ul>
<li><code>term</code>: candidate’s term</li>
<li><code>candidateId</code>: candidate requesting vote</li>
<li><code>lastLogIndex</code>: index of candidate’s last log entry</li>
<li><code>lastLogTerm</code>: term of candidate’s last log entry</li>
</ul>
</li>
<li>Results:
<ul>
<li><code>term</code>: currentTerm, for candidate to update itself</li>
<li><code>voteGranted</code>: true means candidate received vote</li>
</ul>
</li>
<li>Receiver implementation:
<ul>
<li>Reply <code>false</code> if <code>term &lt; currentTerm</code></li>
<li>If votedFor is <code>null</code> or <code>candidateId</code>, and candidate’s log is at least as up-to-date as receiver’s log, grant vote</li>
</ul>
</li>
</ol>
<h2 id="log-replication"><a class="markdownIt-Anchor" href="#log-replication"></a> Log replication</h2>
<h3 id="how-are-client-requests-handled"><a class="markdownIt-Anchor" href="#how-are-client-requests-handled"></a> How are client requests handled?</h3>
<ol>
<li>Each client request contains a command to be executed by the replicated state machines.</li>
<li>The leader appends the command to its log as a new entry, then issues <code>AppendEntries</code> RPCs in parallel to each of the other servers to replicate the entry.</li>
<li>When the entry has been safely replicated, the leader applies the entry to its state machine and returns the result of that execution to the client.</li>
<li>If followers crash or run slowly, or if network packets are lost, the leader retries <code>AppendEntries</code> RPCs indefinitely (even after it has responded to the client) until all followers eventually store all log entries.</li>
</ol>
<h3 id="what-is-stored-in-a-log-entry"><a class="markdownIt-Anchor" href="#what-is-stored-in-a-log-entry"></a> What is stored in a log entry?</h3>
<ol>
<li>A command for state machine</li>
<li>A term number when entry was received by leader.</li>
<li>An index to identify its position in the log. The index of the first log is 1.</li>
</ol>
<h3 id="how-to-apply-a-log-entry-to-the-state-machines"><a class="markdownIt-Anchor" href="#how-to-apply-a-log-entry-to-the-state-machines"></a> How to apply a log entry to the state machines?</h3>
<ol>
<li>The leader decides when it is safe to apply a log entry to the state machines.
<ul>
<li>Such an entry is called <code>committed</code>.</li>
<li>Raft guarantees that committed entries are durable and will eventually be executed by all of the available state machines.</li>
</ul>
</li>
<li>A log entry is committed once the leader that created the entry has replicated it on a majority of the servers.</li>
<li>It also commits all preceding entries in the leader’s log, including entries created by previous leaders.</li>
<li>The leader keeps track of the highest index it knows to be committed, and it includes that index in future <code>AppendEntries</code> RPCs, including heartbeats, so that the other servers eventually find out that they should commit some new entries.</li>
<li>Once a follower learns that a log entry is committed, it applies the entry to its local state machine in log order.</li>
</ol>
<h3 id="how-to-determine-the-consistency-between-logs"><a class="markdownIt-Anchor" href="#how-to-determine-the-consistency-between-logs"></a> How to determine the consistency between logs?</h3>
<ol>
<li><strong>Log Matching property</strong>: If two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index.</li>
<li>The Log Matching property is maintained through the following properties:
<ul>
<li>If two entries in different logs have the same index and term, then they store the same command.</li>
<li>If two entries in different logs have the same index and term, then the logs are identical in all preceding entries.</li>
</ul>
</li>
</ol>
<h3 id="how-to-check-the-consistency-in-appendentries-rpcs"><a class="markdownIt-Anchor" href="#how-to-check-the-consistency-in-appendentries-rpcs"></a> How to check the consistency in AppendEntries RPCs?</h3>
<ol>
<li>When sending an <code>AppendEntries</code> RPC, the leader includes the index and term of the entry in its log that immediately precedes the new entries.</li>
<li>If the follower does not find an entry in its log with the same index and term, then it refuses the new entries.</li>
</ol>
<h3 id="what-kinds-of-inconsistency-may-incur"><a class="markdownIt-Anchor" href="#what-kinds-of-inconsistency-may-incur"></a> What kinds of inconsistency may incur?</h3>
<ol>
<li>Leader crashes can leave the logs inconsistent. The old leader may not have fully replicated all of the entries in its log.</li>
<li>A follower may be missing entries that are present on the leader, it may have extra entries that are not present on the leader, or both.</li>
</ol>
<h3 id="how-does-leader-handle-follower-inconsistencies"><a class="markdownIt-Anchor" href="#how-does-leader-handle-follower-inconsistencies"></a> How does leader handle follower inconsistencies?</h3>
<ol>
<li><strong>Leader Append-Only</strong> property: a leader never overwrites or deletes entries in its log.</li>
<li>The leader handles inconsistencies by forcing the followers’ logs to duplicate its own. Namely, conflicting entries in follower logs will be overwritten with entries from the leader’s log.</li>
<li>The leader must find the latest log entry where the two logs agree, delete any entries in the follower’s log after that point, and send the follower all of the leader’s entries after that point.</li>
<li>All of these actions happen in response to the consistency check performed by AppendEntries RPCs.</li>
<li>A leader does not need to take any special actions to restore log consistency when it comes to power. It just begins normal operation, and the logs automatically converge in response to failures of the AppendEntries consistency check.</li>
</ol>
<h3 id="how-does-appendentries-rpc-perform-consistency-check"><a class="markdownIt-Anchor" href="#how-does-appendentries-rpc-perform-consistency-check"></a> How does AppendEntries RPC perform consistency check?</h3>
<ol>
<li>The leader maintains a nextIndex for each follower. When a leader first comes to power, it initializes all nextIndex values to the index just after the last one in its log, i.e. assuming all followers are as up-to-date as itself.</li>
<li>If a follower’s log is inconsistent with the leader’s, the consistency check will fail in the next <code>AppendEntries</code> RPC.</li>
<li>After a rejection, the leader decrements nextIndex and retries the <code>AppendEntries</code> RPC.</li>
<li>Eventually <code>nextIndex</code> will reach a point where the leader and follower logs match. When this happens, <code>AppendEntries</code> will succeed, which removes any conflicting entries in the follower’s log and appends entries from the leader’s log (if any).</li>
<li>Once <code>AppendEntries</code> succeeds, the follower’s log is consistent with the leader’s, and it will remain that way for the rest of the term.</li>
</ol>
<h3 id="how-to-handle-uncommited-entries-from-previous-leaders"><a class="markdownIt-Anchor" href="#how-to-handle-uncommited-entries-from-previous-leaders"></a> How to handle uncommited entries from previous leaders?</h3>
<ol>
<li>If a leader crashes before committing an entry, future leaders will attempt to finish replicating the entry.</li>
<li>A leader cannot immediately conclude that an entry from a previous term <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">term_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> is committed once it is stored on a majority of servers.
<ul>
<li>There could have other entries in a <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">term_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> between <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">term_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> and the leader’s current term <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">term_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>.</li>
<li>If nothing in the leaders current term has reached agreement, after the leader dies, that server with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">term_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> may become the new leader, and it can overwrite entries of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">term_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> although those entries are committed since <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>i</mi></msub><mo>&gt;</mo><mi>t</mi><mi>e</mi><mi>r</mi><msub><mi>m</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">term_i&gt;term_p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.9011879999999999em;vertical-align:-0.286108em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span></li>
</ul>
</li>
<li>Raft never commits log entries from previous terms by counting replicas.</li>
<li>Only log entries from the leader’s current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property.</li>
</ol>
<h3 id="appendentries-rpc"><a class="markdownIt-Anchor" href="#appendentries-rpc"></a> AppendEntries RPC</h3>
<ol>
<li>This is invoked by leader to replicate log entries; also used as heartbeat.</li>
<li>Arguments:
<ul>
<li><code>term</code>: leader’s term</li>
<li><code>leaderId</code>: so follower can redirect clients</li>
<li><code>prevLogIndex</code>: index of log entry immediately preceding new ones.</li>
<li><code>prevLogTerm</code>: term of <code>prevLogIndex</code> entry</li>
<li><code>entries[]</code>: log entries to store (empty for hearbeat; may send more than one for efficiency)</li>
<li><code>leaderCommit</code>: leader’s <code>commitIndex</code></li>
</ul>
</li>
<li>Results:
<ul>
<li><code>term</code>: <code>currentTerm</code>, for leader to update itself</li>
<li><code>success</code>: true if follower contrained entry matching <code>prevLogIndex</code> and <code>prevLogTerm</code></li>
</ul>
</li>
<li>Receiver implementation:
<ul>
<li>Reply <code>false</code> if <code>term &lt; currentTerm</code></li>
<li>Reply <code>false</code> if log doesn’t contrain an entry at <code>prevLogIndex</code> whose term matches <code>prevLogTerm</code></li>
<li>If an existing entry conflicts with a new one (same index but different terms), delete the existing entry and all that follow it.</li>
<li>Append any new entries not already in the log.</li>
<li>If <code>leaderCommit &gt; commitIndex</code>, set <code>commitIndex = min(leaderCommit, index of last new entry)</code>.</li>
</ul>
</li>
</ol>
<h2 id="log-compaction"><a class="markdownIt-Anchor" href="#log-compaction"></a> Log compaction</h2>
<h3 id="how-does-raft-compact-logs"><a class="markdownIt-Anchor" href="#how-does-raft-compact-logs"></a> How does Raft compact logs?</h3>
<ol>
<li>In snapshotting, the entire current system state is written to a snapshot on stable storage.</li>
<li>Once a server completes writing a snapshot, it may delete all log entries up through the last included index, as well as any prior snapshot.</li>
<li>Each server takes snapshots independently, covering just the committed entries in its log.</li>
<li>Data still only flows from leaders to followers, just followers can now reorganize their data.</li>
</ol>
<h3 id="how-to-handle-the-appendentries-that-requires-compated-entries"><a class="markdownIt-Anchor" href="#how-to-handle-the-appendentries-that-requires-compated-entries"></a> How to handle the AppendEntries that requires compated entries?</h3>
<ol>
<li>Raft also includes a small amount of metadata in the snapshot.
<ul>
<li>The <code>last included index</code> is the index of the last entry in the log that the snapshot replaces (the last entry the state machine had applied).</li>
<li>The <code>last included term</code> is the term of this entry.</li>
</ul>
</li>
<li>When the leader has already discarded the next log entry that it needs to send to a follower.</li>
<li>This situation is unlikely in normal operation: a follower that has kept up with the leader would already have this entry. However, an exceptionally slow follower or a new server joining the cluster would not.</li>
</ol>
<h3 id="how-to-install-the-snapshot-from-leader"><a class="markdownIt-Anchor" href="#how-to-install-the-snapshot-from-leader"></a> How to install the snapshot from leader?</h3>
<ol>
<li>The leader uses a new RPC called <code>InstallSnapshot</code> to send snapshots to followers that are too far behind.</li>
<li>When a follower receives a snapshot with this RPC, it must decide what to do with its existing log entries.</li>
<li>If the snapshot contains new information not already in the recipient’s log
<ul>
<li>The follower discards its entire log</li>
<li>It is all superseded by the snapshot and may possibly have uncommitted entries that conflict with the snapshot.</li>
</ul>
</li>
<li>If instead the follower receives a snapshot that describes a prefix of its log
<ul>
<li>This could be due to retransmission or by mistake.</li>
<li>Log entries covered by the snapshot are deleted but entries following the snapshot are still valid and must be retained.</li>
</ul>
</li>
</ol>
<h3 id="when-should-a-server-to-snapshot"><a class="markdownIt-Anchor" href="#when-should-a-server-to-snapshot"></a> When should a server to snapshot?</h3>
<ol>
<li>If a server snapshots too often, it wastes disk bandwidth and energy; if it snapshots too infrequently, it risks exhausting its storage capacity, and it increases the time required to replay the log during restarts.</li>
<li>One simple strategy is to take a snapshot when the log reaches a fixed size in bytes.</li>
<li>If this size is set to be significantly larger than the expected size of a snapshot, then the disk bandwidth overhead for snapshotting will be small.</li>
</ol>
<h3 id="how-to-reduce-the-delays-of-normal-operations-caused-by-a-snapshot"><a class="markdownIt-Anchor" href="#how-to-reduce-the-delays-of-normal-operations-caused-by-a-snapshot"></a> How to reduce the delays of normal operations caused by a snapshot?</h3>
<ol>
<li>The solution is to use copy-on-write techniques so that new updates can be accepted without impacting the snapshot being written.</li>
<li>The operating system’s copy-on-write support (e.g., fork on Linux) can be used to create an in-memory snapshot of the entire state machine.</li>
</ol>
<h3 id="installsnapshot-rpc"><a class="markdownIt-Anchor" href="#installsnapshot-rpc"></a> InstallSnapshot RPC</h3>
<ol>
<li>This is invoked by leader to send chunks of snapshot to a follower. Leaders always send chunks in order.</li>
<li>Arguments:
<ul>
<li><code>term</code>: leader’s term</li>
<li><code>leaderId</code>: so follower can redirect clients</li>
<li><code>lastIncludedIndex</code>: the snapshot replaces all entries up through and including this index</li>
<li><code>lastIncludedTerm</code>: term of <code>lastIncludedIndex</code></li>
<li><code>offset</code>: byte offset where chunk is positioned in the snapshot file
<ul>
<li>The whole snapshot file may be large, and hence divided into several chunks.</li>
</ul>
</li>
<li><code>data[]</code>: raw bytes of the snapshot chunk, starting at offset</li>
<li><code>done</code>: <code>true</code> if this is the last chunk</li>
</ul>
</li>
<li>Results:
<ul>
<li><code>term</code>: <code>currentTerm</code>, for leader to update itself</li>
</ul>
</li>
<li>Receiver implementation:
<ul>
<li>Reply immediately if <code>term &lt; currentTerm</code></li>
<li>Create new snapshot file if first chunk (offset is 0)</li>
<li>Write data into snapshot file at given offset</li>
<li>Reply and wait for more data chunks if done is <code>false</code>.</li>
<li>Save snapshot file, discard any existing or partial snapshot with smaller index</li>
<li>If existing log entry has same index and term as snapshot’s last included entry, retain log entries following it and reply</li>
<li>Discard the entire log</li>
<li>Reset state machine using snapshot contents (and load snapshot’s cluster configuration)</li>
</ul>
</li>
</ol>
<h2 id="client-interaction"><a class="markdownIt-Anchor" href="#client-interaction"></a> Client interaction</h2>
<h3 id="how-does-client-find-cluster-leader"><a class="markdownIt-Anchor" href="#how-does-client-find-cluster-leader"></a> How does client find cluster leader?</h3>
<ol>
<li>When a client first starts up
<ul>
<li>It connects to a randomly-chosen server.</li>
<li>If the client’s first choice is not the leader, that server will reject the client’s request and supply information about the most recent leader it has heard from.</li>
</ul>
</li>
<li>If the leader crashes
<ul>
<li>Client requests will time out.</li>
<li>Clients then try again with randomly-chosen servers.</li>
</ul>
</li>
</ol>
<h3 id="how-to-prevent-raft-execute-a-command-multiple-times"><a class="markdownIt-Anchor" href="#how-to-prevent-raft-execute-a-command-multiple-times"></a> How to prevent Raft execute a command multiple times?</h3>
<ol>
<li>Our goal for Raft is to implement linearizable semantics, i.e. each operation appears to execute instantaneously, exactly once, at some point between its invocation and its response.
<ul>
<li>One case of executing a command multiple times is that if the leader crashes after committing the log entry but before responding to the client, the client will retry the command with a new leader, causing it to be executed a second time.</li>
</ul>
</li>
<li>The solution is for clients to assign unique serial numbers to every command.</li>
<li>Then, the state machine tracks the latest serial number processed for each client, along with the associated response.</li>
<li>If it receives a command whose serial number has already been executed, it responds immediately without re-executing the request.</li>
</ol>
<h3 id="how-to-prevent-returning-stale-date-to-a-read-only-operation"><a class="markdownIt-Anchor" href="#how-to-prevent-returning-stale-date-to-a-read-only-operation"></a> How to prevent returning stale date to a read-only operation?</h3>
<ol>
<li>The reason for stale reading is that the leader responding to the request might have been superseded by a newer leader of which it is unaware.</li>
<li>A leader must have the latest information on which entries are committed.
<ul>
<li>The Leader Completeness Property guarantees that a leader has all committed entries, but at the start of its term, it may not know which those are.</li>
<li>To find out, it needs to commit an entry from its term.</li>
<li>Raft handles this by having each leader commit a blank <em>no-op</em> entry into the log at the start of its term.</li>
</ul>
</li>
<li>A leader must check whether it has been deposed before processing a read-only request, since its information may be stale if a more recent leader has been elected).
<ul>
<li>Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests.</li>
<li>Alternatively, the leader could rely on the heartbeat mechanism to provide a form of lease, but this would rely on timing for safety (it assumes bounded clock skew).</li>
</ul>
</li>
</ol>
<h1 id="experiements-and-results"><a class="markdownIt-Anchor" href="#experiements-and-results"></a> Experiements and results</h1>
<ol>
<li>The main goal of Raft is to propose a consensus algorithm which is easier to understand than Paxos. Hence the author measured the understandability of this model through scores of learning students.</li>
<li>A most important measure of a new system is its correctness. The author proved the correctness of Raft with a formal specification.</li>
<li>Finally, the author also measured the performance of Raft, which is similar to other consensus algorithms.</li>
<li>The election timeout will effect the performance of the system through the performance of leader election. Hence, the author measured how will the randomization and base election timeout effect the performance.
<ul>
<li>A small amount of randomization in the election timeout is enough to avoid split votes in elections. Using more randomness improves worst-case behavior.</li>
<li>Downtime can be reduced by reducing the election timeout.
<ul>
<li>However, lowering the timeouts beyond 12 - 14 ms violates Raft’s timing requirement: leaders have difficulty broadcasting heartbeats before other servers start new elections. This can cause unnecessary leader changes and lower overall system availability.</li>
<li>The author recommends using a conservative election timeout such as 150–300ms; such timeouts are unlikely to cause unnecessary leader changes and will still provide good availability.</li>
</ul>
</li>
<li>
<img src="/imgs/Distributed/Raft/02.png" style="zoom:33%;" />
</li>
</ul>
</li>
</ol>
<h1 id="reproduce-and-unmentioned-parts"><a class="markdownIt-Anchor" href="#reproduce-and-unmentioned-parts"></a> Reproduce and unmentioned parts</h1>
<p>Reference to the Lab 2, 3 and 4 of MIT 6.824.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Fault-Tolerance-VM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Fault-Tolerance-VM/" class="post-title-link" itemprop="url">Fault Tolerance VM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:11:02" itemprop="dateCreated datePublished" datetime="2023-09-26T13:11:02+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-04 16:28:34" itemprop="dateModified" datetime="2023-10-04T16:28:34+08:00">2023-10-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/vm-ft.pdf">The Design of a Practical System for Fault-Tolerance Virtual Machines</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#ft-design">FT design</a>
<ul>
<li><a href="#primary-backup-structure">Primary-backup structure</a>
<ul>
<li><a href="#what-is-the-usual-way-to-implement-fault-tolerance-via-primarybackup-approach">What is the usual way to implement fault-tolerance via primary/backup approach?</a></li>
<li><a href="#what-is-the-difference-between-physical-servers-and-vm-in-state-machine-level">What is the difference between physical servers and VM in state machine level?</a></li>
<li><a href="#what-is-the-basic-structure-of-ft-vms">What is the basic structure of FT VMs?</a></li>
</ul>
</li>
<li><a href="#ft-protocol">FT protocol</a>
<ul>
<li><a href="#how-does-vmware-backup-vm-replay">How does VMware backup VM replay?</a></li>
<li><a href="#what-if-the-backup-vm-executes-in-a-way-different-from-the-primary-vm">What if the backup VM executes in a way different from the primary VM?</a></li>
<li><a href="#will-the-output-rule-affect-vm-eg-stop-its-execution">Will the Output Rule affect VM, e.g. stop its execution?</a></li>
<li><a href="#what-is-the-subtleties-of-executing-disk-reads-on-the-backup-vm">What is the subtleties of executing disk reads on the backup VM?</a></li>
</ul>
</li>
<li><a href="#detecting-and-responding-to-failure">Detecting and responding to failure</a>
<ul>
<li><a href="#how-to-handle-duplicate-outputs">How to handle duplicate outputs?</a></li>
<li><a href="#how-to-handle-backup-vm-failure">How to handle backup VM failure?</a></li>
<li><a href="#how-to-handle-primary-vm-failure">How to handle primary VM failure?</a></li>
<li><a href="#after-a-failover-how-will-the-new-primary-vm-communicate-with-external-world">After a failover, how will the new primary VM communicate with external world?</a></li>
<li><a href="#how-to-detect-failure-of-primary-or-backup-vms">How to detect failure of primary or backup VMs?</a></li>
<li><a href="#how-to-avoid-split-brain-problems">How to avoid split-brain problems?</a></li>
</ul>
</li>
<li><a href="#alternative-non-shared-disk">Alternative: Non-shared disk</a>
<ul>
<li><a href="#what-is-the-difference-between-non-shared-disk-and-shared-disk">What is the difference between non-shared disk and shared disk?</a></li>
<li><a href="#in-what-case-non-shared-disk-will-be-useful">In what case non-shared disk will be useful?</a></li>
<li><a href="#what-is-the-disadvantage-of-non-shared-disk">What is the disadvantage of non-shared disk?</a></li>
<li><a href="#how-to-solve-the-split-brain-situation">How to solve the split-brain situation?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#implementation">Implementation</a>
<ul>
<li><a href="#starting-and-restarting">Starting and restarting</a>
<ul>
<li><a href="#what-requirements-need-to-be-satisfied-by-the-startup-mechanism">What requirements need to be satisfied by the startup mechanism?</a></li>
<li><a href="#how-to-implement-the-startup-mechanism">How to implement the startup mechanism?</a></li>
<li><a href="#how-to-choose-a-server-on-which-to-run-the-backup-vm">How to choose a server on which to run the backup VM?</a></li>
</ul>
</li>
<li><a href="#logging-channel">Logging channel</a>
<ul>
<li><a href="#how-to-control-primary-sending-log-entries-and-backup-receiving-entries">How to control primary sending log entries, and backup receiving entries?</a></li>
<li><a href="#what-if-the-log-buffer-of-the-primary-is-full">What if the log buffer of the primary is full?</a></li>
<li><a href="#what-is-the-main-cause-of-the-buffer-of-primary-being-full">What is the main cause of the buffer of primary being full?</a></li>
<li><a href="#how-to-prevent-the-backup-vm-from-getting-too-far-behind-the-primary">How to prevent the backup VM from getting too far behind the primary?</a></li>
</ul>
</li>
<li><a href="#special-operations">Special operations</a>
<ul>
<li><a href="#how-to-deal-with-control-operations">How to deal with control operations?</a></li>
<li><a href="#how-to-implement-the-vmotion-for-primary-and-backup-vms">How to implement the VMotion for primary and backup VMs?</a></li>
</ul>
</li>
<li><a href="#issues-for-disk-ios">Issues for disk IOs</a>
<ul>
<li><a href="#how-many-kind-of-races-may-occur">How many kind of races may occur?</a></li>
<li><a href="#how-to-solve-the-non-determinism-caused-by-several-io-operations">How to solve the non-determinism caused by several IO operations?</a></li>
<li><a href="#how-to-solve-the-non-determinism-caused-by-io-operations-and-applicationos">How to solve the non-determinism caused by IO operations and application/OS?</a></li>
<li><a href="#how-the-newly-promoted-primary-vm-handle-those-outstanding-ios">How the newly-promoted primary VM handle those outstanding IOs?</a></li>
</ul>
</li>
<li><a href="#issues-for-network-io">Issues for network IO</a>
<ul>
<li><a href="#how-to-solve-the-non-determinism-caused-by-asynchronous-updates">How to solve the non-determinism caused by asynchronous updates?</a></li>
<li><a href="#how-can-we-optimize-the-network-performance-while-running-ft">How can we optimize the network performance while running FT?</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiments-and-results">Experiments and results</a></li>
</ul>
</p>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Contribution
<ul>
<li>This paper implemented a system providing fault tolerance virtual machine (VM) based on the approach of replicating the execution of a primary VM vis a backup VM on another server. The system automatically restores redundancy after faulure.</li>
<li>It reduces performance of real applications by less than 10%. The data bandwidth needed to keep the primary and secondary VM executing in lockstep is less than 20 Mb/s for several real applications, which allows for the possibility of implementing fault tolerance over longer distance.</li>
<li>The system automatically restores redundancy after a failure by starting a new backup viretual machine on any available server in the local cluster.</li>
</ul>
</li>
<li>Limitation
<ul>
<li>Only support uni-processor VMs. Recording and replaying the execution of a multi-processor VM have significant performance issues because nearly every access to shared memory can be a non-deterministic operation.</li>
<li>Only attempt to deal with fail-stop failure, which are server failures that can be detected before the failing server causes an incorrect externally visible action.</li>
</ul>
</li>
<li>Challenges
<ul>
<li>Correctly capturing all the input and non-determinism necessary to ensure deterministic execution of a backup virtual machine.</li>
<li>Correctly applying the inputs and non-determinism to the backup virtual machine.</li>
<li>Doing so in a manner that doesn’t degrade performance.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="ft-design"><a class="markdownIt-Anchor" href="#ft-design"></a> FT design</h2>
<h3 id="primary-backup-structure"><a class="markdownIt-Anchor" href="#primary-backup-structure"></a> Primary-backup structure</h3>
<h4 id="what-is-the-usual-way-to-implement-fault-tolerance-via-primarybackup-approach"><a class="markdownIt-Anchor" href="#what-is-the-usual-way-to-implement-fault-tolerance-via-primarybackup-approach"></a> What is the usual way to implement fault-tolerance via primary/backup approach?</h4>
<ol>
<li>
<p>The backup server is always available to take over is the primary server fails.</p>
<ul>
<li>The problem is that the state of the backup server must be kept nearly identical to the primary server at all times. We say that the two VMs are in virtual lock-step.</li>
</ul>
</li>
<li>
<p>One way is to ship changes to all state of the primary. The bandwidth needed to send can be very large.</p>
</li>
<li>
<p>Another method is the state-machine approach.</p>
<ul>
<li>
<p>The idea is to model the servers as deterministic state machcines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order.</p>
</li>
<li>
<p>Some operations are not deterministic. Extra coordination must be used to ensure that they receive a primary and backup are kept in sync.</p>
</li>
<li>
<p>The extra information is far less than the amount of state (mainly memory updates) that is changing in the primary.</p>
</li>
</ul>
</li>
</ol>
<h4 id="what-is-the-difference-between-physical-servers-and-vm-in-state-machine-level"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-physical-servers-and-vm-in-state-machine-level"></a> What is the difference between physical servers and VM in state machine level?</h4>
<ol>
<li>
<p>Implementing coordication to ensure deterministic execution of physical servers is difficult, particularly as processor frequencies increase.</p>
</li>
<li>
<p>VM running on top of a hypervisor can be considered a well-defined state machine.</p>
</li>
<li>
<p>VMs still have non-deterministic operations. Hypervisor is able to capture all the necessary information about non-deterministic operations on the primary VM and to replay these operations correctly on the backup VM.</p>
</li>
</ol>
<h4 id="what-is-the-basic-structure-of-ft-vms"><a class="markdownIt-Anchor" href="#what-is-the-basic-structure-of-ft-vms"></a> What is the basic structure of FT VMs?</h4>
<ol>
<li>
<p>The virtual disks for the VMs are on shared storage, and accessible to the primary and backup VM for input and output.</p>
</li>
<li>
<p>Only the primary VM advertises its presence on the network, so all network inputs come to the primary VM. So does all other inputs.</p>
</li>
<li>
<p>All inputs, including incoming network packets, disk reads, keyboard and mouse, only come to the primary VM. And  the primary VM sends all inputs it received to the backup VM via a network connection known as the logging channel.</p>
<img src="/imgs/Distributed/FTVM/01.png" style="zoom:33%;" />
</li>
</ol>
<h3 id="ft-protocol"><a class="markdownIt-Anchor" href="#ft-protocol"></a> FT protocol</h3>
<h4 id="how-does-vmware-backup-vm-replay"><a class="markdownIt-Anchor" href="#how-does-vmware-backup-vm-replay"></a> How does VMware backup VM replay?</h4>
<ol>
<li>VMware deterministic replay records the inputs of a VM and all possible non-determinism associated with the VM execution in a stream of log entries written to a log file.</li>
<li>For non-deterministic operations, sufficient infomation is logged to allow the operation to be reproduced with the same state change and output.</li>
<li>For non-deterministic events such as timer or IO completion interrupts, the exact instruction at which the event occurred is also recorded. During replay, the event is delivered at the same point in the instruction stream.</li>
<li>VMware deterministic replay has no need to use epochs where non-deterministic events are only delievered at the end. Each interrupt is recorded as it occurs and effciently delivered at the appropriate instruction while being replayed.</li>
<li>Instead of writing the log entries to disk, we send them to the backup VM via the logging channel. The backup VM replays the entries in real time.</li>
</ol>
<h4 id="what-if-the-backup-vm-executes-in-a-way-different-from-the-primary-vm"><a class="markdownIt-Anchor" href="#what-if-the-backup-vm-executes-in-a-way-different-from-the-primary-vm"></a> What if the backup VM executes in a way different from the primary VM?</h4>
<ol>
<li>
<p>The <em>Output Requirement</em>: if the backup VM ever takes over after a failure of the primary, the backup VM will continue executing in a way that is entirely consistent with all outputs that the primary VM has sent to the external world.</p>
</li>
<li>
<p>The Output Requirement can be ensured by</p>
<ul>
<li>
<p>delaying any external output (typically a network packet) until the backup VM has received all information that will allow it to replay execution at least to the point of that output operation.</p>
</li>
<li>
<p>One necessary condition is that the backup VM must have received all log entries generated prior to the output operation.</p>
</li>
</ul>
</li>
<li>
<p>If we create a special log entry at each output operation. Then, the Output Requirement may be enforced by the Output Rule.</p>
<ul>
<li><em>Output Rule</em>: the primary VM may not send an output to the external world, until the backup VM has received and acknowledged the log entry associated with the operation producing the output.</li>
</ul>
</li>
</ol>
<h4 id="will-the-output-rule-affect-vm-eg-stop-its-execution"><a class="markdownIt-Anchor" href="#will-the-output-rule-affect-vm-eg-stop-its-execution"></a> Will the Output Rule affect VM, e.g. stop its execution?</h4>
<ol>
<li>It does not say anything about stopping the execution of the primary VM. We need only delay the sending of the output, but the VM itself can continue execution.</li>
<li>Since operating systems do non-blocking network and disk outputs with asynchronous interrupts to indicate completion, the VM can easily continue execution and will not necessarily be immediately affected by the delay in the output.</li>
</ol>
<h4 id="what-is-the-subtleties-of-executing-disk-reads-on-the-backup-vm"><a class="markdownIt-Anchor" href="#what-is-the-subtleties-of-executing-disk-reads-on-the-backup-vm"></a> What is the subtleties of executing disk reads on the backup VM?</h4>
<ol>
<li>
<p>By default, the primary VM will send the results of the disk read to the backup VM via the logging channel.</p>
</li>
<li>
<p>Executing disk read on the backup VM can greatly reduce the traffic on the logging channel for workloads that do a lot of disk reads. It may also slow down the backup VM’s execution.</p>
</li>
<li>
<p>Some extra work must be done to deal with failed disk read operations.</p>
<ul>
<li>
<p>If the primary succeeds while the backup fails, the backup needs to keep retrying until succeess.</p>
</li>
<li>
<p>If the backup succeeds while the primary fails, the contents of the target memory must be sent to the backup via the logging channel, since the contents of memory will be undetermined and not necessarily replicated by a successful disk read by the backup VM.</p>
</li>
</ul>
</li>
<li>
<p>If the primary VM does a read to a particular disk location followed fairly soon by a write to the same disk location, then the disk write must be delayed until the backup VM has executed the first disk read.</p>
</li>
</ol>
<h3 id="detecting-and-responding-to-failure"><a class="markdownIt-Anchor" href="#detecting-and-responding-to-failure"></a> Detecting and responding to failure</h3>
<h4 id="how-to-handle-duplicate-outputs"><a class="markdownIt-Anchor" href="#how-to-handle-duplicate-outputs"></a> How to handle duplicate outputs?</h4>
<ol>
<li>We cannot guarantee that all outputs are produced exactly once in a failover situation.</li>
<li>The network infrastructure (e.g. TCP) is designed to deal with lost packets and duplicate packets.</li>
</ol>
<h4 id="how-to-handle-backup-vm-failure"><a class="markdownIt-Anchor" href="#how-to-handle-backup-vm-failure"></a> How to handle backup VM failure?</h4>
<p>The primary VM will go live, i.e. leave recording mode, stop sending entries on the logging channel and start executing normally.</p>
<h4 id="how-to-handle-primary-vm-failure"><a class="markdownIt-Anchor" href="#how-to-handle-primary-vm-failure"></a> How to handle primary VM failure?</h4>
<ol>
<li>The backup VM will continue replaying its execution from the log entries until it has consumed the last log entry.</li>
<li>The backup VM will stop replaying mode and start executing as a normal VM. The backup VM has been promoted to the primary VM, and is now missing a backup VM.</li>
</ol>
<h4 id="after-a-failover-how-will-the-new-primary-vm-communicate-with-external-world"><a class="markdownIt-Anchor" href="#after-a-failover-how-will-the-new-primary-vm-communicate-with-external-world"></a> After a failover, how will the new primary VM communicate with external world?</h4>
<ol>
<li>VMware FT automatically advertises the MAC address of the new primary VM on the network, so that physical network switches will know  on what server that new primary VM is located.</li>
<li>The newly promoted primary VM may need to reissue some disk IOs.</li>
</ol>
<h4 id="how-to-detect-failure-of-primary-or-backup-vms"><a class="markdownIt-Anchor" href="#how-to-detect-failure-of-primary-or-backup-vms"></a> How to detect failure of primary or backup VMs?</h4>
<ol>
<li>VMware FT uses UDP heartbeating between servers that are running fault-tolerant VMs to detect when a server may have crashed.</li>
<li>In addition, VMware FT monitors the logging traffic that is sent from the primary to the backup VM and the acknowledgments sent from the backup VM to the primary VM. Because of regular timer interrupts, the logging traffic should be regular and never stop for a functioning guest OS.</li>
</ol>
<h4 id="how-to-avoid-split-brain-problems"><a class="markdownIt-Anchor" href="#how-to-avoid-split-brain-problems"></a> How to avoid split-brain problems?</h4>
<ol>
<li>When either a primary or backup VM wants to go live, it executes an atomic test-and-set operation on the shared virtual disk.</li>
<li>If the operation succeeds, the VM is allowed to go live.</li>
<li>If the operation fails, then the other VM must have already gone live, so the current VM actually halts itself (“commits suicide”).</li>
</ol>
<h3 id="alternative-non-shared-disk"><a class="markdownIt-Anchor" href="#alternative-non-shared-disk"></a> Alternative: Non-shared disk</h3>
<h4 id="what-is-the-difference-between-non-shared-disk-and-shared-disk"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-non-shared-disk-and-shared-disk"></a> What is the difference between non-shared disk and shared disk?</h4>
<ol>
<li>In shared disk, any write to the shared disk is considered a communication to external world. Writes to the shared disk must be delayed.</li>
<li>In non-shared disk, the virtual disks are essentially considered part of the internal state of each VM. Disk writes of the primary do not have to be delayed according to the Output Rule.</li>
</ol>
<h4 id="in-what-case-non-shared-disk-will-be-useful"><a class="markdownIt-Anchor" href="#in-what-case-non-shared-disk-will-be-useful"></a> In what case non-shared disk will be useful?</h4>
<ol>
<li>Shared storage is not accessible to the primary and backup VMs.</li>
<li>This may be the case because shared storage is unavailable or too expensive, or because the servers running the primary and backup VMs are far apart.</li>
</ol>
<h4 id="what-is-the-disadvantage-of-non-shared-disk"><a class="markdownIt-Anchor" href="#what-is-the-disadvantage-of-non-shared-disk"></a> What is the disadvantage of non-shared disk?</h4>
<ol>
<li>The two copies of the virtual disks must be explicitly synced up in some manner when fault tolerance is first enabled.</li>
<li>The disks can get out of sync after a failure, so they must be explicitly resynced when the backup VM is restarted after a failure.</li>
</ol>
<h4 id="how-to-solve-the-split-brain-situation"><a class="markdownIt-Anchor" href="#how-to-solve-the-split-brain-situation"></a> How to solve the split-brain situation?</h4>
<ol>
<li>
<p>There may be no shared storage to use for dealing with it. The system could use some other external tiebreaker.</p>
<ul>
<li>A third-party server that both servers can talk to.</li>
</ul>
</li>
<li>
<p>If the servers are part of a cluster with more than two nodes, the system could alternatively use a majority algorithm.</p>
<ul>
<li>A VM would only be allowed to go live if it is running on a server that is part of a communication sub-cluster that contains a majority of the original nodes.</li>
</ul>
</li>
</ol>
<h2 id="implementation"><a class="markdownIt-Anchor" href="#implementation"></a> Implementation</h2>
<h3 id="starting-and-restarting"><a class="markdownIt-Anchor" href="#starting-and-restarting"></a> Starting and restarting</h3>
<h4 id="what-requirements-need-to-be-satisfied-by-the-startup-mechanism"><a class="markdownIt-Anchor" href="#what-requirements-need-to-be-satisfied-by-the-startup-mechanism"></a> What requirements need to be satisfied by the startup mechanism?</h4>
<ol>
<li>We also want to use it to restart a backup VM after a failure. Hence, this mechanism must be usable for a running primary VM that is in an arbitrary state.</li>
<li>We would prefer that the mechanism does not significantly disrupt that execution of the primary VM.</li>
</ol>
<h4 id="how-to-implement-the-startup-mechanism"><a class="markdownIt-Anchor" href="#how-to-implement-the-startup-mechanism"></a> How to implement the startup mechanism?</h4>
<ol>
<li>VMware FT adapted a modified VMware VMotion that allows the migration of a running VM from one server to another server with minimal disruption. However, after migration, the VMotion will destroy the local VM.</li>
<li>The FT VMotion clones a VM to a remote host rather than migrating it without destroying the local VM.</li>
<li>The FT VMotion also sets up a logging channel, and causes the source VM to enter logging mode as the primary, and the destination VM to enter replay mode as the new backup.</li>
</ol>
<h4 id="how-to-choose-a-server-on-which-to-run-the-backup-vm"><a class="markdownIt-Anchor" href="#how-to-choose-a-server-on-which-to-run-the-backup-vm"></a> How to choose a server on which to run the backup VM?</h4>
<ol>
<li>The primary Vm informs the clustering service that it needs a new backup.</li>
<li>The clustering service determines the best server on which to run the backup VM based on resource usage and other constraints and invokes an FT VMotion to create the new backup VM.</li>
<li>VMware FT typically can re-establish VM redundancy within minutes of a server failure, all without any noticeable interruption in the execution of a fault-tolerant VM.</li>
</ol>
<h3 id="logging-channel"><a class="markdownIt-Anchor" href="#logging-channel"></a> Logging channel</h3>
<h4 id="how-to-control-primary-sending-log-entries-and-backup-receiving-entries"><a class="markdownIt-Anchor" href="#how-to-control-primary-sending-log-entries-and-backup-receiving-entries"></a> How to control primary sending log entries, and backup receiving entries?</h4>
<ol>
<li>The hypervisors maintain a large buffer for logging entries for the primary and backup VMs.</li>
<li>The contents of the primary’s log buffer are flushed out to the logging channel as soon as possible, and log entries are read into the backup’s log buffer from the logging channel as soon as they arrive.</li>
<li>The backup sends acknowledgments back to the primary each time that it reads some log entries from the network into its log buffer.</li>
</ol>
<h4 id="what-if-the-log-buffer-of-the-primary-is-full"><a class="markdownIt-Anchor" href="#what-if-the-log-buffer-of-the-primary-is-full"></a> What if the log buffer of the primary is full?</h4>
<ol>
<li>It must stop execution until log entries can be flushed out.</li>
<li>This stop in execution is a natural flow-control mechanism that slows down the primary VM when it is producing log entries at too fast a rate.</li>
<li>This pause can affect clients of the VM, and we must minimize the possibility that the primary log buffer fills up.</li>
</ol>
<h4 id="what-is-the-main-cause-of-the-buffer-of-primary-being-full"><a class="markdownIt-Anchor" href="#what-is-the-main-cause-of-the-buffer-of-primary-being-full"></a> What is the main cause of the buffer of primary being full?</h4>
<ol>
<li>One biggest reason is that the backup VM is executing too slowly and therefore sonsuming log entries too slowly.</li>
<li>In general, the backup VM must be able to replay an execution at roughly the same speed as sthe primary VM is recording the execution.</li>
<li>The overhead of recording and replaying in VMware deterministic replay is roughly the same.</li>
<li>If the server hosting the backup VM is heavily loaded with other VMs (and hence overcommitted on resources), the backup VM may not be able to get enough CPU and memory resources to execute as fast as the primary VM.</li>
</ol>
<h4 id="how-to-prevent-the-backup-vm-from-getting-too-far-behind-the-primary"><a class="markdownIt-Anchor" href="#how-to-prevent-the-backup-vm-from-getting-too-far-behind-the-primary"></a> How to prevent the backup VM from getting too far behind the primary?</h4>
<ol>
<li>When sending acknowledgments, we also send additional information to determine the real-time execution lag between the primary and backup VMs.</li>
<li>Typically the execution lag is less than 100 milliseconds.</li>
<li>If the backup VM starts having a significant execution lag (e.g. more than 1 second), VMware FT starts slowing down the primary VM by informing the scheduler to give it a slightly smally amount of CPU.</li>
<li>Such slowdowns are very rare, and typically happen only when the system is under extreme stress.</li>
</ol>
<h3 id="special-operations"><a class="markdownIt-Anchor" href="#special-operations"></a> Special operations</h3>
<h4 id="how-to-deal-with-control-operations"><a class="markdownIt-Anchor" href="#how-to-deal-with-control-operations"></a> How to deal with control operations?</h4>
<ol>
<li>
<p>Most control operations shouls be applied to both machines.</p>
<ul>
<li>
<p>If the primary VM is explicitly powered off, the backup VM should be stopped as well, and not attempt to go live.</p>
</li>
<li>
<p>Any resource management change on the primary should be applied to the backup.</p>
</li>
</ul>
</li>
<li>
<p>The only operation that can be done independently on the primary and backup VMs is VMotion.</p>
<ul>
<li>
<p>The primary and backup VMs can be VMotioned independently to other hosts.</p>
</li>
<li>
<p>VMware FT ensures that neither VM is moved to the server where the other VM is.</p>
</li>
</ul>
</li>
</ol>
<h4 id="how-to-implement-the-vmotion-for-primary-and-backup-vms"><a class="markdownIt-Anchor" href="#how-to-implement-the-vmotion-for-primary-and-backup-vms"></a> How to implement the VMotion for primary and backup VMs?</h4>
<ol>
<li>
<p>For a normal VMotion, it requires that all outstanding disk IOs be quiesced just as the final switchover on the VMotion occurs.</p>
</li>
<li>
<p>For a primary VM,</p>
<ul>
<li>
<p>The quiescing is easily handled by waiting until the physical IOs completeand delivering these completions to the VM.</p>
</li>
<li>
<p>The backup VM must disconnect from the source primary and re-connect to the destination primary at the appropriate time.</p>
</li>
</ul>
</li>
<li>
<p>For a backup VM,</p>
<ul>
<li>
<p>There is no easy way to cause all IOs to be completed at any required point, since the backup VM must replay the primary VM’s execution and complete IOs at the same execution point.</p>
</li>
<li>
<p>When a backup VM is at the final switchover point for a VMotion, it requests via the logging channel that the primary VM temporarily quiesce all its IOs.</p>
</li>
</ul>
</li>
</ol>
<h3 id="issues-for-disk-ios"><a class="markdownIt-Anchor" href="#issues-for-disk-ios"></a> Issues for disk IOs</h3>
<h4 id="how-many-kind-of-races-may-occur"><a class="markdownIt-Anchor" href="#how-many-kind-of-races-may-occur"></a> How many kind of races may occur?</h4>
<ol>
<li>
<p>The first kind is caused by several IO operations.</p>
<ul>
<li>
<p>One reason is that disk operations are non-blocking and can execute in parallel. Simultaneous disk operations access the same disk location causing races.</p>
</li>
<li>
<p>The other reason is that DMA directly to/from the memory of the VM. Simultaneous disk operations access the same memory pages.</p>
</li>
</ul>
</li>
<li>
<p>The second kind is caused by IO operations and non-IO operations.</p>
<ul>
<li>The disk operations directly access the memory of a VM via DMA. Hence a disk operation accesses the same memory pages as an aplication or OS in a VM causing races.</li>
</ul>
</li>
</ol>
<h4 id="how-to-solve-the-non-determinism-caused-by-several-io-operations"><a class="markdownIt-Anchor" href="#how-to-solve-the-non-determinism-caused-by-several-io-operations"></a> How to solve the non-determinism caused by several IO operations?</h4>
<p>We should detect any such IO races, and force such racing disk operations to execute sequentially in the same way on the primary and backup.</p>
<h4 id="how-to-solve-the-non-determinism-caused-by-io-operations-and-applicationos"><a class="markdownIt-Anchor" href="#how-to-solve-the-non-determinism-caused-by-io-operations-and-applicationos"></a> How to solve the non-determinism caused by IO operations and application/OS?</h4>
<ol>
<li>
<p>We need to set up page protection termporarily on pages that are targets of disk operations.</p>
</li>
<li>
<p>The page protections result in a trap if the VM happens to make an access to a page that is also the target of an outstanding disk operation, and the VM can be paused until the disk operation completes.</p>
</li>
<li>
<p>Changing MMU protections on pages is expensive, we use bounce buffers.</p>
<ul>
<li>
<p>A bounce buffer is a temporary buffer that has the same size as the memory being accessed by a disk operation.</p>
</li>
<li>
<p>A disk read operation is modified to read the specified data to the bounce buffer, and the data is copied to guest memory only as the IO completion is delivered.</p>
</li>
<li>
<p>For a disk write operation, the data to besent is first copied to the bounce buffer, and the disk write is modified to write data from the bounce buffer.</p>
</li>
</ul>
</li>
<li>
<p>The bounce buffer can slow down disk operations, but noticeable performance loss is not seen.</p>
</li>
</ol>
<h4 id="how-the-newly-promoted-primary-vm-handle-those-outstanding-ios"><a class="markdownIt-Anchor" href="#how-the-newly-promoted-primary-vm-handle-those-outstanding-ios"></a> How the newly-promoted primary VM handle those outstanding IOs?</h4>
<ol>
<li>There is no way for the newly-promoted primary VM to be sure if the disk IOs were issued to the disk or completed  successfully.</li>
<li>We could send an error completion that indicates that each IO failed, since it is acceptable to return an error even if the IO completed successfully. However, the guest OS might not respond well to errors from its local disk.</li>
<li>We can re-issue the pending IOs during the go-live process of the backup VM. Because we have eliminated all races and all IOs specify derectly which memory and disk blocks are accessed, these disk operations can be re-issued even if they have already completed successfully.</li>
</ol>
<h3 id="issues-for-network-io"><a class="markdownIt-Anchor" href="#issues-for-network-io"></a> Issues for network IO</h3>
<h4 id="how-to-solve-the-non-determinism-caused-by-asynchronous-updates"><a class="markdownIt-Anchor" href="#how-to-solve-the-non-determinism-caused-by-asynchronous-updates"></a> How to solve the non-determinism caused by asynchronous updates?</h4>
<ol>
<li>
<p>In normal VM, the hypervisor asynchronously updating the state of the virtual machine’s network device.</p>
</li>
<li>
<p>For FT</p>
<ul>
<li>
<p>The code that asynchronously updates VM ring buffers with incoming packets has been modified to force the guest to trap to the hypervisor, where it can log the updates and then apply them to the VM.</p>
</li>
<li>
<p>The code that normally pull packets out of transmit queues asynchronously is diabled, and instead transmits are done through a trap to the hypervisor.</p>
</li>
</ul>
</li>
</ol>
<h4 id="how-can-we-optimize-the-network-performance-while-running-ft"><a class="markdownIt-Anchor" href="#how-can-we-optimize-the-network-performance-while-running-ft"></a> How can we optimize the network performance while running FT?</h4>
<ol>
<li>
<p>Reduce VM trapand interrupts with clustering optimizations.</p>
<ul>
<li>
<p>When the VM is streaming data at a sufficient bit rate, the hypervisor can do one transmit trap per group of packets and, in the best case, zero traps, since it can transmit the packets as part of receiving new packets.</p>
</li>
<li>
<p>The hypervisor can reduce the number of interrupts to the VM for incoming packets by only posting the interrupt for a group of packets.</p>
</li>
</ul>
</li>
<li>
<p>Reduce the delay for transmitted packets.</p>
<ul>
<li>
<p>The key is to reduce the time required to send a log message to the backup and get an acknowledgment.</p>
</li>
<li>
<p>It is ensured that sending and receiving log entries and acknowledgments can all be done without any thread context switch.</p>
</li>
<li>
<p>The VMware vSphere hypervisor allows functions to be registered with the TCP stack that will be called from a deferred-execution context (similar to a tasklet in Linux) whenever TCP data is received.</p>
</li>
<li>
<p>When the primary VM enqueues a packet to be transmitted, we force an immediate log flush of the associated output log entry by scheduling a defferred-execution context to do the flush.</p>
</li>
</ul>
</li>
</ol>
<h1 id="experiments-and-results"><a class="markdownIt-Anchor" href="#experiments-and-results"></a> Experiments and results</h1>
<p>One important banchmark is the performance ratio between non-FT and FT systems and logging bandwidth between primary and backup. The performance ratio can show how efficient the FT protocol is, and logging bandwidth is usually a bottleneck of the system. The author measured them all as the following table. We can see that FT protocol only decreases the performance by less than 10%</p>
<img src="/imgs/Distributed/FTVM/02.png" style="zoom:30%;" />
<p>The typical idle logging bandwidth is 0.5-1.5 Mbits/sec. The idle bandwidth is largely the result of recording the delivery of timer interrupts. For a VM with an active workload, the logging bandwidth is dominated by the network and disk inputs that must be sent to the backup - the network packets that are received and the disk blocks that are read from disk. Hence the logging bandwidth can be much higher than those measured in table for applications that have very high network receive or disk read bandwidth. For these kinds of applications, the bandwidth of the logging channel could be a bottleneck.</p>
<p>The author also measured the bandwidth of logging channels with different capacities, as shown below. When FT is enabled for receive workloads, the loging bandwidth is very large, since all incomming network packets must be sent on the logging channel. When FT is enabled for transmit workloads, the logging bandwidth is much lower. Overall, FT can limit network bandwidths significantly at very high transmit and receive rates, but high absolute rates are still achievable.</p>
<img src="/imgs/Distributed/FTVM/03.png" style="zoom:30%;" />

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/GFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/GFS/" class="post-title-link" itemprop="url">GFS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:06:36" itemprop="dateCreated datePublished" datetime="2023-09-26T13:06:36+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-29 16:21:10" itemprop="dateModified" datetime="2024-02-29T16:21:10+08:00">2024-02-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/gfs.pdf">The Google File System</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#abstract">Abstract</a></li>
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#overview">Overview</a>
<ul>
<li><a href="#what-operations-are-supported">What operations are supported?</a></li>
<li><a href="#architecture">Architecture</a>
<ul>
<li><a href="#what-does-the-system-consist-of">What does the system consist of?</a></li>
<li><a href="#what-does-master-need-to-do">What does master need to do?</a></li>
<li><a href="#how-to-prevent-the-master-becoming-a-bottleneck">How to prevent the master becoming a bottleneck?</a></li>
<li><a href="#what-is-the-advantage-of-large-chunk-size">What is the advantage of large chunk size?</a></li>
<li><a href="#what-is-the-disadvantage-of-large-chunk-size">What is the disadvantage of large chunk size?</a></li>
<li><a href="#when-will-hot-spots-problem-emerge">When will hot spots problem emerge?</a></li>
</ul>
</li>
<li><a href="#consistency">Consistency</a>
<ul>
<li><a href="#how-do-we-define-a-file-region-being-consistent-or-defined">How do we define a file region being consistent or defined?</a></li>
<li><a href="#how-many-consistency-rules-should-be-considered">How many consistency rules should be considered?</a></li>
<li><a href="#how-do-gfs-guarantees-the-second-rule">How do GFS guarantees the second rule?</a></li>
<li><a href="#what-is-the-side-effect-of-clients-caching-chunk-locations">What is the side-effect of clients caching chunk locations?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#data-mutation">Data mutation</a>
<ul>
<li><a href="#control-data-flow">Control &amp; data flow</a>
<ul>
<li><a href="#how-do-we-minimize-management-overhead-at-the-master-of-data-mutation">How do we minimize management overhead at the master of data mutation?</a></li>
<li><a href="#what-does-leases-change">What does leases change?</a></li>
<li><a href="#how-does-the-control-flow">How does the control flow?</a></li>
<li><a href="#how-does-primary-and-secondary-servers-write-data">How does primary and secondary servers write data?</a></li>
<li><a href="#what-would-the-system-do-if-write-fails">What would the system do if write fails?</a></li>
<li><a href="#what-if-a-write-is-large-or-straddles-a-chunk-boundary">What if a write is large or straddles a chunk boundary?</a></li>
<li><a href="#how-to-prevent-primary-become-bottleneck-of-pushing-data">How to prevent primary become bottleneck of pushing data?</a></li>
<li><a href="#how-to-minimize-latency-of-pushing-data">How to minimize latency of pushing data?</a></li>
</ul>
</li>
<li><a href="#write-and-record-append">Write and record append</a>
<ul>
<li><a href="#what-is-the-difference-between-write-and-record-append">What is the difference between write and record append?</a></li>
<li><a href="#how-does-typical-writing-happen">How does typical writing happen?</a></li>
<li><a href="#how-does-readers-deal-with-occasional-padding-and-duplicates">How does readers deal with occasional padding and duplicates?</a></li>
</ul>
</li>
<li><a href="#atomic-record-appends">Atomic record appends</a>
<ul>
<li><a href="#what-if-appending-causes-the-current-chunk-to-exceed-the-maximum-size">What if appending causes the current chunk to exceed the maximum size?</a></li>
<li><a href="#what-if-appending-fails-at-some-chunkservers">What if appending fails at some chunkservers?</a></li>
</ul>
</li>
<li><a href="#snapshot">Snapshot</a>
<ul>
<li><a href="#what-does-snapshot-do">What does snapshot do?</a></li>
<li><a href="#how-does-snapshot-be-implemented">How does snapshot be implemented?</a></li>
<li><a href="#how-does-clients-write-a-chunk-after-snapshot">How does clients write a chunk after snapshot?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#master">Master</a>
<ul>
<li><a href="#basic-operations">Basic operations</a>
<ul>
<li><a href="#how-does-client-communicate-with-master-specifically">How does client communicate with master specifically?</a></li>
<li><a href="#what-metadata-does-master-need-to-store">What metadata does master need to store?</a></li>
<li><a href="#how-to-persist">How to persist?</a></li>
<li><a href="#how-does-gfs-manage-namespace">How does GFS manage namespace?</a></li>
<li><a href="#how-does-gfs-design-locking-scheme">How does GFS design locking scheme?</a></li>
</ul>
</li>
<li><a href="#replica-management">Replica management</a>
<ul>
<li><a href="#how-to-place-replicas">How to place replicas?</a></li>
<li><a href="#what-factors-are-considered-when-create-a-new-chunk">What factors are considered when create a new chunk?</a></li>
<li><a href="#what-if-the-number-of-available-replicas-of-a-chunk-falls-below-a-user-specified-goal">What if the number of available replicas of a chunk falls below a user-specified goal?</a></li>
<li><a href="#what-if-cloning-traffic-from-overwhelming-client-traffic">What if cloning traffic from overwhelming client traffic?</a></li>
<li><a href="#how-to-keep-the-placement-of-replicas-in-balance">How to keep the placement of replicas in balance?</a></li>
</ul>
</li>
<li><a href="#deletion">Deletion</a>
<ul>
<li><a href="#how-to-delete-a-file">How to delete a file?</a></li>
<li><a href="#what-are-the-advantages-and-disadvantages-of-lazy-deletion-over-eager-deletion">What are the advantages and disadvantages of lazy deletion over eager deletion?</a></li>
<li><a href="#how-to-address-the-issues-of-reusing">How to address the issues of reusing?</a></li>
<li><a href="#how-to-handle-the-possible-stale-replicas">How to handle the possible stale replicas?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#fault-tolerance">Fault tolerance</a>
<ul>
<li><a href="#how-to-handle-master-failure">How to handle master failure?</a></li>
<li><a href="#why-cannot-recover-data-using-other-chunk-replicas-why-each-chunkserver-must-independently-verify-the-integrity">Why cannot recover data using other chunk replicas? Why each chunkserver must independently verify the integrity?</a></li>
<li><a href="#how-to-ensure-data-integrity">How to ensure data integrity?</a></li>
<li><a href="#how-to-read-data-with-checksum">How to read data with checksum?</a></li>
<li><a href="#how-to-write-data-with-checksum">How to write data with checksum?</a></li>
<li><a href="#what-is-included-in-the-diagnostic-logs">What is included in the diagnostic logs?</a></li>
</ul>
</li>
<li><a href="#other-parts-unmentioned">Other parts (unmentioned)</a>
<ul>
<li><a href="#to-sum-up-what-is-the-metadata-of-master-and-where-are-they">To sum up, what is the metadata of master, and where are they?</a></li>
<li><a href="#what-is-the-cause-of-split-brain-how-to-solve-it">What is the cause of split brain? How to solve it?</a></li>
<li><a href="#why-gfs-doesnt-overwrite-those-failed-records-immediately-but-leaving-padding-and-duplicates">Why GFS doesn’t overwrite those failed records immediately, but leaving padding and duplicates?</a></li>
<li><a href="#gfs-is-a-weak-consistency-system-how-can-we-upgrade-it-to-a-strong-consistency-system">GFS is a weak consistency system, how can we upgrade it to a strong consistency system?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiments-and-results">Experiments and results</a>
<ul>
<li><a href="#micro-benchmarks">Micro-benchmarks</a>
<ul>
<li><a href="#read">Read</a></li>
<li><a href="#write">Write</a></li>
<li><a href="#record-append">Record append</a></li>
</ul>
</li>
<li><a href="#real-world-clusters">Real world clusters</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h1>
<ol>
<li><strong>Main idea</strong>:</li>
<li><strong>Key findings</strong>:</li>
<li><strong>The system</strong>:</li>
<li><strong>Evaluation</strong>:</li>
</ol>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Contribution: provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients.</li>
<li>Difference points in design space
<ul>
<li>This system integreted constant monitoring, error detection, fault tolerance, and automatic recovery.</li>
<li>Files are huge by traditional standards. Design assumptions and parameters such as I/O operation and block sizes have to be revisited.</li>
<li>Most files are mutated by appending new data rather than overwriting existing data. Random writes within a file are practically non-existent. Once written, the files are only read, and often only seuqentially.</li>
</ul>
</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> Overview</h2>
<h3 id="what-operations-are-supported"><a class="markdownIt-Anchor" href="#what-operations-are-supported"></a> What operations are supported?</h3>
<ol>
<li>Usual operations: <code>create</code>, <code>delete</code>, <code>open</code>, <code>close</code>, <code>read</code>, and <code>write</code></li>
<li><code>snapshot</code>: creates a copy of a file or a directory tree at low cost</li>
<li><code>record append</code>: allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity</li>
</ol>
<h3 id="architecture"><a class="markdownIt-Anchor" href="#architecture"></a> Architecture</h3>
<img src="/imgs/Distributed/GFS/01.png" style="zoom:33%;" />
<h4 id="what-does-the-system-consist-of"><a class="markdownIt-Anchor" href="#what-does-the-system-consist-of"></a> What does the system consist of?</h4>
<ol>
<li>Consist of a signle <em>master</em> and multiple <em>chunkservers</em> and is accessed by multiple <em>clients</em>.</li>
<li>Files are divided into fixed-size chunks. Each chunk is identified by an immutable and globally unique 64 bit <em>chunk handle</em> assigned by the master at the timeof chunk creation.</li>
<li>Chunkservers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range. Each chunk is replicated on multiple chunkservers, three by default.</li>
<li>Neither the client nor the chunkserver caches file data. Caches offer little benefit while causing coherence issues. But clients do cache metadata.</li>
</ol>
<h4 id="what-does-master-need-to-do"><a class="markdownIt-Anchor" href="#what-does-master-need-to-do"></a> What does master need to do?</h4>
<ol>
<li>
<p>The master maintains all file system metadata, and controls system-wide activities.</p>
<ul>
<li>
<p>Metadata includes namespace, access control information, the mapping from files to chunks, and the current locations of chunks.</p>
</li>
<li>
<p>System-wide activities includes chunk lease management, garbage collection of orphaned chunks, and chunk migration between chunkservers.</p>
</li>
</ul>
</li>
<li>
<p>The master periodically comminicates with each chunkserver in HeartBeat messages to give it instructions and collect its state.</p>
</li>
<li>
<p>Clients interact with the master for metadata operations, but all data-bearing communication goes directly to the chunkservers.</p>
</li>
</ol>
<h4 id="how-to-prevent-the-master-becoming-a-bottleneck"><a class="markdownIt-Anchor" href="#how-to-prevent-the-master-becoming-a-bottleneck"></a> How to prevent the master becoming a bottleneck?</h4>
<ol>
<li>
<p>The idea is to minimize its involvement in reads and writes.</p>
</li>
<li>
<p>A client asks the master which chunkservers it should contact, and caches this information for a limited time and interacts with the chunkservers directly for subsequent operations.</p>
</li>
</ol>
<h4 id="what-is-the-advantage-of-large-chunk-size"><a class="markdownIt-Anchor" href="#what-is-the-advantage-of-large-chunk-size"></a> What is the advantage of large chunk size?</h4>
<ol>
<li>
<p>Reduce clients’ need to interact with the master because reads and writes on the same chunk require only one initial request to the master for chunk location information.</p>
</li>
<li>
<p>Reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period of time.</p>
</li>
<li>
<p>Reduce the size of the metadata stored on the master.</p>
</li>
</ol>
<h4 id="what-is-the-disadvantage-of-large-chunk-size"><a class="markdownIt-Anchor" href="#what-is-the-disadvantage-of-large-chunk-size"></a> What is the disadvantage of large chunk size?</h4>
<ol>
<li>
<p>Wasting space due to internal fragmentation. This can be eased through lazy space allocation.</p>
</li>
<li>
<p>A small file consists of a small number of chunks, perhaps just one. The chunkservers storing those chunks may become hot spots if many clients are accessing the same file. This have not been a major issue.</p>
</li>
</ol>
<h4 id="when-will-hot-spots-problem-emerge"><a class="markdownIt-Anchor" href="#when-will-hot-spots-problem-emerge"></a> When will hot spots problem emerge?</h4>
<ol>
<li>
<p>A more common case is that an executable was written to GFS as a single-chunk file and then started on hundreds of machines at the same time.</p>
</li>
<li>
<p>We can fix this problem by storing such executables with a higher replication factor.</p>
</li>
</ol>
<h3 id="consistency"><a class="markdownIt-Anchor" href="#consistency"></a> Consistency</h3>
<h4 id="how-do-we-define-a-file-region-being-consistent-or-defined"><a class="markdownIt-Anchor" href="#how-do-we-define-a-file-region-being-consistent-or-defined"></a> How do we define a file region being consistent or defined?</h4>
<ol>
<li>
<p>A file region is consistent if all clients will always see the same data, regardless of which replicas they read from.</p>
</li>
<li>
<p>A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety.</p>
</li>
<li>
<p>Concurrent successful mutations leave the region undefined but consistent: all clients see the same data, but it may not reflect what any one mutation has written. Typically, it consists of mingled fragments from multiple mutations.</p>
</li>
</ol>
<h4 id="how-many-consistency-rules-should-be-considered"><a class="markdownIt-Anchor" href="#how-many-consistency-rules-should-be-considered"></a> How many consistency rules should be considered?</h4>
<ol>
<li>File namespace mutations (e.g. file creation) are atomic.</li>
<li>After a sequence of successful mutations, the mutated file region is guaranteed to be defined and contain the data written by the last mutation.</li>
</ol>
<h4 id="how-do-gfs-guarantees-the-second-rule"><a class="markdownIt-Anchor" href="#how-do-gfs-guarantees-the-second-rule"></a> How do GFS guarantees the second rule?</h4>
<ol>
<li>Applying mutations to a chunk in the same order on all its replicas.</li>
<li>Using chunk version numbers to detect any replica that has become stale because it has missed mutations while its chunkserver was down.</li>
<li>Stale replicas will never be involved in a mutation or given to clients asking the master for chunk locations. They are garbage collected at the earliest opportunity.</li>
</ol>
<h4 id="what-is-the-side-effect-of-clients-caching-chunk-locations"><a class="markdownIt-Anchor" href="#what-is-the-side-effect-of-clients-caching-chunk-locations"></a> What is the side-effect of clients caching chunk locations?</h4>
<ol>
<li>They may read from a stale replica before that information is refreshed.</li>
<li>This window is limited by the cache entry’s timeout and dthe next open of the file.</li>
<li>As most of files are append-only, a stale replica usually returns a premature end of chunk rather than outdated data.</li>
</ol>
<h2 id="data-mutation"><a class="markdownIt-Anchor" href="#data-mutation"></a> Data mutation</h2>
<h3 id="control-data-flow"><a class="markdownIt-Anchor" href="#control-data-flow"></a> Control &amp; data flow</h3>
<h4 id="how-do-we-minimize-management-overhead-at-the-master-of-data-mutation"><a class="markdownIt-Anchor" href="#how-do-we-minimize-management-overhead-at-the-master-of-data-mutation"></a> How do we minimize management overhead at the master of data mutation?</h4>
<ol>
<li>We use leases to maintain a consistent mutation order across replicas.</li>
<li>The master grants a chunk lease to one of the replicas, which we call the primary. The primary picks a serial order for all mutations to the chunk. All replicas follow this order when applying mutations.</li>
<li>The client caches who is the lease of a certain chunk for future mutations. It needs to contact the master again only when the primary becomes unreachable or replies that it no longer holds a lease.</li>
</ol>
<h4 id="what-does-leases-change"><a class="markdownIt-Anchor" href="#what-does-leases-change"></a> What does leases change?</h4>
<ol>
<li>A lease has an initial timeout of 60 seconds. However, as long as the chunk is being mutated, the primary can request and typically receive extensions from the master indefinitely.</li>
<li>These extension requests and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunkservers.</li>
<li>The master may sometimes try to revoke a lease before it expires (e.g., when the master wants to disable mutations on a file that is being renamed).</li>
<li>Even if the master loses communication with a primary, it can safely grant a new lease to another replica after the old lease expires.</li>
</ol>
<h4 id="how-does-the-control-flow"><a class="markdownIt-Anchor" href="#how-does-the-control-flow"></a> How does the control flow?</h4>
<ol>
<li>
<p>the client asks the master which chunkserver holds the current lease for the chunk and the locations of the other replicas. If no one has a lease, the master grants one to a replica it chooses</p>
</li>
<li>
<p>the master replies with the identity of the primary and the locations of the other (secondary) replicas.</p>
</li>
<li>
<p>The client pushes the data to all the replicas in any order, instead of only sending to the lease.</p>
</li>
<li>
<p>once all the replicas have acknowledged receiving the data, the client sends a write request to the primary.</p>
</li>
<li>
<p>the primary forwards the write request to all secondary replicas.</p>
</li>
<li>
<p>the secondaries all reply to the primary indicating that they have completed the operation.</p>
</li>
<li>
<p>the primary replies to the client. Any errors encountered at any of the replicas are reported to the client.</p>
<img src="/imgs/Distributed/GFS/02.png" style="zoom:25%;" />
</li>
</ol>
<h4 id="how-does-primary-and-secondary-servers-write-data"><a class="markdownIt-Anchor" href="#how-does-primary-and-secondary-servers-write-data"></a> How does primary and secondary servers write data?</h4>
<ol>
<li>
<p>Each chunkserver will store the data from client in an internal LRU buffer cache until the data is used or aged out.</p>
</li>
<li>
<p>The write request from client to primary identifies the data pushed earlier to all of the replicas.</p>
</li>
<li>
<p>The primary assigns consecutive serial numbers to all the mutations it receives, possibly from multiple clients, which provides the necessary serialization.</p>
</li>
<li>
<p>The primary applies the mutation to its own local state in serial number order.</p>
</li>
</ol>
<h4 id="what-would-the-system-do-if-write-fails"><a class="markdownIt-Anchor" href="#what-would-the-system-do-if-write-fails"></a> What would the system do if write fails?</h4>
<ol>
<li>
<p>If it had failed at the primary, it would not have been assigned a serial number and forwarded.</p>
</li>
<li>
<p>In other cases, the write may have succeeded at the primary and an arbitrary subset of the secondary replicas. The client request is considered to have failed, and the modified region is left in an inconsistent state.</p>
</li>
<li>
<p>The client code handles such errors by retrying the failed mutation. It will make a few attempts at steps 3 through 7 before falling back to a retry from the beginning of the write.</p>
</li>
</ol>
<h4 id="what-if-a-write-is-large-or-straddles-a-chunk-boundary"><a class="markdownIt-Anchor" href="#what-if-a-write-is-large-or-straddles-a-chunk-boundary"></a> What if a write is large or straddles a chunk boundary?</h4>
<ol>
<li>GFS client code breaks it down into multiple write operations.</li>
<li>They all follow the control flow described above but may be interleaved with and overwritten by concurrent operations from other clients.</li>
<li>The shared file region may end up containing fragments from different clients, although the replicas will be identical because the individual operations are completed successfully in the same order on all replicas.</li>
</ol>
<h4 id="how-to-prevent-primary-become-bottleneck-of-pushing-data"><a class="markdownIt-Anchor" href="#how-to-prevent-primary-become-bottleneck-of-pushing-data"></a> How to prevent primary become bottleneck of pushing data?</h4>
<ol>
<li>While control flows from the client to the primary and then to all secondaries, data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion.</li>
<li>By decoupling the data flow from the control flow, we can improve performance by scheduling the expensive data flow based on the network topology regardless of which chunkserver is the primary.</li>
<li>Our goals are to fully utilize each machine’s network bandwidth, avoid network bottlenecks and high-latency links, and minimize the latency to push through all the data.</li>
<li>Each machine forwards the data to the “closest” machine in the network topology that has not received it.</li>
<li>Our network topology is simple enough that “distances” can be accurately estimated from IP addresses.</li>
</ol>
<h4 id="how-to-minimize-latency-of-pushing-data"><a class="markdownIt-Anchor" href="#how-to-minimize-latency-of-pushing-data"></a> How to minimize latency of pushing data?</h4>
<ol>
<li>We minimize latency by pipelining the data transfer over TCP connections. Once a chunkserver receives some data, it starts forwarding immediately.</li>
<li>Pipelining is especially helpful to us because we use a switched network with full-duplex links. Sending the data immediately does not reduce the receive rate.</li>
</ol>
<h3 id="write-and-record-append"><a class="markdownIt-Anchor" href="#write-and-record-append"></a> Write and record append</h3>
<h4 id="what-is-the-difference-between-write-and-record-append"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-write-and-record-append"></a> What is the difference between write and record append?</h4>
<ol>
<li>
<p>A write causes data to be written at an application-specified file offset.</p>
</li>
<li>
<p>A record append causes data (the “record”) to be appended atomically at least once even in the presence of concurrent mutations, but at an offset of GFS’s choosing.</p>
<ul>
<li>
<p>The offset is returned to the client and marks the beginning of a defined region that contains the record.</p>
</li>
<li>
<p>GFS may insert padding or record duplicates in between. They occupy regions considered to be inconsistent and are typically dwarfed by the amount of user data.</p>
</li>
</ul>
</li>
<li>
<p>A “regular” append is merely a write at an offset that the client believes to be the current end of file.</p>
</li>
</ol>
<h4 id="how-does-typical-writing-happen"><a class="markdownIt-Anchor" href="#how-does-typical-writing-happen"></a> How does typical writing happen?</h4>
<ol>
<li>A writer generates a file from beginning to end. It atomically renames the file to a permanent name after writing all the data, or periodically checkpoints how much has been successfully written.</li>
<li>Checkpoints may also include application-level checksums. Readers verify and process only the file region up to the last checkpoint, which is known to be in the defined state.</li>
<li>Checkpointing allows writers to restart incrementally and keeps readers from processing successfully written file data that is still incomplete.</li>
</ol>
<h4 id="how-does-readers-deal-with-occasional-padding-and-duplicates"><a class="markdownIt-Anchor" href="#how-does-readers-deal-with-occasional-padding-and-duplicates"></a> How does readers deal with occasional padding and duplicates?</h4>
<ol>
<li>Each record prepared by the writer contains extra information like checksums so that its validity can be verified.</li>
<li>A reader can identify and discard extra padding and record fragments using the checksums.</li>
<li>If it cannot tolerate the occasional duplicates, it can filter them out using unique identifiers in the records, which are often needed anyway to name corresponding application entities such as web documents.</li>
</ol>
<h3 id="atomic-record-appends"><a class="markdownIt-Anchor" href="#atomic-record-appends"></a> Atomic record appends</h3>
<h4 id="what-if-appending-causes-the-current-chunk-to-exceed-the-maximum-size"><a class="markdownIt-Anchor" href="#what-if-appending-causes-the-current-chunk-to-exceed-the-maximum-size"></a> What if appending causes the current chunk to exceed the maximum size?</h4>
<ol>
<li>The primary checks to see if appending the record to the current chunk would cause the chunk to exceed the maximum size.</li>
<li>If so, it pads the chunk to the maximum size, tells secondaries to do the same, and replies to the client indicating that the operation should be retried on the next chunk.</li>
<li>If the record fits within the maximum size, which is the common case, the primary appends the data to its replica, tells the secondaries to write the data at the exact offset where it has, and finally replies success to the client.</li>
</ol>
<h4 id="what-if-appending-fails-at-some-chunkservers"><a class="markdownIt-Anchor" href="#what-if-appending-fails-at-some-chunkservers"></a> What if appending fails at some chunkservers?</h4>
<ol>
<li>Replicas of the same chunk may contain different data possibly including duplicates of the same record in whole or in part.</li>
<li>GFS does not guarantee that all replicas are bytewise identical. It only guarantees that the data is written at least once as an atomic unit.</li>
<li>If there is any secondary chunkserver that can successfully append the record, the primary is succeed. Next time, the primary can choose an offset after the failed record.</li>
<li>Hence, after this, all replicas are at least as long as the end of record and therefore any future record will be assigned a higher offset or a different chunk even if a different replica later becomes the primary.</li>
</ol>
<h3 id="snapshot"><a class="markdownIt-Anchor" href="#snapshot"></a> Snapshot</h3>
<h4 id="what-does-snapshot-do"><a class="markdownIt-Anchor" href="#what-does-snapshot-do"></a> What does snapshot do?</h4>
<ol>
<li>The snapshot operation makes a copy of a file or a directory tree (the “source”) almost instantaneously, while minimizing any interruptions of ongoing mutations.</li>
<li>Users use it to quickly create branch copies of huge data sets (and often copies of those copies, recursively), or to checkpoint the current state before experimenting with changes that can later be committed or rolled back easily.</li>
</ol>
<h4 id="how-does-snapshot-be-implemented"><a class="markdownIt-Anchor" href="#how-does-snapshot-be-implemented"></a> How does snapshot be implemented?</h4>
<ol>
<li>
<p>It use standard copy-on-write techniques.</p>
</li>
<li>
<p>Master revokes leases on the chunks in the files it is about to snapshot.</p>
<ul>
<li>
<p>This ensures that any subsequent writes to these chunks will require an interaction with the master to find the lease holder.</p>
</li>
<li>
<p>And this will give the master an opportunity to create a new copy of the chunk first.</p>
</li>
</ul>
</li>
<li>
<p>Master logs the operation to disk.</p>
</li>
<li>
<p>It then applies this log record to its in-memory state by duplicating the metadata for the source file or directory tree. The newly created snapshot files point to the same chunks as the source files.</p>
</li>
</ol>
<h4 id="how-does-clients-write-a-chunk-after-snapshot"><a class="markdownIt-Anchor" href="#how-does-clients-write-a-chunk-after-snapshot"></a> How does clients write a chunk after snapshot?</h4>
<ol>
<li>The first time a client wants to write to a chunk C after the snapshot operation, it sends a request to the master to find the current lease holder.</li>
<li>The master notices that the reference count for chunk C is greater than one. It defers replying to the client request and instead picks a new chunk handle C’.</li>
<li>It then asks each chunkserver that has a current replica of C to create a new chunk called C’.</li>
<li>By creating the new chunk on the same chunkservers as the original, we ensure that the data can be copied locally, not over the network.</li>
<li>The master grants one of the replicas a lease on the new chunk C’ and replies to the client, which can write the chunk normally.</li>
</ol>
<h2 id="master"><a class="markdownIt-Anchor" href="#master"></a> Master</h2>
<h3 id="basic-operations"><a class="markdownIt-Anchor" href="#basic-operations"></a> Basic operations</h3>
<h4 id="how-does-client-communicate-with-master-specifically"><a class="markdownIt-Anchor" href="#how-does-client-communicate-with-master-specifically"></a> How does client communicate with master specifically?</h4>
<ol>
<li>The client translates the file name and byte offset specified by the application into a chunk index within the file.</li>
<li>It sends the master a requeust containing the file name and chunk index.</li>
<li>The master replies with the corresponding chunk handle and locations of the replicas.</li>
<li>The client caches this information using the file name and chunk index as the key.</li>
</ol>
<h4 id="what-metadata-does-master-need-to-store"><a class="markdownIt-Anchor" href="#what-metadata-does-master-need-to-store"></a> What metadata does master need to store?</h4>
<ol>
<li>
<p>Stored persistently: the file and chunk namespace, the mapping from files to chunks</p>
<ul>
<li>
<p>The master will scan periodically through its entire state in the background</p>
</li>
<li>
<p>Periodic scanning is to implement chunk garbage collection, re-replication in the presence of chunkserver failures, and chunk migration to balance load and disk space usage across chunkservers.</p>
</li>
<li>
<p>The number of chunks and hence the capacity of the whole system is limited by how much memory the master has. But not a serious limitation for less than <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>64</mn></mrow><annotation encoding="application/x-tex">64</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord">4</span></span></span></span> bytes of metadata for each chunk.</p>
</li>
</ul>
</li>
<li>
<p>No need to store persistently: the locations of each chunk’s replicas.</p>
<ul>
<li>
<p>The master asks each chunkserver about its chunks at master startup and whenever a chunkserver joins the cluster.</p>
</li>
<li>
<p>The master can keep itself up-to-date thereafter because it controls all chunk placement and monitors chunkserver status.</p>
</li>
</ul>
</li>
<li>
<p>Operation record</p>
<ul>
<li>
<p>The namespace and mapping are kept persistent by logging mutations to operation log</p>
</li>
<li>
<p>It is stored on the master’s local disk and replicated on remote machines</p>
</li>
</ul>
</li>
</ol>
<h4 id="how-to-persist"><a class="markdownIt-Anchor" href="#how-to-persist"></a> How to persist?</h4>
<ol>
<li>Operation log
<ul>
<li>The operation log contains a historical record of critical metadata changes. Not only is it the only persistent record of critical metadata, it also serves as a logical time line that defines the order of concurrent operations.</li>
<li>The system respond to a client operation only after flushing the corresponding log record to disk both locally and remotely.</li>
<li>The master batches several log records together before flushing thereby reducing the impact of flushing and replication on overall system thoughput.</li>
</ul>
</li>
<li>Checkpoint
<ul>
<li>To minimize startup time, we must keep the log small. The master checkpoints its state whenever the log grows beyond a certain size. It can recover by loading the latest checkpoint and replaying only the records after that.</li>
<li>The checkpoint is in a compact B-tree like form.</li>
<li>The master switches to a new log file and creates the new checkpoint in a separate thread.</li>
<li>Older checkpoints and log files can be freely deleted, though we keep a few around to guard against catastrophes. A failure during checkpointing does not affect correctness because the recovery code detects and skips incomplete checkpoints.</li>
</ul>
</li>
</ol>
<h4 id="how-does-gfs-manage-namespace"><a class="markdownIt-Anchor" href="#how-does-gfs-manage-namespace"></a> How does GFS manage namespace?</h4>
<ol>
<li>GFS does not have a per-directory data structure that lists all the files in that directory. Nor does it support aliases for the same file or directory.</li>
<li>GFS logically represents its namespace as a lookup table mapping full pathnames to metadata. With prefix compression, this table can be efficiently represented in memory.</li>
<li>Each node in the namespace tree (either an absolute file name or an absolute directory name) has an associated read-write lock.</li>
</ol>
<h4 id="how-does-gfs-design-locking-scheme"><a class="markdownIt-Anchor" href="#how-does-gfs-design-locking-scheme"></a> How does GFS design locking scheme?</h4>
<ol>
<li>If a master operation involves a certain file or directory, it will acquire read-locks on all the parent directories, and either a read-lock or a write-lock on the leaf file or directory that it will operate directly.</li>
<li>File creation does not require a write lock on the parent directory because there is no “directory”, or inode-like, data structure to be protected from modification.</li>
<li>This locking scheme allows concurrent mutations in the same directory.</li>
</ol>
<h3 id="replica-management"><a class="markdownIt-Anchor" href="#replica-management"></a> Replica management</h3>
<h4 id="how-to-place-replicas"><a class="markdownIt-Anchor" href="#how-to-place-replicas"></a> How to place replicas?</h4>
<ol>
<li>There are two purposes: maximize data reliability and availability, and maximize network bandwidth utilization.</li>
<li>It is not enough to spread replicas across machines, which only guards against disk or machine failures and fully utilizes each machine’s network bandwidth.</li>
<li>We must also spread chunk replicas across racks. This ensures that some replicas of a chunk will survive and remain available even if an entire rack is damaged or offline. It also means that traffic, especially reads, for a chunk can exploit the aggregate bandwidth of multiple racks.</li>
<li>On the other hand, write traffic has to flow through multiple racks, a tradeoff we make willingly.</li>
<li>An even safer way is to spread across data centers in different cities. It can guards against a city-level catastrophe.</li>
</ol>
<h4 id="what-factors-are-considered-when-create-a-new-chunk"><a class="markdownIt-Anchor" href="#what-factors-are-considered-when-create-a-new-chunk"></a> What factors are considered when create a new chunk?</h4>
<ol>
<li>We want to place new replicas on chunkservers with below-average disk space utilization. Over time this will equalize disk utilization across chunkservers.</li>
<li>We want to limit the number of “recent” creations on each chunkserver. Although creation itself is cheap, it reliably predicts imminent heavy write traffic because chunks are created when demanded by writes.</li>
<li>We want to spread replicas of a chunk across racks.</li>
</ol>
<h4 id="what-if-the-number-of-available-replicas-of-a-chunk-falls-below-a-user-specified-goal"><a class="markdownIt-Anchor" href="#what-if-the-number-of-available-replicas-of-a-chunk-falls-below-a-user-specified-goal"></a> What if the number of available replicas of a chunk falls below a user-specified goal?</h4>
<ol>
<li>
<p>The master would re-replicate the chunk.</p>
</li>
<li>
<p>If there are many chunks below their goal, the master picks the highest priority chunk considering some factors and “clones” it by instructing some chunkserver to copy the chunk data directly from an existing valid replica.</p>
<ul>
<li>
<p>How far it is from its replication goal.</p>
</li>
<li>
<p>Prefer to first re-replicate chunks for live files as opposed to chunks that belong to recently deleted files.</p>
</li>
<li>
<p>To minimize the impact of failures on running applications, we boost the priority of any chunk that is blocking client progress.</p>
</li>
</ul>
</li>
</ol>
<h4 id="what-if-cloning-traffic-from-overwhelming-client-traffic"><a class="markdownIt-Anchor" href="#what-if-cloning-traffic-from-overwhelming-client-traffic"></a> What if cloning traffic from overwhelming client traffic?</h4>
<ol>
<li>The master limits the numbers of active clone operations both for the cluster and for each chunkserver.</li>
<li>Each chunkserver limits the amount of bandwidth it spends on each clone operation by throttling its read requests to the source chunkserver.</li>
</ol>
<h4 id="how-to-keep-the-placement-of-replicas-in-balance"><a class="markdownIt-Anchor" href="#how-to-keep-the-placement-of-replicas-in-balance"></a> How to keep the placement of replicas in balance?</h4>
<ol>
<li>It examines the current replica distribution and moves replicas for better disk space and load balancing.</li>
<li>The master gradually fills up a new chunkserver rather than instantly swamps it with new chunks and the heavy write traffic that comes with them.</li>
<li>The master must also choose which existing replica to remove. It prefers to remove those on chunkservers with below-average free space.</li>
</ol>
<h3 id="deletion"><a class="markdownIt-Anchor" href="#deletion"></a> Deletion</h3>
<h4 id="how-to-delete-a-file"><a class="markdownIt-Anchor" href="#how-to-delete-a-file"></a> How to delete a file?</h4>
<ol>
<li>
<p>GFS does not immediately reclaim the available physical storage, it is just renamed to a hidden name that includes the deletion timestamp. It does so only lazily during regular garbage collection at both the file and chunk levels.</p>
</li>
<li>
<p>During the master’s regular scan of the file system namespace, it removes any such hidden files if they have existed for more than three days (the interval is configurable).</p>
<ul>
<li>
<p>Until then, the file can still be read under the new, special name and can be undeleted by renaming it back to normal.</p>
</li>
<li>
<p>When the hidden file is removed from the namespace, its in-memory metadata is erased. This effectively severs its links to all its chunks.</p>
</li>
</ul>
</li>
<li>
<p>In a similar regular scan of the chunk namespace, the master identifies orphaned chunks (i.e., those not reachable from any file) and erases the metadata for those chunks.</p>
<ul>
<li>
<p>In a HeartBeat message exchanged with the master, each chunkserver reports a subset of the chunks it has, and the master replies with the identity of all chunks that are no longer present in the master’s metadata.</p>
</li>
<li>
<p>The chunkserver is free to delete its replicas of such chunks.</p>
</li>
</ul>
</li>
</ol>
<h4 id="what-are-the-advantages-and-disadvantages-of-lazy-deletion-over-eager-deletion"><a class="markdownIt-Anchor" href="#what-are-the-advantages-and-disadvantages-of-lazy-deletion-over-eager-deletion"></a> What are the advantages and disadvantages of lazy deletion over eager deletion?</h4>
<ol>
<li>
<p>It is simple and reliable in a large-scale distributed system where component failures are common.</p>
</li>
<li>
<p>It merges storage reclamation into the regular background activities of the master.</p>
</li>
<li>
<p>The delay in reclaiming storage provides a safety net against accidental, irreversible deletion.</p>
</li>
<li>
<p>The delay sometimes hinders user effort to fine tune usage when storage is tight.</p>
</li>
<li>
<p>Applications that repeatedly create and delete temporary files may not be able to reuse the storage right away.</p>
</li>
</ol>
<h4 id="how-to-address-the-issues-of-reusing"><a class="markdownIt-Anchor" href="#how-to-address-the-issues-of-reusing"></a> How to address the issues of reusing?</h4>
<ol>
<li>
<p>Expediting storage reclamation if a deleted file is explicitly deleted again.</p>
</li>
<li>
<p>Allow users to apply different replication and reclamation policies to different parts of the namespace.</p>
</li>
</ol>
<h4 id="how-to-handle-the-possible-stale-replicas"><a class="markdownIt-Anchor" href="#how-to-handle-the-possible-stale-replicas"></a> How to handle the possible stale replicas?</h4>
<ol>
<li>
<p>For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas.</p>
</li>
<li>
<p>Whenever the master grants a new lease on a chunk, it increases the chunk version number and informs the up-to-date replicas. This occurs before any client is notified and therefore before it can start writing to the chunk.</p>
</li>
<li>
<p>If one replica is currently unavailable, its chunk version number will not be advanced. The master will detect that this chunkserver has a stale replica when the chunkserver restarts and reports its set of chunks and their associated version numbers.</p>
</li>
<li>
<p>The master removes stale replicas in its regular garbage collection. Before that, it effectively considers a stale replica not to exist at all when it replies to client requests for chunk information.</p>
</li>
<li>
<p>The master includes the chunk version number when it informs clients which chunkserver holds a lease on a chunk or when it instructs a chunkserver to read the chunk from another chunkserver in a cloning operation.</p>
</li>
</ol>
<h2 id="fault-tolerance"><a class="markdownIt-Anchor" href="#fault-tolerance"></a> Fault tolerance</h2>
<h3 id="how-to-handle-master-failure"><a class="markdownIt-Anchor" href="#how-to-handle-master-failure"></a> How to handle master failure?</h3>
<ol>
<li>
<p>The master state is replicated for reliability.</p>
<ul>
<li>
<p>When it fails, it can restart almost instantly.</p>
</li>
<li>
<p>When its machine or disk fails, monitoring infrastructure outside GFS starts a new master process elsewhere with the replicated operation log.</p>
</li>
<li>
<p>Clients use only the canonical name of the master, which is a DNS alias that can be changed if the master is relocated to another machine.</p>
</li>
</ul>
</li>
<li>
<p>“Shadow” masters provide read-only access to the file system even when the primary master is down.</p>
<ul>
<li>
<p>They enhance read availability for files that are not being actively mutated or applications that do not mind getting slightly stale results.</p>
</li>
<li>
<p>Since file content is read from chunkservers, applications do not observe stale file content. What could be stale within short windows is file metadata.</p>
</li>
<li>
<p>To keep itself informed, a shadow master reads a replica of the growing operation log and applies the same sequence of changes to its data structures exactly as the primary does.</p>
</li>
<li>
<p>It depends on the primary master only for replica location updates resulting from the primary’s decisions to create and delete replicas.</p>
</li>
</ul>
</li>
</ol>
<h3 id="why-cannot-recover-data-using-other-chunk-replicas-why-each-chunkserver-must-independently-verify-the-integrity"><a class="markdownIt-Anchor" href="#why-cannot-recover-data-using-other-chunk-replicas-why-each-chunkserver-must-independently-verify-the-integrity"></a> Why cannot recover data using other chunk replicas? Why each chunkserver must independently verify the integrity?</h3>
<ol>
<li>
<p>It would be impractical to detect corruption by comparing replicas across chunkservers.</p>
</li>
<li>
<p>Divergent replicas may be legal: the semantics of GFS mutations, in particular atomic record append, does not guarantee identical replicas.</p>
</li>
</ol>
<h3 id="how-to-ensure-data-integrity"><a class="markdownIt-Anchor" href="#how-to-ensure-data-integrity"></a> How to ensure data integrity?</h3>
<ol>
<li>
<p>Each chunkserver uses checksumming to detect corruption of stored data. A chunk is broken up into 64 KB blocks. Each has a corresponding 32 bit checksum.</p>
</li>
<li>
<p>Checksums are kept in memory and stored persistently with logging, separate from user data.</p>
</li>
<li>
<p>During idle periods, chunkservers can scan and verify the contents of inactive chunks.</p>
</li>
</ol>
<h3 id="how-to-read-data-with-checksum"><a class="markdownIt-Anchor" href="#how-to-read-data-with-checksum"></a> How to read data with checksum?</h3>
<ol>
<li>
<p>the chunkserver verifies the checksum of data blocks that overlap the read range before returning any data to the requester, whether a client or another chunkserver.</p>
</li>
<li>
<p>If a block does not match the recorded checksum, the chunkserver returns an error to the requestor and reports the mismatch to the master.</p>
</li>
<li>
<p>In response, the requestor will read from other replicas, while the master will clone the chunk from another replica.</p>
</li>
<li>
<p>After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica.</p>
</li>
</ol>
<h3 id="how-to-write-data-with-checksum"><a class="markdownIt-Anchor" href="#how-to-write-data-with-checksum"></a> How to write data with checksum?</h3>
<ol>
<li>
<p>For writes that append to the end of a chunk, we just incrementally update the checksum for the last partial checksum block, and compute new checksums for any brand new checksum blocks filled by the append.</p>
<ul>
<li>Even if the last partial checksum block is already corrupted and we fail to detect it now, the new checksum value will not match the stored data, and the corruption will be detected as usual when the block is next read.</li>
</ul>
</li>
<li>
<p>If a write overwrites an existing range of the chunk, we must read and verify the first and last blocks of the range being overwritten, then perform the write, and finally compute and record the new checksums.</p>
<ul>
<li>If we do not verify the first and last blocks before overwriting them partially, the new checksums may hide corruption that exists in the regions not being overwritten.</li>
</ul>
</li>
</ol>
<h3 id="what-is-included-in-the-diagnostic-logs"><a class="markdownIt-Anchor" href="#what-is-included-in-the-diagnostic-logs"></a> What is included in the diagnostic logs?</h3>
<ol>
<li>
<p>GFS servers generate diagnostic logs that record many significant events (such as chunkservers going up and down) and all RPC requests and replies.</p>
</li>
<li>
<p>The RPC logs include the exact requests and responses sent on the wire, except for the file data being read or written.</p>
</li>
</ol>
<h2 id="other-parts-unmentioned"><a class="markdownIt-Anchor" href="#other-parts-unmentioned"></a> Other parts (unmentioned)</h2>
<h3 id="to-sum-up-what-is-the-metadata-of-master-and-where-are-they"><a class="markdownIt-Anchor" href="#to-sum-up-what-is-the-metadata-of-master-and-where-are-they"></a> To sum up, what is the metadata of master, and where are they?</h3>
<ol>
<li>
<p>File name: this is an array of chunk handles. It is stored on disk.</p>
</li>
<li>
<p>Handle: it contains a list of chunkservers, version number, primary, and lease expiration.</p>
<ul>
<li>
<p>Only the version number is stored on disk, due to the rest can be restored by asking chunkservers when master is recovered.</p>
</li>
<li>
<p>Given that there might have stale chunks, we cannot ask chunkservers for the version number of a chunk.</p>
</li>
</ul>
</li>
<li>
<p>Lops and checkpoints are stored on disk.</p>
</li>
</ol>
<h3 id="what-is-the-cause-of-split-brain-how-to-solve-it"><a class="markdownIt-Anchor" href="#what-is-the-cause-of-split-brain-how-to-solve-it"></a> What is the cause of split brain? How to solve it?</h3>
<ol>
<li>Split brain is caused by network partition, the master cannot talk to primary while the primary can talk to clients. Hence the master mistakingly designates two primary for the same chunk.</li>
<li>The master knowswhen the lease will expire, so when the master cannot talk to the primary, it will wait until the lease expired before assign another primary.</li>
</ol>
<h3 id="why-gfs-doesnt-overwrite-those-failed-records-immediately-but-leaving-padding-and-duplicates"><a class="markdownIt-Anchor" href="#why-gfs-doesnt-overwrite-those-failed-records-immediately-but-leaving-padding-and-duplicates"></a> Why GFS doesn’t overwrite those failed records immediately, but leaving padding and duplicates?</h3>
<p>Because When it starts to write the next record, it may not know the fate of prior record.</p>
<h3 id="gfs-is-a-weak-consistency-system-how-can-we-upgrade-it-to-a-strong-consistency-system"><a class="markdownIt-Anchor" href="#gfs-is-a-weak-consistency-system-how-can-we-upgrade-it-to-a-strong-consistency-system"></a> GFS is a weak consistency system, how can we upgrade it to a strong consistency system?</h3>
<ol>
<li>Primary detects duplicate requests to ensure the failed write doesn’t show up twice</li>
<li>When primary asks a secondary to do something, the secondary actually does it and doesn’t just return error (except the secondary has a permanent damage, in which case, it should be removed)</li>
<li>The secondary doesn’t expose data to readers until the primary is sure that all the secondaries really will be execute the append.</li>
<li>When primary crashes, there will have been some last set of operations that primary had launched to the secondaries, but primary crashed before ensure all operations are done. The new primary need to explicitly resync with all secondaries.</li>
</ol>
<h1 id="experiments-and-results"><a class="markdownIt-Anchor" href="#experiments-and-results"></a> Experiments and results</h1>
<h2 id="micro-benchmarks"><a class="markdownIt-Anchor" href="#micro-benchmarks"></a> Micro-benchmarks</h2>
<p>The author first tested several micro-benchmarks, i.e. reads, writes, and record appends. These tests are that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> clients do those operation simultaneously. For reading, they read from the file system; for writing, they write to distinct files; for appending, they append to a single file. And test the aggregate throughputs of the system, comparing them with the network limit. The results are as following:</p>
<p><img src="/imgs/Distributed/GFS/03.png" alt="" /></p>
<h3 id="read"><a class="markdownIt-Anchor" href="#read"></a> Read</h3>
<p>The reading efficiency drops because as the number of readers increases, so does the probability that multiple readers simultaneously read from the same chunkserver.</p>
<h3 id="write"><a class="markdownIt-Anchor" href="#write"></a> Write</h3>
<p>The limit plateaus of write rate at 67 MB/s because we need to write each byte to 3 of the 16 chunkservers, each with a 12.5 MB/s input connection.</p>
<p>The main culprit for a low write rate with only one client is the network stack. It does not interact very well with the pipelining scheme we use for pushing data to chunk replicas. Delays in propagating data from one replica to another reduce the overall write rate.</p>
<p>As the number of clients grows, it becomes more likely that multiple clients write concurrently to the same chunkserver as the number of clients increases. Moreover, collision is more likely for 16 writers than for 16 readers because each write involves three different replicas.</p>
<p>Writes are slower than we would like. In practice this has not been a major problem because even though it increases the latencies as seen by individual clients, it does not significantly affect the aggregate write bandwidth delivered by the system to a large number of clients.</p>
<h3 id="record-append"><a class="markdownIt-Anchor" href="#record-append"></a> Record append</h3>
<p>The performance of record appends is limited by the network bandwidth of the chunkservers that store the last chunk of the file, independent of the number of clients.</p>
<p>The append rate drops mostly due to congestion and variances in network transfer rates seen by different clients.</p>
<p>The chunkserver network congestion in our experiment is not a significant issue in practice because a client can make progress on writing one file while the chunkservers for another file are busy.</p>
<h2 id="real-world-clusters"><a class="markdownIt-Anchor" href="#real-world-clusters"></a> Real world clusters</h2>
<p>The author also measured the performance of real world clusters. First, the author measured their storage usage and size of metadata. Then, the read rate, write rate and the rate of operations sent to the master were measured. The results show that master can easily keep up with this rate, and therefore is not a bottleneck for these workloads.</p>
<p><img src="/imgs/Distributed/GFS/04.png" style="zoom: 33%;" /><img src="/imgs/Distributed/GFS/05.png" style="zoom: 31%;" /></p>
<p>After a chunkserver fails, some chunks will become underreplicated and must be cloned to restore their replication levels. To test the recovery time, the author killed a single chunkserver containing 15000 chunks of 600 GB data.</p>
<p>To limit the impact on running applications and provide leeway for scheduling decisions, our default parameters limit this cluster to 91 concurrent clonings (40% of the number of chunkservers) where each clone operation is allowed to consume at most 6.25 MB/s (50 Mbps). All chunks were restored in 23.2 minutes, at an effective replication rate of 440 MB/s.</p>
<p>Finally, the author also measured the workload of chunkserver and master, and breakdown the workload of chunkserver by size and same of master by type. The table 4 shows the distribution of operations by size, and the table 5 shows the total amount of data transferred in operations of various size.</p>
<p><img src="/imgs/Distributed/GFS/06.png" style="zoom:25%;" /><img src="/imgs/Distributed/GFS/07.png" style="zoom:25%;" /><img src="/imgs/Distributed/GFS/08.png" style="zoom: 31%;" /></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/MapReduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/MapReduce/" class="post-title-link" itemprop="url">MapReduce</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:00:23" itemprop="dateCreated datePublished" datetime="2023-09-26T13:00:23+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-25 20:06:38" itemprop="dateModified" datetime="2024-02-25T20:06:38+08:00">2024-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/mapreduce.pdf">MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p><ul class="markdownIt-TOC">
<li><a href="#abstract">Abstract</a></li>
<li><a href="#purpose">Purpose</a></li>
<li><a href="#model">Model</a>
<ul>
<li><a href="#paper-ideas">Paper ideas</a>
<ul>
<li><a href="#what-does-users-need-to-do">What does users need to do?</a></li>
<li><a href="#what-does-run-time-system-need-to-do">What does run-time system need to do?</a></li>
<li><a href="#how-does-the-system-run">How does the system run?</a></li>
<li><a href="#what-does-master-need-to-do">What does master need to do?</a></li>
<li><a href="#how-to-handle-worker-failure">How to handle worker failure?</a></li>
<li><a href="#how-to-handle-master-failure">How to handle master failure?</a></li>
<li><a href="#how-to-partition-reduce-tasks">How to partition reduce tasks?</a></li>
<li><a href="#how-do-we-ensure-that-nobody-observes-partially-written-files-in-the-presence-of-crashes">How do we ensure that nobody observes partially written files in the presence of crashes?</a></li>
<li><a href="#how-to-handle-straggler-problem">How to handle straggler problem?</a></li>
</ul>
</li>
<li><a href="#reproduce">Reproduce</a>
<ul>
<li><a href="#how-do-we-assign-map-tasks">How do we assign map tasks?</a></li>
<li><a href="#how-do-we-assign-reduce-tasks">How do we assign reduce tasks?</a></li>
<li><a href="#when-can-workers-stop-requesting-for-more-map-tasksreduce-tasks">When can workers stop requesting for more map tasks/reduce tasks?</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#experiments-and-results">Experiments and results</a></li>
</ul>
</p>
<h1 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h1>
<ol>
<li><strong>Main idea</strong>: Asking user to write simple Map function and Reduce function. The distributed executing framework supporting parallel scheduling is provided as a standard library that will run those functions automatically.</li>
<li><strong>Key findings</strong>: Map and Reduce are easier to write and depends on the specific application. Despite their different implementations, their distribution and parallelism are similar and can be handled by a common framework.</li>
<li><strong>The system</strong>: A master will assign and monitor each tasks and nodes while several workers will perform Map phase and Reduce phase sequentially.</li>
<li><strong>Evaluation</strong>: The system is evaluated on grep and sort tasks with benchmarks of disk I/O speed and duration of each phase.</li>
</ol>
<h1 id="purpose"><a class="markdownIt-Anchor" href="#purpose"></a> Purpose</h1>
<ol>
<li>Issues: Hard to parallel computation, distribute data, and handle server failures</li>
<li>Contribution: Proposed an interface where users only need to write relatively simple Map function and Reduce function, and the system will parallel and distribute automatically.</li>
</ol>
<h1 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h1>
<h2 id="paper-ideas"><a class="markdownIt-Anchor" href="#paper-ideas"></a> Paper ideas</h2>
<h3 id="what-does-users-need-to-do"><a class="markdownIt-Anchor" href="#what-does-users-need-to-do"></a> What does users need to do?</h3>
<ol>
<li>Users need to provide a Map function and a Reduce function.</li>
<li>Map function will read the original data as key-value pairs, take one pair as input each time, and output intermediate key-value pairs, i.e. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>a</mi><mi>p</mi><mo stretchy="false">(</mo><mi>k</mi><mn>1</mn><mo separator="true">,</mo><mi>v</mi><mn>1</mn><mo stretchy="false">)</mo><mo>→</mo><mi>l</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>k</mi><mn>2</mn><mo separator="true">,</mo><mi>v</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">map(k1,v1)\rightarrow list(k2,v2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">2</span><span class="mclose">)</span></span></span></span></li>
<li>Reduce function will take the intermediate-key and a list of all intermediate-values for that key as input, and merge these values to form a smaller set of values, i.e. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>c</mi><mi>e</mi><mo stretchy="false">(</mo><mi>k</mi><mn>2</mn><mo separator="true">,</mo><mi>l</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>v</mi><mn>2</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>→</mo><mi>l</mi><mi>i</mi><mi>s</mi><mi>t</mi><mo stretchy="false">(</mo><mi>v</mi><mn>2</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">reduce(k2,list(v2))\rightarrow list(v2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">2</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord">2</span><span class="mclose">)</span></span></span></span></li>
<li>Users need to implement the Mapper and Reducer as interface provided by the system, and pass to the MapReduce specification. After passing the input and output files, invoke the <code>MapReduce</code> function to execute.</li>
</ol>
<h3 id="what-does-run-time-system-need-to-do"><a class="markdownIt-Anchor" href="#what-does-run-time-system-need-to-do"></a> What does run-time system need to do?</h3>
<ol>
<li>Partition data</li>
<li>Schedule across a set of machines</li>
<li>Handle machine failures</li>
<li>Manage inter-machine communication.</li>
</ol>
<h3 id="how-does-the-system-run"><a class="markdownIt-Anchor" href="#how-does-the-system-run"></a> How does the system run?</h3>
<ol>
<li>When the <code>MapReduce</code> function is invoked, one <strong>master</strong> process and several <strong>worker</strong> processes will be forked.</li>
<li>Master will assign work to workers, either a Map work, or a Reduce work.</li>
<li>Master tries to make most of the <code>(3) read</code> run locally. In the <code>(5) remote read</code>, network communication is inevitable.<br />
<img src="/imgs/Distributed/MapReduce/01.png" style="zoom: 50%;" /></li>
</ol>
<h3 id="what-does-master-need-to-do"><a class="markdownIt-Anchor" href="#what-does-master-need-to-do"></a> What does master need to do?</h3>
<ol>
<li>Master pings every worker periodically, and marks those no response in a certain amount of time as failed.</li>
<li>Track the state of each map task and reduce task (idle, in-progress or completed), and the identity of the worker machine (for non-idle tasks).</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> is the number of map tasks, while <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span> is the number of reduce tasks. The master must make <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>M</mi><mo>+</mo><mi>R</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(M+R)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mclose">)</span></span></span></span> scheduling decisions, and keeps <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>M</mi><mo>∗</mo><mi>R</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(M*R)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mclose">)</span></span></span></span> state in memory (all map task/reduce task pair).</li>
</ol>
<h3 id="how-to-handle-worker-failure"><a class="markdownIt-Anchor" href="#how-to-handle-worker-failure"></a> How to handle worker failure?</h3>
<ol>
<li>What kinds of worker failure need re-execution?
<ul>
<li>Any tasks in progress</li>
<li>Completed map tasks also need to be re-executed, since their output is stored on the local disks and is inaccessible.</li>
<li>Completed reduce tasks don’t need to be re-executed, since their output is stored on the global file system.</li>
</ul>
</li>
<li>Master will mark the state of those tasks that need re-execution to idle, and can assign them to other workers in the future.</li>
</ol>
<h3 id="how-to-handle-master-failure"><a class="markdownIt-Anchor" href="#how-to-handle-master-failure"></a> How to handle master failure?</h3>
<ol>
<li>One way is to make the master write periodic checkpoints of the master data structure.</li>
<li>Given that there is only a single master, its failure is unlikely. Therefore another way is to abort the MapReduce computation if the master fails, and clients can try again later. (This is the way the author takes)</li>
</ol>
<h3 id="how-to-partition-reduce-tasks"><a class="markdownIt-Anchor" href="#how-to-partition-reduce-tasks"></a> How to partition reduce tasks?</h3>
<p>The number of reduce tasks/output files (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>) is specified by the users. The default partitioning uses hashing, namely partition according to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>a</mi><mi>s</mi><mi>h</mi><mo stretchy="false">(</mo><mi>k</mi><mi>e</mi><mi>y</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>m</mi><mi>o</mi><mi>d</mi><mtext> </mtext><mi>R</mi></mrow><annotation encoding="application/x-tex">hash(key)\ mod\ R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mspace"> </span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span></span></span></span>.</p>
<h3 id="how-do-we-ensure-that-nobody-observes-partially-written-files-in-the-presence-of-crashes"><a class="markdownIt-Anchor" href="#how-do-we-ensure-that-nobody-observes-partially-written-files-in-the-presence-of-crashes"></a> How do we ensure that nobody observes partially written files in the presence of crashes?</h3>
<p>Each worker first write their results to a temporary file. Then rename it once all writtens are completed.</p>
<h3 id="how-to-handle-straggler-problem"><a class="markdownIt-Anchor" href="#how-to-handle-straggler-problem"></a> How to handle straggler problem?</h3>
<ol>
<li>Straggler: a machine that takes an unusually long time to complete on of the last few map or reduce tasks. This may be caused by a bad disk, its scheduling system scheduling it a different other tasks.</li>
<li>So when a MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks.</li>
</ol>
<h2 id="reproduce"><a class="markdownIt-Anchor" href="#reproduce"></a> Reproduce</h2>
<p>This reproduce part is based on the Lab 1 of MIT 6.824.</p>
<h3 id="how-do-we-assign-map-tasks"><a class="markdownIt-Anchor" href="#how-do-we-assign-map-tasks"></a> How do we assign map tasks?</h3>
<p>Each worker will request for more map tasks when it becomes idle, and the master will assign files directly to them.</p>
<h3 id="how-do-we-assign-reduce-tasks"><a class="markdownIt-Anchor" href="#how-do-we-assign-reduce-tasks"></a> How do we assign reduce tasks?</h3>
<ol>
<li>When worker is notified that there is no more map tasks, they will begin to request for reduce tasks. This time, the master won’t assign files directly, instead, master will only assign a number in the range from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">R-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>. Then each worker will try to read intermediate files from each workers according to its number automatically.</li>
<li>This requires those map workers store their output in a previously agreed file name for reduce workers to request.</li>
</ol>
<h3 id="when-can-workers-stop-requesting-for-more-map-tasksreduce-tasks"><a class="markdownIt-Anchor" href="#when-can-workers-stop-requesting-for-more-map-tasksreduce-tasks"></a> When can workers stop requesting for more map tasks/reduce tasks?</h3>
<ol>
<li>Only after all map tasks is completed, workers can stop requesting for more map tasks and begin to request for reduce tasks, since reduce tasks may depend on those unfinished map tasks. So the lifecycle of workers can be partitioned into two phase, the map phase and the reduce phase.</li>
<li>Also, only after all reduce tasks is completed, workers can stop requesting for more reduce tasks and quit the program. This is because those executing, yet uncompleted, tasks may fail, and when that happens, we need other workers to re-execute those tasks.</li>
<li>Similarly, reduce workers cannot delete those intermediate files right after they read them. Because if they fail, their successor need to read those files.</li>
</ol>
<h1 id="experiments-and-results"><a class="markdownIt-Anchor" href="#experiments-and-results"></a> Experiments and results</h1>
<ol>
<li><strong>What to notice when configurate cluster?</strong>
<ul>
<li>Need to reserve some memory for other tasks running on the cluster. The author reserved <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mn>1.5</mn></mrow><annotation encoding="application/x-tex">1-1.5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">5</span></span></span></span> GB out of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span></span></span></span> GB.</li>
<li>Best test when the CPUs, disks, and network were mostly idle.</li>
</ul>
</li>
<li>The author tested two representative situations, grep and sort.</li>
<li>In the grep test, the execution time includes a minute of startup overhead over <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>150</mn></mrow><annotation encoding="application/x-tex">150</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">5</span><span class="mord">0</span></span></span></span> seconds of total time. The overhead is due to the propagation of the program to all worker machines, and delays interacting with GFS to open the set of input files and to get the information needed for the locality optimization.<br />
<img src="/imgs/Distributed/MapReduce/02.png" style="zoom: 33%;" /></li>
<li>In the sort test,
<ul>
<li>It only consists of less than <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn></mrow><annotation encoding="application/x-tex">50</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">0</span></span></span></span> lines of user code</li>
<li>The entire computation time including startup overhead is similar to the best reported result at that time.</li>
<li>The author tested three kind of rates, the rate of reading by map workers (<em>input rate</em>), the rate of communicating intermediate files between map workers and reduce workers (<em>shuffle rate</em>), and the rate of writing output files by reduce workers (<em>output rate</em>). These are the I/O parts which affect the perfornmence significantly.
<ul>
<li>The input rate is less than grep, because sort map tasks spend more time writing intermediate output to their local disks.</li>
<li>The input rate is higher than the shuffle rate and the output rate because of locality optimization.</li>
<li>The shuffle rate is higher than the output rate because the output phase writes replicas due to the mechanism for reliability  of the underlying file system.</li>
</ul>
</li>
<li>The author also tested when <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">0</span><span class="mord">0</span></span></span></span> out of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1746</mn></mrow><annotation encoding="application/x-tex">1746</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">7</span><span class="mord">4</span><span class="mord">6</span></span></span></span> workers are killed several minutes. The underlying cluster scheduler immediately restarted new worker processes on these mechines (only the processes were killed, the machines were still functioning properly). The entire computation time increases of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.80556em;vertical-align:-0.05556em;"></span><span class="mord">5</span><span class="mord">%</span></span></span></span> over the normal execution time.<br />
<img src="/imgs/Distributed/MapReduce/03.png" style="zoom:50%;" /></li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/03/Courses/15445/17-Distributed-OLAP-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/03/Courses/15445/17-Distributed-OLAP-Databases/" class="post-title-link" itemprop="url">17 Distributed OLAP Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-03 12:05:46" itemprop="dateCreated datePublished" datetime="2023-09-03T12:05:46+08:00">2023-09-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:27" itemprop="dateModified" datetime="2023-09-26T14:01:27+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#how-should-we-divide-tables">How should we divide tables?</a></li>
<li><a href="#execution-models">Execution models</a>
<ul>
<li><a href="#how-to-execute-when-required-data-are-in-different-nodes">How to execute when required data are in different nodes?</a></li>
<li><a href="#how-to-handle-query-fault">How to handle query fault?</a></li>
<li><a href="#when-pushing-query-to-data-what-are-the-queries">When pushing query to data, what are the queries?</a></li>
</ul>
</li>
<li><a href="#distributed-join-algorithms">Distributed Join Algorithms</a>
<ul>
<li><a href="#how-to-perform-distributed-joins">How to perform distributed joins?</a></li>
<li><a href="#what-is-semi-join">What is semi-join?</a></li>
</ul>
</li>
<li><a href="#cloud-systems">Cloud systems</a>
<ul>
<li><a href="#what-is-the-difference-for-dbmss-running-in-cloud-systems">What is the difference for DBMSs running in cloud systems?</a></li>
<li><a href="#how-to-store-data-of-different-schema">How to store data of different schema?</a></li>
<li><a href="#how-to-share-data-between-systems">How to share data between systems?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="how-should-we-divide-tables"><a class="markdownIt-Anchor" href="#how-should-we-divide-tables"></a> How should we divide tables?</h1>
<ol>
<li>The first schema is star schema.
<ul>
<li>Star schemas contain two types of tables: fact tables and dimension tables.</li>
<li>The fact table contains multiple “events” that occur in the application. It will contain the minimal unique information per event.</li>
<li>The rest of the attributes will be foreign key references to outer dimension tables.</li>
<li>In a star schema, there can only be one dimension-level out from the fact table.</li>
</ul>
</li>
<li>The second schema is snowflake schema.
<ul>
<li>It allows for more than one dimension out from the fact table.</li>
</ul>
</li>
<li>Snowflake schemas take up less storage space. Denormalized data models may incur integrity and consistency violations.</li>
<li>Snowflake schemas require more joins to get the data needed for a query. Queries on star schemas will usually be faster.</li>
</ol>
<h1 id="execution-models"><a class="markdownIt-Anchor" href="#execution-models"></a> Execution models</h1>
<h2 id="how-to-execute-when-required-data-are-in-different-nodes"><a class="markdownIt-Anchor" href="#how-to-execute-when-required-data-are-in-different-nodes"></a> How to execute when required data are in different nodes?</h2>
<ol>
<li>The first approach is to push query to data.
<ul>
<li>Send the query (or a portion of it) to the node that contains the data.</li>
<li>The result is then sent back to where the query is being executed, which uses local data and the data sent to it, to complete the query.</li>
<li>Perform as much filtering and processing as possible where data resides before transmitting over network.</li>
<li>This is more common in a shared nothing system.</li>
</ul>
</li>
<li>The second approach is to pull data to query.
<ul>
<li>Bring the data to the node that is executing a query that needs it for processing.</li>
<li>This is normally what a shared disk system would do.</li>
<li>The problem with this is that the size of the data relative to the size of the query could be very different.</li>
</ul>
</li>
</ol>
<h2 id="how-to-handle-query-fault"><a class="markdownIt-Anchor" href="#how-to-handle-query-fault"></a> How to handle query fault?</h2>
<ol>
<li>The data that a node receives from remote sources are cached in the buffer pool.
<ul>
<li>When buffer pool run out of memory, the DBMS can write some pages out to disk in some ephemeral pages.</li>
<li>This allows the DBMS to support intermediate results that are large than the amount of memory available.</li>
<li>Ephemeral pages are not persisted after a restart.</li>
</ul>
</li>
<li>Most shared-nothing distributed OLAP DBMSs are designed to assume that nodes do not fail during query execution.
<ul>
<li>If one node fails during query execution, then the whole query fails.</li>
</ul>
</li>
<li>The DBMS could take a snapshot of the intermediate results for a query during execution to allow it to recover if nodes fail.
<ul>
<li>Most of the DMBSs do not want to pay the penalty of writing intermediate results to disk for quick queries.</li>
<li>This is adopted if the query takes a long time.</li>
</ul>
</li>
<li>In shared-disk distributed OLAP DMBSs, they can write results to the shared disk to prevent performing the same calculation again. However, the frequency of writing to disk is also a trade-off.</li>
</ol>
<h2 id="when-pushing-query-to-data-what-are-the-queries"><a class="markdownIt-Anchor" href="#when-pushing-query-to-data-what-are-the-queries"></a> When pushing query to data, what are the queries?</h2>
<ol>
<li>The DBMSs only need to push fragments of the query instead of the whole query.</li>
<li>The first approach is pushing physical operators.
<ul>
<li>Generate a single query plan and then break it up into partition-specific  fragments.</li>
<li>Most systems implement this approach.</li>
</ul>
</li>
<li>The second approach is pushing another SQL query.
<ul>
<li>The DBMS will rewrite original query into partition-specific queries.</li>
<li>This approach allows for local optimization at each node.</li>
</ul>
</li>
</ol>
<h1 id="distributed-join-algorithms"><a class="markdownIt-Anchor" href="#distributed-join-algorithms"></a> Distributed Join Algorithms</h1>
<h2 id="how-to-perform-distributed-joins"><a class="markdownIt-Anchor" href="#how-to-perform-distributed-joins"></a> How to perform distributed joins?</h2>
<ol>
<li>One approach is to put entire tables on a single node and then perform the join.
<ul>
<li>You lose the parallelism of a distributed DBMS.</li>
<li>It has expensive data transferring over the network.</li>
</ul>
</li>
<li>To join tables, the DBMS needs to get the proper tuples on the same node. Once the data is at the node, the DBMS then executes the single-node join algorithms.</li>
<li>The best scenario is that one table is replicated at every node.
<ul>
<li>Each node joins its local data in parallel and then sends their results to a coordinating node.</li>
</ul>
</li>
<li>The second scenario is that tables are partitioned on the join attribute.
<ul>
<li>Each node only needs to acquire tuples of the second table that will match the tuples of the left table that are already in it.</li>
<li>Each node performs the join on local data and then sends to a coordinator node for coalescing.</li>
</ul>
</li>
<li>The third scenario is that both tables are partitioned on different keys while one of the tables is small.
<ul>
<li>The DBMS will broadcast that table to all nodes.</li>
</ul>
</li>
<li>The worst scenario is that both tables are not partitioned on the join key.
<ul>
<li>The DBMS copies the tables by shuffling them across nodes.</li>
</ul>
</li>
</ol>
<h2 id="what-is-semi-join"><a class="markdownIt-Anchor" href="#what-is-semi-join"></a> What is semi-join?</h2>
<ol>
<li>If the result only contains columns sfrom the left table, the DBMSs use semi-join to minimize the amount of data sent during joins.</li>
<li>This is like a projection pushdown. The DBMS transmits only the necessary columns of the left table to other nodes.</li>
<li>For DBMSs that do not support <code>SEMI JOIN</code>, we can fake it with <code>EXISTS</code>.
<ul>
<li>Wrap the predicates of <code>WHERE</code>  clause with <code>EXISTS (SELECT 1 from S WHERE predicates)</code>.</li>
</ul>
</li>
</ol>
<h1 id="cloud-systems"><a class="markdownIt-Anchor" href="#cloud-systems"></a> Cloud systems</h1>
<h2 id="what-is-the-difference-for-dbmss-running-in-cloud-systems"><a class="markdownIt-Anchor" href="#what-is-the-difference-for-dbmss-running-in-cloud-systems"></a> What is the difference for DBMSs running in cloud systems?</h2>
<ol>
<li>The first approach is managed DBMS.
<ul>
<li>No significant modification to the DBMS to be aware that it is running in a cloud environment.</li>
</ul>
</li>
<li>The second approach is cloud-native DBMS.
<ul>
<li>The system is designed explicitly to run in a cloud environment.</li>
<li>Usually based on a shared-disk architecture.</li>
</ul>
</li>
<li>A “serverless” DBMS evicts tenants when they become idle, rather than always maintaining compute resources for each customer.</li>
</ol>
<h2 id="how-to-store-data-of-different-schema"><a class="markdownIt-Anchor" href="#how-to-store-data-of-different-schema"></a> How to store data of different schema?</h2>
<ol>
<li>A Data Lake is a centralized repository for storing large amounts of structured, semi-structured, and un- structured data without having to define a schema or ingest the data into proprietary internal formats.</li>
<li>Data lakes are usually faster at ingesting data, as they do not require transformation right away. Yet, when they are fetching data, they need to look up the catalog before transforming data into the desired schema.</li>
<li>They do require the user to write their own transformation piplines.</li>
</ol>
<h2 id="how-to-share-data-between-systems"><a class="markdownIt-Anchor" href="#how-to-share-data-between-systems"></a> How to share data between systems?</h2>
<ol>
<li>Most DBMSs use a proprietary on-disk binary file format for their databases.</li>
<li>The only way to share data between systems is to convert data into a common text-based format.</li>
<li>There are new open-source binary file formats that make it easier to access data across systems.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/02/Courses/15445/16-Distributed-OLTP-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/02/Courses/15445/16-Distributed-OLTP-Databases/" class="post-title-link" itemprop="url">16 Distributed OLTP Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-02 00:02:00" itemprop="dateCreated datePublished" datetime="2023-09-02T00:02:00+08:00">2023-09-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:19" itemprop="dateModified" datetime="2023-09-26T14:01:19+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#what-is-the-difference-between-oltp-and-olap">What is the difference between OLTP and OLAP?</a></li>
<li><a href="#2-phase-commit-2pc">2-Phase Commit (2PC)</a>
<ul>
<li><a href="#what-is-the-procedure-of-2pc">What is the procedure of 2PC?</a></li>
<li><a href="#what-would-happen-if-some-nodes-crash-during-2pc">What would happen if some nodes crash during 2PC?</a></li>
<li><a href="#how-to-recover-from-crash-during-2pc">How to recover from crash during 2PC?</a></li>
<li><a href="#how-can-we-optimize-2pc-to-reduce-communication">How can we optimize 2PC to reduce communication?</a></li>
<li><a href="#what-is-the-difference-betwen-2pc-and-paxos">What is the difference betwen 2PC and Paxos?</a></li>
</ul>
</li>
<li><a href="#replication">Replication</a>
<ul>
<li><a href="#how-can-we-execute-readwrite-with-replication">How can we execute read/write with replication?</a></li>
<li><a href="#when-should-notify-application-of-result">When should notify application of result?</a></li>
<li><a href="#when-should-propagate-updates-between-nodes">When should propagate updates between nodes?</a></li>
<li><a href="#what-should-be-sent-to-the-followers">What should be sent to the followers?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="what-is-the-difference-between-oltp-and-olap"><a class="markdownIt-Anchor" href="#what-is-the-difference-between-oltp-and-olap"></a> What is the difference between OLTP and OLAP?</h1>
<ol>
<li>OLTP is the front-end databases that communicate and interact with the outside world, e.g. applications. And OLAP is the back-end databases that used to analyze the data in those front-end databases.</li>
<li>OLTP often executes repetitive short-lived read/write txns, while OLAP executes long-running, read-only queries involving complex joins.</li>
<li>Before going into OLAP system, data in OLTP databases need an intermediate step called ETL, or Extract, Transform, and Load, which combines the OLTP databases into a universal schema for the data warehouse.</li>
</ol>
<h1 id="2-phase-commit-2pc"><a class="markdownIt-Anchor" href="#2-phase-commit-2pc"></a> 2-Phase Commit (2PC)</h1>
<h2 id="what-is-the-procedure-of-2pc"><a class="markdownIt-Anchor" href="#what-is-the-procedure-of-2pc"></a> What is the procedure of 2PC?</h2>
<ol>
<li>When application sends a commit request to the coordinator, the coordinator tells other nodes to go into the first phase, prepare phase.</li>
<li>When participants are ready, they will reply to the coordinator with acknowledgement.</li>
<li>If the coordinator receives acknoledgement from all participants, they can begin the second phase, commit phase.
<ul>
<li>And the coordinator will response to application with success after receiving all acknowledgement of commit phase from all participants.</li>
</ul>
</li>
<li>If the coordinator receives abort messages from any of the participants, the coordinator responses to application with abort message and begins the abort phase.
<ul>
<li>As usual, participants need to reply acknowledgement to coordinator in abort phase.</li>
</ul>
</li>
</ol>
<h2 id="what-would-happen-if-some-nodes-crash-during-2pc"><a class="markdownIt-Anchor" href="#what-would-happen-if-some-nodes-crash-during-2pc"></a> What would happen if some nodes crash during 2PC?</h2>
<ol>
<li>If the coordinator crashes, participants must decide what to do after a timeout, either abort or elect a new coordinator.
<ul>
<li>The system is not available during this timeout.</li>
</ul>
</li>
<li>If one of the participants crashes, coordinator assumes that it responded with an abort if it hasn’t sent an acknowledgement yet.
<ul>
<li>Nodes use a timeout to determine that participant is dead.</li>
</ul>
</li>
</ol>
<h2 id="how-to-recover-from-crash-during-2pc"><a class="markdownIt-Anchor" href="#how-to-recover-from-crash-during-2pc"></a> How to recover from crash during 2PC?</h2>
<ol>
<li>Each node records the inbound/outbound messages and outcome of each phase in a non-volatile storage log.</li>
<li>On recovery, examine the log for 2PC messages
<ul>
<li>If local txn in prepared state, contact coordinator.</li>
<li>If local txn not in prepared, abort it.</li>
<li>If local txn was committing and node is the coordinator, send <code>COMMIT</code> message to nodes.</li>
</ul>
</li>
</ol>
<h2 id="how-can-we-optimize-2pc-to-reduce-communication"><a class="markdownIt-Anchor" href="#how-can-we-optimize-2pc-to-reduce-communication"></a> How can we optimize 2PC to reduce communication?</h2>
<ol>
<li>The first method is early prepare voting.
<ul>
<li>If you send a query to a remote node that you know will be the last one you execute there, then that node will also return their vote for the prepare phase with the query result.</li>
<li>This is rare due to rarely write database application with the idea of last query.</li>
<li>However, this can be used in RPC sorts of things where we can sure that this process will terminate after executed certain query.</li>
</ul>
</li>
<li>The second method is early ACK after prepare.
<ul>
<li>If all nodes vote to commit a txn, the coordinator can send the client an acknowledgement that their txn was successful before the commit phase finishes, i.e. send acknowledgement to client after receiving all acknowledgement from participants.</li>
<li>This could cause a small windown of client receiving success, yet cannot see modifications from servers due to commit phase is not finished yet.</li>
</ul>
</li>
</ol>
<h2 id="what-is-the-difference-betwen-2pc-and-paxos"><a class="markdownIt-Anchor" href="#what-is-the-difference-betwen-2pc-and-paxos"></a> What is the difference betwen 2PC and Paxos?</h2>
<ol>
<li>2-phase commit is a degenerate case of Paxos.
<ul>
<li>Paxos uses <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>F</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2F + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> coordinators and makes progress as long as at least <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">F + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> of them are working properly.</li>
<li>2-phase commit sets <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">F = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>.</li>
</ul>
</li>
<li>2-phase commit blocks if coordinator fails after the prepare message is sent, until coordinator recovers.<br />
Paxos remains non-blocking if a majority participants are alive, provided there is a sufficiently long period without further failures.</li>
<li>2-phase commit requires all nodes to agree on the commit while Paxos only requires majority agreement.</li>
<li>In 2-phase commit, each node may have different data, executing different commands. However, in Paxos, all nodes are only replications.</li>
</ol>
<h1 id="replication"><a class="markdownIt-Anchor" href="#replication"></a> Replication</h1>
<h2 id="how-can-we-execute-readwrite-with-replication"><a class="markdownIt-Anchor" href="#how-can-we-execute-readwrite-with-replication"></a> How can we execute read/write with replication?</h2>
<ol>
<li>The first approach is primary-replica.
<ul>
<li>All updates go to a designated primary for each object. The primary propagates updates to its replicas without an atomic commit protocol.</li>
<li>Read-only txns may be allowed to access replicas.</li>
<li>If the primary goes down, then hold an election to select a new primary.</li>
</ul>
</li>
<li>The second approach is multi-primary.
<ul>
<li>Txns can update data objects at any replica.</li>
<li>Replicas must synchronize with each other using an atomic commit protocol.</li>
</ul>
</li>
</ol>
<h2 id="when-should-notify-application-of-result"><a class="markdownIt-Anchor" href="#when-should-notify-application-of-result"></a> When should notify application of result?</h2>
<ol>
<li>When a txn commits on a replicated database, the DBMS decides whether it must wait for that txn’s changes to propagate to other nodes before it can send the acknowledgement to application.</li>
<li>The first propagation level is synchronous, which leads to strong consistency.
<ul>
<li>The primary sends updates to replicas and then waits for them to acknowledge that they fully applied (i.e., logged) the changes before sending acknowledgement to application.</li>
</ul>
</li>
<li>The second propagation level is asynchronous, which leads to eventual consistency.
<ul>
<li>The primary immediately returns the acknowledgement to the client without waiting for replicas to apply the changes.</li>
</ul>
</li>
</ol>
<h2 id="when-should-propagate-updates-between-nodes"><a class="markdownIt-Anchor" href="#when-should-propagate-updates-between-nodes"></a> When should propagate updates between nodes?</h2>
<ol>
<li>The first approach is continuous.
<ul>
<li>The DBMS sends log messages immediately as it generates them.</li>
<li>It also needs to send a commit/abort message.</li>
</ul>
</li>
<li>The second approach is on commit.
<ul>
<li>The DBMS only sends the log messages for a txn to the replicas once the txn is commits.</li>
<li>Do not waste time sending log records for aborted txns.</li>
<li>It assumes that a txn’s log records fits entirely in memory.</li>
</ul>
</li>
</ol>
<h2 id="what-should-be-sent-to-the-followers"><a class="markdownIt-Anchor" href="#what-should-be-sent-to-the-followers"></a> What should be sent to the followers?</h2>
<ol>
<li>The first choice is active-active.
<ul>
<li>A txn executes at each replica independently.</li>
<li>Need to check at the end whether the txn ends up with the same result at each replica.</li>
</ul>
</li>
<li>The second choice is active-passive.
<ul>
<li>Each txn executes at a single location and propagates the changes to the replica.</li>
<li>Can either do physical or logical replication.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/31/Courses/15445/15-Introduction-to-Distributed-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/31/Courses/15445/15-Introduction-to-Distributed-Databases/" class="post-title-link" itemprop="url">15 Introduction to Distributed Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-31 23:53:22" itemprop="dateCreated datePublished" datetime="2023-08-31T23:53:22+08:00">2023-08-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:14" itemprop="dateModified" datetime="2023-09-26T14:01:14+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#system-architecture">System architecture</a>
<ul>
<li><a href="#what-will-system-architecture-affect">What will system architecture affect?</a></li>
<li><a href="#what-is-shared-memory-architecture">What is shared memory architecture?</a></li>
<li><a href="#what-is-shared-disk-architecture">What is shared disk architecture?</a></li>
<li><a href="#what-is-shared-nothing-architecture">What is shared nothing architecture?</a></li>
</ul>
</li>
<li><a href="#design-issues">Design issues</a>
<ul>
<li><a href="#what-are-the-design-issues-of-distributed-database">What are the design issues of distributed database?</a></li>
<li><a href="#what-do-nodes-look-like">What do nodes look like?</a></li>
<li><a href="#how-to-coordinate-execution">How to coordinate execution?</a></li>
</ul>
</li>
<li><a href="#partitioning-schemes">Partitioning Schemes</a>
<ul>
<li><a href="#what-do-we-desire-when-partitioning-database">What do we desire when partitioning database?</a></li>
<li><a href="#how-can-we-partition-database">How can we partition database?</a></li>
<li><a href="#how-can-we-optimize-horizontal-partitioning">How can we optimize horizontal partitioning?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="system-architecture"><a class="markdownIt-Anchor" href="#system-architecture"></a> System architecture</h1>
<h2 id="what-will-system-architecture-affect"><a class="markdownIt-Anchor" href="#what-will-system-architecture-affect"></a> What will system architecture affect?</h2>
<ol>
<li>A distributed DBMS’s system architecture specifies what shared resources are directly accessible to CPUs.</li>
<li>This affects how CPUs coordinate with each other and where they retrieve/store objects in the database.</li>
<li>There are four architectures: shared everything, shared memory, shared disk and shared nothing.</li>
<li>In shared everything architecture, CPU, memories and disks are all in local. This is more of a parallel architecture than a distributed architecture.</li>
</ol>
<h2 id="what-is-shared-memory-architecture"><a class="markdownIt-Anchor" href="#what-is-shared-memory-architecture"></a> What is shared memory architecture?</h2>
<ol>
<li>CPUs have access to common memory address space via a fast interconnect.</li>
<li>Each processor has a global view of all the in-memory data structures.
<ul>
<li>Each process’s scope of memory is the same memory address space, which can be modified by multiple processes.</li>
</ul>
</li>
<li>Each DBMS instance on a processor must “know” about the other instances.</li>
<li>In practice, most DBMSs do not use this architecture, as it is provided at the OS / kernel level.</li>
</ol>
<img src="/imgs/15445/Distributed/shared_mem.png" width="30%">
<h2 id="what-is-shared-disk-architecture"><a class="markdownIt-Anchor" href="#what-is-shared-disk-architecture"></a> What is shared disk architecture?</h2>
<ol>
<li>All CPUs can access a single logical disk directly via an interconnect, but each have their own private memories.</li>
<li>It can scale execution layer independently from the storage layer.
<ul>
<li>The advantage of shared disk over shared nothing is that it can easily scale up with compute layer and storage layer independent.</li>
<li>What we want to persist after crash is in the storage layer.</li>
<li>Theoretically, we can kill or add front-end nodes without losing database or add storage disks / change storage layer with modfying compute nodes.</li>
</ul>
</li>
<li>It must send messages between CPUs to learn about their current state.</li>
<li>This architecture is commonly used in OLAP systems. Many DBSMs begin to think that shared disk architecture is better than shared nothing.</li>
</ol>
<img src="/imgs/15445/Distributed/shared_disk.png" width="30%">
<h2 id="what-is-shared-nothing-architecture"><a class="markdownIt-Anchor" href="#what-is-shared-nothing-architecture"></a> What is shared nothing architecture?</h2>
<ol>
<li>Each DBMS instance has its own CPU, memory, and local disk.</li>
<li>Nodes only communicate with each other via network.
<ul>
<li>When executing a query that requires data from different nodes, the DBMS can either send data up to the node connected with application, or that node can ask another node to execute the query and return the result.</li>
</ul>
</li>
<li>All data in the database are sharded into different nodes.
<ul>
<li>When adding a new node into the architecture, that node is initially empty. The DBMS must re-shard data so that they are distributed evenly.</li>
<li>It is more difficult to increase capacity because the DBMS has to physically move data to new nodes.</li>
<li>It might have small window for queries to receive false positive due to that part of data was in that node and now moving to another node.</li>
</ul>
</li>
<li>It is also difficult to ensure consistency across all nodes in the DBMS, since the nodes must coordinate with each other on the state of transactions.</li>
<li>However, it can potentially achieve better performance and are more efficient then other types of distributed DBMS architectures.</li>
</ol>
<img src="/imgs/15445/Distributed/shared_nothing.png" width="30%">
<h1 id="design-issues"><a class="markdownIt-Anchor" href="#design-issues"></a> Design issues</h1>
<h2 id="what-are-the-design-issues-of-distributed-database"><a class="markdownIt-Anchor" href="#what-are-the-design-issues-of-distributed-database"></a> What are the design issues of distributed database?</h2>
<ol>
<li>How does the application find data?</li>
<li>Where does the application send queries?</li>
<li>How to execute queries on distributed data? Push query to data? Or pull data to query?</li>
<li>How does the DBMS ensure correctness?</li>
<li>How do we divide the database across resources?</li>
</ol>
<h2 id="what-do-nodes-look-like"><a class="markdownIt-Anchor" href="#what-do-nodes-look-like"></a> What do nodes look like?</h2>
<ol>
<li>The first approach is homogenous nodes.
<ul>
<li>Every node in the cluster can perform the same set of tasks albeit on potentially different partitions of data.</li>
<li>Makes provisioning and failover “easier” since any node can replace other nodes.</li>
</ul>
</li>
<li>The second approach is heterogenous nodes.
<ul>
<li>Nodes are assigned specific tasks.</li>
<li>It can allow a single physical node to host multiple “virtual” node types for dedicated tasks.</li>
<li>A design of heterogenous nodes have two kinds of nodes router and config server.
<ul>
<li>Router can directly access shards of database yet it does not know where are that data they want.</li>
<li>Config server knows the data contained in each shards. However, it does not responsible for retrieving them.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="how-to-coordinate-execution"><a class="markdownIt-Anchor" href="#how-to-coordinate-execution"></a> How to coordinate execution?</h2>
<ol>
<li>If our DBMS supports multi-operation and distributed transactions, we need a way to coordinate their execution in the system.</li>
<li>The first approach is centerlized coordinator. There is a centralized coordinator that receives commands from application.
<ul>
<li>The first design requires applications to handle transactions.
<ul>
<li>The client communicates with the coordinator to acquire locks on the partitions that the client wants to access.</li>
<li>Once it receives an acknowledgement from the coordinator, the client sends its queries to those partitions.</li>
<li>Once all queries for a given transaction are done, the client sends a commit request to the coordinator.</li>
<li>The coordinator then communicates with the partitions involved in the transaction to determine whether the transaction is allowed to commit.</li>
</ul>
</li>
<li>Another design uses a middleware to accept query requests and routes queries to correct partitions.</li>
</ul>
</li>
<li>The second approach is decentralized.
<ul>
<li>The client directly sends queries to one of the partitions which will be the leader node of that transaction.</li>
<li>The leader node will coordinate the following communicating with other partitions and committing.</li>
</ul>
</li>
</ol>
<h1 id="partitioning-schemes"><a class="markdownIt-Anchor" href="#partitioning-schemes"></a> Partitioning Schemes</h1>
<h2 id="what-do-we-desire-when-partitioning-database"><a class="markdownIt-Anchor" href="#what-do-we-desire-when-partitioning-database"></a> What do we desire when partitioning database?</h2>
<ol>
<li>Applications should not be required to know where data is physically located in a distributed DBMS.
<ul>
<li>Any query that run on a single-node DBMS should produce the same result on a distributed DBMS.</li>
<li>The DBMS executes query fragments on each partition and then combines the results to produce a single answer.</li>
</ul>
</li>
<li>In practice, developers need to be aware of the communication costs of queries to avoid excessively “expensive” data movement.</li>
<li>The DBMS can partition a database physically for shared nothing or logically for shared disk.
<ul>
<li>In Logical partitioning, each node is responsible for certain designated data. They cannot access data out of their duty. Though data are actually stored in independent storage nodes which computation nodes all can access.</li>
<li>In physical partitioning, data out of their duty cannot be accessed directly since they are stored in the local disk of other nodes.</li>
</ul>
</li>
</ol>
<h2 id="how-can-we-partition-database"><a class="markdownIt-Anchor" href="#how-can-we-partition-database"></a> How can we partition database?</h2>
<ol>
<li>The first method is the naive table partitioning.
<ul>
<li>Assign an entire table to a single node.</li>
<li>It assumes that each node has enough storage space for an entire table.</li>
<li>This is ideal if queries never join data across tables stored on different nodes and access patterns are uniform.</li>
</ul>
</li>
<li>The second method is vertical partitioning.
<ul>
<li>Split a table’s attributes into separate partitions.</li>
<li>It must store tuple information to reconstruct the original record.</li>
</ul>
</li>
<li>The third method is horizontal partitioning.
<ul>
<li>Split a table’s tuples into disjoint subsets based on some partitioning key and scheme.</li>
<li>Choose column(s) that divides the database equally in terms of size, load, or usage.</li>
<li>It can partition based on hashing, ranges, or predicates.</li>
</ul>
</li>
</ol>
<h2 id="how-can-we-optimize-horizontal-partitioning"><a class="markdownIt-Anchor" href="#how-can-we-optimize-horizontal-partitioning"></a> How can we optimize horizontal partitioning?</h2>
<ol>
<li>The main problem is that when adding a new storage nodes, the DBMS needs to reshuffle data.</li>
<li>We can use the consistent hashing.
<ul>
<li>The hashing value is betwen <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> forming a circle.</li>
<li>Each nodes are assigned a value betwen <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>. Data are stored in the node with the closest value to its hashing values going in clock-wise order.</li>
<li>When adding a node, only one node needs to transmit data to the new node, instead of transmit data between all pairs of nodes.</li>
</ul>
</li>
<li>With consistent hashing, we can support replication easily.
<ul>
<li>Just store data in the first batch of nodes with closest value in clock-wise order.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/30/Courses/15445/14-Database-Recovery/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/30/Courses/15445/14-Database-Recovery/" class="post-title-link" itemprop="url">14 Database Recovery</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-30 19:12:36" itemprop="dateCreated datePublished" datetime="2023-08-30T19:12:36+08:00">2023-08-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:09" itemprop="dateModified" datetime="2023-09-26T14:01:09+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#what-are-the-main-ideas-of-aries">What are the main ideas of ARIES?</a></li>
<li><a href="#record-logs">Record logs</a>
<ul>
<li><a href="#what-are-the-lsns">What are the LSNs?</a></li>
<li><a href="#how-to-handle-transaction-commit">How to handle transaction commit?</a></li>
<li><a href="#how-to-handle-transaction-abort">How to handle transaction abort?</a></li>
</ul>
</li>
<li><a href="#fuzzy-checkpoints">Fuzzy checkpoints</a>
<ul>
<li><a href="#how-can-we-improve-the-naive-checkpoints">How can we improve the naive checkpoints?</a></li>
<li><a href="#how-can-we-checkpoint-without-stalling-transactions">How can we checkpoint without stalling transactions?</a></li>
</ul>
</li>
<li><a href="#recovery">Recovery</a>
<ul>
<li><a href="#how-does-aries-recover-from-crash">How does ARIES recover from crash?</a></li>
<li><a href="#what-does-analysis-phase-do">What does analysis phase do?</a></li>
<li><a href="#what-does-redo-phase-do">What does redo phase do?</a></li>
<li><a href="#what-does-undo-phase-do">What does undo phase do?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="what-are-the-main-ideas-of-aries"><a class="markdownIt-Anchor" href="#what-are-the-main-ideas-of-aries"></a> What are the main ideas of ARIES?</h1>
<ol>
<li>ARIES: Algorithms for Recovery and Isolation Exploiting Semantics</li>
<li>Write-Ahead Logging:
<ul>
<li>Any change is recorded in log on stable storage before the database change is written to disk.</li>
<li>Must use steal and no-force buffer pool policies.
<ul>
<li>Logs are forced to be flushed into disk while modified pages are not.</li>
<li>Force is also correct, but it damages runtime performance, which makes no one uses it.</li>
</ul>
</li>
</ul>
</li>
<li>Repeating History During Redo: On DBMS restart, retrace actions and restore database to exact state before crash.</li>
<li>Logging Changes During Undo: Record undo actions to log to ensure action is not repeated in the event of repeated failures.</li>
</ol>
<h1 id="record-logs"><a class="markdownIt-Anchor" href="#record-logs"></a> Record logs</h1>
<h2 id="what-are-the-lsns"><a class="markdownIt-Anchor" href="#what-are-the-lsns"></a> What are the LSNs?</h2>
<ol>
<li>Every log record now includes a globally unique, monotonically increasing log sequence number (LSN).
<ul>
<li>LSNs represent the physical order that transactions make changes to the database.</li>
</ul>
</li>
<li>Various components in the system keep track of LSNs that pertain to them
<ul>
<li>In memory, the system uses <code>flushedLSN</code> to track the last LSN in log on disk.</li>
<li>In each page in disk, <code>pageLSN</code> is used to track the newest update to that page while <code>recLSN</code> is tracking the oldest update to that page since it was last flushed.</li>
<li>Each transaction maintains <code>lastLSN</code> representing the latest record of that transaction.</li>
<li>In disk, there is also a <code>MasterRecord</code> meaning the LSN of latest checkpoint.</li>
</ul>
</li>
<li>Before the DBMS can write page x to disk, it must flush the log at least to the point where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>L</mi><mi>S</mi><msub><mi>N</mi><mi>x</mi></msub><mo>≤</mo><mi>f</mi><mi>l</mi><mi>u</mi><mi>s</mi><mi>h</mi><mi>e</mi><mi>d</mi><mi>L</mi><mi>S</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">pageLSN_x ≤ flushedLSN</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">e</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">u</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>.</li>
<li>Update the <code>pageLSN</code> every time a transaction modifies a record in the page.</li>
<li>Update the <code>flushedLSN</code> in memory every time the DBMS writes out the WAL buffer to disk.</li>
</ol>
<h2 id="how-to-handle-transaction-commit"><a class="markdownIt-Anchor" href="#how-to-handle-transaction-commit"></a> How to handle transaction commit?</h2>
<ol>
<li>When a transaction commits, the DBMS writes a <code>COMMIT</code> record to log and guarantees that all log records up to transaction’s <code>COMMIT</code> record are flushed to disk.
<ul>
<li>Log flushes are sequential, synchronous writes to disk.</li>
</ul>
</li>
<li>When the commit succeeds, write a special <code>TXN-END</code> record to log.
<ul>
<li>Indicates that no new log record for a transaction will appear in the log ever again.</li>
<li>This does not need to be flushed immediately.</li>
</ul>
</li>
</ol>
<h2 id="how-to-handle-transaction-abort"><a class="markdownIt-Anchor" href="#how-to-handle-transaction-abort"></a> How to handle transaction abort?</h2>
<ol>
<li>Another <code>prevLSN</code> is added to log records pointing to the previous LSN for that transaction to make it easy to walk through its records.</li>
<li>First write an <code>ABORT</code> record to log for the transaction.
<ul>
<li>Following that, we need to records steps taken to undo the transaction.
<ul>
<li>A <code>CLR</code> describes the actions taken to undo the actions of a previous update record.</li>
<li>It has all the fields of an update log record plus the <code>undoNext</code> pointer pointing to the next-to-be-undone LSN.</li>
<li><code>CLR</code>s are added to log records but the DBMS does not wait for them to be flushed before notifying the application that the transaction aborted.</li>
</ul>
</li>
<li>Lastly, write a <code>TXN-END</code> record.</li>
</ul>
</li>
<li>To add <code>CLR</code> records, we need to analyze the transaction’s updates in reverse order.
<ul>
<li>For each update record, write a <code>CLR</code> entry to the log, and restore old value.</li>
<li><code>CLR</code>s never need to be undone.</li>
</ul>
</li>
</ol>
<h1 id="fuzzy-checkpoints"><a class="markdownIt-Anchor" href="#fuzzy-checkpoints"></a> Fuzzy checkpoints</h1>
<h2 id="how-can-we-improve-the-naive-checkpoints"><a class="markdownIt-Anchor" href="#how-can-we-improve-the-naive-checkpoints"></a> How can we improve the naive checkpoints?</h2>
<ol>
<li>The naive checkpoint needs to halt the start of any new transactions and wait until all active transactions finish executing.</li>
<li>We can only pause modifying transactions while the DBMS takes the checkpoint.
<ul>
<li>It can be done through preventing queries from acquiring write latch on table/index pages.</li>
<li>Don’t have to wait until all transactions finish before taking the checkpoint.</li>
</ul>
</li>
<li>We must record internal state as of the beginning of the checkpoint.
<ul>
<li>Active Transaction Table (ATT): What transactions are running at the time we took checkpoint.</li>
<li>Dirty Page Table (DPT): What pages are dirty.</li>
</ul>
</li>
<li>ATT is maintained at runtime and recovery. There is a entry per currently active transaction
<ul>
<li>Each entry contains
<ul>
<li><code>transactionId</code>: Unique transaction identifier</li>
<li><code>status</code>: The current “mode” of the transaction</li>
<li><code>lastLSN</code>: Most recent LSN created by transaction.</li>
</ul>
</li>
<li>Remove entry after the <code>TXN-END</code> record.</li>
<li>Txn Status Codes
<ul>
<li><code>R</code>: Running</li>
<li><code>C</code>: Committing</li>
<li><code>U</code>: Candidate for undo</li>
</ul>
</li>
<li><code>U</code> is the default mode in recovery.
<ul>
<li>When replaying the log, we cannot see what’s come up ahead, assuming not gonna see a transaction commit record.</li>
<li>Flip to commit when see a transaction commit record</li>
</ul>
</li>
</ul>
</li>
<li>DPT contains one entry per dirty page in the buffer pool including <code>recLSN</code>.
<ul>
<li><code>recLSN</code> is the LSN of the log record that first caused the page to be dirty.</li>
</ul>
</li>
<li>Each checkpoint record includes ATT and DPT in it.</li>
</ol>
<h2 id="how-can-we-checkpoint-without-stalling-transactions"><a class="markdownIt-Anchor" href="#how-can-we-checkpoint-without-stalling-transactions"></a> How can we checkpoint without stalling transactions?</h2>
<ol>
<li>In fuzzy checkpoint, there are two kind of records to track checkpoint boundaries.
<ul>
<li><code>CHECKPOINT-BEGIN</code> indicates start of checkpoint. Recovery begins from here.</li>
<li><code>CHECKPOINT-END</code> contains ATT and DPT at the moment of <code>CHECKPOINT-BEGIN</code>.</li>
</ul>
</li>
<li>The <code>LSN</code> of the <code>CHECKPOINT-BEGIN</code> record is written to the <code>MasterRecord</code> when it completes.</li>
<li>Any transaction that begins after the checkpoint starts is excluded from the ATT in the <code>CHECKPOINT-END</code> record.</li>
</ol>
<h1 id="recovery"><a class="markdownIt-Anchor" href="#recovery"></a> Recovery</h1>
<h2 id="how-does-aries-recover-from-crash"><a class="markdownIt-Anchor" href="#how-does-aries-recover-from-crash"></a> How does ARIES recover from crash?</h2>
<ol>
<li>The first phase is analysis.
<ul>
<li>Examine the WAL in forward direction starting at MasterRecord to identify dirty pages in the buffer pool and active transactions at the time of the crash.</li>
<li>This is to figure out which transactions committed or failed since checkpoint.</li>
</ul>
</li>
<li>The second phase is redo phase.
<ul>
<li>Repeat all actions starting from an appropriate point in the log in forward direction.</li>
<li>This phase is to repeat all actions, even transactions that will abort.</li>
</ul>
</li>
<li>The last phase is undo.
<ul>
<li>Reverse the actions of transactions that did not commit before the crash in reverse order.</li>
<li>This phase is to reverse effects of failed transactions.</li>
</ul>
</li>
<li>If crashes happen during recovery, just run recovery again.</li>
</ol>
<img src="/imgs/15445/Recovery/aries.png" width="20%">
<h2 id="what-does-analysis-phase-do"><a class="markdownIt-Anchor" href="#what-does-analysis-phase-do"></a> What does analysis phase do?</h2>
<ol>
<li>Scan log forward from the <code>CHECKPOINT-END</code> of the last successful checkpoint.</li>
<li>If the DBMS finds a <code>TXN-END</code> record, remove its corresponding transaction from ATT.</li>
<li>For all other records,
<ul>
<li>If transaction not in ATT, add it with status <code>UNDO</code>. On commit, change transaction status to <code>COMMIT</code>.</li>
<li>For update log records, if page <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> not in DPT, add <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> to DPT, set its <code>recLSN=LSN</code>.</li>
</ul>
</li>
<li>At end of the Analysis Phase,
<ul>
<li>ATT identifies which transactions were active at time of crash.</li>
<li>DPT identifies which dirty pages might not have made it to disk.</li>
</ul>
</li>
</ol>
<h2 id="what-does-redo-phase-do"><a class="markdownIt-Anchor" href="#what-does-redo-phase-do"></a> What does redo phase do?</h2>
<ol>
<li>The goal is to repeat history to reconstruct the database state at the moment of the crash.</li>
<li>Scan forward from the log record containing smallest <code>recLSN</code> in DPT.</li>
<li>For each update log record or <code>CLR</code> with a given LSN, redo the action unless affected page is not in DPT, or affected page is in DPT but that record’s LSN is less than the page’s <code>recLSN</code>.
<ul>
<li>If the affected page is not in DPT, that means that that modification is already flushed in disk.</li>
<li>If that record’s LSN is less than the page’s <code>recLSN</code>, that means that that page is dirty but due to some updates after the record. The modification of that record is also flushed into the disk.</li>
</ul>
</li>
<li>To redo an action,
<ul>
<li>Reapply logged update.</li>
<li>Set <code>pageLSN</code> to log record’s LSN.</li>
<li>No additional logging, no forced flushes.</li>
<li>To improve performance, we can assume that it is not going to crash again and flush all changes to disk asynchronously in the background.</li>
</ul>
</li>
<li>At the end of Redo Phase, write <code>TXN-END</code> log records for all transactions with status <code>C</code> and remove them from the ATT.</li>
</ol>
<h2 id="what-does-undo-phase-do"><a class="markdownIt-Anchor" href="#what-does-undo-phase-do"></a> What does undo phase do?</h2>
<ol>
<li>Undo all transactions that were active at the time of crash and therefore will never commit.</li>
<li>These are all the transactions with <code>U</code> status in the ATT after the Analysis Phase.</li>
<li>Process them in reverse LSN order using the <code>lastLSN</code> to speed up traversal.</li>
<li>Write a <code>CLR</code> for every modification.</li>
<li>To improve performance,
<ul>
<li>Lazily rollback changes before new transactions access pages.</li>
<li>Rewrite the application to avoid long-running transactions, which will never be used.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/29/Courses/15445/13-Database-Logging/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/29/Courses/15445/13-Database-Logging/" class="post-title-link" itemprop="url">13 Database Logging</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-29 11:42:20" itemprop="dateCreated datePublished" datetime="2023-08-29T11:42:20+08:00">2023-08-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:04" itemprop="dateModified" datetime="2023-09-26T14:01:04+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#crash">Crash</a>
<ul>
<li><a href="#how-to-recover-from-crash">How to recover from crash?</a></li>
<li><a href="#what-are-the-possible-classification-of-failures">What are the possible classification of failures?</a></li>
</ul>
</li>
<li><a href="#naive-solution">Naive solution</a>
<ul>
<li><a href="#what-buffer-pool-policies-can-we-choose">What buffer pool policies can we choose?</a></li>
<li><a href="#what-are-the-pros-and-cons-of-no-steal-and-force-policy">What are the pros and cons of no-steal and force policy?</a></li>
<li><a href="#how-does-shadow-paging-work">How does shadow paging work?</a></li>
<li><a href="#what-are-the-disadvantages-of-shadow-paging">What are the disadvantages of shadow paging?</a></li>
<li><a href="#how-does-journal-file-work">How does journal file work?</a></li>
</ul>
</li>
<li><a href="#write-ahead-log">Write-ahead log</a>
<ul>
<li><a href="#what-is-the-main-idea-of-wal">What is the main idea of WAL?</a></li>
<li><a href="#how-to-write-under-wal-protocol">How to write under WAL protocol?</a></li>
<li><a href="#how-to-reduce-flushing-the-log-buffer">How to reduce flushing the log buffer?</a></li>
<li><a href="#how-to-store-changes">How to store changes?</a></li>
<li><a href="#what-is-log-structured-system">What is log-structured system?</a></li>
<li><a href="#how-to-prevent-wal-from-growing-forever">How to prevent WAL from growing forever?</a></li>
<li><a href="#what-is-the-problem-of-this-naive-checkpoint-protocol">What is the problem of this naive checkpoint protocol?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="crash"><a class="markdownIt-Anchor" href="#crash"></a> Crash</h1>
<h2 id="how-to-recover-from-crash"><a class="markdownIt-Anchor" href="#how-to-recover-from-crash"></a> How to recover from crash?</h2>
<ol>
<li>Recovery algorithms are techniques to ensure database consistency, transaction atomicity, and durability despite failures.</li>
<li>The DBMS needs to ensure the following:
<ul>
<li>The changes for any transaction are durable once the DBMS has told somebody that it committed.</li>
<li>No partial changes are durable if the transaction aborted.</li>
</ul>
</li>
<li>Recovery algorithms have two parts:
<ul>
<li>The first is runtime part: Actions during normal transaction processing to ensure that the DBMS can recover from a failure.</li>
<li>The second is startup part: Actions after a failure to recover the database to a state that ensures atomicity, consistency, and durability.</li>
</ul>
</li>
</ol>
<h2 id="what-are-the-possible-classification-of-failures"><a class="markdownIt-Anchor" href="#what-are-the-possible-classification-of-failures"></a> What are the possible classification of failures?</h2>
<ol>
<li>Transaction failures:
<ul>
<li>Logical errors: Transaction cannot complete due to some internal error condition (e.g., integrity constraint violation).</li>
<li>Internal state errors: DBMS must terminate an active transaction due to an error condition (e.g., deadlock).</li>
</ul>
</li>
<li>System failures:
<ul>
<li>Software failure: Problem with the OS or DBMS implementation (e.g., uncaught divide-by-zero exception).</li>
<li>Hardware failure: The computer hosting the DBMS crashes (e.g., power plug gets pulled).</li>
<li>Fail-stop assumption: Non-volatile storage contents are assumed to not be corrupted by system crash.</li>
</ul>
</li>
<li>Storage media failure:
<ul>
<li>Non-repairable hardware failure: A head crash or similar disk failure destroys all or part of non-volatile storage.</li>
<li>Destruction is assumed to be detectable (e.g., disk controller use checksums to detect failures).</li>
<li>No DBMS can recover from this! Database must be restored from archived version.</li>
<li>Stable storage is a non-existent form of non-volatile storage that survives all possible failures scenarios.</li>
</ul>
</li>
</ol>
<h1 id="naive-solution"><a class="markdownIt-Anchor" href="#naive-solution"></a> Naive solution</h1>
<h2 id="what-buffer-pool-policies-can-we-choose"><a class="markdownIt-Anchor" href="#what-buffer-pool-policies-can-we-choose"></a> What buffer pool policies can we choose?</h2>
<ol>
<li>Steal policy:
<ul>
<li>Whether the DBMS allows an uncommitted transaction to overwrite the most recent committed value of an object in non-volatile storage.</li>
<li>Steal: Is allowed</li>
<li>No-steal: Is not allowed</li>
</ul>
</li>
<li>Force policy:
<ul>
<li>Whether the DBMS requires that all updates made by a transaction are reflected on non-volatile storage before the transaction can commit.</li>
<li>Force: Is required</li>
<li>No-force: Is not required</li>
</ul>
</li>
<li>Undo: The process of removing the effects of an incomplete or aborted transaction.</li>
<li>Redo: The process of re-applying the effects of a committed transaction for durability.</li>
<li>Stead and no-force policy has the best runtime performance since it does not need to wait for conflicted uncommitted transactions and not necessarily to flush when commit.</li>
<li>No-steal and force policy has the best recovery performance since it does not need undo and redo anything.</li>
</ol>
<h2 id="what-are-the-pros-and-cons-of-no-steal-and-force-policy"><a class="markdownIt-Anchor" href="#what-are-the-pros-and-cons-of-no-steal-and-force-policy"></a> What are the pros and cons of no-steal and force policy?</h2>
<ol>
<li>This approach is the easiest to implement:
<ul>
<li>Never have to undo changes of an aborted transaction because the changes were not written to disk.</li>
<li>Never have to redo changes of a committed transaction because all the changes are guaranteed to be written to disk at commit time (assuming atomic hardware writes).</li>
</ul>
</li>
<li>An uncommitted transaction and another committing transaction may have written the same page. Then the buffer pool manager needs to copy that page with only modifications from the committing transaction and writes that copied page to non-volatile storage.</li>
<li>The disadvantages are as following:
<ul>
<li>Copying data is expensive.</li>
<li>The buffer pool manager needs to be aware of the context.</li>
<li>Cannot support write sets that exceed the amount of physical memory available.</li>
</ul>
</li>
</ol>
<h2 id="how-does-shadow-paging-work"><a class="markdownIt-Anchor" href="#how-does-shadow-paging-work"></a> How does shadow paging work?</h2>
<ol>
<li>Instead of copying the entire database, the DBMS copies pages on write to create two versions
<ul>
<li>Master: Contains only changes from committed transactions. Read-only transactions access the current master.</li>
<li>Shadow: Temporary database with changes made from uncommitted transactions.</li>
</ul>
</li>
<li>Active modifying transaction copies the page table as the shadow page table.
<ul>
<li>When it tries to modify a page, it will copy that page, and modify the shadow page table to point to the new copied page.</li>
<li>To install updates when a transaction commits, overwrite the root so it points to the shadow, thereby swapping the master and shadow. Then DMBS needs to do some garbage collection.</li>
</ul>
</li>
<li>The buffer pool policy is no-steal and force.</li>
<li>To support rollbacks and recovery, the DBMS needs to Remove the shadow pages. Leave the master and the DB root pointer alone on undo phase, and no need to redo at all.</li>
</ol>
<h2 id="what-are-the-disadvantages-of-shadow-paging"><a class="markdownIt-Anchor" href="#what-are-the-disadvantages-of-shadow-paging"></a> What are the disadvantages of shadow paging?</h2>
<ol>
<li>Copying the entire page table is expensive:
<ul>
<li>We can use a page table structured like a B+tree (LMDB). There is no need to copy entire tree, only need to copy paths in the tree that lead to updated leaf nodes.</li>
</ul>
</li>
<li>Commit overhead is high:
<ul>
<li>Need to flush every updated page, page table, and root.</li>
<li>Data gets fragmented, which is bad for sequential scans.</li>
<li>Require the DBMS to perform writes to random non-contiguous pages on disk.</li>
<li>Need garbage collection.</li>
</ul>
</li>
<li>Only supports one writer transaction at a time or transactions in a batch. If two concurrent writer write on the same page, they all copies from master version causing only one write is reflected on the committed database.</li>
</ol>
<h2 id="how-does-journal-file-work"><a class="markdownIt-Anchor" href="#how-does-journal-file-work"></a> How does journal file work?</h2>
<ol>
<li>When a transaction modifies a page, the DBMS copies the original page to a separate journal file before overwriting master version.</li>
<li>After restarting, if a journal file exists, then the DBMS restores it to undo changes from uncommitted transactions.</li>
</ol>
<h1 id="write-ahead-log"><a class="markdownIt-Anchor" href="#write-ahead-log"></a> Write-ahead log</h1>
<h2 id="what-is-the-main-idea-of-wal"><a class="markdownIt-Anchor" href="#what-is-the-main-idea-of-wal"></a> What is the main idea of WAL?</h2>
<ol>
<li>Maintain a log file separate from data files that contains the changes that transactions make to database.
<ul>
<li>Assume that the log is on stable storage.</li>
<li>Log contains enough information to perform the necessary undo and redo actions to restore the database.</li>
</ul>
</li>
<li>DBMS must write to disk the log file records that correspond to changes made to a database object before it can flush that object to disk.
<ul>
<li>The buffer pool policy is steal and no-force.</li>
</ul>
</li>
</ol>
<h2 id="how-to-write-under-wal-protocol"><a class="markdownIt-Anchor" href="#how-to-write-under-wal-protocol"></a> How to write under WAL protocol?</h2>
<ol>
<li>The DBMS stages all a transaction’s log records in volatile storage backed by buffer pool.
<ul>
<li>All log records pertaining to an updated page are written to non-volatile storage before the page itself is over-written in non-volatile storage.</li>
<li>A transaction is not considered committed until all its log records have been written to stable storage.</li>
</ul>
</li>
<li>A <code>&lt;BEGIN&gt;</code> record is written to the log for each transaction to mark its starting point.
<ul>
<li>Most DBMS only writes <code>&lt;BEGIN&gt;</code> on first write command of a transaction instead of on the beginning.</li>
</ul>
</li>
<li>When a transaction finishes, the DBMS will write a <code>&lt;COMMIT&gt;</code> record on the log, and make sure that all log records are flushed before it returns an acknowledgement to application.</li>
<li>Each log entry contains information about the change to a single object
<ul>
<li>Transaction ID, object ID, before value (for undo) and after value (for redo).</li>
</ul>
</li>
</ol>
<h2 id="how-to-reduce-flushing-the-log-buffer"><a class="markdownIt-Anchor" href="#how-to-reduce-flushing-the-log-buffer"></a> How to reduce flushing the log buffer?</h2>
<ol>
<li>Flushing the log buffer to disk every time a transaction commits will become a bottleneck.</li>
<li>The DBMS can use the group commit optimization to batch multiple log flushes together to amortize overhead.
<ul>
<li>When the buffer is full, flush it to disk. Or if there is a timeout.</li>
<li>Log records from different transaction are mixed. This is fine since we can sort them out by recorded transaction ID.</li>
</ul>
</li>
</ol>
<h2 id="how-to-store-changes"><a class="markdownIt-Anchor" href="#how-to-store-changes"></a> How to store changes?</h2>
<ol>
<li>The first logging scheme is physical logging.
<ul>
<li>Record the byte-level changes made to a specific page.</li>
</ul>
</li>
<li>The second is logical logging.
<ul>
<li>Record the high-level operations executed by transactions, e.g. queries.</li>
<li>Logical logging requires less data written in each log record than physical logging.</li>
<li>Difficult to implement recovery with logical logging if you have concurrent transactions running at lower isolation levels.
<ul>
<li>The crash may happen in the middle of a query.</li>
<li>It is hard to determine which parts of the database may have been modified by a query before crash.</li>
</ul>
</li>
<li>Also takes longer to recover because you must re-execute every transaction all over again.</li>
</ul>
</li>
<li>The last is physiological logging.
<ul>
<li>Hybrid approach with byte-level changes for a single tuple identified by page ID and slot number.</li>
</ul>
</li>
</ol>
<h2 id="what-is-log-structured-system"><a class="markdownIt-Anchor" href="#what-is-log-structured-system"></a> What is log-structured system?</h2>
<ol>
<li>Log-structured DBMSs do not have dirty pages.
<ul>
<li>Any page retrieved from disk is immutable.</li>
<li>All modifications are reflected through logs.</li>
</ul>
</li>
<li>The DBMS buffers log records in in-memory pages (MemTable).
<ul>
<li>If this buffer is full, it must be flushed to disk. But it may contain changes from uncommitted transactions.</li>
</ul>
</li>
<li>These DBMSs still maintain a separate WAL to recreate the MemTable on crash.</li>
</ol>
<h2 id="how-to-prevent-wal-from-growing-forever"><a class="markdownIt-Anchor" href="#how-to-prevent-wal-from-growing-forever"></a> How to prevent WAL from growing forever?</h2>
<ol>
<li>If the WAL will grow forever, after a crash, the DBMS must replay the entire log, which will take a long time.</li>
<li>The DBMS periodically takes a checkpoint where it flushes all buffers out to disk.</li>
<li>In blocking / consistent checkpoint protocol,
<ul>
<li>Procedure
<ul>
<li>Pause all queries</li>
<li>Flush all WAL records in memory to disk</li>
<li>Flush all modified pages in the buffer pool to disk</li>
<li>Write a <code>&lt;CHECKPOINT&gt;</code> entry to WAL and flush to disk</li>
<li>Resume queries</li>
</ul>
</li>
<li>On recovery, we can use the <code>&lt;CHECKPOINT&gt;</code> record as the starting point for analyzing the WAL.
<ul>
<li>Any transaction that committed before the checkpoint is ignored.</li>
<li>Redo transactions that is committed after checkpoint and undo transactions that is not committed before the crash.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="what-is-the-problem-of-this-naive-checkpoint-protocol"><a class="markdownIt-Anchor" href="#what-is-the-problem-of-this-naive-checkpoint-protocol"></a> What is the problem of this naive checkpoint protocol?</h2>
<ol>
<li>The DBMS must stall transactions when it takes a checkpoint to ensure a consistent snapshot.
<ul>
<li>Checkpointing too often causes the runtime performance to degrade due to system spends too much time flushing buffers.</li>
<li>But waiting a long time is just as bad since the checkpoint will be large and slow, which makes recovery time much longer.</li>
</ul>
</li>
<li>Scanning the log to find uncommitted transactions can take a long time.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/22/Courses/15445/12-Multi-Version-Concurrency-Control/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/22/Courses/15445/12-Multi-Version-Concurrency-Control/" class="post-title-link" itemprop="url">12 Multi-Version Concurrency Control</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-22 15:34:19" itemprop="dateCreated datePublished" datetime="2023-08-22T15:34:19+08:00">2023-08-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-09-26 14:01:00" itemprop="dateModified" datetime="2023-09-26T14:01:00+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><ul class="markdownIt-TOC">
<li><a href="#multi-version-logic">Multi-version logic</a>
<ul>
<li><a href="#what-does-multi-version-mean">What does multi-version mean?</a></li>
<li><a href="#how-to-maintain-multi-version-logically">How to maintain multi-version logically?</a></li>
<li><a href="#what-isolation-can-mvcc-support">What isolation can MVCC support?</a></li>
</ul>
</li>
<li><a href="#design-decisions">Design decisions</a>
<ul>
<li><a href="#what-concurrency-control-protocol-can-be-used">What concurrency control protocol can be used?</a></li>
<li><a href="#how-are-versions-stored">How are versions stored?</a></li>
<li><a href="#how-to-perform-garbage-collection">How to perform garbage collection?</a></li>
<li><a href="#how-to-manage-indexes">How to manage indexes?</a></li>
<li><a href="#how-to-delete-a-tuple">How to delete a tuple?</a></li>
</ul>
</li>
</ul>
</p>
<h1 id="multi-version-logic"><a class="markdownIt-Anchor" href="#multi-version-logic"></a> Multi-version logic</h1>
<h2 id="what-does-multi-version-mean"><a class="markdownIt-Anchor" href="#what-does-multi-version-mean"></a> What does multi-version mean?</h2>
<ol>
<li>
<p>The DBMS maintains multiple physical versions of a single logical object in the database.</p>
<ul>
<li>
<p>When a txn writes to an object, the DBMS creates a new version of that object.</p>
</li>
<li>
<p>When a txn reads an object, it reads the newest version that existed when the txn started.</p>
</li>
</ul>
</li>
<li>
<p>Writers do not block readers and readers do not block writers.</p>
</li>
<li>
<p>Read-only txns can read a consistent snapshot without acquiring locks using timestamps to determine visibility.</p>
</li>
<li>
<p>Multi-version can easily support time-travel queries on a snapshot version on database.</p>
</li>
</ol>
<h2 id="how-to-maintain-multi-version-logically"><a class="markdownIt-Anchor" href="#how-to-maintain-multi-version-logically"></a> How to maintain multi-version logically?</h2>
<ol>
<li>Each version describes the current version number, the value for this version and the range of lifetime, i.e. the begin and end timestamps.</li>
<li>End timestamp is marked infinity for the newest version.</li>
<li>When a transaction writes an object:
<ul>
<li>First it creates a new entry with a new version number and setting its begin timestamp as its timestamp and end timestamp being infinity.</li>
<li>Then it marks the end timestamp of last version as its timestamp.</li>
<li>Physically this transaction should modifiy the last version to point to this new version. Hence it need to wait for the transaction creating last version to commit to begin its own commit phase.</li>
</ul>
</li>
</ol>
<h2 id="what-isolation-can-mvcc-support"><a class="markdownIt-Anchor" href="#what-isolation-can-mvcc-support"></a> What isolation can MVCC support?</h2>
<ol>
<li><code>SNAPSHOT ISOLATION</code> is another isolation supported by Oracle.
<ul>
<li>It guarantees that all reads made in a transaction see a consistent snapshot of the database that existed at the time the transaction started.</li>
<li>A transaction will commit only if its writes do not conflict with any concurrent updates made since that snapshot.</li>
</ul>
</li>
<li>It is susceptible to write skew anomaly.
<ul>
<li>Two concurrent transactions modify different objects resulting in race conditions.</li>
<li>If a transaction wants to modify all values to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> while another transaction wants to modifiy all values to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>. The first transaction changes all <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>s and the second transaction changes all <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>s. When their result merges to the database, it would be the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>s and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>s are flipped instead of all being <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>s or <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>s.</li>
</ul>
</li>
</ol>
<h1 id="design-decisions"><a class="markdownIt-Anchor" href="#design-decisions"></a> Design decisions</h1>
<h2 id="what-concurrency-control-protocol-can-be-used"><a class="markdownIt-Anchor" href="#what-concurrency-control-protocol-can-be-used"></a> What concurrency control protocol can be used?</h2>
<ol>
<li>All aforementioned protocols can be used in MVCC.</li>
<li>Timestamp ordering assigns transactions timestamps to determine what they can see.</li>
<li>Optimistic concurrency control uses private workspace for new versions.</li>
<li>Two-phase locking requires transactions to acquire appropriate lock on physical version before they can read/write a logical tuple.</li>
</ol>
<h2 id="how-are-versions-stored"><a class="markdownIt-Anchor" href="#how-are-versions-stored"></a> How are versions stored?</h2>
<ol>
<li>The DBMS uses the tuples’ pointer field to create a version chain per logical tuple.
<ul>
<li>This allows the DBMS to find the version that is visible to a particular txn at runtime.</li>
<li>Indexes always point to the “head” of the chain.</li>
</ul>
</li>
<li>The first approach is append-only storage: New versions are appended to the same table space.
<ul>
<li>The versions of different logical tuples are inter-mixed.</li>
<li>On every update, append a new version of the tuple into an empty space in the table.</li>
<li>If the chain is from oldest-to-newest, the DBMS must traverse chain on look-ups. However, the update do not need to update index.</li>
<li>Or, the chain can be from oldest-to-newest. The pros and cons are contrary with last scenario.</li>
</ul>
</li>
<li>The second approach is time-travel storage: Old versions are copied to separate table space.
<ul>
<li>On every update, copy the current version to the time- travel table. Update pointers. Then Overwrite master version in the main table and update pointers.</li>
</ul>
</li>
<li>The third approach is delta storage: The original values of the modified attributes are copied into a separate delta record space.
<ul>
<li>On every update, copy only the values that were modified to the delta storage and overwrite the master version.</li>
<li>Txns can recreate old versions by applying the delta in reverse order.</li>
</ul>
</li>
</ol>
<h2 id="how-to-perform-garbage-collection"><a class="markdownIt-Anchor" href="#how-to-perform-garbage-collection"></a> How to perform garbage collection?</h2>
<ol>
<li>The DBMS needs to remove reclaimable physical versions from the database over time.
<ul>
<li>Reclaimable means that no active txn in the DBMS can see that version (SI), or the version was created by an aborted txn.</li>
<li>To support time-travel queries, the DBMS can only reclaim versions created by an aborted txn.</li>
</ul>
</li>
<li>To look for expired versions, the implementation has two choices.</li>
<li>The first approach is tuple-level: Find old versions by examining tuples directly.
<ul>
<li>In background vacuuming manner, separate thread(s) periodically scan the table and look for reclaimable versions. This design works with any storage.</li>
<li>In cooperative cleaning manner, worker threads identify reclaimable versions as they traverse version chain. It only works with oldest-to-newest.</li>
</ul>
</li>
<li>The second approach is transaction-level: Txns keep track of their old versions on updates so the DBMS does not have to scan tuples to determine visibility.
<ul>
<li>Each txn keeps track of its read/write set. On commit/abort, the txn provides this information to a centralized vacuum worker.</li>
<li>The DBMS periodically determines when all versions created by a finished txn are no longer visible.</li>
</ul>
</li>
</ol>
<h2 id="how-to-manage-indexes"><a class="markdownIt-Anchor" href="#how-to-manage-indexes"></a> How to manage indexes?</h2>
<ol>
<li>Primary key indexes point to version chain head.
<ul>
<li>How often the DBMS must update the primary key index depends on whether the system creates new versions when a tuple is updated.</li>
<li>If a txn updates a tuple’s primary key attribute(s), then this is treated as a <code>DELETE</code> followed by an <code>INSERT</code>.</li>
</ul>
</li>
<li>Secondary indexes may use the physical address to the version chain head the same way as primary key index.</li>
<li>The secondary indexes may also use logical pointers.
<ul>
<li>It uses a fixed identifier per tuple that does not change, e.g. primary key or tuple ID.</li>
<li>This would require an extra indirection layer.</li>
</ul>
</li>
</ol>
<h2 id="how-to-delete-a-tuple"><a class="markdownIt-Anchor" href="#how-to-delete-a-tuple"></a> How to delete a tuple?</h2>
<ol>
<li>The DBMS physically deletes a tuple from the database only when all versions of a logically deleted tuple are not visible.</li>
<li>If a tuple is deleted, then there cannot be a new version of that tuple after the newest version.</li>
<li>There are two ways to denote that tuple has been logically delete at some point in time.
<ul>
<li>The first deleted flag way is to maintain a flag to indicate that the logical tuple has been deleted after the newest physical version. The flag can either be in tuple header or a separate column.</li>
<li>The second tombstone tuple way is to create an empty physical version to indicate that a logical tuple is deleted.
<ul>
<li>It uses a separate pool for tombstone tuples with only a special bit pattern in version chain pointer to reduce the storage overhead.</li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/about/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/about/">1</a><a class="page-number" href="/about/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/about/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/about/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/about/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
