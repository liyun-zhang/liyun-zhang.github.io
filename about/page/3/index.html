<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/about/page/3/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/about/page/3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"about/page/3/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/Fault-Tolerance-VM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/Fault-Tolerance-VM/" class="post-title-link" itemprop="url">Fault Tolerance VM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:11:02" itemprop="dateCreated datePublished" datetime="2023-09-26T13:11:02+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-15 19:31:55" itemprop="dateModified" datetime="2024-03-15T19:31:55+08:00">2024-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/vm-ft.pdf">The Design of a Practical System for Fault-Tolerance Virtual Machines</a></p>
<p>@[toc]</p>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Contribution<ul>
<li>This paper implemented a system providing fault tolerance virtual machine (VM) based on replicating the execution of a primary VM vis a backup VM on another server. The system automatically restores redundancy after failure. </li>
<li>It reduces the performance of real applications by less than 10%. The data bandwidth needed to keep the primary and secondary VM executing in lockstep is less than 20 Mb/s for several real applications, which allows for implementing fault tolerance over a longer distance. </li>
<li>The system automatically restores redundancy after a failure by starting a new backup virtual machine on any available server in the local cluster. </li>
</ul>
</li>
<li>Limitation<ul>
<li>Only support uni-processor VMs. Recording and replaying the execution of a multi-processor VM have significant performance issues because nearly every access to shared memory can be a non-deterministic operation. </li>
<li>Only attempt to deal with fail-stop failure, which are server failures that can be detected before the failing server causes an incorrect externally visible action. </li>
</ul>
</li>
<li>Challenges<ul>
<li>Correctly capturing all the input and non-determinism necessary to ensure deterministic execution of a backup virtual machine. </li>
<li>Correctly applying the inputs and non-determinism to the backup virtual machine. </li>
<li>Doing so in a manner that doesn’t degrade performance. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="FT-design"><a href="#FT-design" class="headerlink" title="FT design"></a>FT design</h2><h3 id="Primary-backup-structure"><a href="#Primary-backup-structure" class="headerlink" title="Primary-backup structure"></a>Primary-backup structure</h3><h4 id="What-is-the-usual-way-to-implement-fault-tolerance-via-a-primary-backup-approach"><a href="#What-is-the-usual-way-to-implement-fault-tolerance-via-a-primary-backup-approach" class="headerlink" title="What is the usual way to implement fault tolerance via a primary/backup approach?"></a>What is the usual way to implement fault tolerance via a primary/backup approach?</h4><ol>
<li><p>The backup server can always take over if the primary server fails. </p>
<ul>
<li>The problem is that the state of the backup server must be kept nearly identical to the primary server at all times. We say that the two VMs are in virtual lock-step. </li>
</ul>
</li>
<li><p>One way is to ship changes to all states of the primary. The bandwidth needed to send can be very large. </p>
</li>
<li><p>Another method is the state-machine approach. </p>
<ul>
<li><p>The idea is to model the servers as deterministic state machines that are kept in sync by starting them from the same initial state and ensuring that they receive the same input requests in the same order. </p>
</li>
<li><p>Some operations are not deterministic. Extra coordination must be used to ensure that they receive a primary and backup are kept in sync. </p>
</li>
<li><p>The extra information is far less than the state (mainly memory updates) changing in the primary. </p>
</li>
</ul>
</li>
</ol>
<h4 id="What-is-the-difference-between-physical-servers-and-VMs-at-the-state-machine-level"><a href="#What-is-the-difference-between-physical-servers-and-VMs-at-the-state-machine-level" class="headerlink" title="What is the difference between physical servers and VMs at the state machine level?"></a>What is the difference between physical servers and VMs at the state machine level?</h4><ol>
<li><p>Implementing coordination to ensure deterministic execution of physical servers is challenging, particularly as processor frequencies increase. </p>
</li>
<li><p>VM running on a hypervisor can be considered a well-defined state machine. </p>
</li>
<li><p>VMs still have non-deterministic operations. The hypervisor can capture all the necessary information about non-deterministic operations on the primary VM and replay these operations correctly on the backup VM. </p>
</li>
</ol>
<h4 id="What-is-the-basic-structure-of-FT-VMs"><a href="#What-is-the-basic-structure-of-FT-VMs" class="headerlink" title="What is the basic structure of FT VMs?"></a>What is the basic structure of FT VMs?</h4><ol>
<li>The virtual disks for the VMs are on shared storage and accessible to the primary and backup VM for input and output. </li>
<li>Only the primary VM advertises its presence on the network, so all network inputs come to the primary VM. So do all other inputs. </li>
<li><p>All inputs, including incoming network packets, disk reads, keyboard, and mouse, only come to the primary VM. The primary VM sends all inputs it receives to the backup VM via a network connection known as the logging channel. </p>
<p><img src="/imgs/Distributed/FTVM/01.png" style="zoom:33%;" /></p>
</li>
</ol>
<h3 id="FT-protocol"><a href="#FT-protocol" class="headerlink" title="FT protocol"></a>FT protocol</h3><h4 id="How-does-VMware-backup-VM-replay"><a href="#How-does-VMware-backup-VM-replay" class="headerlink" title="How does VMware backup VM replay?"></a>How does VMware backup VM replay?</h4><ol>
<li>VMware deterministic replay records the inputs of a VM and all possible non-determinism associated with the VM execution in a stream of log entries written to a log file. </li>
<li>For non-deterministic operations, sufficient information is logged to allow the operation to be reproduced with the same state change and output. </li>
<li>The exact instruction at which the event occurred is also recorded for non-deterministic events such as timer or IO completion interrupts. The event is delivered at the same point in the instruction stream during replay. </li>
<li>VMware deterministic replay does not need to use epochs where non-deterministic events are only delivered at the end. Each interrupt is recorded as it occurs and efficiently delivered with the appropriate instruction while being replayed. </li>
<li>Instead of writing the log entries to disk, we send them to the backup VM via the logging channel. The backup VM replays the entries in real-time. </li>
</ol>
<h4 id="What-if-the-backup-VM-executes-in-a-way-different-from-the-primary-VM"><a href="#What-if-the-backup-VM-executes-in-a-way-different-from-the-primary-VM" class="headerlink" title="What if the backup VM executes in a way different from the primary VM?"></a>What if the backup VM executes in a way different from the primary VM?</h4><ol>
<li><p>The <em>Output Requirement</em>: if the backup VM ever takes over after a primary failure, the backup VM will continue executing in a way that is entirely consistent with all outputs that the primary VM has sent to the external world. </p>
</li>
<li><p>The Output Requirement can be ensured by </p>
<ul>
<li><p>Delaying any external output (typically a network packet) until the backup VM has received all information will allow it to replay execution at least to the point of that output operation. </p>
</li>
<li><p>One necessary condition is that the backup VM must have received all log entries generated before the output operation. </p>
</li>
</ul>
</li>
<li><p>Suppose we create a special log entry at each output operation. Then, the Output Requirement may be enforced by the Output Rule. </p>
<ul>
<li><em>Output Rule</em>: the primary VM may not send an output to the external world until the backup VM has received and acknowledged the log entry associated with the operation producing the output. </li>
</ul>
</li>
</ol>
<h4 id="Will-the-Output-Rule-affect-the-VM-e-g-stop-its-execution"><a href="#Will-the-Output-Rule-affect-the-VM-e-g-stop-its-execution" class="headerlink" title="Will the Output Rule affect the VM, e.g., stop its execution?"></a>Will the Output Rule affect the VM, e.g., stop its execution?</h4><ol>
<li>It does not say anything about stopping the execution of the primary VM. We need only delay the sending of the output, but the VM itself can continue execution. </li>
<li>Since operating systems do non-blocking network and disk outputs with asynchronous interrupts to indicate completion, the VM can easily continue execution and will not necessarily be immediately affected by the delay in the output. </li>
</ol>
<h4 id="What-are-the-subtleties-of-executing-disk-reads-on-the-backup-VM"><a href="#What-are-the-subtleties-of-executing-disk-reads-on-the-backup-VM" class="headerlink" title="What are the subtleties of executing disk reads on the backup VM?"></a>What are the subtleties of executing disk reads on the backup VM?</h4><ol>
<li><p>By default, the primary VM will send the results of the disk read to the backup VM via the logging channel. </p>
</li>
<li><p>Executing disk reads on the backup VM can greatly reduce the traffic on the logging channel for workloads that do a lot of disk reads. It may also slow down the backup VM’s execution. </p>
</li>
<li><p>Some extra work must be done to deal with failed disk read operations. </p>
<ul>
<li><p>If the primary succeeds while the backup fails, the backup must keep retrying until success. </p>
</li>
<li><p>Suppose the backup succeeds while the primary fails. In that case, the contents of the target memory must be sent to the backup via the logging channel since the contents of the memory will be undetermined and not necessarily replicated by a successful disk read by the backup VM. </p>
</li>
</ul>
</li>
<li><p>If the primary VM does a read to a particular disk location, followed pretty soon by a write to the exact disk location, then the disk write must be delayed until the backup VM has executed the first disk read. </p>
</li>
</ol>
<h3 id="Detecting-and-responding-to-failure"><a href="#Detecting-and-responding-to-failure" class="headerlink" title="Detecting and responding to failure"></a>Detecting and responding to failure</h3><h4 id="How-to-handle-duplicate-outputs"><a href="#How-to-handle-duplicate-outputs" class="headerlink" title="How to handle duplicate outputs?"></a>How to handle duplicate outputs?</h4><ol>
<li>We cannot guarantee that all outputs are produced exactly once in a failover situation. </li>
<li>The network infrastructure (e.g., TCP) is designed to deal with lost packets and duplicate packets. </li>
</ol>
<h4 id="How-to-handle-backup-VM-failure"><a href="#How-to-handle-backup-VM-failure" class="headerlink" title="How to handle backup VM failure?"></a>How to handle backup VM failure?</h4><p>The primary VM will go live, i.e., leave recording mode, stop sending entries on the logging channel, and start executing normally. </p>
<h4 id="How-to-handle-primary-VM-failure"><a href="#How-to-handle-primary-VM-failure" class="headerlink" title="How to handle primary VM failure?"></a>How to handle primary VM failure?</h4><ol>
<li>The backup VM will continue replaying its execution from the log entries until it has consumed the last log entry. </li>
<li>The backup VM will stop replaying mode and execute as a regular VM. The backup VM has been promoted to the primary VM and is now missing a backup VM. </li>
</ol>
<h4 id="After-a-failover-how-will-the-new-primary-VM-communicate-with-the-external-world"><a href="#After-a-failover-how-will-the-new-primary-VM-communicate-with-the-external-world" class="headerlink" title="After a failover, how will the new primary VM communicate with the external world?"></a>After a failover, how will the new primary VM communicate with the external world?</h4><ol>
<li>VMware FT automatically advertises the MAC address of the new primary VM on the network so that physical network switches will know on what server that new primary VM is located. </li>
<li>The newly promoted primary VM may need to reissue some disk IOs. </li>
</ol>
<h4 id="How-to-detect-failure-of-primary-or-backup-VMs"><a href="#How-to-detect-failure-of-primary-or-backup-VMs" class="headerlink" title="How to detect failure of primary or backup VMs?"></a>How to detect failure of primary or backup VMs?</h4><ol>
<li>VMware FT uses UDP heartbeating between servers running fault-tolerant VMs to detect when a server may have crashed. </li>
<li>In addition, VMware FT monitors the logging traffic sent from the primary to the backup VM and the acknowledgments sent from the backup VM to the primary VM. Because of regular timer interrupts, the logging traffic should be regular and never stop for a functioning guest OS. </li>
</ol>
<h4 id="How-to-avoid-split-brain-problems"><a href="#How-to-avoid-split-brain-problems" class="headerlink" title="How to avoid split-brain problems?"></a>How to avoid split-brain problems?</h4><ol>
<li>When a primary or backup VM wants to go live, it executes an atomic test-and-set operation on the shared virtual disk. </li>
<li>If the operation succeeds, the VM is allowed to go live. </li>
<li>If the operation fails, the other VM must have already gone live, so the current VM halts itself (“commits suicide”). </li>
</ol>
<h3 id="Alternative-Non-shared-disk"><a href="#Alternative-Non-shared-disk" class="headerlink" title="Alternative: Non-shared disk"></a>Alternative: Non-shared disk</h3><h4 id="What-is-the-difference-between-non-shared-disks-and-shared-disks"><a href="#What-is-the-difference-between-non-shared-disks-and-shared-disks" class="headerlink" title="What is the difference between non-shared disks and shared disks?"></a>What is the difference between non-shared disks and shared disks?</h4><ol>
<li>In a shared disk, any write to the shared disk is considered a communication to the external world. Writes to the shared disk must be delayed. </li>
<li>In non-shared disks, the virtual disks are essentially considered part of the internal state of each VM. Disk writes of the primary do not have to be delayed according to the Output Rule. </li>
</ol>
<h4 id="In-what-case-will-a-non-shared-disk-be-useful"><a href="#In-what-case-will-a-non-shared-disk-be-useful" class="headerlink" title="In what case will a non-shared disk be useful?"></a>In what case will a non-shared disk be useful?</h4><ol>
<li>Shared storage is not accessible to the primary and backup VMs. </li>
<li>This may be because shared storage is unavailable, too expensive, or because the servers running the primary and backup VMs are far apart. </li>
</ol>
<h4 id="What-is-the-disadvantage-of-non-shared-disks"><a href="#What-is-the-disadvantage-of-non-shared-disks" class="headerlink" title="What is the disadvantage of non-shared disks?"></a>What is the disadvantage of non-shared disks?</h4><ol>
<li>The two copies of the virtual disks must be explicitly synced up in some manner when fault tolerance is first enabled. </li>
<li>The disks can get out of sync after a failure, so they must be explicitly resynced when the backup VM is restarted after a failure. </li>
</ol>
<h4 id="How-to-solve-the-split-brain-situation"><a href="#How-to-solve-the-split-brain-situation" class="headerlink" title="How to solve the split-brain situation?"></a>How to solve the split-brain situation?</h4><ol>
<li><p>There may be no shared storage to use for dealing with it. The system could use some other external tiebreaker. </p>
<ul>
<li>A third-party server that both servers can talk to. </li>
</ul>
</li>
<li><p>If the servers are part of a cluster with more than two nodes, the system could use a majority algorithm alternatively. </p>
<ul>
<li>A VM would only be allowed to go live if it runs on a server that is part of a communication sub-cluster containing most of the original nodes. </li>
</ul>
</li>
</ol>
<h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><h3 id="Starting-and-restarting"><a href="#Starting-and-restarting" class="headerlink" title="Starting and restarting"></a>Starting and restarting</h3><h4 id="What-requirements-need-to-be-satisfied-by-the-startup-mechanism"><a href="#What-requirements-need-to-be-satisfied-by-the-startup-mechanism" class="headerlink" title="What requirements need to be satisfied by the startup mechanism?"></a>What requirements need to be satisfied by the startup mechanism?</h4><ol>
<li>We also want to use it to restart a backup VM after a failure. Hence, this mechanism must be usable for a running primary VM in an arbitrary state. </li>
<li>We prefer that the mechanism does not significantly disrupt the execution of the primary VM. </li>
</ol>
<h4 id="How-to-implement-the-startup-mechanism"><a href="#How-to-implement-the-startup-mechanism" class="headerlink" title="How to implement the startup mechanism?"></a>How to implement the startup mechanism?</h4><ol>
<li>VMware FT adapted a modified VMware VMotion that allows the migration of a running VM from one server to another with minimal disruption. However, after migration, the VMotion will destroy the local VM. </li>
<li>The FT VMotion clones a VM to a remote host rather than migrating it without destroying the local VM. </li>
<li>The FT VMotion also sets up a logging channel and causes the source VM to enter logging mode as the primary and the destination VM to enter replay mode as the new backup. </li>
</ol>
<h4 id="How-to-choose-a-server-on-which-to-run-the-backup-VM"><a href="#How-to-choose-a-server-on-which-to-run-the-backup-VM" class="headerlink" title="How to choose a server on which to run the backup VM?"></a>How to choose a server on which to run the backup VM?</h4><ol>
<li>The primary VM informs the clustering service that it needs a new backup. </li>
<li>The clustering service determines the best server to run the backup VM based on resource usage and other constraints and invokes an FT VMotion to create the new backup VM. </li>
<li>VMware FT typically can re-establish VM redundancy within minutes of a server failure, all without any noticeable interruption in executing a fault-tolerant VM. </li>
</ol>
<h3 id="Logging-channel"><a href="#Logging-channel" class="headerlink" title="Logging channel"></a>Logging channel</h3><h4 id="How-to-control-primary-sending-log-entries-and-backup-receiving-entries"><a href="#How-to-control-primary-sending-log-entries-and-backup-receiving-entries" class="headerlink" title="How to control primary sending log entries and backup receiving entries?"></a>How to control primary sending log entries and backup receiving entries?</h4><ol>
<li>The hypervisors maintain a large buffer for logging entries for the primary and backup VMs. </li>
<li>The contents of the primary’s log buffer are flushed out to the logging channel as soon as possible, and log entries are read into the backup’s log buffer from the logging channel as soon as they arrive. </li>
<li>The backup sends acknowledgments back to the primary each time that it reads some log entries from the network into its log buffer. </li>
</ol>
<h4 id="What-if-the-primary-log-buffer-is-full"><a href="#What-if-the-primary-log-buffer-is-full" class="headerlink" title="What if the primary log buffer is full?"></a>What if the primary log buffer is full?</h4><ol>
<li>It must stop execution until log entries can be flushed out. </li>
<li>This stop in execution is a natural flow-control mechanism that slows down the primary VM when it produces log entries at a rate that is too fast. </li>
<li>This pause can affect VM clients, and we must minimize the possibility that the primary log buffer fills up. </li>
</ol>
<h4 id="What-is-the-main-cause-of-the-buffer-of-primary-being-full"><a href="#What-is-the-main-cause-of-the-buffer-of-primary-being-full" class="headerlink" title="What is the main cause of the buffer of primary being full?"></a>What is the main cause of the buffer of primary being full?</h4><ol>
<li>One of the biggest reasons is that the backup VM is executing too slowly and consuming log entries too slowly. </li>
<li>In general, the backup VM must be able to replay an execution at roughly the same speed as the primary VM recording the execution. </li>
<li>The overhead of recording and replaying in VMware deterministic replay is roughly the same. </li>
<li>Suppose the server hosting the backup VM is heavily loaded with other VMs (and hence overcommitted on resources). In that case, the backup VM may not get enough CPU and memory resources to execute as fast as the primary VM. </li>
</ol>
<h4 id="How-to-prevent-the-backup-VM-from-getting-too-far-behind-the-primary"><a href="#How-to-prevent-the-backup-VM-from-getting-too-far-behind-the-primary" class="headerlink" title="How to prevent the backup VM from getting too far behind the primary?"></a>How to prevent the backup VM from getting too far behind the primary?</h4><ol>
<li>When sending acknowledgments, we also send additional information to determine the real-time execution lag between the primary and backup VMs. </li>
<li>Typically, the execution lag is less than 100 milliseconds. </li>
<li>If the backup VM starts having a significant execution lag (e.g., more than 1 second), VMware FT starts slowing down the primary VM by informing the scheduler to give it a slightly small amount of CPU. </li>
<li>Such slowdowns are rare and typically happen only when the system is under extreme stress. </li>
</ol>
<h3 id="Special-operations"><a href="#Special-operations" class="headerlink" title="Special operations"></a>Special operations</h3><h4 id="How-to-deal-with-control-operations"><a href="#How-to-deal-with-control-operations" class="headerlink" title="How to deal with control operations?"></a>How to deal with control operations?</h4><ol>
<li><p>Most control operations should be applied to both machines. </p>
<ul>
<li><p>If the primary VM is explicitly powered off, the backup VM should also be stopped and not attempted to go live. </p>
</li>
<li><p>Any resource management change on the primary should be applied to the backup. </p>
</li>
</ul>
</li>
<li><p>VMotion is the only operation that can be done independently on the primary and backup VMs. </p>
<ul>
<li><p>The primary and backup VMs can be VMotioned independently to other hosts. </p>
</li>
<li><p>VMware FT ensures that neither VM is moved to the server where the other VM is. </p>
</li>
</ul>
</li>
</ol>
<h4 id="How-to-implement-the-VMotion-for-primary-and-backup-VMs"><a href="#How-to-implement-the-VMotion-for-primary-and-backup-VMs" class="headerlink" title="How to implement the VMotion for primary and backup VMs?"></a>How to implement the VMotion for primary and backup VMs?</h4><ol>
<li><p>A normal VMotion requires that all outstanding disk IOs be quiesced just as the final switchover on the VMotion occurs. </p>
</li>
<li><p>For a primary VM, </p>
<ul>
<li><p>The quiescing is easily handled by waiting until the physical IOs are complete and delivering these completions to the VM. </p>
</li>
<li><p>The backup VM must disconnect from the source primary and re-connect to the destination primary at the appropriate time. </p>
</li>
</ul>
</li>
<li><p>For a backup VM, </p>
<ul>
<li><p>There is no easy way to cause all IOs to be completed at any required point since the backup VM must replay the primary VM’s execution and complete IOs at the same execution point. </p>
</li>
<li><p>When a backup VM is at the final switchover point for a VMotion, it requests via the logging channel that the primary VM temporarily quiesce all its IOs. </p>
</li>
</ul>
</li>
</ol>
<h3 id="Issues-for-disk-IOs"><a href="#Issues-for-disk-IOs" class="headerlink" title="Issues for disk IOs"></a>Issues for disk IOs</h3><h4 id="How-many-kinds-of-races-may-occur"><a href="#How-many-kinds-of-races-may-occur" class="headerlink" title="How many kinds of races may occur?"></a>How many kinds of races may occur?</h4><ol>
<li><p>The first kind is caused by several IO operations. </p>
<ul>
<li><p>One reason is that disk operations are non-blocking and can execute in parallel. Simultaneous disk operations access the same disk location, causing races. </p>
</li>
<li><p>The other reason is that DMA is directly to/from the VM’s memory. Simultaneous disk operations access the same memory pages. </p>
</li>
</ul>
</li>
<li><p>The second kind is caused by IO operations and non-IO operations. </p>
<ul>
<li>The disk operations directly access the memory of a VM via DMA. Hence, a disk operation accesses the same memory pages as an application or OS in a VM, causing races. </li>
</ul>
</li>
</ol>
<h4 id="How-to-solve-the-non-determinism-caused-by-several-IO-operations"><a href="#How-to-solve-the-non-determinism-caused-by-several-IO-operations" class="headerlink" title="How to solve the non-determinism caused by several IO operations?"></a>How to solve the non-determinism caused by several IO operations?</h4><p>We should detect any such IO races and force such racing disk operations to execute sequentially in the same way on the primary and backup. </p>
<h4 id="How-to-solve-the-non-determinism-caused-by-IO-operations-and-application-OS"><a href="#How-to-solve-the-non-determinism-caused-by-IO-operations-and-application-OS" class="headerlink" title="How to solve the non-determinism caused by IO operations and application/OS?"></a>How to solve the non-determinism caused by IO operations and application/OS?</h4><ol>
<li><p>We need to temporarily set up page protection on pages that are targets of disk operations. </p>
</li>
<li><p>The page protections result in a trap if the VM happens to access a page that is also the target of an outstanding disk operation, and the VM can be paused until the disk operation completes. </p>
</li>
<li><p>Changing MMU protections on pages is expensive; we use bounce buffers. </p>
<ul>
<li><p>A bounce buffer is a temporary buffer that is the same size as the memory accessed by a disk operation. </p>
</li>
<li><p>A disk read operation is modified to read the specified data to the bounce buffer, and the data is copied to guest memory only as the IO completion is delivered. </p>
</li>
<li><p>For a disk write operation, the data to be sent is first copied to the bounce buffer, and the disk write is modified to write data from the bounce buffer. </p>
</li>
</ul>
</li>
<li><p>The bounce buffer can slow down disk operations, but a noticeable performance loss is not seen. </p>
</li>
</ol>
<h4 id="How-does-the-newly-promoted-primary-VM-handle-those-outstanding-IOs"><a href="#How-does-the-newly-promoted-primary-VM-handle-those-outstanding-IOs" class="headerlink" title="How does the newly-promoted primary VM handle those outstanding IOs?"></a>How does the newly-promoted primary VM handle those outstanding IOs?</h4><ol>
<li>There is no way for the newly-promoted primary VM to be sure if the disk IOs were issued to the disk or completed successfully. </li>
<li>We could send an error completion that indicates that each IO failed since it is acceptable to return an error even if the IO was completed successfully. However, the guest OS might not respond well to errors from its local disk. </li>
<li>We can re-issue the pending IOs during the backup VM go-live process. Because we have eliminated all races and all IOs specify directly which memory and disk blocks are accessed, these disk operations can be re-issued even if they have already been completed successfully. </li>
</ol>
<h3 id="Issues-for-network-IO"><a href="#Issues-for-network-IO" class="headerlink" title="Issues for network IO"></a>Issues for network IO</h3><h4 id="How-to-solve-the-non-determinism-caused-by-asynchronous-updates"><a href="#How-to-solve-the-non-determinism-caused-by-asynchronous-updates" class="headerlink" title="How to solve the non-determinism caused by asynchronous updates?"></a>How to solve the non-determinism caused by asynchronous updates?</h4><ol>
<li><p>In a normal VM, the hypervisor asynchronously updates the state of the virtual machine’s network device. </p>
</li>
<li><p>For FT</p>
<ul>
<li><p>The code that asynchronously updates VM ring buffers with incoming packets has been modified to force the guest to trap them in the hypervisor, which can log the updates and then apply them to the VM. </p>
</li>
<li><p>The code that typically pulls packets out of transmit queues asynchronously is disabled, and instead, transmits are done through a trap to the hypervisor. </p>
</li>
</ul>
</li>
</ol>
<h4 id="How-can-we-optimize-the-network-performance-while-running-FT"><a href="#How-can-we-optimize-the-network-performance-while-running-FT" class="headerlink" title="How can we optimize the network performance while running FT?"></a>How can we optimize the network performance while running FT?</h4><ol>
<li><p>Reduce VM traps and interrupts with clustering optimizations. </p>
<ul>
<li><p>When the VM is streaming data at a sufficient bit rate, the hypervisor can do one transmit trap per group of packets and, in the best case, zero traps since it can transmit the packets as part of receiving new packets. </p>
</li>
<li><p>The hypervisor can reduce the number of interrupts to the VM for incoming packets by only posting the interrupt for a group of packets. </p>
</li>
</ul>
</li>
<li><p>Reduce the delay for transmitted packets. </p>
<ul>
<li><p>The key is to reduce the time required to send a log message to the backup and get an acknowledgment. </p>
</li>
<li><p>It is ensured that sending and receiving log entries and acknowledgments can all be done without any thread context switch. </p>
</li>
<li><p>The VMware vSphere hypervisor allows functions to be registered with the TCP stack that will be called from a deferred execution context (similar to a tasklet in Linux) whenever TCP data is received. </p>
</li>
<li><p>When the primary VM enqueues a packet to be transmitted, we force an immediate log flush of the associated output log entry by scheduling a deferred execution context to do the flush. </p>
</li>
</ul>
</li>
</ol>
<h1 id="Experiments-and-results"><a href="#Experiments-and-results" class="headerlink" title="Experiments and results"></a>Experiments and results</h1><p>One important benchmark is the performance ratio between non-FT and FT systems and logging bandwidth between primary and backup. The performance ratio can show how efficient the FT protocol is, and logging bandwidth is usually a system bottleneck. The author measured them all in the following table. We can see that the FT protocol only decreases the performance by less than 10%</p>
<p><img src="/imgs/Distributed/FTVM/02.png" style="zoom:30%;" /></p>
<p>The typical idle logging bandwidth is 0.5-1.5 Mbits/sec. The idle bandwidth primarily results from recording the delivery of timer interrupts. For a VM with an active workload, the logging bandwidth is dominated by the network and disk inputs that must be sent to the backup - the network packets received and the disk blocks read from the disk. Hence, the logging bandwidth can be much higher than those measured in the table for applications with high network receive or disk read bandwidth. For these kinds of applications, the bandwidth of the logging channel could be a bottleneck. </p>
<p>The author also measured the bandwidth of logging channels with different capacities, as shown below. When FT is enabled to receive workloads, the logging bandwidth is very large since all incoming network packets must be sent to the logging channel. The logging bandwidth is much lower when FT is enabled for transmit workloads. Overall, FT can limit network bandwidths significantly at very high transmit and receive rates, but high absolute rates are still achievable. </p>
<p><img src="/imgs/Distributed/FTVM/03.png" style="zoom:30%;" /></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/GFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/GFS/" class="post-title-link" itemprop="url">GFS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:06:36" itemprop="dateCreated datePublished" datetime="2023-09-26T13:06:36+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-15 16:33:24" itemprop="dateModified" datetime="2024-03-15T16:33:24+08:00">2024-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/gfs.pdf">The Google File System</a></p>
<p>@[toc]</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ol>
<li><strong>Main idea</strong>: </li>
<li><strong>Key findings</strong>: </li>
<li><strong>The system</strong>: </li>
<li><strong>Evaluation</strong>: </li>
</ol>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Contribution: provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. </li>
<li>Difference points in design space.<ul>
<li>This system integrated constant monitoring, error detection, fault tolerance, and automatic recovery. </li>
<li>Files are enormous by traditional standards. Design assumptions and parameters such as I/O operation and block sizes have to be revisited. </li>
<li>Most files are mutated by appending new data rather than overwriting existing data. Random writes within a file are practically nonexistent. Once written, the files are only read, and often only sequentially. </li>
</ul>
</li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h3 id="What-operations-are-supported"><a href="#What-operations-are-supported" class="headerlink" title="What operations are supported?"></a>What operations are supported?</h3><ol>
<li>Usual operations: <code>create</code>, <code>delete</code>, <code>open</code>, <code>close</code>, <code>read</code>, and <code>write</code></li>
<li><code>snapshot</code>: creates a copy of a file or a directory tree at a low cost</li>
<li><code>record append</code>: allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity</li>
</ol>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p><img src="/imgs/Distributed/GFS/01.png" style="zoom:33%;" /></p>
<h4 id="What-does-the-system-consist-of"><a href="#What-does-the-system-consist-of" class="headerlink" title="What does the system consist of?"></a>What does the system consist of?</h4><ol>
<li>It consists of a single <em>master</em> and multiple <em>chunkservers</em> and is accessed by multiple <em>clients</em>. </li>
<li>Files are divided into fixed-size chunks. Each chunk is identified by an immutable and globally unique 64-bit chunk handle assigned by the master at the time of chunk creation. </li>
<li>Chunkservers store chunks on local disks as Linux files and read or write chunk data specified by a chunk handle and byte range. Each chunk is replicated on multiple chunkservers, three by default. </li>
<li>Neither the client nor the chunkserver caches file data. Caches offer little benefit while causing coherence issues. But clients do cache metadata. </li>
</ol>
<h4 id="What-does-the-master-need-to-do"><a href="#What-does-the-master-need-to-do" class="headerlink" title="What does the master need to do?"></a>What does the master need to do?</h4><ol>
<li><p>The master maintains all file system metadata and controls system-wide activities. </p>
<ul>
<li>Metadata includes namespace, access control information, the mapping from files to chunks, and the current locations of chunks. </li>
</ul>
</li>
</ol>
<ul>
<li>System-wide activities include chunk lease management, garbage collection of orphaned chunks, and chunk migration between chunkservers. </li>
</ul>
<ol>
<li><p>The master periodically communicates with each chunkserver in HeartBeat messages to give instructions and collect its state. </p>
</li>
<li><p>Clients interact with the master for metadata operations, but all data-bearing communication goes directly to the chunkservers. </p>
</li>
</ol>
<h4 id="How-can-the-master-be-prevented-from-becoming-a-bottleneck"><a href="#How-can-the-master-be-prevented-from-becoming-a-bottleneck" class="headerlink" title="How can the master be prevented from becoming a bottleneck?"></a>How can the master be prevented from becoming a bottleneck?</h4><ol>
<li><p>The idea is to minimize its involvement in reads and writes. </p>
</li>
<li><p>A client asks the master which chunkservers to contact, caches this information for a limited time, and interacts directly with the chunkservers for subsequent operations. </p>
</li>
</ol>
<h4 id="What-is-the-advantage-of-large-chunk-size"><a href="#What-is-the-advantage-of-large-chunk-size" class="headerlink" title="What is the advantage of large chunk size?"></a>What is the advantage of large chunk size?</h4><ol>
<li><p>Reduce clients’ need to interact with the master because reads and writes on the same chunk require only one initial request to the master for chunk location information. </p>
</li>
<li><p>Reduce network overhead by keeping a persistent TCP connection to the chunkserver over an extended period. </p>
</li>
<li><p>Reduce the size of the metadata stored on the master. </p>
</li>
</ol>
<h4 id="What-is-the-disadvantage-of-large-chunk-size"><a href="#What-is-the-disadvantage-of-large-chunk-size" class="headerlink" title="What is the disadvantage of large chunk size?"></a>What is the disadvantage of large chunk size?</h4><ol>
<li><p>Wasting space due to internal fragmentation. This can be eased through lazy space allocation. </p>
</li>
<li><p>A small file consists of several chunks, perhaps just one. The chunkservers storing those chunks may become hot spots if many clients access the same file. This has not been a major issue. </p>
</li>
</ol>
<h4 id="When-will-the-hot-spot-problem-emerge"><a href="#When-will-the-hot-spot-problem-emerge" class="headerlink" title="When will the hot spot problem emerge?"></a>When will the hot spot problem emerge?</h4><ol>
<li><p>A more common case is that an executable was written to GFS as a single-chunk file and then simultaneously started on hundreds of machines. </p>
</li>
<li><p>We can fix this problem by storing such executables with a higher replication factor. </p>
</li>
</ol>
<h3 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h3><h4 id="How-do-we-define-a-file-region-as-being-consistent-or-defined"><a href="#How-do-we-define-a-file-region-as-being-consistent-or-defined" class="headerlink" title="How do we define a file region as being consistent or defined?"></a>How do we define a file region as being consistent or defined?</h4><ol>
<li><p>A file region is consistent if all clients always see the same data, regardless of which replicas they read from. </p>
</li>
<li><p>A region is defined after a file data mutation if consistent, and clients will see what the mutation writes. </p>
</li>
<li><p>Concurrent successful mutations leave the region undefined but consistent: all clients see the same data, but it may not reflect what any one mutation has written. Typically, it consists of mingled fragments from multiple mutations. </p>
</li>
</ol>
<h4 id="How-many-consistency-rules-should-be-considered"><a href="#How-many-consistency-rules-should-be-considered" class="headerlink" title="How many consistency rules should be considered?"></a>How many consistency rules should be considered?</h4><ol>
<li>File namespace mutations (e.g., file creation) are atomic. </li>
<li>After a sequence of successful mutations, the mutated file region is guaranteed to be defined and contain the data written by the last mutation. </li>
</ol>
<h4 id="How-does-GFS-guarantee-the-second-rule"><a href="#How-does-GFS-guarantee-the-second-rule" class="headerlink" title="How does GFS guarantee the second rule?"></a>How does GFS guarantee the second rule?</h4><ol>
<li>Applying mutations to a chunk in the same order on all its replicas. </li>
<li>Using chunk version numbers to detect any replica that has become stale because it has missed mutations while its chunkserver was down. </li>
<li>Stale replicas will never be involved in a mutation or given to clients asking the master for chunk locations. They are garbage collected at the earliest opportunity. </li>
</ol>
<h4 id="What-is-the-side-effect-of-clients-caching-chunk-locations"><a href="#What-is-the-side-effect-of-clients-caching-chunk-locations" class="headerlink" title="What is the side-effect of clients caching chunk locations?"></a>What is the side-effect of clients caching chunk locations?</h4><ol>
<li>They may read from a stale replica before that information is refreshed.  </li>
<li>This window is limited by the cache entry’s timeout and the next opening of the file. </li>
<li>As most files are append-only, a stale replica usually returns a premature end of chunk rather than outdated data. </li>
</ol>
<h2 id="Data-mutation"><a href="#Data-mutation" class="headerlink" title="Data mutation"></a>Data mutation</h2><h3 id="Control-amp-data-flow"><a href="#Control-amp-data-flow" class="headerlink" title="Control &amp; data flow"></a>Control &amp; data flow</h3><h4 id="How-do-we-minimize-management-overhead-at-the-master-of-data-mutation"><a href="#How-do-we-minimize-management-overhead-at-the-master-of-data-mutation" class="headerlink" title="How do we minimize management overhead at the master of data mutation?"></a>How do we minimize management overhead at the master of data mutation?</h4><ol>
<li>We use leases to maintain a consistent mutation order across replicas. </li>
<li>The master grants a chunk lease to one of the replicas, which we call the primary. The primary picks a serial order for all mutations to the chunk. All replicas follow this order when mutations are applied. </li>
<li>The client caches who leases a certain chunk for future mutations. It needs to contact the master again only when the primary becomes unreachable or replies that it no longer holds a lease. </li>
</ol>
<h4 id="What-do-leases-change"><a href="#What-do-leases-change" class="headerlink" title="What do leases change?"></a>What do leases change?</h4><ol>
<li>A lease has an initial timeout of 60 seconds. However, as long as the chunk is being mutated, the primary can request and typically receive extensions from the master indefinitely. </li>
<li>These extension requests and grants are piggybacked on the HeartBeat messages regularly exchanged between the master and all chunkservers. </li>
<li>The master may sometimes try to revoke a lease before it expires (e.g. when the master wants to disable mutations on a file that is being renamed). </li>
<li>Even if the master loses communication with a primary, it can safely grant a new lease to another replica after the old lease expires. </li>
</ol>
<h4 id="How-does-the-control-flow"><a href="#How-does-the-control-flow" class="headerlink" title="How does the control flow?"></a>How does the control flow?</h4><ol>
<li><p>The client asks the master which chunkserver holds the current lease for the chunk and the locations of the other replicas. If no one has a lease, the master grants one to a replica it chooses</p>
</li>
<li><p>The master replies with the identity of the primary and the locations of the other (secondary) replicas. </p>
</li>
<li><p>The client pushes the data to all the replicas in any order instead of only sending it to the lease. </p>
</li>
<li><p>Once all the replicas have acknowledged receiving the data, the client sends a write request to the primary. </p>
</li>
<li><p>The primary forwards the write request to all secondary replicas. </p>
</li>
<li><p>The secondaries reply to the primary, indicating they have completed the operation. </p>
</li>
<li><p>The primary replies to the client. Any errors encountered at any of the replicas are reported to the client. </p>
<p><img src="/imgs/Distributed/GFS/02.png" style="zoom:25%;" /></p>
</li>
</ol>
<h4 id="How-do-primary-and-secondary-servers-write-data"><a href="#How-do-primary-and-secondary-servers-write-data" class="headerlink" title="How do primary and secondary servers write data?"></a>How do primary and secondary servers write data?</h4><ol>
<li><p>Each chunkserver will store the data from the client in an internal LRU buffer cache until the data is used or aged out. </p>
</li>
<li><p>The write request from the client to primary identifies the data pushed earlier to all of the replicas. </p>
</li>
<li><p>The primary assigns consecutive serial numbers to all the mutations it receives, possibly from multiple clients, which provides the necessary serialization. </p>
</li>
<li><p>The primary applies the mutation to its local state in serial number order. </p>
</li>
</ol>
<h4 id="What-would-the-system-do-if-the-write-fails"><a href="#What-would-the-system-do-if-the-write-fails" class="headerlink" title="What would the system do if the write fails?"></a>What would the system do if the write fails?</h4><ol>
<li><p>It would not have been assigned a serial number and forwarded if it had failed at the primary. </p>
</li>
<li><p>In other cases, the write may have succeeded at the primary and an arbitrary subset of the secondary replicas. The client request is considered to have failed, and the modified region is left in an inconsistent state. </p>
</li>
<li><p>The client code handles such errors by retrying the failed mutation. It will make a few attempts at steps 3 through 7 before falling back to a retry from the beginning of the write. </p>
</li>
</ol>
<h4 id="What-if-a-write-is-large-or-straddles-a-chunk-boundary"><a href="#What-if-a-write-is-large-or-straddles-a-chunk-boundary" class="headerlink" title="What if a write is large or straddles a chunk boundary?"></a>What if a write is large or straddles a chunk boundary?</h4><ol>
<li>GFS client code breaks it down into multiple write operations. </li>
<li>They all follow the control flow described above but may be interleaved with and overwritten by concurrent operations from other clients. </li>
<li>The shared file region may end up containing fragments from different clients. However, the replicas will be identical because the individual operations are completed successfully in the same order on all replicas. </li>
</ol>
<h4 id="How-to-prevent-the-primary-become-bottleneck-of-pushing-data"><a href="#How-to-prevent-the-primary-become-bottleneck-of-pushing-data" class="headerlink" title="How to prevent the primary become bottleneck of pushing data?"></a>How to prevent the primary become bottleneck of pushing data?</h4><ol>
<li>While control flows from the client to the primary and then to all secondaries, data is pushed linearly along a carefully picked chain of chunkservers in a pipelined fashion. </li>
<li>Decoupling the data flow from the control flow can improve performance by scheduling the expensive data flow based on the network topology, regardless of which chunkserver is the primary. </li>
<li>We aim to fully utilize each machine’s network bandwidth, avoid network bottlenecks and high-latency links, and minimize the latency to push through all the data. </li>
<li>Each machine forwards the data to the “closest” machine in the network topology that has not received it. </li>
<li>Our network topology is simple enough that “distances” can be accurately estimated from IP addresses.</li>
</ol>
<h4 id="How-to-minimize-the-latency-of-pushing-data"><a href="#How-to-minimize-the-latency-of-pushing-data" class="headerlink" title="How to minimize the latency of pushing data?"></a>How to minimize the latency of pushing data?</h4><ol>
<li>We minimize latency by pipelining the data transfer over TCP connections. Once a chunkserver receives some data, it starts forwarding immediately. </li>
<li>Pipelining is especially helpful because we use a switched network with full-duplex links. Sending the data immediately does not reduce the receive rate. </li>
</ol>
<h3 id="Write-and-record-append"><a href="#Write-and-record-append" class="headerlink" title="Write and record append"></a>Write and record append</h3><h4 id="What-is-the-difference-between-write-and-record-append"><a href="#What-is-the-difference-between-write-and-record-append" class="headerlink" title="What is the difference between write and record append?"></a>What is the difference between write and record append?</h4><ol>
<li><p>A write causes data to be written at an application-specified file offset. </p>
</li>
<li><p>A record append causes data (the “record”) to be appended atomically at least once, even in the presence of concurrent mutations, but at an offset of GFS’s choosing. </p>
<ul>
<li><p>The offset is returned to the client and marks the beginning of a defined region that contains the record. </p>
</li>
<li><p>GFS may insert padding or record duplicates in between. They occupy regions considered inconsistent and are typically dwarfed by the amount of user data. </p>
</li>
</ul>
</li>
<li><p>A “regular” append is merely a write at an offset that the client believes to be the current end of the file. </p>
</li>
</ol>
<h4 id="How-does-typical-writing-happen"><a href="#How-does-typical-writing-happen" class="headerlink" title="How does typical writing happen?"></a>How does typical writing happen?</h4><ol>
<li>A writer generates a file from beginning to end. After writing all the data, it atomically renames the file to a permanent name or periodically checkpoints how much has been successfully written. </li>
<li>Checkpoints may also include application-level checksums. Readers verify and process only the file region up to the last checkpoint, which is in the defined state. </li>
<li>Checkpointing allows writers to restart incrementally and keeps readers from processing successfully written file data that is still incomplete. </li>
</ol>
<h4 id="How-do-readers-deal-with-occasional-padding-and-duplicates"><a href="#How-do-readers-deal-with-occasional-padding-and-duplicates" class="headerlink" title="How do readers deal with occasional padding and duplicates?"></a>How do readers deal with occasional padding and duplicates?</h4><ol>
<li>Each record prepared by the writer contains extra information like checksums to verify its validity. </li>
<li>A reader can identify and discard extra padding and record fragments using the checksums. </li>
<li>Suppose it cannot tolerate the occasional duplicates. In that case, it can filter them out using unique identifiers in the records, often needed to name corresponding application entities such as web documents. </li>
</ol>
<h3 id="Atomic-record-appends"><a href="#Atomic-record-appends" class="headerlink" title="Atomic record appends"></a>Atomic record appends</h3><h4 id="What-if-appending-causes-the-current-chunk-to-exceed-the-maximum-size"><a href="#What-if-appending-causes-the-current-chunk-to-exceed-the-maximum-size" class="headerlink" title="What if appending causes the current chunk to exceed the maximum size?"></a>What if appending causes the current chunk to exceed the maximum size?</h4><ol>
<li>The primary checks if appending the record to the current chunk would cause the chunk to exceed the maximum size. </li>
<li>If so, it pads the chunk to the maximum size, tells secondaries to do the same, and replies to the client, indicating that the operation should be retried on the next chunk. </li>
<li>Suppose the record fits within the maximum size. In that case, the typical case is that the primary appends the data to its replica tells the secondaries to write the data at the exact offset where it has been, and finally replies successfully to the client. </li>
</ol>
<h4 id="What-if-appending-fails-at-some-chunkservers"><a href="#What-if-appending-fails-at-some-chunkservers" class="headerlink" title="What if appending fails at some chunkservers?"></a>What if appending fails at some chunkservers?</h4><ol>
<li>Replicas of the same chunk may contain different data, possibly including duplicates of the same record as a whole or in part. </li>
<li>GFS does not guarantee that all replicas are bytewise identical. It only guarantees that the data is written at least once as an atomic unit. </li>
<li>The primary has succeeded if any secondary chunkserver can successfully append the record. Next time, the primary can choose an offset after the failed record. </li>
<li>Hence, after this, all replicas are at least as long as the end of the record, and therefore, any future record will be assigned a higher offset or a different chunk, even if a different replica later becomes the primary. </li>
</ol>
<h3 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h3><h4 id="What-does-snapshot-do"><a href="#What-does-snapshot-do" class="headerlink" title="What does snapshot do?"></a>What does snapshot do?</h4><ol>
<li>The snapshot operation makes a copy of a file or a directory tree (the “source”) almost instantaneously while minimizing any interruptions of ongoing mutations. </li>
<li>Users use it to quickly create branch copies of massive data sets (often copies of those copies, recursively) or to checkpoint the current state before experimenting with changes that can later be committed or rolled back easily. </li>
</ol>
<h4 id="How-does-snapshot-be-implemented"><a href="#How-does-snapshot-be-implemented" class="headerlink" title="How does snapshot be implemented?"></a>How does snapshot be implemented?</h4><ol>
<li><p>It uses standard copy-on-write techniques. </p>
</li>
<li><p>Master revokes leases on the chunks in the files it is about to snapshot. </p>
<ul>
<li><p>This ensures that any subsequent writes to these chunks will require an interaction with the master to find the leaseholder. </p>
</li>
<li><p>This will give the master an opportunity to create a new copy of the chunk first.</p>
</li>
</ul>
</li>
<li><p>The master logs the operation to disk. </p>
</li>
<li>It then applies this log record to its in-memory state by duplicating the metadata for the source file or directory tree. The newly created snapshot files point to the same chunks as the source files. </li>
</ol>
<h4 id="How-do-clients-write-a-chunk-after-a-snapshot"><a href="#How-do-clients-write-a-chunk-after-a-snapshot" class="headerlink" title="How do clients write a chunk after a snapshot?"></a>How do clients write a chunk after a snapshot?</h4><ol>
<li>The first time a client wants to write to a chunk C after the snapshot operation, it sends a request to the master to find the current lease holder. </li>
<li>The master notices that the reference count for chunk C is greater than one. It defers replying to the client request and instead picks a new chunk handle C’. </li>
<li>It then asks each chunkserver that has a current replica of C to create a new chunk called C’. </li>
<li>By creating the new chunk on the same chunkservers as the original, we ensure that the data can be copied locally, not over the network. </li>
<li>The master grants one of the replicas a lease on the new chunk C’ and replies to the client, which can write the chunk normally. </li>
</ol>
<h2 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h2><h3 id="Basic-operations"><a href="#Basic-operations" class="headerlink" title="Basic operations"></a>Basic operations</h3><h4 id="How-does-the-client-communicate-with-the-master-specifically"><a href="#How-does-the-client-communicate-with-the-master-specifically" class="headerlink" title="How does the client communicate with the master specifically?"></a>How does the client communicate with the master specifically?</h4><ol>
<li>The client translates the file name and byte offset specified by the application into a chunk index within the file. </li>
<li>It sends the master a request containing the file name and chunk index. </li>
<li>The master replies with the corresponding chunk handle and locations of the replicas. </li>
<li>The client caches this information using the file name and chunk index as the key. </li>
</ol>
<h4 id="What-metadata-does-the-master-need-to-store"><a href="#What-metadata-does-the-master-need-to-store" class="headerlink" title="What metadata does the master need to store?"></a>What metadata does the master need to store?</h4><ol>
<li><p>Stored persistently: the file and chunk namespace, the mapping from files to chunks</p>
<ul>
<li><p>The master will scan periodically through its entire state in the background.</p>
</li>
<li><p>Periodic scanning is to implement chunk garbage collection, re-replication in the presence of chunkserver failures, and chunk migration to balance load and disk space usage across chunkservers. </p>
</li>
<li><p>The number of chunks and, hence, the whole system’s capacity is limited by how much memory the master has. But not a serious limitation for less than $64$ bytes of metadata for each chunk. </p>
</li>
</ul>
</li>
<li><p>There is no need to persistently store the locations of each chunk’s replicas. </p>
<ul>
<li><p>The master asks each chunkserver about its chunks at master startup and whenever a chunkserver joins the cluster. </p>
</li>
<li><p>The master can keep itself up-to-date after that because it controls all chunk placement and monitors chunkserver status. </p>
</li>
</ul>
</li>
<li><p>Operation record</p>
<ul>
<li><p>The namespace and mapping are persistent by logging mutations into the operation log. </p>
</li>
<li><p>It is stored on the master’s local disk and replicated on remote machines.</p>
</li>
</ul>
</li>
</ol>
<h4 id="How-to-persist"><a href="#How-to-persist" class="headerlink" title="How to persist?"></a>How to persist?</h4><ol>
<li>Operation log<ul>
<li>The operation log contains a historical record of critical metadata changes. It is the only persistent record of critical metadata and serves as a logical timeline that defines the order of concurrent operations. </li>
<li>The system responds to a client operation only after flushing the corresponding log record locally and remotely to disk. </li>
<li>The master batches several log records together before flushing, reducing the impact of flushing and replication on overall system thoughput. </li>
</ul>
</li>
<li>Checkpoint<ul>
<li>To minimize startup time, we must keep the log small. The master checkpoints its state whenever the log grows beyond a certain size. It can recover by loading the latest checkpoint and replaying only the records afterward. </li>
<li>The checkpoint is in a compact B-tree-like form. </li>
<li>The master switches to a new log file and creates the latest checkpoint in a separate thread. </li>
<li>Older checkpoints and log files can be freely deleted, though we keep a few around to guard against catastrophes. A failure during checkpointing does not affect correctness because the recovery code detects and skips incomplete checkpoints. </li>
</ul>
</li>
</ol>
<h4 id="How-does-GFS-manage-namespace"><a href="#How-does-GFS-manage-namespace" class="headerlink" title="How does GFS manage namespace?"></a>How does GFS manage namespace?</h4><ol>
<li>GFS does not have a per-directory data structure that lists all the files in that directory. Nor does it support aliases for the same file or directory. </li>
<li>GFS logically represents its namespace as a lookup table mapping full pathnames to metadata. With prefix compression, this table can be efficiently represented in memory. </li>
<li>Each node in the namespace tree (an absolute file name or an absolute directory name) has an associated read-write lock. </li>
</ol>
<h4 id="How-does-GFS-design-the-locking-scheme"><a href="#How-does-GFS-design-the-locking-scheme" class="headerlink" title="How does GFS design the locking scheme?"></a>How does GFS design the locking scheme?</h4><ol>
<li>Suppose a master operation involves a specific file or directory. In that case, it will acquire read-locks on all the parent directories and either a read-lock or a write-lock on the leaf file or directory that it will operate directly. </li>
<li>File creation does not require a write lock on the parent directory because there is no “directory” or inode-like data structure to be protected from modification. </li>
<li>This locking scheme allows concurrent mutations in the same directory. </li>
</ol>
<h3 id="Replica-management"><a href="#Replica-management" class="headerlink" title="Replica management"></a>Replica management</h3><h4 id="How-to-place-replicas"><a href="#How-to-place-replicas" class="headerlink" title="How to place replicas?"></a>How to place replicas?</h4><ol>
<li>There are two purposes: maximize data reliability and availability and maximize network bandwidth utilization. </li>
<li>It is not enough to spread replicas across machines, which only guards against disk or machine failures and fully utilizes each machine’s network bandwidth. </li>
<li>We must also spread chunk replicas across racks. This ensures that some replicas of a chunk will survive and remain available even if an entire rack is damaged or offline. It also means that traffic, especially reads, for a chunk can exploit the aggregate bandwidth of multiple racks. </li>
<li>On the other hand, write traffic has to flow through multiple racks, a tradeoff we make willingly. </li>
<li>An even safer way is to spread across data centers in different cities. It can guard against a city-level catastrophe. </li>
</ol>
<h4 id="What-factors-are-considered-when-creating-a-new-chunk"><a href="#What-factors-are-considered-when-creating-a-new-chunk" class="headerlink" title="What factors are considered when creating a new chunk?"></a>What factors are considered when creating a new chunk?</h4><ol>
<li>We want to place new replicas on chunkservers with below-average disk space utilization. Over time, this will equalize disk utilization across chunkservers. </li>
<li>We want to limit the number of “recent” creations on each chunkserver. Although creation itself is cheap, it reliably predicts imminent heavy write traffic because chunks are created when demanded by writes. </li>
<li>We want to spread replicas of a chunk across racks. </li>
</ol>
<h4 id="What-if-the-number-of-available-chunk-replicas-falls-below-a-user-specified-goal"><a href="#What-if-the-number-of-available-chunk-replicas-falls-below-a-user-specified-goal" class="headerlink" title="What if the number of available chunk replicas falls below a user-specified goal?"></a>What if the number of available chunk replicas falls below a user-specified goal?</h4><ol>
<li><p>The master would re-replicate the chunk. </p>
</li>
<li><p>Suppose there are many chunks below their goal. In that case, the master picks the highest priority chunk considering some factors and “clones” it by instructing some chunkserver to copy the chunk data directly from an existing valid replica. </p>
<ul>
<li><p>How far it is from its replication goal. </p>
</li>
<li><p>Prefer to re-replicate chunks for live files instead of chunks belonging to recently deleted files. </p>
</li>
<li><p>To minimize the impact of failures on running applications, we boost the priority of any chunk blocking client progress. </p>
</li>
</ul>
</li>
</ol>
<h4 id="What-if-cloning-traffic-from-overwhelming-client-traffic"><a href="#What-if-cloning-traffic-from-overwhelming-client-traffic" class="headerlink" title="What if cloning traffic from overwhelming client traffic?"></a>What if cloning traffic from overwhelming client traffic?</h4><ol>
<li>The master limits the number of active clone operations for the cluster and each chunkserver. </li>
<li>Each chunkserver limits its bandwidth on each clone operation by throttling its read requests to the source chunkserver.</li>
</ol>
<h4 id="How-to-keep-the-placement-of-replicas-in-balance"><a href="#How-to-keep-the-placement-of-replicas-in-balance" class="headerlink" title="How to keep the placement of replicas in balance?"></a>How to keep the placement of replicas in balance?</h4><ol>
<li>It examines the current replica distribution and moves replicas for better disk space and load balancing. </li>
<li>The master gradually fills up a new chunkserver rather than instantly swamps it with new chunks and the heavy write traffic that comes with them. </li>
<li>The master must also choose which existing replica to remove. It prefers to remove those on chunkservers with below-average free space. </li>
</ol>
<h3 id="Deletion"><a href="#Deletion" class="headerlink" title="Deletion"></a>Deletion</h3><h4 id="How-to-delete-a-file"><a href="#How-to-delete-a-file" class="headerlink" title="How to delete a file?"></a>How to delete a file?</h4><ol>
<li><p>GFS does not immediately reclaim the available physical storage; it is just renamed to a hidden name that includes the deletion timestamp. It does so only lazily during regular garbage collection at both the file and chunk levels. </p>
</li>
<li><p>During the master’s regular scan of the file system namespace, any such hidden files are removed if they have existed for more than three days (the interval is configurable). </p>
<ul>
<li><p>Until then, the file can still be read under the new, special name and undeleted by renaming it to normal. </p>
</li>
<li><p>When the hidden file is removed from the namespace, its in-memory metadata is erased. This effectively severs its links to all its chunks.</p>
</li>
</ul>
</li>
<li><p>In a similar regular scan of the chunk namespace, the master identifies orphaned chunks (i.e., those not reachable from any file) and erases the metadata for those chunks. </p>
<ul>
<li><p>In a HeartBeat message exchanged with the master, each chunkserver reports a subset of the chunks it has, and the master replies with the identity of all chunks that are no longer present in the master’s metadata. </p>
</li>
<li><p>The chunkserver is free to delete its replicas of such chunks. </p>
</li>
</ul>
</li>
</ol>
<h4 id="What-are-the-advantages-and-disadvantages-of-lazy-deletion-over-eager-deletion"><a href="#What-are-the-advantages-and-disadvantages-of-lazy-deletion-over-eager-deletion" class="headerlink" title="What are the advantages and disadvantages of lazy deletion over eager deletion?"></a>What are the advantages and disadvantages of lazy deletion over eager deletion?</h4><ol>
<li><p>It is simple and reliable in a large-scale distributed system where component failures are common. </p>
</li>
<li><p>It merges storage reclamation into the master’s regular background activities. </p>
</li>
<li><p>The delay in reclaiming storage protects against accidental, irreversible deletion. </p>
</li>
<li><p>The delay sometimes hinders user effort to fine-tune usage when storage is tight. </p>
</li>
<li><p>Applications that repeatedly create and delete temporary files may be unable to reuse the storage immediately. </p>
</li>
</ol>
<h4 id="How-to-address-the-issues-of-reusing"><a href="#How-to-address-the-issues-of-reusing" class="headerlink" title="How to address the issues of reusing?"></a>How to address the issues of reusing?</h4><ol>
<li><p>Expediting storage reclamation if a deleted file is explicitly deleted again. </p>
</li>
<li><p>Allow users to apply different replication and reclamation policies to different namespace parts. </p>
</li>
</ol>
<h4 id="How-to-handle-the-possible-stale-replicas"><a href="#How-to-handle-the-possible-stale-replicas" class="headerlink" title="How to handle the possible stale replicas?"></a>How to handle the possible stale replicas?</h4><ol>
<li><p>For each chunk, the master maintains a chunk version number to distinguish between up-to-date and stale replicas. </p>
</li>
<li><p>Whenever the master grants a new lease on a chunk, the chunk version number increases and informs the up-to-date replicas. This occurs before any client is notified before it can start writing to the chunk. </p>
</li>
<li><p>If one replica is unavailable, its chunk version number will not be advanced. The master will detect that this chunkserver has a stale replica when the chunkserver restarts and reports its set of chunks and associated version numbers. </p>
</li>
<li><p>The master removes stale replicas in its regular garbage collection. Before that, it effectively considers a stale replica not to exist when it replies to client requests for chunk information. </p>
</li>
<li>The master includes the chunk version number when it informs clients which chunkserver holds a lease on a chunk or instructs a chunkserver to read the chunk from another chunkserver in a cloning operation. </li>
</ol>
<h2 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault tolerance"></a>Fault tolerance</h2><h3 id="How-to-handle-master-failure"><a href="#How-to-handle-master-failure" class="headerlink" title="How to handle master failure?"></a>How to handle master failure?</h3><ol>
<li><p>The master state is replicated for reliability. </p>
<ul>
<li><p>When it fails, it can restart almost instantly. </p>
</li>
<li><p>When its machine or disk fails, monitoring infrastructure outside GFS starts a new master process elsewhere with the replicated operation log. </p>
</li>
<li><p>Clients use only the canonical name of the master, which is a DNS alias that can be changed if the master is relocated to another machine.</p>
</li>
</ul>
</li>
<li><p>“Shadow” masters provide read-only access to the file system even when the primary master is down. </p>
<ul>
<li><p>They enhance read availability for files not being actively mutated or applications that do not mind getting slightly stale results. </p>
</li>
<li><p>Since file content is read from chunkservers, applications do not observe stale content. What could be stale within short windows is file metadata. </p>
</li>
<li><p>To keep itself informed, a shadow master reads a replica of the growing operation log and applies the same sequence of changes to its data structures exactly as the primary does.</p>
</li>
<li><p>It depends on the primary master only for replica location updates resulting from the primary’s decisions to create and delete replicas.</p>
</li>
</ul>
</li>
</ol>
<h3 id="Why-cannot-recover-data-using-other-chunk-replicas-Why-each-chunkserver-must-independently-verify-the-integrity"><a href="#Why-cannot-recover-data-using-other-chunk-replicas-Why-each-chunkserver-must-independently-verify-the-integrity" class="headerlink" title="Why cannot recover data using other chunk replicas? Why each chunkserver must independently verify the integrity?"></a>Why cannot recover data using other chunk replicas? Why each chunkserver must independently verify the integrity?</h3><ol>
<li><p>It would be impractical to detect corruption by comparing replicas across chunkservers. </p>
</li>
<li><p>Divergent replicas may be legal: the semantics of GFS mutations, in particular atomic record append, do not guarantee identical replicas. </p>
</li>
</ol>
<h3 id="How-to-ensure-data-integrity"><a href="#How-to-ensure-data-integrity" class="headerlink" title="How to ensure data integrity?"></a>How to ensure data integrity?</h3><ol>
<li><p>Each chunkserver uses checksumming to detect corruption of stored data. A chunk is broken up into 64 KB blocks. Each has a corresponding 32-bit checksum. </p>
</li>
<li><p>Checksums are kept in memory and stored persistently with logging, separate from user data. </p>
</li>
<li><p>During idle periods, chunkservers can scan and verify the contents of inactive chunks. </p>
</li>
</ol>
<h3 id="How-to-read-data-with-checksum"><a href="#How-to-read-data-with-checksum" class="headerlink" title="How to read data with checksum?"></a>How to read data with checksum?</h3><ol>
<li><p>The chunkserver verifies the checksum of data blocks that overlap the read range before returning any data to the requester, whether a client or another chunkserver. </p>
</li>
<li><p>If a block does not match the recorded checksum, the chunkserver returns an error to the requestor and reports the mismatch to the master. </p>
</li>
<li><p>In response, the requestor will read from other replicas, while the master will clone the chunk from another replica. </p>
</li>
<li><p>After a valid new replica is in place, the master instructs the chunkserver that reported the mismatch to delete its replica. </p>
</li>
</ol>
<h3 id="How-to-write-data-with-checksum"><a href="#How-to-write-data-with-checksum" class="headerlink" title="How to write data with checksum?"></a>How to write data with checksum?</h3><ol>
<li><p>For writes that append to the end of a chunk, we incrementally update the checksum for the last partial checksum block and compute new checksums for any new checksum blocks filled by the append. </p>
<ul>
<li>Even if the last partial checksum block is already corrupted and we fail to detect it now, the new checksum value will not match the stored data, and the corruption will be detected as usual when the block is next read.</li>
</ul>
</li>
<li><p>If a write overwrites an existing range of the chunk, we must read and verify the first and last blocks of the range being overwritten, then perform the write, and finally compute and record the new checksums. </p>
<ul>
<li>If we do not verify the first and last blocks before partially overwriting them, the new checksums may hide corruption in the regions not being overwritten. </li>
</ul>
</li>
</ol>
<h3 id="What-is-included-in-the-diagnostic-logs"><a href="#What-is-included-in-the-diagnostic-logs" class="headerlink" title="What is included in the diagnostic logs?"></a>What is included in the diagnostic logs?</h3><ol>
<li><p>GFS servers generate diagnostic logs that record significant events (such as chunkservers going up and down) and all RPC requests and replies. </p>
</li>
<li><p>The RPC logs include the exact requests and responses sent on the wire, except for the file data being read or written. </p>
</li>
</ol>
<h2 id="Other-parts-unmentioned"><a href="#Other-parts-unmentioned" class="headerlink" title="Other parts (unmentioned)"></a>Other parts (unmentioned)</h2><h3 id="To-sum-up-what-is-the-metadata-of-the-master-and-where-are-they"><a href="#To-sum-up-what-is-the-metadata-of-the-master-and-where-are-they" class="headerlink" title="To sum up, what is the metadata of the master, and where are they?"></a>To sum up, what is the metadata of the master, and where are they?</h3><ol>
<li><p>File name: this is an array of chunk handles. It is stored on disk. </p>
</li>
<li><p>Handle: it contains a list of chunkservers, version number, primary, and lease expiration. </p>
<ul>
<li><p>Only the version number is stored on disk; the rest can be restored by asking chunkservers when the master is recovered. </p>
</li>
<li><p>Given that there might be stale chunks, we cannot ask chunkservers for the version number of a chunk. </p>
</li>
</ul>
</li>
<li><p>Lops and checkpoints are stored on disk. </p>
</li>
</ol>
<h3 id="What-is-the-cause-of-a-split-brain-How-to-solve-it"><a href="#What-is-the-cause-of-a-split-brain-How-to-solve-it" class="headerlink" title="What is the cause of a split-brain? How to solve it?"></a>What is the cause of a split-brain? How to solve it?</h3><ol>
<li>The split brain is caused by network partition; the master cannot talk to the primary, while the primary can talk to clients. Hence the master mistakingly designates two primary for the same chunk. </li>
<li>The master knows when the lease will expire, so when the master cannot talk to the primary, it will wait until the lease expires before assigning another primary. </li>
</ol>
<h3 id="Why-does-GFS-not-overwrite-those-failed-records-immediately-but-leave-padding-and-duplicates"><a href="#Why-does-GFS-not-overwrite-those-failed-records-immediately-but-leave-padding-and-duplicates" class="headerlink" title="Why does GFS not overwrite those failed records immediately but leave padding and duplicates?"></a>Why does GFS not overwrite those failed records immediately but leave padding and duplicates?</h3><p>When it starts to write the next record, it may not know the fate of the prior record. </p>
<h3 id="GFS-is-a-weak-consistency-system-how-can-we-upgrade-it-to-a-strong-one"><a href="#GFS-is-a-weak-consistency-system-how-can-we-upgrade-it-to-a-strong-one" class="headerlink" title="GFS is a weak consistency system; how can we upgrade it to a strong one?"></a>GFS is a weak consistency system; how can we upgrade it to a strong one?</h3><ol>
<li>Primary detects duplicate requests to ensure the failed write doesn’t show up twice.</li>
<li>When the primary asks a secondary to do something, the secondary actually does it and doesn’t just return an error (except the secondary has permanent damage, in which case, it should be removed)</li>
<li>The secondary only exposes data to readers once the primary is sure that all the secondaries will execute the append. </li>
<li>When the primary crashes, there will be some last set of operations that the primary launched for the secondaries. Still, the primary crashed before ensuring all operations are done—the new primary needs to explicitly resync with all secondaries. </li>
</ol>
<h1 id="Experiments-and-results"><a href="#Experiments-and-results" class="headerlink" title="Experiments and results"></a>Experiments and results</h1><h2 id="Micro-benchmarks"><a href="#Micro-benchmarks" class="headerlink" title="Micro-benchmarks"></a>Micro-benchmarks</h2><p>The author first tested several micro-benchmarks, i.e., reads, writes, and record appends. These tests are that $N$ clients do those operations simultaneously. For reading, they read from the file system; for writing, they write to distinct files; for appending, they append to a single file. And test the aggregate throughputs of the system, comparing them with the network limit. The results are as follows: </p>
<p><img src="/imgs/Distributed/GFS/03.png" alt=""></p>
<h3 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h3><p>The reading efficiency drops because as the number of readers increases, so does the probability that multiple readers simultaneously read from the same chunkserver. </p>
<h3 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h3><p>The limit plateaus of the write rate is 67 MB/s because we need to write each byte to 3 of the 16 chunkservers, each with a 12.5 MB/s input connection. </p>
<p>The network stack is the main culprit for a low write rate with only one client. It does not interact well with the pipelining scheme we use to push data to chunk replicas. Delays in propagating data from one replica to another reduce the overall write rate. </p>
<p>As the number of clients grows, it becomes more likely that multiple clients will write concurrently to the same chunkserver as the number of clients increases. Moreover, the collision has more potential for 16 writers than 16 readers because each involves three replicas. </p>
<p>Writes are slower than we would like. In practice, this has not been a major problem because even though it increases the latencies seen by individual clients, it does not significantly affect the aggregate write bandwidth delivered by the system to a large number of clients. </p>
<h3 id="Record-append"><a href="#Record-append" class="headerlink" title="Record append"></a>Record append</h3><p>The performance of record appends is limited by the network bandwidth of the chunkservers that store the last chunk of the file, independent of the number of clients. </p>
<p>The append rate drops primarily due to congestion and variances in network transfer rates seen by different clients. </p>
<p>The chunkserver network congestion in our experiment is not a significant issue in practice because a client can progress on writing one file. In contrast, the chunkservers for another file are busy. </p>
<h2 id="Real-world-clusters"><a href="#Real-world-clusters" class="headerlink" title="Real-world clusters"></a>Real-world clusters</h2><p>The author also measured the performance of real-world clusters. First, the author measured their storage usage and the size of metadata. Then, the read rate, write rate, and the rate of operations sent to the master were measured. The results show that the master can easily keep up with this rate and, therefore, is not a bottleneck for these workloads. </p>
<p><img src="/imgs/Distributed/GFS/04.png" style="zoom: 33%;" /><img src="/imgs/Distributed/GFS/05.png" style="zoom: 31%;" /></p>
<p>After a chunkserver fails, some chunks become underreplicated and must be cloned to restore their replication levels. To test the recovery time, the author killed a single chunkserver containing 15000 chunks of 600 GB data. </p>
<p>To limit the impact on running applications and provide leeway for scheduling decisions, our default parameters limit this cluster to 91 concurrent clonings (40% of the number of chunkservers) where each clone operation is allowed to consume at most 6.25 MB/s (50 Mbps). All chunks were restored in 23.2 minutes at an effective replication rate of 440 MB/s. </p>
<p>Finally, the author also measured the workload of chunkserver and master and broke down the workload of chunkserver by size, the same as that of master by type. Table 4 shows the distribution of operations by size, and Table 5 shows the total amount of data transferred in operations of various sizes. </p>
<p><img src="/imgs/Distributed/GFS/06.png" style="zoom:25%;" /><img src="/imgs/Distributed/GFS/07.png" style="zoom:25%;" /><img src="/imgs/Distributed/GFS/08.png" style="zoom: 31%;" /></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/26/Paper/Distributed/MapReduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/26/Paper/Distributed/MapReduce/" class="post-title-link" itemprop="url">MapReduce</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-26 13:00:23" itemprop="dateCreated datePublished" datetime="2023-09-26T13:00:23+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-15 19:49:43" itemprop="dateModified" datetime="2024-03-15T19:49:43+08:00">2024-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/" itemprop="url" rel="index"><span itemprop="name">Paper Notebook</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper-Notebook/Distributed-System/" itemprop="url" rel="index"><span itemprop="name">Distributed System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Paper: <a target="_blank" rel="noopener" href="http://nil.csail.mit.edu/6.824/2020/papers/mapreduce.pdf">MapReduce: Simplified Data Processing on Large Clusters</a></p>
<p>@[toc]</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><ol>
<li><strong>Main idea</strong>: Asking the user to write a simple Map and Reduce functions. The distributed executing framework supporting parallel scheduling is provided as a standard library that will run those functions automatically. </li>
<li><strong>Key findings</strong>: Map and Reduce are easier to write and depend on the specific application. Despite their different implementations, their distribution and parallelism are similar and can be handled by a common framework. </li>
<li><strong>The system</strong>: A master will assign and monitor each task and node while several workers will perform the Map and Reduce phases sequentially. </li>
<li><strong>Evaluation</strong>: The system is evaluated on grep and sort tasks with benchmarks of disk I/O speed and duration of each phase. </li>
</ol>
<h1 id="Purpose"><a href="#Purpose" class="headerlink" title="Purpose"></a>Purpose</h1><ol>
<li>Issues: Hard to parallel computation, distribute data, and handle server failures</li>
<li>Contribution: Proposed an interface where users only need to write relatively simple Map and Reduce functions, and the system will parallel and distribute automatically. </li>
</ol>
<h1 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h1><h2 id="Paper-ideas"><a href="#Paper-ideas" class="headerlink" title="Paper ideas"></a>Paper ideas</h2><h3 id="What-do-users-need-to-do"><a href="#What-do-users-need-to-do" class="headerlink" title="What do users need to do?"></a>What do users need to do?</h3><ol>
<li>Users need to provide a Map function and a Reduce function. </li>
<li>The map function will read the original data as key-value pairs, take one pair as input each time, and output intermediate key-value pairs, i.e. $map(k1,v1)\rightarrow list(k2,v2)$</li>
<li>The reduce function will take the intermediate key and a list of all intermediate values for that key as input and merge these values to form a smaller set of values, i.e. $reduce(k2,list(v2))\rightarrow list(v2)$</li>
<li>Users must implement the Mapper and Reducer as an interface provided by the system and pass them to the MapReduce specification. After passing the input and output files, invoke the <code>MapReduce</code> function to execute. </li>
</ol>
<h3 id="What-does-the-run-time-system-need-to-do"><a href="#What-does-the-run-time-system-need-to-do" class="headerlink" title="What does the run-time system need to do?"></a>What does the run-time system need to do?</h3><ol>
<li>Partition data</li>
<li>Schedule across a set of machines.</li>
<li>Handle machine failures</li>
<li>Manage inter-machine communication. </li>
</ol>
<h3 id="How-does-the-system-run"><a href="#How-does-the-system-run" class="headerlink" title="How does the system run?"></a>How does the system run?</h3><ol>
<li>When the <code>MapReduce</code> function is invoked, one <strong>master</strong> and several <strong>worker</strong> processes will be forked. </li>
<li>The master will assign work to workers, either a Map or Reduce work. </li>
<li>Master tries to make most of the <code>(3) read</code> run locally. In the <code>(5) remote read</code>, network communication is inevitable.<br><img src="/imgs/Distributed/MapReduce/01.png" style="zoom: 50%;" /></li>
</ol>
<h3 id="What-does-the-master-need-to-do"><a href="#What-does-the-master-need-to-do" class="headerlink" title="What does the master need to do?"></a>What does the master need to do?</h3><ol>
<li>Master pings every worker periodically and marks those who have not responded in a certain amount of time as failed. </li>
<li>Track the state of each map task and reduce task (idle, in progress, or completed) and the identity of the worker machine (for non-idle tasks). </li>
<li>$M$ is the number of map tasks, while $R$ is the number of reduce tasks. The master must make $O(M+R)$ scheduling decisions, and keeps $O(M*R)$ state in memory (all map task/reduce task pair). </li>
</ol>
<h3 id="How-to-handle-worker-failure"><a href="#How-to-handle-worker-failure" class="headerlink" title="How to handle worker failure?"></a>How to handle worker failure?</h3><ol>
<li>What kinds of worker failure need re-execution? <ul>
<li>Any tasks in progress</li>
<li>Completed map tasks must also be re-executed since their output is stored on the local disks and is inaccessible. </li>
<li>Completed reduce tasks don’t need to be re-executed since their output is stored on the global file system. </li>
</ul>
</li>
<li>The master will mark the state of those tasks that need re-execution to idle and can assign them to other workers in the future. </li>
</ol>
<h3 id="How-to-handle-master-failure"><a href="#How-to-handle-master-failure" class="headerlink" title="How to handle master failure?"></a>How to handle master failure?</h3><ol>
<li>One way is to make the master write periodic checkpoints of the master data structure. </li>
<li>Given that there is only a single master, its failure is unlikely. Therefore, another way is to abort the MapReduce computation if the master fails, and clients can try again later. (This is the way the author takes)</li>
</ol>
<h3 id="How-to-partition-reduce-tasks"><a href="#How-to-partition-reduce-tasks" class="headerlink" title="How to partition reduce tasks?"></a>How to partition reduce tasks?</h3><p>The number of reduce tasks/output files ($R$) is specified by the users. The default partitioning uses hashing, namely partitioning according to $hash(key)\ mod\ R$. </p>
<h3 id="How-do-we-ensure-that-nobody-observes-partially-written-files-during-crashes"><a href="#How-do-we-ensure-that-nobody-observes-partially-written-files-during-crashes" class="headerlink" title="How do we ensure that nobody observes partially written files during crashes?"></a>How do we ensure that nobody observes partially written files during crashes?</h3><p>Each worker first writes their results to a temporary file. Then, rename it once all writes are completed. </p>
<h3 id="How-to-handle-straggler-problems"><a href="#How-to-handle-straggler-problems" class="headerlink" title="How to handle straggler problems?"></a>How to handle straggler problems?</h3><ol>
<li>Straggler: a machine that takes an unusually long to complete one of the last few map or reduce tasks. A bad disk may cause this; its scheduling system schedules it for different tasks. </li>
<li>So when a MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks. </li>
</ol>
<h2 id="Reproduce"><a href="#Reproduce" class="headerlink" title="Reproduce"></a>Reproduce</h2><p>This reproduction part is based on Lab 1 of MIT 6.824. </p>
<h3 id="How-do-we-assign-map-tasks"><a href="#How-do-we-assign-map-tasks" class="headerlink" title="How do we assign map tasks?"></a>How do we assign map tasks?</h3><p>   Each worker will request for more map tasks when it becomes idle, and the master will assign files directly to them. </p>
<h3 id="How-do-we-assign-reduce-tasks"><a href="#How-do-we-assign-reduce-tasks" class="headerlink" title="How do we assign reduce tasks?"></a>How do we assign reduce tasks?</h3><ol>
<li>When workers are notified that there are no more map tasks, they request reduce tasks. This time, the master won’t assign files directly. Instead, the master will only assign a number in the range from $0$ to $R-1$. Then, each worker will try to read intermediate files from each worker according to their number automatically. </li>
<li>This requires those map workers to store their output in a previously agreed file name to reduce workers’ requests. </li>
</ol>
<h3 id="When-can-workers-stop-requesting-more-map-tasks-reduce-tasks"><a href="#When-can-workers-stop-requesting-more-map-tasks-reduce-tasks" class="headerlink" title="When can workers stop requesting more map tasks/reduce tasks?"></a>When can workers stop requesting more map tasks/reduce tasks?</h3><ol>
<li>Only after all map tasks are completed can workers stop requesting more map tasks and begin to request reduced tasks since reduced tasks may depend on those unfinished map tasks. So, workers’ lifecycles can be partitioned into two phases: the map phase and the reduce phase.</li>
<li>Also, only after all reduced tasks are completed can workers stop requesting more reduced tasks and quit the program. This is because those executing yet uncompleted tasks may fail, and when that happens, we need other workers to re-execute those tasks. </li>
<li>Similarly, reduce workers cannot delete those intermediate files right after they read them. Because if they fail, their successor needs to read those files. </li>
</ol>
<h1 id="Experiments-and-results"><a href="#Experiments-and-results" class="headerlink" title="Experiments and results"></a>Experiments and results</h1><ol>
<li><strong>What to notice when configuring a cluster?</strong><ul>
<li>Need to reserve some memory for other tasks running on the cluster. The author reserved $1-1.5$ GB out of $4$ GB. </li>
<li>Best test when the CPUs, disks, and network were mostly idle. </li>
</ul>
</li>
<li>The author tested two representative situations, grep and sort. </li>
<li>In the grep test, the execution time includes a minute of startup overhead over $150$ seconds of total time. The overhead is due to the propagation of the program to all worker machines and delays interacting with GFS to open the set of input files and to get the information needed for the locality optimization.<br><img src="/imgs/Distributed/MapReduce/02.png" style="zoom: 33%;" /></li>
<li>In the sort test, <ul>
<li>It only consists of less than $50$ lines of user code</li>
<li>The entire computation time, including startup overhead, is similar to the best-reported result. </li>
<li>The author tested three kinds of rates: the rate of reading by map workers (<em>input rate</em>), the rate of communicating intermediate files between map workers and reduce workers (<em>shuffle rate</em>), and the rate of writing output files by reduce workers (<em>output rate</em>). These are the I/O parts that affect the performance significantly. <ul>
<li>The input rate is less than grep because sort map tasks require more time to write intermediate output to their local disks. </li>
<li>Because of locality optimization, the input rate is higher than the shuffle rate and the output rate. </li>
<li>The shuffle rate is higher than the output rate because the output phase writes replicas due to the mechanism for reliability of the underlying file system. </li>
</ul>
</li>
<li>The author also tested when $200$ out of $1746$ workers are killed for several minutes. The underlying cluster scheduler immediately restarted new worker processes on these machines (only the processes were killed; the machines were still functioning correctly). The entire computation time increases of $5\%$ over the normal execution time.<br><img src="/imgs/Distributed/MapReduce/03.png" style="zoom:50%;" /></li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/03/Courses/15445/17-Distributed-OLAP-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/03/Courses/15445/17-Distributed-OLAP-Databases/" class="post-title-link" itemprop="url">17 Distributed OLAP Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-03 12:05:46" itemprop="dateCreated datePublished" datetime="2023-09-03T12:05:46+08:00">2023-09-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 17:00:07" itemprop="dateModified" datetime="2024-03-16T17:00:07+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="How-should-we-divide-tables"><a href="#How-should-we-divide-tables" class="headerlink" title="How should we divide tables?"></a>How should we divide tables?</h1><ol>
<li>The first schema is the star schema. <ul>
<li>Star schemas contain two types of tables: fact tables and dimension tables. </li>
<li>The fact table contains multiple “events” that occur in the application. It will contain minimal unique information per event. </li>
<li>The remaining attributes will be foreign key references to outer dimension tables. </li>
<li>In a star schema, there can only be one dimension-level out of the fact table. </li>
</ul>
</li>
<li>The second schema is the snowflake schema. <ul>
<li>It allows for more than one dimension out of the fact table. </li>
</ul>
</li>
<li>Snowflake schemas take up less storage space. Denormalized data models may incur integrity and consistency violations. </li>
<li>Snowflake schemas require more joins to get the data needed for a query. Queries on star schemas will usually be faster. </li>
</ol>
<h1 id="Execution-models"><a href="#Execution-models" class="headerlink" title="Execution models"></a>Execution models</h1><h2 id="How-to-execute-when-required-data-are-in-different-nodes"><a href="#How-to-execute-when-required-data-are-in-different-nodes" class="headerlink" title="How to execute when required data are in different nodes?"></a>How to execute when required data are in different nodes?</h2><ol>
<li>The first approach is to push the query to data. <ul>
<li>Send the query (or a portion of it) to the node that contains the data. </li>
<li>The result is then sent back to where the query is being executed, which uses local data and the data sent to it to complete the query. </li>
<li>Perform as much filtering and processing as possible where data resides before transmitting over the network. </li>
<li>This is more common in a shared-nothing system. </li>
</ul>
</li>
<li>The second approach is to pull data to query. <ul>
<li>Bring the data to the node and execute a query that needs processing. </li>
<li>This usually is what a shared disk system would do. </li>
<li>The problem with this is that the data size relative to the query size could be very different. </li>
</ul>
</li>
</ol>
<h2 id="How-to-handle-query-faults"><a href="#How-to-handle-query-faults" class="headerlink" title="How to handle query faults?"></a>How to handle query faults?</h2><ol>
<li>The data a node receives from remote sources is cached in the buffer pool.<ul>
<li>When the buffer pool runs out of memory, the DBMS can write some pages out to disk in some ephemeral pages. </li>
<li>This allows the DBMS to support intermediate results larger than the amount of memory available. </li>
<li>Ephemeral pages are not persisted after a restart. </li>
</ul>
</li>
<li>Most shared-nothing distributed OLAP DBMSs are designed to assume that nodes do not fail during query execution. <ul>
<li>If one node fails during query execution, then the whole query fails. </li>
</ul>
</li>
<li>The DBMS could take a snapshot of the intermediate results for a query during execution to allow it to recover if nodes fail. <ul>
<li>Most DMBSs do not want to pay the penalty of writing intermediate results to disk for quick queries. </li>
<li>This is adopted if the query takes a long time. </li>
</ul>
</li>
<li>In shared-disk distributed OLAP DMBSs, they can write results to the shared disk to prevent performing the same calculation again. However, the frequency of writing to disk is also a trade-off. </li>
</ol>
<h2 id="When-pushing-a-query-to-data-what-are-the-queries"><a href="#When-pushing-a-query-to-data-what-are-the-queries" class="headerlink" title="When pushing a query to data, what are the queries?"></a>When pushing a query to data, what are the queries?</h2><ol>
<li>The DBMSs only need to push fragments of the query instead of the whole query. </li>
<li>The first approach is pushing physical operators. <ul>
<li>Generate a single query plan and then break it up into partition-specific fragments. </li>
<li>Most systems implement this approach. </li>
</ul>
</li>
<li>The second approach is pushing another SQL query. <ul>
<li>The DBMS will rewrite the original query into partition-specific queries. </li>
<li>This approach allows for local optimization at each node. </li>
</ul>
</li>
</ol>
<h1 id="Distributed-Join-Algorithms"><a href="#Distributed-Join-Algorithms" class="headerlink" title="Distributed Join Algorithms"></a>Distributed Join Algorithms</h1><h2 id="How-to-perform-distributed-joins"><a href="#How-to-perform-distributed-joins" class="headerlink" title="How to perform distributed joins?"></a>How to perform distributed joins?</h2><ol>
<li>One approach is to put entire tables on a single node and then perform the join. <ul>
<li>You lose the parallelism of a distributed DBMS. </li>
<li>It has expensive data transferring over the network. </li>
</ul>
</li>
<li>The DBMS needs to get the proper tuples on the same node to join tables. The DBMS executes the single-node join algorithms once the data is at the node. </li>
<li>The best scenario is that one table is replicated at every node. <ul>
<li>Each node joins its local data in parallel and then sends its results to a coordinating node. </li>
</ul>
</li>
<li>The second scenario is that tables are partitioned on the join attribute. <ul>
<li>Each node only needs to acquire tuples of the second table that will match the tuples of the left table already in it. </li>
<li>Each node performs the join on local data and then sends it to a coordinator node for coalescing.</li>
</ul>
</li>
<li>The third scenario is that both tables are partitioned on different keys while one of the tables is small. <ul>
<li>The DBMS will broadcast that table to all nodes. </li>
</ul>
</li>
<li>The worst scenario is that neither table is partitioned on the join key. <ul>
<li>The DBMS copies the tables by shuffling them across nodes. </li>
</ul>
</li>
</ol>
<h2 id="What-is-semi-join"><a href="#What-is-semi-join" class="headerlink" title="What is semi-join?"></a>What is semi-join?</h2><ol>
<li>If the result only contains columns from the left table, the DBMSs use semi-join to minimize the amount of data sent during joins. </li>
<li>This is like a projection pushdown. The DBMS transmits only the necessary columns of the left table to other nodes. </li>
<li>For DBMSs that do not support <code>SEMI JOIN</code>, we can fake it with <code>EXISTS</code>. <ul>
<li>Wrap the <code>WHERE</code> clause predicates with <code>EXISTS (SELECT 1 from S WHERE predicates)</code>. </li>
</ul>
</li>
</ol>
<h1 id="Cloud-systems"><a href="#Cloud-systems" class="headerlink" title="Cloud systems"></a>Cloud systems</h1><h2 id="What-is-the-difference-for-DBMSs-running-in-cloud-systems"><a href="#What-is-the-difference-for-DBMSs-running-in-cloud-systems" class="headerlink" title="What is the difference for DBMSs running in cloud systems?"></a>What is the difference for DBMSs running in cloud systems?</h2><ol>
<li>The first approach is managed DBMS. <ul>
<li>No significant modification to the DBMS to be aware that it is running in a cloud environment. </li>
</ul>
</li>
<li>The second approach is cloud-native DBMS. <ul>
<li>The system is designed explicitly to run in a cloud environment. </li>
<li>Usually based on a shared-disk architecture. </li>
</ul>
</li>
<li>A “serverless” DBMS evicts tenants when they become idle rather than always maintaining compute resources for each customer. </li>
</ol>
<h2 id="How-to-store-data-of-different-schema"><a href="#How-to-store-data-of-different-schema" class="headerlink" title="How to store data of different schema?"></a>How to store data of different schema?</h2><ol>
<li>A Data Lake is a centralized repository for storing large amounts of structured, semi-structured, and unstructured data without defining a schema or ingesting the data into proprietary internal formats. </li>
<li>Data lakes are usually faster at ingesting data, as they do not immediately require transformation. Yet, when fetching data, they must look up the catalog before transforming it into the desired schema. </li>
<li>They require the user to write their transformation pipelines.</li>
</ol>
<h2 id="How-to-share-data-between-systems"><a href="#How-to-share-data-between-systems" class="headerlink" title="How to share data between systems?"></a>How to share data between systems?</h2><ol>
<li>Most DBMSs use a proprietary on-disk binary file format for their databases. </li>
<li>The only way to share data between systems is to convert data into a common text-based format. </li>
<li>New open-source binary file formats make it easier to access data across systems. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/09/02/Courses/15445/16-Distributed-OLTP-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/09/02/Courses/15445/16-Distributed-OLTP-Databases/" class="post-title-link" itemprop="url">16 Distributed OLTP Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-02 00:02:00" itemprop="dateCreated datePublished" datetime="2023-09-02T00:02:00+08:00">2023-09-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 16:55:17" itemprop="dateModified" datetime="2024-03-16T16:55:17+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="What-is-the-difference-between-OLTP-and-OLAP"><a href="#What-is-the-difference-between-OLTP-and-OLAP" class="headerlink" title="What is the difference between OLTP and OLAP?"></a>What is the difference between OLTP and OLAP?</h1><ol>
<li>OLTP is the front-end database communicating and interacting with the outside world, e.g., applications. OLAP is the back-end database used to analyze the data in those front-end databases. </li>
<li>OLTP often executes repetitive, short-lived read/write transactions, while OLAP executes long-running, read-only queries involving complex joins. </li>
<li>Before going into the OLAP system, data in OLTP databases need an intermediate step called ETL, or Extract, Transform, and Load, which combines the OLTP databases into a universal schema for the data warehouse.</li>
</ol>
<h1 id="2-Phase-Commit-2PC"><a href="#2-Phase-Commit-2PC" class="headerlink" title="2-Phase Commit (2PC)"></a>2-Phase Commit (2PC)</h1><h2 id="What-is-the-procedure-of-2PC"><a href="#What-is-the-procedure-of-2PC" class="headerlink" title="What is the procedure of 2PC?"></a>What is the procedure of 2PC?</h2><ol>
<li>When an application sends a commit request to the coordinator, the coordinator tells other nodes to go into the first phase, the preparation phase. </li>
<li>When participants are ready, they will reply to the coordinator with acknowledgment. </li>
<li>If the coordinator receives an acknowledgment from all participants, they can begin the second commit phase. <ul>
<li>The coordinator will respond to the application successfully after receiving all participants’ acknowledgment of the commit phase. </li>
</ul>
</li>
<li>If the coordinator receives abort messages from any participants, the coordinator responds to the application with an abort message and begins the abort phase. <ul>
<li>As usual, participants need to reply with acknowledgment to the coordinator in the abort phase. </li>
</ul>
</li>
</ol>
<h2 id="What-would-happen-if-some-nodes-crash-during-2PC"><a href="#What-would-happen-if-some-nodes-crash-during-2PC" class="headerlink" title="What would happen if some nodes crash during 2PC?"></a>What would happen if some nodes crash during 2PC?</h2><ol>
<li>If the coordinator crashes, participants must decide what to do after a timeout: either abort or elect a new coordinator. <ul>
<li>The system is not available during this timeout. </li>
</ul>
</li>
<li>If one of the participants crashes, the coordinator assumes it responded with an abort if it hasn’t sent an acknowledgment yet. <ul>
<li>Nodes use a timeout to determine that the participant is dead.</li>
</ul>
</li>
</ol>
<h2 id="How-to-recover-from-a-crash-during-2PC"><a href="#How-to-recover-from-a-crash-during-2PC" class="headerlink" title="How to recover from a crash during 2PC?"></a>How to recover from a crash during 2PC?</h2><ol>
<li>Each node records each phase’s inbound/outbound messages and outcomes in a non-volatile storage log. </li>
<li>On recovery, examine the log for 2PC messages.<ul>
<li>If the local transaction is prepared, contact the coordinator. </li>
<li>If the local transaction is not prepared, abort it. </li>
<li>If the local transaction is committed and the node is the coordinator, send a <code>COMMIT</code> message to nodes. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-optimize-2PC-to-reduce-communication"><a href="#How-can-we-optimize-2PC-to-reduce-communication" class="headerlink" title="How can we optimize 2PC to reduce communication?"></a>How can we optimize 2PC to reduce communication?</h2><ol>
<li>The first method is early prepare voting. <ul>
<li>If you send a query to a remote node that you know will be the last one you execute there, then that node will also return their vote for the prepare phase with the query result. </li>
<li>This is rare because database applications are rarely written with the idea of the last query. </li>
<li>However, this can be used in RPC sorts of things where we can be sure that this process will terminate after executing a certain query. </li>
</ul>
</li>
<li>The second method is early ACK after preparation. <ul>
<li>Suppose all nodes vote to commit a transaction. In that case, the coordinator can send the client an acknowledgment that their transaction was successful before the commit phase finishes, i.e., send an acknowledgment to the client after receiving all participant acknowledgments. </li>
<li>This could cause a small window of clients receiving success yet not seeing modifications from servers because the commit phase is not yet finished. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-difference-between-2PC-and-Paxos"><a href="#What-is-the-difference-between-2PC-and-Paxos" class="headerlink" title="What is the difference between 2PC and Paxos?"></a>What is the difference between 2PC and Paxos?</h2><ol>
<li>2-phase commit is a degenerate case of Paxos. <ul>
<li>Paxos uses $2F + 1$ coordinators and makes progress as long as at least $F + 1$ of them are working properly. </li>
<li>2-phase commit sets $F = 0$. </li>
</ul>
</li>
<li>2-phase commit blocks if the coordinator fails after the prepared message is sent until the coordinator recovers.<br>Paxos remains non-blocking if a majority of participants are alive, provided there is a sufficiently long period without further failures. </li>
<li>A 2-phase commit requires all nodes to agree on the commit, while Paxos only requires majority agreement. </li>
<li>In a 2-phase commit, each node may have different data, executing different commands. However, in Paxos, all nodes are only replications. </li>
</ol>
<h1 id="Replication"><a href="#Replication" class="headerlink" title="Replication"></a>Replication</h1><h2 id="How-can-we-execute-read-write-with-replication"><a href="#How-can-we-execute-read-write-with-replication" class="headerlink" title="How can we execute read/write with replication?"></a>How can we execute read/write with replication?</h2><ol>
<li>The first approach is primary-replica. <ul>
<li>All updates go to a designated primary for each object. The primary propagates updates to its replicas without an atomic commit protocol. </li>
<li>Read-only transactions may allow access to replicas. </li>
<li>If the primary goes down, then hold an election to select a new primary. </li>
</ul>
</li>
<li>The second approach is multi-primary. <ul>
<li>Transactions can update data objects at any replica.</li>
<li>Replicas must synchronize with each other using an atomic commit protocol. </li>
</ul>
</li>
</ol>
<h2 id="When-should-notify-the-application-of-the-result"><a href="#When-should-notify-the-application-of-the-result" class="headerlink" title="When should notify the application of the result?"></a>When should notify the application of the result?</h2><ol>
<li>When a transaction is committed on a replicated database, the DBMS decides whether to wait for its changes to propagate to other nodes before sending the acknowledgment to the application. </li>
<li>The first propagation level is synchronous, which leads to strong consistency. <ul>
<li>The primary sends updates to replicas and then waits for them to acknowledge that they fully applied (i.e., logged) the changes before sending an acknowledgment to the application. </li>
</ul>
</li>
<li>The second propagation level is asynchronous, which leads to eventual consistency. <ul>
<li>The primary immediately returns the acknowledgment to the client without waiting for replicas to apply the changes. </li>
</ul>
</li>
</ol>
<h2 id="When-should-propagate-updates-between-nodes"><a href="#When-should-propagate-updates-between-nodes" class="headerlink" title="When should propagate updates between nodes?"></a>When should propagate updates between nodes?</h2><ol>
<li>The first approach is continuous. <ul>
<li>The DBMS sends log messages immediately as it generates them. </li>
<li>It also needs to send a commit/abort message. </li>
</ul>
</li>
<li>The second approach is to commit. <ul>
<li>The DBMS only sends the log messages for a transaction to the replicas once the transaction is committed.</li>
<li>Do not waste time sending log records for aborted transactions. </li>
<li>It assumes that a transaction’s log records fit entirely in memory. </li>
</ul>
</li>
</ol>
<h2 id="What-should-be-sent-to-the-followers"><a href="#What-should-be-sent-to-the-followers" class="headerlink" title="What should be sent to the followers?"></a>What should be sent to the followers?</h2><ol>
<li>The first choice is active-active. <ul>
<li>A transaction executes at each replica independently. </li>
<li>Check at the end whether the transaction ends with the same result at each replica. </li>
</ul>
</li>
<li>The second choice is active-passive. <ul>
<li>Each transaction executes at a single location and propagates the changes to the replica.</li>
<li>Can either do physical or logical replication.</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/31/Courses/15445/15-Introduction-to-Distributed-Databases/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/31/Courses/15445/15-Introduction-to-Distributed-Databases/" class="post-title-link" itemprop="url">15 Introduction to Distributed Databases</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-31 23:53:22" itemprop="dateCreated datePublished" datetime="2023-08-31T23:53:22+08:00">2023-08-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 16:48:43" itemprop="dateModified" datetime="2024-03-16T16:48:43+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="System-architecture"><a href="#System-architecture" class="headerlink" title="System architecture"></a>System architecture</h1><h2 id="What-will-system-architecture-affect"><a href="#What-will-system-architecture-affect" class="headerlink" title="What will system architecture affect?"></a>What will system architecture affect?</h2><ol>
<li>A distributed DBMS’s system architecture specifies what shared resources are directly accessible to CPUs. </li>
<li>This affects how CPUs coordinate with each other and where they retrieve/store objects in the database. </li>
<li>There are four architectures: shared everything, shared memory, shared disk, and shared nothing. </li>
<li>In shared everything architecture, CPU, memories, and disks are all local. This is more of a parallel architecture than a distributed architecture. </li>
</ol>
<h2 id="What-is-shared-memory-architecture"><a href="#What-is-shared-memory-architecture" class="headerlink" title="What is shared memory architecture?"></a>What is shared memory architecture?</h2><ol>
<li>CPUs have access to common memory address space via a fast interconnect. </li>
<li>Each processor has a global view of all the in-memory data structures. <ul>
<li>Each process’s memory scope is the same memory address space multiple processes can modify. </li>
</ul>
</li>
<li>Each DBMS instance on a processor must “know” about the other instances. </li>
<li>In practice, most DBMSs do not use this architecture, as it is provided at the OS/kernel level. </li>
</ol>
<p><img src="/imgs/15445/Distributed/shared_mem.png" width="30%"></p>
<h2 id="What-is-shared-disk-architecture"><a href="#What-is-shared-disk-architecture" class="headerlink" title="What is shared disk architecture?"></a>What is shared disk architecture?</h2><ol>
<li>All CPUs can access a single logical disk directly via an interconnect, but each has its own private memories.</li>
<li>It can scale the execution layer independently from the storage layer. <ul>
<li>The advantage of shared disk over shared nothing is that it can easily scale up with the compute and storage layers independently. </li>
<li>What we want to persist after a crash is in the storage layer. </li>
<li>Theoretically, we can kill or add front-end nodes without losing the database or, add storage disks / change the storage layer by modifying compute nodes. </li>
</ul>
</li>
<li>It must send messages between CPUs to learn about their current state. </li>
<li>This architecture is commonly used in OLAP systems. Many DBSMs begin to think that shared disk architecture is better than shared nothing. </li>
</ol>
<p><img src="/imgs/15445/Distributed/shared_disk.png" width="30%"></p>
<h2 id="What-is-shared-nothing-architecture"><a href="#What-is-shared-nothing-architecture" class="headerlink" title="What is shared-nothing architecture?"></a>What is shared-nothing architecture?</h2><ol>
<li>Each DBMS instance has its own CPU, memory, and local disk. </li>
<li>Nodes only communicate with each other via network. <ul>
<li>When executing a query that requires data from different nodes, the DBMS can either send data up to the node connected with the application or that node can ask another node to execute the query and return the result. </li>
</ul>
</li>
<li>All data in the database are sharded into different nodes. <ul>
<li>When adding a new node into the architecture, that node is initially empty. The DBMS must re-shard data so that they are distributed evenly. </li>
<li>It is more difficult to increase capacity because the DBMS has to move data to new nodes physically. </li>
<li>It might have a small window for queries to receive false positives because part of the data was in that node and is now moving to another node. </li>
</ul>
</li>
<li>It is also difficult to ensure consistency across all nodes in the DBMS since the nodes must coordinate with each other regarding the state of transactions. </li>
<li>However, it can potentially achieve better performance and be more efficient than other types of distributed DBMS architectures. </li>
</ol>
<p><img src="/imgs/15445/Distributed/shared_nothing.png" width="30%"></p>
<h1 id="Design-issues"><a href="#Design-issues" class="headerlink" title="Design issues"></a>Design issues</h1><h2 id="What-are-the-design-issues-of-distributed-databases"><a href="#What-are-the-design-issues-of-distributed-databases" class="headerlink" title="What are the design issues of distributed databases?"></a>What are the design issues of distributed databases?</h2><ol>
<li>How does the application find data? </li>
<li>Where does the application send queries? </li>
<li>How to execute queries on distributed data? Push query to data? Or pull data to query? </li>
<li>How does the DBMS ensure correctness? </li>
<li>How do we divide the database across resources? </li>
</ol>
<h2 id="What-do-nodes-look-like"><a href="#What-do-nodes-look-like" class="headerlink" title="What do nodes look like?"></a>What do nodes look like?</h2><ol>
<li>The first approach is homogenous nodes. <ul>
<li>Every node in the cluster can perform the same set of tasks, albeit on potentially different partitions of data. </li>
<li>Makes provisioning and failover “easier” since any node can replace other nodes. </li>
</ul>
</li>
<li>The second approach is heterogeneous nodes. <ul>
<li>Nodes are assigned specific tasks. </li>
<li>It can allow a single physical node to host multiple “virtual” node types for dedicated tasks. </li>
<li>A heterogeneous node design has two kinds of nodes: router and config server. <ul>
<li>The router can directly access shards of the database, yet it does not know where the data is wanted. </li>
<li>The config server knows the data contained in each shard. However, it is not responsible for retrieving them. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="How-to-coordinate-execution"><a href="#How-to-coordinate-execution" class="headerlink" title="How to coordinate execution?"></a>How to coordinate execution?</h2><ol>
<li>If our DBMS supports multi-operation and distributed transactions, we need a way to coordinate their execution in the system. </li>
<li>The first approach is a centralized coordinator. A centralized coordinator receives commands from the application. <ul>
<li>The first design requires applications to handle transactions. <ul>
<li>The client communicates with the coordinator to acquire locks on the partitions that the client wants to access. </li>
<li>Once it receives an acknowledgment from the coordinator, the client sends its queries to those partitions. </li>
<li>Once all queries for a given transaction are done, the client sends a commit request to the coordinator. </li>
<li>The coordinator then communicates with the partitions involved in the transaction to determine whether the transaction is allowed to be committed. </li>
</ul>
</li>
<li>Another design uses middleware to accept query requests and route queries to correct partitions. </li>
</ul>
</li>
<li>The second approach is decentralized. <ul>
<li>The client directly sends queries to one of the partitions, which will be the leader node for that transaction. </li>
<li>The leader node will coordinate communicating with other partitions and committing. </li>
</ul>
</li>
</ol>
<h1 id="Partitioning-Schemes"><a href="#Partitioning-Schemes" class="headerlink" title="Partitioning Schemes"></a>Partitioning Schemes</h1><h2 id="What-do-we-desire-when-partitioning-a-database"><a href="#What-do-we-desire-when-partitioning-a-database" class="headerlink" title="What do we desire when partitioning a database?"></a>What do we desire when partitioning a database?</h2><ol>
<li>Applications should not be required to know where data is physically located in a distributed DBMS. <ul>
<li>Any query that runs on a single-node DBMS should produce the same result on a distributed DBMS. </li>
<li>The DBMS executes query fragments on each partition and then combines the results to produce a single answer. </li>
</ul>
</li>
<li>In practice, developers need to be aware of the communication costs of queries to avoid excessively “expensive” data movement. </li>
<li>The DBMS can partition a database physically for shared nothing or logically for shared disk. <ul>
<li>In Logical partitioning, each node is responsible for certain designated data. They cannot access data out of their duty. However, data are stored in independent storage nodes, which computation nodes can all access. </li>
<li>In physical partitioning, data out of their duty cannot be accessed directly since they are stored in the local disk of other nodes. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-partition-the-database"><a href="#How-can-we-partition-the-database" class="headerlink" title="How can we partition the database?"></a>How can we partition the database?</h2><ol>
<li>The first method is the naive table partitioning. <ul>
<li>Assign an entire table to a single node.</li>
<li>It assumes that each node has enough storage space for an entire table.</li>
<li>This is ideal if queries never join data across tables stored on different nodes and access patterns are uniform. </li>
</ul>
</li>
<li>The second method is vertical partitioning. <ul>
<li>Split a table’s attributes into separate partitions. </li>
<li>It must store tuple information to reconstruct the original record. </li>
</ul>
</li>
<li>The third method is horizontal partitioning. <ul>
<li>Split a table’s tuples into disjoint subsets based on some partitioning key and scheme. </li>
<li>Choose column(s) that divides the database equally in terms of size, load, or usage. </li>
<li>It can partition based on hashing, ranges, or predicates. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-optimize-horizontal-partitioning"><a href="#How-can-we-optimize-horizontal-partitioning" class="headerlink" title="How can we optimize horizontal partitioning?"></a>How can we optimize horizontal partitioning?</h2><ol>
<li>The main problem is that the DBMS needs to reshuffle data when adding a new storage node. </li>
<li>We can use the consistent hashing. <ul>
<li>The hashing value is between $0$ and $1$ forming a circle. </li>
<li>Each nodes are assigned a value between $0$ and $1$. Data are stored in the node with the closest value to its hashing values going in clockwise order. </li>
<li>When adding a node, only one node needs to transmit data to the new node instead of transmitting data between all pairs of nodes. </li>
</ul>
</li>
<li>With consistent hashing, we can support replication easily. <ul>
<li>Store data in the first batch of nodes with the closest value in clockwise order. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/30/Courses/15445/14-Database-Recovery/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/30/Courses/15445/14-Database-Recovery/" class="post-title-link" itemprop="url">14 Database Recovery</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-30 19:12:36" itemprop="dateCreated datePublished" datetime="2023-08-30T19:12:36+08:00">2023-08-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 16:30:32" itemprop="dateModified" datetime="2024-03-16T16:30:32+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="What-are-the-main-ideas-of-ARIES"><a href="#What-are-the-main-ideas-of-ARIES" class="headerlink" title="What are the main ideas of ARIES?"></a>What are the main ideas of ARIES?</h1><ol>
<li>ARIES: Algorithms for Recovery and Isolation Exploiting Semantics</li>
<li>Write-Ahead Logging:<ul>
<li>Any change is recorded in the log on stable storage before the database change is written to disk. </li>
<li>Must use steal and no-force buffer pool policies. <ul>
<li>Logs are forced to be flushed into the disk, while modified pages are not. </li>
<li>Force is also correct, but it damages runtime performance, making no one use it. </li>
</ul>
</li>
</ul>
</li>
<li>Repeating History During Redo: On DBMS restart, retrace actions and restore the database to the exact state before the crash. </li>
<li>Logging Changes During Undo: Record undo actions to log to ensure action is not repeated in the event of repeated failures. </li>
</ol>
<h1 id="Record-logs"><a href="#Record-logs" class="headerlink" title="Record logs"></a>Record logs</h1><h2 id="What-are-the-LSNs"><a href="#What-are-the-LSNs" class="headerlink" title="What are the LSNs?"></a>What are the LSNs?</h2><ol>
<li>Every log record now includes a globally unique, monotonically increasing log sequence number (LSN).<ul>
<li>LSNs represent the physical order in which transactions make changes to the database. </li>
</ul>
</li>
<li>Various components in the system keep track of LSNs that pertain to them.<ul>
<li>In memory, the system uses <code>flushedLSN</code> to track the last LSN in the log on disk. </li>
<li>In each disk page, <code>pageLSN</code> is used to track the newest update to that page, while <code>recLSN</code> is tracking the oldest update to that page since it was last flushed. </li>
<li>Each transaction maintains the <code>lastLSN</code> representing the latest record of that transaction. </li>
<li>There is also a MasterRecord in the disk, meaning the LSN of the latest checkpoint. </li>
</ul>
</li>
<li>Before the DBMS can write page x to disk, it must flush the log at least to the point where $pageLSN_x ≤ flushedLSN$. </li>
<li>Update the <code>pageLSN</code> every time a transaction modifies a record in the page. </li>
<li>Update the <code>flushedLSN</code> in memory every time the DBMS writes out the WAL buffer to disk. </li>
</ol>
<h2 id="How-to-handle-transaction-commits"><a href="#How-to-handle-transaction-commits" class="headerlink" title="How to handle transaction commits?"></a>How to handle transaction commits?</h2><ol>
<li>When a transaction is committed, the DBMS writes a <code>COMMIT</code> record to log and guarantees that all log records up to the transaction’s <code>COMMIT</code> record are flushed to disk. <ul>
<li>Log flushes are sequential, synchronous writes to disk. </li>
</ul>
</li>
<li>When the commit succeeds, write a special <code>TXN-END</code> record to log. <ul>
<li>Indicates that no new log record for a transaction will appear in the log ever again. </li>
<li>This does not need to be flushed immediately. </li>
</ul>
</li>
</ol>
<h2 id="How-to-handle-transaction-abort"><a href="#How-to-handle-transaction-abort" class="headerlink" title="How to handle transaction abort?"></a>How to handle transaction abort?</h2><ol>
<li>Another <code>prevLSN</code> is added to log records pointing to the previous LSN for that transaction to make it easy to walk through its records. </li>
<li>First, write an <code>ABORT</code> record to log for the transaction. <ul>
<li>Following that, we must record the steps taken to undo the transaction. <ul>
<li>A <code>CLR</code> describes the actions taken to undo the actions of a previous update record. </li>
<li>It has all the fields of an update log record plus the <code>undoNext</code> pointer pointing to the next-to-be-undone LSN. </li>
<li><code>CLR</code>s are added to log records, but the DBMS does not wait for them to be flushed before notifying the application that the transaction was aborted. </li>
</ul>
</li>
<li>Lastly, write a <code>TXN-END</code> record. </li>
</ul>
</li>
<li>We need to analyze the transaction’s updates in reverse order to add <code>CLR</code> records. <ul>
<li>Write a <code>CLR</code> entry to the log for each update record and restore the old value. </li>
<li><code>CLR</code>s never need to be undone. </li>
</ul>
</li>
</ol>
<h1 id="Fuzzy-checkpoints"><a href="#Fuzzy-checkpoints" class="headerlink" title="Fuzzy checkpoints"></a>Fuzzy checkpoints</h1><h2 id="How-can-we-improve-the-naive-checkpoints"><a href="#How-can-we-improve-the-naive-checkpoints" class="headerlink" title="How can we improve the naive checkpoints?"></a>How can we improve the naive checkpoints?</h2><ol>
<li>The naive checkpoint needs to halt the start of any new transactions and wait until all active transactions finish executing. </li>
<li>We can only pause modifying transactions while the DBMS takes the checkpoint. <ul>
<li>This can be done by preventing queries from acquiring a write latch on table/index pages. </li>
<li>Don’t have to wait until all transactions finish before taking the checkpoint. </li>
</ul>
</li>
<li>We must record the internal state as of the beginning of the checkpoint. <ul>
<li>Active Transaction Table (ATT): What transactions were running when we took the checkpoint. </li>
<li>Dirty Page Table (DPT): What pages are dirty. </li>
</ul>
</li>
<li>ATT is maintained at runtime and recovery. There is an entry per currently active transaction.<ul>
<li>Each entry contains<ul>
<li><code>transactionId</code>: Unique transaction identifier</li>
<li><code>status</code>: The current “mode” of the transaction</li>
<li><code>lastLSN</code>: The most recent LSN created by the transaction. </li>
</ul>
</li>
<li>Remove the entry after the <code>TXN-END</code> record. </li>
<li>Txn Status Codes<ul>
<li><code>R</code>: Running</li>
<li><code>C</code>: Committing</li>
<li><code>U</code>: Candidate for undo</li>
</ul>
</li>
<li><code>U</code> is the default mode in recovery. <ul>
<li>When replaying the log, we cannot see what’s coming up, assuming we will not see a transaction commit record. </li>
<li>Flip to commit when we see a transaction commit record.</li>
</ul>
</li>
</ul>
</li>
<li>DPT contains one entry per dirty page in the buffer pool, including <code>recLSN</code>. <ul>
<li><code>recLSN</code> is the LSN of the log record that first caused the page to be dirty. </li>
</ul>
</li>
<li>Each checkpoint record includes ATT and DPT in it. </li>
</ol>
<h2 id="How-can-we-checkpoint-without-stalling-transactions"><a href="#How-can-we-checkpoint-without-stalling-transactions" class="headerlink" title="How can we checkpoint without stalling transactions?"></a>How can we checkpoint without stalling transactions?</h2><ol>
<li>In a fuzzy checkpoint, two kinds of records track checkpoint boundaries. <ul>
<li><code>CHECKPOINT-BEGIN</code> indicates the start of the checkpoint. Recovery begins from here. </li>
<li><code>CHECKPOINT-END</code> contains ATT and DPT at the moment of <code>CHECKPOINT-BEGIN</code>. </li>
</ul>
</li>
<li>The <code>LSN</code> of the <code>CHECKPOINT-BEGIN</code> record is written in the MasterRecord when it is complete. </li>
<li>Any transaction after the checkpoint starts is excluded from the ATT in the <code>CHECKPOINT-END</code> record. </li>
</ol>
<h1 id="Recovery"><a href="#Recovery" class="headerlink" title="Recovery"></a>Recovery</h1><h2 id="How-does-ARIES-recover-from-the-crash"><a href="#How-does-ARIES-recover-from-the-crash" class="headerlink" title="How does ARIES recover from the crash?"></a>How does ARIES recover from the crash?</h2><ol>
<li>The first phase is analysis. <ul>
<li>Examine the WAL in a forward direction, starting at MasterRecord to identify dirty pages in the buffer pool and active transactions at the time of the crash. </li>
<li>This is to figure out which transactions committed or failed since checkpoint.</li>
</ul>
</li>
<li>The second phase is the redo phase. <ul>
<li>Repeat all actions starting from an appropriate point in the log in the forward direction. </li>
<li>This phase is to repeat all actions, even transactions that will abort. </li>
</ul>
</li>
<li>The last phase is undo. <ul>
<li>Reverse the actions of transactions that were not committed before the crash in reverse order. </li>
<li>This phase is to reverse the effects of failed transactions. </li>
</ul>
</li>
<li>If crashes happen during recovery, just rerun recovery. </li>
</ol>
<p><img src="/imgs/15445/Recovery/aries.png" width="20%"></p>
<h2 id="What-does-the-analysis-phase-do"><a href="#What-does-the-analysis-phase-do" class="headerlink" title="What does the analysis phase do?"></a>What does the analysis phase do?</h2><ol>
<li>Scan the log forward from the <code>CHECKPOINT-END</code> of the last successful checkpoint. </li>
<li>If the DBMS finds a <code>TXN-END</code> record, remove its corresponding transaction from ATT. </li>
<li>For all other records,<ul>
<li>Suppose the transaction is not in ATT; add it with the status <code>UNDO</code>. On commit, change transaction status to <code>COMMIT</code>.</li>
<li>For update log records, if the page $P$ not in DPT, add $P$ to DPT, set its <code>recLSN=LSN</code>.</li>
</ul>
</li>
<li>At the end of the Analysis Phase,<ul>
<li>ATT identifies which transactions were active at the time of the crash. </li>
<li>DPT identifies which dirty pages might not have made it to disk. </li>
</ul>
</li>
</ol>
<h2 id="What-does-the-redo-phase-do"><a href="#What-does-the-redo-phase-do" class="headerlink" title="What does the redo phase do?"></a>What does the redo phase do?</h2><ol>
<li>The goal is to repeat history to reconstruct the database state at the moment of the crash. </li>
<li>Scan forward from the log record containing the smallest <code>recLSN</code> in DPT.</li>
<li>For each update log record or <code>CLR</code> with a given LSN, redo the action unless the affected page is not in DPT or the affected page is in DPT but that record’s LSN is less than the page’s <code>recLSN</code>. <ul>
<li>If the affected page is not in DPT, that modification is already flushed in the disk. </li>
<li>If that record’s LSN is less than the page’s <code>recLSN</code>, that page is dirty due to some updates after the record. The modification of that record is also flushed into the disk. </li>
</ul>
</li>
<li>To redo an action, <ul>
<li>Reapply logged update.</li>
<li>Set <code>pageLSN</code> to log the record’s LSN.</li>
<li>No additional logging, no forced flushes. </li>
<li>To improve performance, we can assume that it will not crash again and that all changes to the disk will be flushed asynchronously in the background.</li>
</ul>
</li>
<li>At the end of the Redo Phase, write <code>TXN-END</code> log records for all transactions with status <code>C</code> and remove them from the ATT. </li>
</ol>
<h2 id="What-does-the-undo-phase-do"><a href="#What-does-the-undo-phase-do" class="headerlink" title="What does the undo phase do?"></a>What does the undo phase do?</h2><ol>
<li>Undo all active transactions at the time of the crash and, therefore, will never commit. </li>
<li>These are all the transactions with <code>U</code> status in the ATT after the Analysis Phase. </li>
<li>Process them in reverse LSN order using the <code>lastLSN</code> to speed up traversal. </li>
<li>Write a <code>CLR</code> for every modification. </li>
<li>To improve performance, <ul>
<li>Lazily rollback changes before new transactions access pages. </li>
<li>Rewrite the application to avoid long-running transactions, which will never be used. </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/29/Courses/15445/13-Database-Logging/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/29/Courses/15445/13-Database-Logging/" class="post-title-link" itemprop="url">13 Database Logging</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-29 11:42:20" itemprop="dateCreated datePublished" datetime="2023-08-29T11:42:20+08:00">2023-08-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 16:09:55" itemprop="dateModified" datetime="2024-03-16T16:09:55+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Crash"><a href="#Crash" class="headerlink" title="Crash"></a>Crash</h1><h2 id="How-to-recover-from-the-crash"><a href="#How-to-recover-from-the-crash" class="headerlink" title="How to recover from the crash?"></a>How to recover from the crash?</h2><ol>
<li>Recovery algorithms are techniques to ensure database consistency, transaction atomicity, and durability despite failures. </li>
<li>The DBMS needs to ensure the following:<ul>
<li>The changes for any transaction are durable once the DBMS has told somebody it has been committed.</li>
<li>No partial changes are durable if the transaction is aborted.</li>
</ul>
</li>
<li>Recovery algorithms have two parts:<ul>
<li>The first is the runtime part: Actions during normal transaction processing to ensure the DBMS can recover from a failure. </li>
<li>The second is the startup part: Actions after a failure to recover the database to a state that ensures atomicity, consistency, and durability. </li>
</ul>
</li>
</ol>
<h2 id="What-are-the-possible-classifications-of-failures"><a href="#What-are-the-possible-classifications-of-failures" class="headerlink" title="What are the possible classifications of failures?"></a>What are the possible classifications of failures?</h2><ol>
<li>Transaction failures:<ul>
<li>Logical errors: Transactions cannot be completed due to internal error conditions (e.g., integrity constraint violation).</li>
<li>Internal state errors: DBMS must terminate an active transaction due to an error condition (e.g., deadlock). </li>
</ul>
</li>
<li>System failures: <ul>
<li>Software failure: Problem with the OS or DBMS implementation (e.g., uncaught divide-by-zero exception). </li>
<li>Hardware failure: The computer hosting the DBMS crashes (e.g., the power plug gets pulled). </li>
<li>Fail-stop assumption: Non-volatile storage contents are assumed not to be corrupted by system crashes. </li>
</ul>
</li>
<li>Storage media failure: <ul>
<li>Non-repairable hardware failure: A head crash or similar disk failure destroys all or part of non-volatile storage. </li>
<li>Destruction is assumed to be detectable (e.g., disk controllers use checksums to detect failures).</li>
<li>No DBMS can recover from this! The database must be restored from the archived version. </li>
<li>Stable storage is a non-existent form of non-volatile storage that survives all possible failure scenarios. </li>
</ul>
</li>
</ol>
<h1 id="Naive-solution"><a href="#Naive-solution" class="headerlink" title="Naive solution"></a>Naive solution</h1><h2 id="What-buffer-pool-policies-can-we-choose"><a href="#What-buffer-pool-policies-can-we-choose" class="headerlink" title="What buffer pool policies can we choose?"></a>What buffer pool policies can we choose?</h2><ol>
<li>Steal policy: <ul>
<li>Whether the DBMS allows an uncommitted transaction to overwrite an object’s most recent committed value in non-volatile storage. </li>
<li>Steal: Is allowed</li>
<li>No-steal: Is not allowed</li>
</ul>
</li>
<li>Force policy:<ul>
<li>Whether the DBMS requires that all updates made by a transaction are reflected on non-volatile storage before the transaction can commit. </li>
<li>Force: Is required</li>
<li>No-force: Is not required</li>
</ul>
</li>
<li>Undo: The process of removing the effects of an incomplete or aborted transaction. </li>
<li>Redo: The process of re-applying the effects of a committed transaction for durability. </li>
<li>Stead and no-force policy has the best runtime performance since it does not need to wait for conflicted uncommitted transactions and does not necessarily have to flush when committed. </li>
<li>The no-steal and force policy has the best recovery performance since it does not need to undo and redo anything. </li>
</ol>
<h2 id="What-are-the-pros-and-cons-of-no-steal-and-force-policy"><a href="#What-are-the-pros-and-cons-of-no-steal-and-force-policy" class="headerlink" title="What are the pros and cons of no-steal and force policy?"></a>What are the pros and cons of no-steal and force policy?</h2><ol>
<li>This approach is the easiest to implement:<ul>
<li>Never have to undo changes of an aborted transaction because the changes were not written to disk. </li>
<li>Never have to redo changes of a committed transaction because all the changes are guaranteed to be written to disk at commit time (assuming atomic hardware writes).</li>
</ul>
</li>
<li>An uncommitted transaction and another committing transaction may have written the same page. Then, the buffer pool manager must copy that page with only modifications from the committing transaction and write that copied page to non-volatile storage. </li>
<li>The disadvantages are as follows:<ul>
<li>Copying data is expensive. </li>
<li>The buffer pool manager needs to be aware of the context. </li>
<li>Cannot support write sets that exceed the amount of physical memory available. </li>
</ul>
</li>
</ol>
<h2 id="How-does-shadow-paging-work"><a href="#How-does-shadow-paging-work" class="headerlink" title="How does shadow paging work?"></a>How does shadow paging work?</h2><ol>
<li>Instead of copying the entire database, the DBMS copies pages on write to create two versions<ul>
<li>Master: Contains only changes from committed transactions. Read-only transactions access the current master. </li>
<li>Shadow: Temporary database with changes made from uncommitted transactions. </li>
</ul>
</li>
<li>Active modifying transaction copies the page table as the shadow page table. <ul>
<li>When it tries to modify a page, it will copy that page and modify the shadow page table to point to the newly copied page. </li>
<li>To install updates when a transaction commits, overwrite the root so it points to the shadow, thereby swapping the master and shadow. Then, DMBS needs to collect some garbage. </li>
</ul>
</li>
<li>The buffer pool policy is no-steal and force. </li>
<li>The DBMS needs to remove the shadow pages to support rollbacks and recovery. Leave the master and the DB root pointer alone in the undo phase; redo is unnecessary. </li>
</ol>
<h2 id="What-are-the-disadvantages-of-shadow-paging"><a href="#What-are-the-disadvantages-of-shadow-paging" class="headerlink" title="What are the disadvantages of shadow paging?"></a>What are the disadvantages of shadow paging?</h2><ol>
<li>Copying the entire page table is expensive: <ul>
<li>We can use a page table structured like a B+tree (LMDB). There is no need to copy the entire tree; only to copy paths in the tree that lead to updated leaf nodes. </li>
</ul>
</li>
<li>Commit overhead is high:<ul>
<li>Need to flush every updated page, page table, and root. </li>
<li>Data gets fragmented, which is bad for sequential scans. </li>
<li>Require the DBMS to perform writes to random non-contiguous pages on disk.</li>
<li>Need garbage collection. </li>
</ul>
</li>
<li>Only supports one writer transaction at a time or transactions in a batch. If two concurrent writers write on the same page, they all copy from the master version, causing only one write to be reflected on the committed database. </li>
</ol>
<h2 id="How-does-a-journal-file-work"><a href="#How-does-a-journal-file-work" class="headerlink" title="How does a journal file work?"></a>How does a journal file work?</h2><ol>
<li>When a transaction modifies a page, the DBMS copies the original page to a separate journal file before overwriting the master version. </li>
<li>After restarting, if a journal file exists, then the DBMS restores it to undo changes from uncommitted transactions. </li>
</ol>
<h1 id="Write-ahead-log"><a href="#Write-ahead-log" class="headerlink" title="Write-ahead log"></a>Write-ahead log</h1><h2 id="What-is-the-main-idea-of-WAL"><a href="#What-is-the-main-idea-of-WAL" class="headerlink" title="What is the main idea of WAL?"></a>What is the main idea of WAL?</h2><ol>
<li>Maintain a log file separate from data files containing the changes transactions make to the database. <ul>
<li>Assume that the log is on stable storage. </li>
<li>The log contains enough information to perform the necessary undo and redo actions to restore the database. </li>
</ul>
</li>
<li>DBMS must write to disk the log file records that correspond to changes made to a database object before it can flush that object to disk. <ul>
<li>The buffer pool policy is steal and no-force. </li>
</ul>
</li>
</ol>
<h2 id="How-to-write-under-WAL-protocol"><a href="#How-to-write-under-WAL-protocol" class="headerlink" title="How to write under WAL protocol?"></a>How to write under WAL protocol?</h2><ol>
<li>The DBMS stages all a transaction’s log records in volatile storage backed by a buffer pool. <ul>
<li>All log records pertaining to an updated page are written to non-volatile storage before the page itself is over-written in non-volatile storage.</li>
<li>A transaction is not considered committed until all its log records have been written to stable storage. </li>
</ul>
</li>
<li>A <code>&lt;BEGIN&gt;</code> record is written to the log for each transaction to mark its starting point. <ul>
<li>Most DBMS only writes <code>&lt;BEGIN&gt;</code> on the first write command of a transaction instead of at the beginning. </li>
</ul>
</li>
<li>When a transaction finishes, the DBMS will write a <code>&lt;COMMIT&gt;</code> record on the log and ensure all log records are flushed before it returns an acknowledgment to the application. </li>
<li>Each log entry contains information about the change to a single object. <ul>
<li>Transaction ID, object ID, before value (for undo) and after value (for redo). </li>
</ul>
</li>
</ol>
<h2 id="How-to-reduce-flushing-the-log-buffer"><a href="#How-to-reduce-flushing-the-log-buffer" class="headerlink" title="How to reduce flushing the log buffer?"></a>How to reduce flushing the log buffer?</h2><ol>
<li>Flushing the log buffer to disk every time a transaction commits will become a bottleneck. </li>
<li>The DBMS can use the group commit optimization to batch multiple log flushes together to amortize overhead.<ul>
<li>When the buffer is full, flush it to disk. Or if there is a timeout. </li>
<li>Log records from different transactions are mixed. This is fine since we can sort them out by recorded transaction ID. </li>
</ul>
</li>
</ol>
<h2 id="How-to-store-changes"><a href="#How-to-store-changes" class="headerlink" title="How to store changes?"></a>How to store changes?</h2><ol>
<li>The first logging scheme is physical logging. <ul>
<li>Record the byte-level changes made to a specific page. </li>
</ul>
</li>
<li>The second is logical logging. <ul>
<li>Record the high-level operations executed by transactions, e.g., queries. </li>
<li>Logical logging requires less data written in each log record than physical logging. </li>
<li>Difficult to implement recovery with logical logging if you have concurrent transactions running at lower isolation levels. <ul>
<li>The crash may happen in the middle of a query. </li>
<li>It is hard to determine which parts of the database may have been modified by a query before the crash. </li>
</ul>
</li>
<li>It also takes longer to recover because you must re-execute every transaction all over again. </li>
</ul>
</li>
<li>The last is physiological logging. <ul>
<li>Hybrid approach with byte-level changes for a single tuple identified by page ID and slot number. </li>
</ul>
</li>
</ol>
<h2 id="What-is-the-log-structured-system"><a href="#What-is-the-log-structured-system" class="headerlink" title="What is the log-structured system?"></a>What is the log-structured system?</h2><ol>
<li>Log-structured DBMSs do not have dirty pages. <ul>
<li>Any page retrieved from the disk is immutable. </li>
<li>All modifications are reflected through logs. </li>
</ul>
</li>
<li>The DBMS buffers log records in in-memory pages (MemTable). <ul>
<li>If this buffer is full, it must be flushed to disk. However, it may contain changes from uncommitted transactions. </li>
</ul>
</li>
<li>These DBMSs maintain a separate WAL to recreate the MemTable on the crash. </li>
</ol>
<h2 id="How-to-prevent-WAL-from-growing-forever"><a href="#How-to-prevent-WAL-from-growing-forever" class="headerlink" title="How to prevent WAL from growing forever?"></a>How to prevent WAL from growing forever?</h2><ol>
<li>If the WAL grows forever after a crash, the DBMS must replay the entire log, which will take a long time. </li>
<li>The DBMS periodically takes a checkpoint where it flushes all buffers out to disk. </li>
<li>In blocking / consistent checkpoint protocol, <ul>
<li>Procedure<ul>
<li>Pause all queries</li>
<li>Flush all WAL records in memory to disk.</li>
<li>Flush all modified pages in the buffer pool to disk.</li>
<li>Write a <code>&lt;CHECKPOINT&gt;</code> entry to WAL and flush it to disk.</li>
<li>Resume queries</li>
</ul>
</li>
<li>On recovery, we can use the <code>&lt;CHECKPOINT&gt;</code> record as the starting point for analyzing the WAL. <ul>
<li>Any transaction that is committed before the checkpoint is ignored. </li>
<li>Redo transactions that are committed after checkpoint and undo transactions not committed before the crash. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="What-is-the-problem-with-this-naive-checkpoint-protocol"><a href="#What-is-the-problem-with-this-naive-checkpoint-protocol" class="headerlink" title="What is the problem with this naive checkpoint protocol?"></a>What is the problem with this naive checkpoint protocol?</h2><ol>
<li>The DBMS must stall transactions when it takes a checkpoint to ensure a consistent snapshot. <ul>
<li>Too often checkpointing causes the runtime performance to degrade because the system spends too much time flushing buffers. </li>
<li>But waiting a long time is just as bad since the checkpoint will be large and slow, which makes recovery time much longer. </li>
</ul>
</li>
<li>Scanning the log to find uncommitted transactions can take a long time. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/22/Courses/15445/12-Multi-Version-Concurrency-Control/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/22/Courses/15445/12-Multi-Version-Concurrency-Control/" class="post-title-link" itemprop="url">12 Multi-Version Concurrency Control</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-22 15:34:19" itemprop="dateCreated datePublished" datetime="2023-08-22T15:34:19+08:00">2023-08-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 16:02:14" itemprop="dateModified" datetime="2024-03-16T16:02:14+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="Multi-version-logic"><a href="#Multi-version-logic" class="headerlink" title="Multi-version logic"></a>Multi-version logic</h1><h2 id="What-does-multi-version-mean"><a href="#What-does-multi-version-mean" class="headerlink" title="What does multi-version mean?"></a>What does multi-version mean?</h2><ol>
<li><p>The DBMS maintains multiple physical versions of a single logical object in the database. </p>
<ul>
<li><p>When a transaction writes to an object, the DBMS creates a new version of that object. </p>
</li>
<li><p>When a transaction reads an object, it reads the newest version that existed when it started. </p>
</li>
</ul>
</li>
<li><p>Writers do not block readers, and readers do not block writers. </p>
</li>
<li><p>Read-only transactions can read a consistent snapshot without acquiring locks using timestamps to determine visibility. </p>
</li>
<li><p>Multi-version can easily support time-travel queries on a snapshot version of the database. </p>
</li>
</ol>
<h2 id="How-to-maintain-multi-version-logically"><a href="#How-to-maintain-multi-version-logically" class="headerlink" title="How to maintain multi-version logically?"></a>How to maintain multi-version logically?</h2><ol>
<li>Each version describes the current version number, the value for this version, and the lifetime range, i.e., the beginning and end timestamps. </li>
<li>The end timestamp is marked infinity for the newest version. </li>
<li>When a transaction writes an object: <ul>
<li>First, it creates a new entry with a new version number and sets its begin timestamp as its timestamp and end timestamp as infinity. </li>
<li>Then, it marks the end timestamp of the last version as its timestamp. </li>
<li>Physically, this transaction should modify the last version to point to this new version. Hence, it must wait for the transaction that has created the previous version to commit before beginning its commit phase. </li>
</ul>
</li>
</ol>
<h2 id="What-isolation-can-MVCC-support"><a href="#What-isolation-can-MVCC-support" class="headerlink" title="What isolation can MVCC support?"></a>What isolation can MVCC support?</h2><ol>
<li><code>SNAPSHOT ISOLATION</code> is another isolation supported by Oracle. <ul>
<li>It guarantees that all reads made in a transaction see a consistent snapshot of the database when the transaction started.</li>
<li>A transaction will commit only if its writes do not conflict with any concurrent updates made since that snapshot.</li>
</ul>
</li>
<li>It is susceptible to write skew anomaly. <ul>
<li>Two concurrent transactions modify different objects, resulting in race conditions. </li>
<li>If a transaction wants to modify all values to $1$ while another transaction wants to modify all values to $0$. The first transaction changes all $0$s, and the second transaction changes all $1$s. When their result merges with the database, it would be the $1$s and $0$s are flipped instead of all being $1$s or $0$s. </li>
</ul>
</li>
</ol>
<h1 id="Design-decisions"><a href="#Design-decisions" class="headerlink" title="Design decisions"></a>Design decisions</h1><h2 id="What-concurrency-control-protocol-can-be-used"><a href="#What-concurrency-control-protocol-can-be-used" class="headerlink" title="What concurrency control protocol can be used?"></a>What concurrency control protocol can be used?</h2><ol>
<li>All aforementioned protocols can be used in MVCC. </li>
<li>Timestamp ordering assigns transaction timestamps to determine what they can see. </li>
<li>Optimistic concurrency control uses a private workspace for new versions. </li>
<li>Two-phase locking requires transactions to acquire an appropriate lock on a physical version before they can read/write a logical tuple. </li>
</ol>
<h2 id="How-are-versions-stored"><a href="#How-are-versions-stored" class="headerlink" title="How are versions stored?"></a>How are versions stored?</h2><ol>
<li>The DBMS uses the tuples’ pointer field to create a version chain per logical tuple. <ul>
<li>This allows the DBMS to find the version visible to a particular transaction at runtime. </li>
<li>Indexes always point to the “head” of the chain. </li>
</ul>
</li>
<li>The first approach is append-only storage: New versions are appended to the same table space. <ul>
<li>The versions of different logical tuples are inter-mixed. </li>
<li>On every update, append a new version of the tuple into an empty space in the table. </li>
<li>If the chain is from oldest to newest, the DBMS must traverse the chain on look-ups. However, the update does not need to update the index. </li>
<li>Or, the chain can be from oldest to newest. The pros and cons are contrary to the last scenario. </li>
</ul>
</li>
<li>The second approach is time-travel storage: Old versions are copied to separate table space.<ul>
<li>On every update, copy the current version to the time-travel table. Update pointers. Then, Overwrite the master version in the main table and update pointers. </li>
</ul>
</li>
<li>The third approach is delta storage: The original values of the modified attributes are copied into a separate delta record space. <ul>
<li>On every update, copy only the values modified to the delta storage and overwrite the master version. </li>
<li>Transactions can recreate old versions by applying the delta in reverse order. </li>
</ul>
</li>
</ol>
<h2 id="How-to-perform-garbage-collection"><a href="#How-to-perform-garbage-collection" class="headerlink" title="How to perform garbage collection?"></a>How to perform garbage collection?</h2><ol>
<li>The DBMS needs to remove reclaimable physical versions from the database over time. <ul>
<li>Reclaimable means that no active transaction in the DBMS can see an aborted transaction created that version (SI) or the version. </li>
<li>The DBMS can only reclaim versions created by an aborted transaction to support time-travel queries. </li>
</ul>
</li>
<li>To look for expired versions, the implementation has two choices. </li>
<li>The first approach is tuple-level: Find old versions by examining tuples directly.<ul>
<li>In a background vacuuming manner, separate thread(s) periodically scan the table and look for reclaimable versions. This design works with any storage. </li>
<li>In a cooperative cleaning manner, worker threads identify reclaimable versions as they traverse the version chain. It only works with oldest-to-newest. </li>
</ul>
</li>
<li>The second approach is transaction-level: transactions keep track of their old versions on updates, so the DBMS does not have to scan tuples to determine visibility. <ul>
<li>Each transaction keeps track of its read/write set. On commit/abort, the transaction provides this information to a centralized vacuum worker. </li>
<li>The DBMS periodically determines when all versions created by a finished transaction are no longer visible. </li>
</ul>
</li>
</ol>
<h2 id="How-to-manage-indexes"><a href="#How-to-manage-indexes" class="headerlink" title="How to manage indexes?"></a>How to manage indexes?</h2><ol>
<li>Primary key indexes point to the version chain head. <ul>
<li>How often the DBMS must update the primary key index depends on whether the system creates new versions when a tuple is updated.</li>
<li>If a transaction updates a tuple’s primary key attribute(s), then this is treated as a <code>DELETE</code> followed by an <code>INSERT</code>. </li>
</ul>
</li>
<li>Secondary indexes may use the physical address to the version chain head like a primary key index. </li>
<li>The secondary indexes may also use logical pointers. <ul>
<li>It uses a fixed identifier per tuple that does not change, e.g., primary key or tuple ID. </li>
<li>This would require an extra indirection layer. </li>
</ul>
</li>
</ol>
<h2 id="How-to-delete-a-tuple"><a href="#How-to-delete-a-tuple" class="headerlink" title="How to delete a tuple?"></a>How to delete a tuple?</h2><ol>
<li>The DBMS physically deletes a tuple from the database only when all versions of a logically deleted tuple are not visible. </li>
<li>If a tuple is deleted, a new version cannot be after the latest version. </li>
<li>There are two ways to denote that a tuple has been logically deleted at some point in time. <ul>
<li>The first way is to maintain a deleted flag to indicate that the logical tuple has been deleted after the newest physical version. The flag can either be in a tuple header or a separate column. </li>
<li>The second tombstone tuple way is to create an empty physical version to indicate that a logical tuple is deleted. <ul>
<li>It uses a separate pool for tombstone tuples with only a special bit pattern in the version chain pointer to reduce the storage overhead.</li>
</ul>
</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2023/08/20/Courses/15445/11-Timestamp-Ordering-Concurrency-Control/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2023/08/20/Courses/15445/11-Timestamp-Ordering-Concurrency-Control/" class="post-title-link" itemprop="url">11 Timestamp Ordering Concurrency Control</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-08-20 23:59:40" itemprop="dateCreated datePublished" datetime="2023-08-20T23:59:40+08:00">2023-08-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 15:48:54" itemprop="dateModified" datetime="2024-03-16T15:48:54+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-445-645-Database-System/" itemprop="url" rel="index"><span itemprop="name">CMU 15-445/645 Database System</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>@[toc]</p>
<h1 id="T-O-Protocols"><a href="#T-O-Protocols" class="headerlink" title="T/O Protocols"></a>T/O Protocols</h1><h2 id="What-is-the-difference-between-2PL-and-T-O"><a href="#What-is-the-difference-between-2PL-and-T-O" class="headerlink" title="What is the difference between 2PL and T/O?"></a>What is the difference between 2PL and T/O?</h2><ol>
<li>2PL determines the serializability order of conflicting operations at runtime while transactions execute, while T/O determines the serializability order of transactions before they execute. <ul>
<li>If $TS(T_i) &lt; TS(T_j)$, then the DBMS must ensure that the execution schedule is equivalent to a serial schedule where $T_i$ appears before $T_j$. </li>
<li>Different schemes assign timestamps at different times during the transaction. </li>
<li>Timestamps can be implemented using different strategies: system/wall clock, logical counter, or hybrid. </li>
</ul>
</li>
<li>2PL is pessimistic. It assumes that conflicts between transactions are very common. <ul>
<li>Hence, it uses locks to prevent conflicts. </li>
</ul>
</li>
<li>Timestamp ordering (T/O) is a more optimistic way. It assumes that conflicts between transactions are rare. <ul>
<li>Hence, it allows each transaction to execute all operations they want and validate their legitimacy after each transaction committing and before applying anything to the main database. </li>
</ul>
</li>
</ol>
<h2 id="Basic-T-O-protocol"><a href="#Basic-T-O-protocol" class="headerlink" title="Basic T/O protocol"></a>Basic T/O protocol</h2><h3 id="What-is-the-main-idea-of-basic-T-O-protocol"><a href="#What-is-the-main-idea-of-basic-T-O-protocol" class="headerlink" title="What is the main idea of basic T/O protocol?"></a>What is the main idea of basic T/O protocol?</h3><ol>
<li>Txns read and write objects without locks.</li>
<li>The timestamp of each transaction is assigned at the <code>BEGIN</code> command. </li>
<li>Every object $X$ is tagged with a timestamp of the last transaction that successfully read/write<ul>
<li>$W-TS(X)$: Write timestamp on $X$</li>
<li>$R-TS(X)$: Read timestamp on $X$</li>
</ul>
</li>
<li>Check timestamps for every operation. If a transaction tries to access an object “from the future,” it aborts and restarts. </li>
</ol>
<h3 id="How-does-basic-T-O-check-each-operation"><a href="#How-does-basic-T-O-check-each-operation" class="headerlink" title="How does basic T/O check each operation?"></a>How does basic T/O check each operation?</h3><ol>
<li>When $T_i$ wants to read $X$:<ul>
<li>If $TS(T_i)&lt;W-TS(X)$, abort $T_i$ and restart it with a new TS to prevent it from starvation. <ul>
<li>This condition means that this $T_i$ is trying to read something from the future. </li>
</ul>
</li>
<li>Else, allow $T_i$ to read $X$, and update $R-TS(X)$ to $max(R-TS(X), TS(T_i))$. </li>
<li>A local copy of $X$ is made to ensure repeatable reads for $T_i$. </li>
</ul>
</li>
<li>When $T_i$ wants to write $X$:<ul>
<li>If $TS(T_i)&lt;R-TS(X)$ or $TS(T_i)&lt;W-TS(X)$, abort and restart $T_i$. <ul>
<li>The first condition means that another transaction from the future cannot see the write from $T_i$ in the past. </li>
<li>The second condition means that another transaction from the future already wrote this object and $T_i$ cannot overwrite it in the past. </li>
</ul>
</li>
<li>Else, allow $T_i$ to write $X$ and update $W-TS(X)$. </li>
<li>Also, a local copy is made. </li>
</ul>
</li>
</ol>
<h3 id="Can-we-optimize-the-write-rule-to-decrease-the-possibility-of-abort"><a href="#Can-we-optimize-the-write-rule-to-decrease-the-possibility-of-abort" class="headerlink" title="Can we optimize the write rule to decrease the possibility of abort?"></a>Can we optimize the write rule to decrease the possibility of abort?</h3><ol>
<li>Thomas write rule: If $TS(T_i) &lt; W-TS(X)$, ignore the write to allow the transaction to continue executing without aborting. <ul>
<li>The thought is that we can see this violation as an immediate write from a future transaction right after the successful write from $T_i$. </li>
<li>The effects are similar, i.e., no one sees what does $T_i$ write. </li>
</ul>
</li>
<li>If $TS(T_i)&lt;R-TS(X)$, we still need to abort $T_i$. </li>
</ol>
<h3 id="What-are-the-issues-of-basic-T-O"><a href="#What-are-the-issues-of-basic-T-O" class="headerlink" title="What are the issues of basic T/O?"></a>What are the issues of basic T/O?</h3><ol>
<li>There is high overhead from copying data to the transaction’s workspace and updating timestamps. Every read requires the transaction to write to the database. </li>
<li>Long-running transactions can get starved. The likelihood that a transaction will read something from a newer transaction increases. </li>
<li>If you assume that conflicts between transactions are rare and that most transactions are short-lived, forcing transactions to acquire locks or update timestamps adds unnecessary overhead. </li>
</ol>
<h2 id="Optimistic-concurrency-control"><a href="#Optimistic-concurrency-control" class="headerlink" title="Optimistic concurrency control"></a>Optimistic concurrency control</h2><h3 id="What-is-the-main-idea-of-OCC"><a href="#What-is-the-main-idea-of-OCC" class="headerlink" title="What is the main idea of OCC?"></a>What is the main idea of OCC?</h3><ol>
<li>OCC assumes that the number of conflicts is low. Especially when: <ul>
<li>All transactions are read-only (ideal).</li>
<li>Txns access disjoint subsets of data.</li>
<li>The database is large, and the workload is not skewed. </li>
</ul>
</li>
<li>The DBMS creates a private workspace for each transaction. <ul>
<li>Any object read is copied into the workspace. Modifications are applied to the workspace. </li>
<li>When a transaction is committed, the DBMS compares the workspace write set to see whether it conflicts with other transactions. </li>
<li>The write set is installed into the “global” database if there are no conflicts. </li>
</ul>
</li>
<li>OCC has three phases:<ul>
<li>Read Phase: Track the read/write sets of transactions and store their writes in a private workspace, i.e., execution of transaction content. </li>
<li>Validation Phase: When a transaction commits, check whether it conflicts with other transactions. </li>
<li>Write Phase: If validation succeeds, apply private changes to the database. Otherwise, abort and restart the transaction. <ul>
<li>Serial Commits: Use a global latch to limit a single transaction to be in the Validation/Write phases at a time. </li>
<li>Parallel Commits: Use fine-grained write latches to support parallel Validation/Write phases. Txns acquire latches in primary key order to avoid deadlocks.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="What-will-happen-in-the-validation-phase"><a href="#What-will-happen-in-the-validation-phase" class="headerlink" title="What will happen in the validation phase?"></a>What will happen in the validation phase?</h3><ol>
<li>When transaction $T_i$ invokes <code>COMMIT</code>, the DBMS checks if it conflicts with other transactions. <ul>
<li>The DBMS needs to guarantee only serializable schedules are permitted.</li>
<li>Check other transactions for RW and WW conflicts and ensure that conflicts are in one direction (e.g., older→younger). </li>
</ul>
</li>
<li>There are two approaches to valid: <ul>
<li>In backward validation, check whether the committing transaction intersects its read/write sets with any transactions already committed.<br><img src="/imgs/15445/TO/backward.png" width="50%"></li>
<li>In forward validation, check whether the committing transaction intersects its read/write sets with any active transactions that have not yet been committed.<br><img src="/imgs/15445/TO/forward.png" width="50%"></li>
</ul>
</li>
<li>Each transaction’s timestamp is assigned at the beginning of the validation phase. Check the timestamp ordering of the committing transaction with all other concerned transactions. When $TS(T_i)&lt;TS(T_j)$, there are only three cases: <ul>
<li>If $T_i$ completes all three phases before $T_j$ begins its execution. This means that there is serial ordering. </li>
<li>If $T_i$ completes before $T_j$ starts its Write phase, then we require that $T_i$ does not write to any object read by $T_j$, i.e. $WriteSet(T_i)\cap ReadSet(T_j)=\empty$. <ul>
<li>At this condition, we can conclude that $T_j$ cannot see anything written by $T_i$. Therefore, if $T_j$ read anything written by $T_i$, it is a violation. </li>
</ul>
</li>
<li>If $T_i$ completes its Read phase before $T_j$​ completes its Read phase, then we require that $T_i$ does not write to any object that is either read or written by $T_j$. <ul>
<li>$WriteSet(T_i) \cap ReadSet(T_j) = \empty$ and $WriteSet(T_i) \cap WriteSet(T_j) = \empty$. </li>
<li>Anything wrote by $T_i$ should be seen by $T_j$ and should not conflict with what $T_j$ intends to write. </li>
<li>OCC wants more than just serializable orders. Similar to the Thomas write rule, if we allow the write sets to have something in common, it would still be serializable, yet in conflict. </li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="What-are-the-issues-of-OCC"><a href="#What-are-the-issues-of-OCC" class="headerlink" title="What are the issues of OCC?"></a>What are the issues of OCC?</h3><ol>
<li>High overhead for copying data locally. </li>
<li>Validation/Write phase bottlenecks.</li>
<li>Aborts are more wasteful than in 2PL because they only occur after a transaction has been executed.</li>
</ol>
<h1 id="The-phantom-problem"><a href="#The-phantom-problem" class="headerlink" title="The phantom problem"></a>The phantom problem</h1><h2 id="What-is-the-phantom-problem"><a href="#What-is-the-phantom-problem" class="headerlink" title="What is the phantom problem?"></a>What is the phantom problem?</h2><ol>
<li>In the above transaction management protocols, we assume that the total number of tuples in a table is fixed, i.e., transactions will not execute insertion or deletion. </li>
<li>Insertions or deletions result in different results for the same range of scan queries, e.g., count, maximum. <ul>
<li>The reason is that transactions can only lock on existing records, not one underway. </li>
</ul>
</li>
</ol>
<h2 id="How-can-we-solve-the-phantom-problem"><a href="#How-can-we-solve-the-phantom-problem" class="headerlink" title="How can we solve the phantom problem?"></a>How can we solve the phantom problem?</h2><ol>
<li>The first approach is to re-execute scans. <ul>
<li>The DBMS tracks the <code>WHERE</code> clause for all queries that the transaction executes. Retain the scan set for every range query in a transaction. </li>
<li>Upon committing, re-execute the scan portion of each query and check whether it generates the same result. </li>
<li>This could double the execution time for all queries, which may be unacceptable. </li>
</ul>
</li>
<li>The second approach is by predicate locking. <ul>
<li>Shared lock on the predicate in a <code>WHERE</code> clause of a <code>SELECT</code> query. </li>
<li>Exclusive lock on the predicate in a <code>WHERE</code> clause of any <code>UPDATE</code>, <code>INSERT</code>, or <code>DELETE</code> query. </li>
<li>Prevent any query changing the result of the locked predicate from executing. </li>
</ul>
</li>
<li>The third approach is index locking. <ul>
<li>Key-value locks only cover a single existing key-value in an index, while gap locks cover those virtual keys for non-existent values. </li>
<li>Key-range locks take multiple key-value and gap locks to lock on a range. </li>
<li>Hierarchical locking allows a transaction to hold wider key-range locks with different locking modes to reduce the number of visits to the lock manager. </li>
</ul>
</li>
<li>If there is no suitable index, then the transaction must obtain the following: <ul>
<li>A lock on every page in the table to prevent a record’s attributes from being changed to fit the predicates. </li>
<li>The lock for the table itself prevents records that fit the predicates from being added or deleted. </li>
</ul>
</li>
</ol>
<h2 id="What-are-isolation-levels"><a href="#What-are-isolation-levels" class="headerlink" title="What are isolation levels?"></a>What are isolation levels?</h2><ol>
<li>We may use a weaker level of consistency to improve scalability. </li>
<li>It provides for greater concurrency at the cost of exposing transactions to uncommitted changes: dirty reads, unrepeatable reads, and phantom reads. </li>
<li>The four isolation levels are as shown below:<br><img src="/imgs/15445/TO/isolation.png" width="50%"></li>
<li>Each isolation level requires different locks to implement:<ul>
<li><code>SERIALIZABLE</code>: Obtain all locks first, plus index locks, plus strict 2PL.</li>
<li><code>REPEATABLE READS</code>: Same as above, but no index locks.</li>
<li><code>READ COMMITTED</code>: Same as above, but S locks are released immediately.</li>
<li><code>READ UNCOMMITTED</code>: Same as above, but allows dirty reads (no S locks). </li>
</ul>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/about/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/about/">1</a><a class="page-number" href="/about/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/about/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/about/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/about/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
