<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/about/page/7/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/about/page/7/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"about/page/7/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">64</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/" class="post-title-link" itemprop="url">04. Work Distribution and Scheduling</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-17 15:35:33" itemprop="dateCreated datePublished" datetime="2022-05-17T15:35:33+08:00">2022-05-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 19:12:41" itemprop="dateModified" datetime="2024-03-16T19:12:41+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="balancing-the-workload"><a class="markdownIt-Anchor" href="#balancing-the-workload"></a> Balancing the workload</h1>
<p>Always implement the simplest solution first, then measure performance to determine if you need to do better.</p>
<h2 id="static-assignment"><a class="markdownIt-Anchor" href="#static-assignment"></a> Static assignment</h2>
<ol>
<li>The assignment of work to threads is pre-determined. But it is not necessarily determined at compile-time; it may depend on runtime parameters.</li>
<li>Benefit: simple, essentially zero runtime overhead</li>
<li>Applicable situation: When the cost (execution time) of work and the amount of work is predictable. The following are some of the most common situations:<br />
When it is known upfront that all work has the same cost<br />
When work is predictable, but not all jobs have the same cost<br />
When statistics about execution time are known</li>
<li>Semi-static assignment: When the work cost is predictable for the near-term future, we can periodically profile itself and re-adjust the assignment.<br />
The assignment is “static” for the interval between re-adjustments</li>
</ol>
<h2 id="dynamic-assignment"><a class="markdownIt-Anchor" href="#dynamic-assignment"></a> Dynamic assignment</h2>
<ol>
<li>
<p>The program determines assignment dynamically at runtime to ensure a well-distributed load. They are often used when the execution time or the total number of tasks is unpredictable.</p>
</li>
<li>
<p>The ISPC task is implemented dynamically.</p>
</li>
</ol>
<h3 id="with-one-queue"><a class="markdownIt-Anchor" href="#with-one-queue"></a> With one queue</h3>
<ol>
<li>
<p>The programmers divide the whole problem into sub-problems (or “tasks”, “work”). A queue shared by all threads is a collection of work to do. A thread will grab another task from the queue whenever it finishes its task.</p>
</li>
<li>
<p>Fine granularity partitioning: each task is small.<br />
This is likely to have a good workload balance, but there is potential for high synchronization costs.</p>
</li>
<li>
<p>Coarse granularity partitioning: each task is larger.<br />
This will decrease synchronization cost and the overhead but may have a worse workload balance.</p>
</li>
<li>
<p>Long tasks should be scheduled first. Thread performing long tasks performs fewer overall tasks but approximately the same amount of work as the other threads. This requires some knowledge of the workload.</p>
</li>
</ol>
<h3 id="with-a-set-of-queues"><a class="markdownIt-Anchor" href="#with-a-set-of-queues"></a> With a set of queues</h3>
<ol>
<li>
<p>When assigned to one queue, all threads have to communicate with each other about the queue.</p>
</li>
<li>
<p>Each thread has its queue, and it only executes tasks in its queue. So, there is no need to communicate with other threads.</p>
</li>
<li>
<p>Initially, the programmer pushes tasks into queues arbitrarily (a bit like a static assignment).<br />
The dynamic is that when a queue is empty, that thread can steal from other still-working threads.<br />
It will steal from a random thread. Every time, it will steal a proportion of the tasks in the target queue, not all of them, and usually more than one task.<br />
A thread is terminated when there is no thread for it to steal; when a steal fails, it will try to steal from other threads until it has tried all of them.</p>
</li>
<li>
<p>Stealing involves communication but at a lower frequency than one queue method. In this way, the local queue access is fast.</p>
</li>
<li>
<p>Sometimes, it is hard to have fully independent tasks, but work in task queues need not be independent.<br />
A task is not removed from the queue and assigned to the worker thread until all task dependencies are satisfied. Workers can submit new tasks (with optional, explicit dependencies) to the task system.</p>
</li>
</ol>
<h1 id="scheduling"><a class="markdownIt-Anchor" href="#scheduling"></a> Scheduling</h1>
<p>In a divide-and-conquer algorithm, there are both dependencies and independencies. Like in quick-sort, both divides depend on the partition, and those two divides are independent.<br />
With Cilk Plus, we can express divide-and-conquer easier.</p>
<h2 id="cilk_spawn"><a class="markdownIt-Anchor" href="#cilk_spawn"></a> cilk_spawn</h2>
<ol>
<li>
<p><code>cilk_spawn</code> is labeled before a function call so that the called function can run concurrently with the code after the call.<br />
The call labeled <code>cilk_spawn</code> is the spawned child, and the rest of the code is the continuation.</p>
</li>
<li>
<p>In divide-and-conquer, there is always a time when the problem size is small enough that the overhead of spawn trumps the benefits of<br />
potential parallelization. Then, we will solve those problems sequentially.</p>
</li>
<li>
<p>The main idea is to expose independent work (potential parallelism) to the system using <code>cilk_spawn</code>.</p>
</li>
<li>
<p><code>cilk_spawn</code> is a bit like <code>pthread_create</code>, and <code>cilk_sync</code> is similar to <code>pthread_join</code>. But the <code>pthread</code> has some problems when too many threads are spawned.<br />
The first is the heavyweight spawn operation. Many more concurrently running threads than cores will cause context-switching overhead, a larger working set than necessary, and less cache locality.</p>
</li>
<li>
<p>The Cilk Plus runtime maintains a pool of worker threads. All threads are created at the application launch.  The machine has exactly as many worker threads as in execution contexts.<br />
If we labeled everything <code>cilk_spawn</code>, the main thread has nothing to do.</p>
</li>
<li>
<p>Each thread in the pool will maintain a work queue to store what word needs to be done.<br />
When a thread goes idle, it will look in the busy thread’s queue for work and move work from the busy thread’s queue to its queue.</p>
</li>
<li>
<p>If the caller thread runs the continuation first, the queue should record the child for later execution, and the child is made available for stealing by other threads.<br />
The caller thread will spawn as many children as possible using this method, like BFS. If there is no stealing, the execution order is very different than<br />
that of program withcilk_spawnremoved.</p>
</li>
<li>
<p>If the caller thread runs the child first, the queue should record continuation for later execution, and continuation is made available for stealing by other threads.<br />
In this method, the caller thread will only create one item to steal.<br />
If no stealing occurs, the thread continually pops continuation from the work queue and enqueues new continuation (like DFS). The execution order is the same as for the program with spawn removed.<br />
If continuation is stolen, the stealing thread spawns the next child.</p>
</li>
<li>
<p>If the continuation is run first, there will be more items to steal; thus, it will be a better advantage of multi-thread.<br />
But if the continuation is run first, the work queue storage for a system with T threads is no more than T times that of stack storage for single-threaded execution and thus saves more space.</p>
</li>
<li>
<p>The work queue is implemented as a dequeue (double-ended queue).<br />
Local thread pushes/pops from the “tail” (bottom), while remote threads steal from the “head” (top).<br />
Reduces contention with local thread: local thread does not access the same part of dequeue as stealing threads.<br />
Do larger work first: in divide-and-conquer, the top of the queue is usually at the beginning of the call tree and is a larger piece of work.<br />
Maximizes locality: in conjunction with the run-child-first policy, the local thread works on the local part of the call tree</p>
</li>
</ol>
<h2 id="cilk_sync"><a class="markdownIt-Anchor" href="#cilk_sync"></a> cilk_sync</h2>
<ol>
<li>
<p><code>cilk_sync</code> is used after those <code>cilk_spawn</code> call code. It will return when all calls spawned by the current function have been completed.</p>
</li>
<li>
<p>There is an implicit <code>cilk_sync</code> at the end of every function containing a cilk_spawn, so when a Cilk function returns, all work associated with that function is complete.</p>
</li>
<li>
<p>If no work has been stolen by other threads, then there’s nothing to do at the sync point, <code>cilk_sync</code> is a no-op. But this is not a common situation.</p>
</li>
<li>
<p>One way to implement sync is with a stalling joint.<br />
The thread that initiates the fork must perform the sync. Therefore, it waits for all spawned work to be complete.<br />
The descriptor for block A was created.<br />
When stealing from the initial thread happens, a descriptor for that stolen work is created to track the number of outstanding spawns for the block and the number of those that have completed.<br />
When all the child spawned for the block is done, this block is considered done, and the descriptor is free. When all the blocks are done, sync is fulfilled.</p>
</li>
<li>
<p>Another way to implement sync is the greedy policy.<br />
When the thread that initiates the fork goes idle, it can look to steal new work.  The last thread to reach the join point continues execution after sync. This will also create a descriptor for those stolen works.<br />
The worker thread that initiated spawn may not be the thread that executes logic after <code>cilk_sync</code>.</p>
</li>
<li>
<p>In greed policy, All threads always attempt to steal if there is nothing to do, and the thread only goes idle if no work to steal is present in the system. But in stalling policy, the initial thread doesn’t steal and only waits until its work is done.</p>
</li>
<li>
<p>The overhead of bookkeeping steals and managing sync points only occurs when steals occur. If large pieces of work are stolen, this should occur infrequently. Most of the time, threads push/pop local work from their local queue.</p>
</li>
<li>
<p>Cilk uses greedy join scheduling.</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/04/20/Courses/CS149/03-Parallel-Programming-Basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/20/Courses/CS149/03-Parallel-Programming-Basics/" class="post-title-link" itemprop="url">03. Parallel Programming Basics</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-20 14:27:45" itemprop="dateCreated datePublished" datetime="2022-04-20T14:27:45+08:00">2022-04-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 18:45:17" itemprop="dateModified" datetime="2024-03-16T18:45:17+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="decomposition"><a class="markdownIt-Anchor" href="#decomposition"></a> Decomposition</h1>
<ol>
<li>
<p>Decomposition: The problem to solve is usually a chunk of work. So the first step we need to do is to decomposite them into subproblems (a.k.a tasks, work to do)</p>
</li>
<li>
<p>We usually want the number of subproblems to be at least as many as processors.</p>
</li>
<li>
<p>The key aspect of decomposition is to identify dependencies. We want subproblems to be independent so that they can be paralleled.</p>
</li>
<li>
<p>Amdahl’s Law: dependencies limit maximum speedup due to parallelism<br />
Let S = the fraction of sequential execution that is inherently sequential (dependencies prevent parallel execution). The maximum speedup due to parallel execution ≤ 1/S.</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>≤</mo><mfrac><mi>t</mi><mrow><mi>s</mi><mi>t</mi><mo>+</mo><mfrac><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>s</mi><mo stretchy="false">)</mo><mi>t</mi></mrow><mi>p</mi></mfrac></mrow></mfrac><mo>=</mo><mfrac><mn>1</mn><mrow><mi>s</mi><mo>+</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>s</mi></mrow><mi>p</mi></mfrac></mrow></mfrac></mrow><annotation encoding="application/x-tex">speedup \le\frac{t}{st+\frac{(1-s)t}{p}}=\frac1{s+\frac{1-s}{p}} 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.6731879999999997em;vertical-align:-1.381108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.29208em;"><span style="top:-2.11em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.2399999999999998em;"><span class="pstrut" style="height:3.01em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.687em;"><span class="pstrut" style="height:3.01em;"></span><span class="mord"><span class="mord mathnormal">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.381108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.537656em;vertical-align:-1.216216em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.264892em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.216216em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li>
<p>In most cases, the programmer is responsible for performing decomposition.</p>
</li>
<li>
<p>When doing the decomposition, it is better to think of partitioning computation instead of data.</p>
</li>
</ol>
<h1 id="assignment"><a class="markdownIt-Anchor" href="#assignment"></a> Assignment</h1>
<ol>
<li>
<p>Assignment: When the subproblems are more than processors, we will group them to form a larger chunk of work and assign the grouped tasks to parallel threads.</p>
</li>
<li>
<p>One goal is to balance the workload so that each processor finishes their work almost simultaneously.<br />
Another goal is to reduce communication costs. Getting data from another processor is nontrivial expensive, either cache miss or waiting for a message.<br />
These two goals are at odds with each other.</p>
</li>
<li>
<p>This step can be performed statically or dynamically during the execution<br />
Static way: Before the processors begin the work, we have already decided how to divide things.<br />
Dynamic way: We work out how to divide on the way as processing.</p>
</li>
<li>
<p>We can choose the static way with the programCount and programIndex assignment or pthread</p>
</li>
<li>
<p>We can choose the dynamic way with foreach if the system chooses the dynamic way or the queue.<br />
In the queue way, we arrange all tasks in a queue, and when a processor has done its job, it will grab another task from the queue. This is an excellent way to balance the workload, but maintaining the queue and acquiring tasks from it might cost some performance.</p>
</li>
</ol>
<h1 id="orchestration"><a class="markdownIt-Anchor" href="#orchestration"></a> Orchestration</h1>
<ol>
<li>
<p>Orchestration: When parallel threads are running, they may need to cooperate. So, we need to let them communicate correctly.</p>
</li>
<li>
<p>We will worry about things like structure communication, synchronization, organizing data structure in memory, and scheduling tasks.</p>
</li>
<li>
<p>The goal is to reduce the costs of communication/sync, preserve the locality of data reference, reduce the overhead of synchronization or communication, etc.</p>
</li>
<li>
<p>In the shared address space model, lock/unlock is commonly used for preserving atomicity, and barriers can divide computation into phases. When threads execute to barriers, they will stop and wait. Until enough threads hit the barrier, those stalled threads can execute rest codes.</p>
</li>
<li>
<p>A commonly used optimization strategy is fewer locks/unlocks and barriers.<br />
Every time we operate a shared variable, we must use the lock/unlock to keep atomicity. We could operate on partial variables locally and then merge the partial results.<br />
When we use a barrier to keep a shared variable valid when it might be changed at the next phase, we can use different shared variables in successive phases. This is to trade off footprint for removing dependencies.</p>
</li>
</ol>
<h1 id="mapping"><a class="markdownIt-Anchor" href="#mapping"></a> Mapping</h1>
<ol>
<li>
<p>Mapping: Finally, we need to map each thread to physical hardware.</p>
</li>
<li>
<p>When we map pthreads to hardware execution context on a CPU core, this is done by the operating system.<br />
When we map ISPC program instances to vector instruction lanes, this is done by the compiler.<br />
When we map CUDA thread blocks to GPU cores, this is done by the hardware.</p>
</li>
<li>
<p>Place related threads (cooperating threads) on the same processor to maximize locality data sharing and minimize costs of comm/sync.<br />
Place unrelated threads on the same processor (one might be bandwidth-limited, and another might be compute-limited) to use the machine more efficiently.</p>
</li>
<li>
<p>Normally, we just let the OS do the mapping as it wants. But sometimes we still want to control the way of mapping.</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/04/18/Courses/CS149/02-Parallel-programming-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/18/Courses/CS149/02-Parallel-programming-models/" class="post-title-link" itemprop="url">02. Parallel programming models</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-18 11:53:04" itemprop="dateCreated datePublished" datetime="2022-04-18T11:53:04+08:00">2022-04-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 18:39:27" itemprop="dateModified" datetime="2024-03-16T18:39:27+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ispc-intel-spmd-program-compiler"><a class="markdownIt-Anchor" href="#ispc-intel-spmd-program-compiler"></a> ISPC (Intel SPMD Program Compiler)</h1>
<h2 id="format-of-ispc"><a class="markdownIt-Anchor" href="#format-of-ispc"></a> Format of ISPC</h2>
<ol>
<li>
<p>ISPC is an SPMD compiler, not an SIMD.</p>
</li>
<li>
<p>The code that needs to be paralleled will be written in a file with the suffix “.ispc” as a function. And we will call that function in the main.cpp.<br />
Call to the ISPC function spawns a gang of ISPC program instances. All instances run ISPC code concurrently. The ISPC function will return when all instances have been completed.<br />
All code in main.cpp will be executed sequentially.</p>
</li>
<li>
<p><code>programCount</code>: the number of simultaneously executing instances in the gang. It is the same for all instances, thus called “uniform value.” This is not set by the programmer but by the run-time system. Programmers can only read it but don’t set that.</p>
</li>
<li>
<p><code>programIndex</code>: the ID of the current instance in the gang. It is a non-uniform value, namely varying.<br />
This is used to assign work to each instance. If we don’t use the programIndex to control the work to be done by each instance, they will all do all the work. Thus, there will be redundancy and no performance improvement.</p>
</li>
<li>
<p><code>uniform</code>: a type modifier. All instances have a copy of the same value for this variable. Its use is purely an optimization. Not needed for correctness.<br />
We cannot directly add a non-uniform value to a uniform value, which will cause a compile-time type error. To do so, we need a reduce_add function from the ISPC library.</p>
</li>
<li>
<p>Those ISPC program instances are not separate threads. The ISPC compiler generates a SIMD thread. So, the <code>programCount</code> is the vector width of the machine.<br />
So, it can only run on one core. And “task” is used to achieve multi-core execution.</p>
</li>
</ol>
<h2 id="ways-of-assignment"><a class="markdownIt-Anchor" href="#ways-of-assignment"></a> Ways of assignment</h2>
<ol>
<li>
<p>Interleaved assignment: the data processed by each instance is discontinuous. Namely, the subscript is:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>I</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi><mo>+</mo><mi>i</mi><mo>×</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo separator="true">,</mo><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mfrac><mrow><mi>N</mi><mo>−</mo><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>I</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi></mrow><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">programIndex+i\times programCount,i\in[0,\frac{N-programIndex}{programCount}) 
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.74285em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.25188em;vertical-align:-0.8804400000000001em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p>
</li>
<li>
<p>Block assignment: the data is split into several chunks, and each instance processes one chunk. So, the data is continuous. Namely, the subscript is:</p>
<p class='katex-block'><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>I</mi><mi>n</mi><mi>d</mi><mi>e</mi><mi>x</mi><mo>×</mo><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>+</mo><mi>i</mi><mo separator="true">,</mo><mi>i</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>c</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo>=</mo><mfrac><mi>N</mi><mrow><mi>p</mi><mi>r</mi><mi>o</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">programIndex\times count+i, i\in[0,count),count=\frac{N}{programCount}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.69841em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.2407700000000004em;vertical-align:-0.8804400000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">u</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804400000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
</li>
<li>
<p>When using the block assignment, there are some situations when the cost of processing later data is more expensive than processing former data, making the assignment uneven. So, the interleaved assignment is less risky.</p>
</li>
<li>
<p>Since ISPC only generates one thread, continuous access to data in block assignments is less cache-friendly than discontinuous access in interleaved assignments.<br />
If there are several threads, the block assignment might be more cache-friendly.</p>
</li>
<li>
<p>The data requested by all instances at the same time is a vector. In interleaved assignment, the data in a vector is memory continuous, while it is not in block assignment. So, in memory access, interleaved assignment is faster.</p>
</li>
<li>
<p><code>foreach (i = 0 ... N)</code> says that these loop iterations can be paralleled, and ISPC implementation assigns iterations to program instances in a gang. The current ISPC implementation will perform a static interleaved assignment.</p>
</li>
</ol>
<h2 id="system-layers"><a class="markdownIt-Anchor" href="#system-layers"></a> System layers</h2>
<ol>
<li>
<p>The layers from up to down are parallel applications, compilers and parallel runtime, operating system, Micro-architecture (hardware implementation)</p>
</li>
<li>
<p>Different parallel models can have various combinations of concerned layers.</p>
</li>
<li>
<p>If we express parallelism with pthread, it goes through all layers. First the parallel application calls <code>pthread_create()</code> to access the pthread library implementation. Then, the pthread library uses System call API to ask for OS support: kernel thread management. Finally, the OS uses <code>x86-64</code> to control modern multi-core CPU.</p>
</li>
<li>
<p>If we express parallelism with ISPC without “task,” it doesn’t need the support of OS. The parallel application uses ISPC language to ask for the service of the ISPC compiler. The compiler will produce machine language of x86-64, including <code>AVX vector instruction</code> to control a single-core CPU.</p>
</li>
</ol>
<h1 id="communication"><a class="markdownIt-Anchor" href="#communication"></a> Communication</h1>
<h2 id="shared-address-space"><a class="markdownIt-Anchor" href="#shared-address-space"></a> Shared Address Space</h2>
<ol>
<li>
<p>The whole machine has a shared space address. When threads aren’t working together, they access their memory space. They can communicate with each other by reading or writing the same data and manipulating synchronization primitives (like locks)</p>
</li>
<li>
<p>This model requires hardware support to implement efficiently. In hardware implementation, any processor can directly reference any memory location.</p>
</li>
<li>
<p>Symmetric (shared-memory) multi-processor (SMP): all processors have uniform memory access time. Namely, accessing an uncached memory address costs the same for all processors.<br />
This is unscalable since the access latency will increase fast with more and more processors and memory chips.<br />
The cores can share memory through a shared L3 cache or a crossbar switch with a die area of one core.</p>
</li>
<li>
<p>Non-uniform memory access (NUMA): All processors can access any memory location, but the cost of memory access (latency and bandwidth) differs for different processors. Each processor has a memory chip that is close to it.<br />
This is more scalable because of the low latency and high bandwidth to local memory.<br />
Cost is the increased programmer effort for performance tuning. Finding and exploiting locality is important to performance (want most memory accesses to be to local memories)</p>
</li>
</ol>
<h2 id="message-passing"><a class="markdownIt-Anchor" href="#message-passing"></a> Message Passing</h2>
<ol>
<li>
<p>Threads operate within their private address spaces and communicate by sending/receiving messages.</p>
</li>
<li>
<p>send: specifies recipient, buffer to be transmitted, and optional message identifier (“tag”)<br />
receive: sender, specifies the buffer to store data, and optional message identifier</p>
</li>
<li>
<p>With this model, we can easily build large-scale parallel machines by interconnecting them. Hardware need not implement system-wide loads and stores to execute message-passing programs; need only be able to communicate messages.<br />
However, the interconnect speed can be the system’s bottleneck.</p>
</li>
<li>
<p>We can implement the message-passing model with shared memory space.<br />
Sending a message is copying memory from message library buffers while receiving the message is copying data from message library buffers.</p>
</li>
<li>
<p>Using less efficient software solutions, we can implement shared address space abstraction on machines without hardware support.<br />
Mark all pages with shared variables as invalid at first, and the page-fault handler issues appropriate network requests</p>
</li>
<li>
<p>Synchronous (blocking) send and receive<br />
Send(): Call returns when the sender receives an acknowledgment that the message data resides in the receiver’s address space.<br />
Recv(): Call returns when data from the received message is copied into the receiver’s address space and acknowledgment sent back to the sender.<br />
So, when using the message-passing model, we must be careful with the order of send() and recv() in each thread because it may easily raise a deadlock.<br />
If the first call of all threads is send(), then no one is receiving, and all are waiting for someone to receive what they have sent.<br />
One common way to program is to arrange one thread to send first and the receiver of that send to receive first.</p>
</li>
<li>
<p>Non-blocking asynchronous send/recv<br />
Send() call returns immediately, while recv() posts intent to receive in the future and returns immediately.<br />
We can use checksend() and checkrecv() to determine the actual status of the send/receipt.<br />
The buffer provided to send() cannot be modified by calling the thread since message processing occurs concurrently with thread execution.</p>
</li>
</ol>
<h2 id="data-parallel"><a class="markdownIt-Anchor" href="#data-parallel"></a> Data Parallel</h2>
<ol>
<li>
<p>Data parallel has a very rigid computation structure. If it works well, it will work very well. But sometimes it just won’t work.</p>
</li>
<li>
<p>Nowadays, data-parallel usually takes the form of SPMD instead of SIMD. Programs perform the same function on different data elements in a collection<br />
<code>map(function, collection)</code>: Synchronization is implicit at the end of the map. Map returns when the function has been applied to all elements of the collection</p>
</li>
<li>
<p>When the function is too complicated, the result might be non-deterministic.<br />
Data-parallel model (foreach) provides no specification of the order in which iterations occur and no primitives for fine-grained mutual exclusion/synchronization.</p>
</li>
<li>
<p>Streams: collections of elements. Elements can be processed independently.<br />
Kernels: side-effect-free functions. Operate element-wise on collections.</p>
</li>
<li>
<p>A stream can be claimed by <code>stream&lt;ElemType&gt; name(N)</code><br />
When defining the kernel function, its parameters are single elements instead of a whole collection. But when we call the kernel function, we pass the streams into the function directly.</p>
</li>
<li>
<p>Benefits of stream programming:<br />
Functions are side-effect-free (cannot write a non-deterministic program)<br />
The compiler knows program data flow: Inputs and outputs of each invocation are known in advance, and thus, prefetching can be employed to hide latency.<br />
When there are multiple kernels, and the output of the last kernel is the input of the next kernel, producer-consumer locality is known in advance.<br />
Implementation can be structured so the second kernel immediately processes the outputs of the first kernel. The values are stored in on-chip buffers/caches and never written to memory, which saves bandwidth.<br />
These optimizations are the responsibility of the stream program compiler. Requires global program analysis.</p>
</li>
<li>
<p>The drawback of stream programming is the need for a library of operators to describe complex data flows so that it might go wrong.</p>
</li>
<li>
<p><code>stream_gather(input, indices, tmp_input)</code>: Put elements in input to tmp_input according to indices. This is called before the kernel function, and the kernel function will deal with the gathered stream.<br />
<code>stream_scatter(tmp_output, indices, output)</code>: Similar to stream_gather, but it is called after the kernel function, which will deal with the original stream.<br />
The parameters of both functions are all stream.</p>
</li>
</ol>
<h2 id="synchronization-and-communication"><a class="markdownIt-Anchor" href="#synchronization-and-communication"></a> Synchronization and Communication</h2>
<ol>
<li>
<p>In shared address space, mutual exclusion is required for shared variables, and barriers are used to express dependencies between computation phases.<br />
They can communicate through implicit loads/stores to shared variables.</p>
</li>
<li>
<p>In the message-passing model, the synchronizations and communications are performed by sending and receiving messages.</p>
</li>
<li>
<p>In the data parallel model, a single logical thread is in control, but the system may parallelize iterations of the forall loop. There is an implicit barrier at the end of the forall loop body.<br />
They can also communicate through implicit loads and stores, like shared address space. There are also some special built-in primitives for more complex communication patterns, e.g., reduce</p>
</li>
</ol>
<h2 id="modern-practice"><a class="markdownIt-Anchor" href="#modern-practice"></a> Modern practice</h2>
<ol>
<li>
<p>Use shared address space programming within a multi-core chip of a cluster and use message passing between chips.<br />
Use the convenience of shared address space where it can be implemented efficiently (within a chip) and require explicit communication elsewhere.</p>
</li>
<li>
<p>Data-parallel-ish programming models support shared-memory style synchronization primitives in kernels. This could permit limited forms of inter-iteration communication.</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/04/13/Courses/CS149/01-Modern-Multicore-Processors/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/13/Courses/CS149/01-Modern-Multicore-Processors/" class="post-title-link" itemprop="url">01. Modern Multicore Processors</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-13 15:48:23" itemprop="dateCreated datePublished" datetime="2022-04-13T15:48:23+08:00">2022-04-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 17:18:34" itemprop="dateModified" datetime="2024-03-16T17:18:34+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="speedup"><a class="markdownIt-Anchor" href="#speedup"></a> Speedup</h1>
<ol>
<li>Speedup (using P processors) = execution time (using 1 processor) / execution time (using P processors)</li>
<li>We can only get a speedup of less than P times when using P processors.<br />
One reason is that although cores are in the same chip when they want to communicate with each other, they have to transmit information through wires, which takes some time.<br />
Another reason is the imbalance in work assignments, which caused one core to have too heavy assignments while others had nothing to do but wait.<br />
Communication costs can dominate a parallel computation, severely limiting speedup. Especially when you assign tasks to too many cores, each core gets little computation.</li>
</ol>
<h1 id="parallelism"><a class="markdownIt-Anchor" href="#parallelism"></a> Parallelism</h1>
<p>Why parallelism?<br />
Before 2004, the performance of single-processors grew exponentially. However, in 2004, Intel hit the Power Density Wall. It goes too hot if you get over 100 watts in a chip and will melt. And the battery won’t hold long.</p>
<h2 id="ilp"><a class="markdownIt-Anchor" href="#ilp"></a> ILP</h2>
<ol>
<li>Instruction-Level Parallelism. Extract several instructions from the same instruction stream, and multiple ALUs perform multiple operations parallel (within a core). This has to be done in the semantics of a program.</li>
<li>Superscalar processor: exploit ILP within an instruction stream. Parallelism is automatically and dynamically discovered by the hardware during execution (not programmer visible)</li>
<li>Example: Pentium 4<br />
Its instruction decoder will yank a whole bunch of instructions out of the instruction stream and map them to a new kind of computation called data flow computation that will track which value is generated and feed which instruction there.<br />
Decoders map them to independent processing units that can each perform a subset of the whole operations.<br />
Control logics try to predict what’s going on and deal with it when it predicts incorrectly. The Branch Target Buffer records all the control flow instructions to predict where they will go again. If it predicts wrong, it will back out and doesn’t commit to those results generated, flushing them away.<br />
Meltdown inspector: This logic leaks information about what other processors are doing.</li>
<li>Most available ILP is exploited by a processor capable of issuing four instructions per clock, and only little performance benefits from building a processor that can issue more.</li>
</ol>
<h2 id="multi-core"><a class="markdownIt-Anchor" href="#multi-core"></a> Multi-Core</h2>
<p>Before the multi-core era, the Majority of chip transistors were used to perform operations that helped a single instruction stream run fast.<br />
More transistors mean a larger cache, smarter out-of-order logic, smarter branch predictor, etc. Also, more transistors get smaller transistors and, thus, higher clock frequencies.</p>
<h3 id="simpler-cores"><a class="markdownIt-Anchor" href="#simpler-cores"></a> Simpler cores</h3>
<ol>
<li>It uses increasing transistor count to add more cores to the processor rather than using transistors to increase the sophistication of processor logic that accelerates a single instruction stream (e.g., out-of-order and speculative operations)</li>
<li>Each core runs a single instruction stream slower than our original large core, but the sum performance is faster. With a smaller core, the communication cost inside of a chip will be cheaper.</li>
<li>However, the original programs express no parallelism and run as one thread on one of the processor cores.</li>
<li>One way to express parallelism is by using pthreads. It will create threads to deal with the tasks we assigned.</li>
<li>Another way is called data-parallel expression. The programmer declares loop iterations independent, and a compiler might automatically generate parallel threaded code.</li>
</ol>
<h3 id="simd-processing"><a class="markdownIt-Anchor" href="#simd-processing"></a> SIMD processing</h3>
<ol>
<li>Amortize the cost/complexity of managing an instruction stream across many ALUs. The same instruction is broadcast to all ALUs, executed in parallel on all ALUs</li>
<li>Each core has an independent vector register set that is different from the regular register set. Each core has multiple ALUs to execute the same instructions for different data.</li>
<li>Only a very structural, carefully written code can be automatically vectorized by compilers.</li>
<li>When conditional executions occur, the condition will run first and create a mask according to the result, which disables the ALUs that should not execute current instructions. Those active ALUs will execute the instructions and create a new mask. This procession will continue until all ALUs have executed all conditional instructions they should execute, and then they can execute the remaining unconditional codes together again.<br />
When a core can handle n elements at the same time, the performance in the worst case is 1 / n of the peak performance.</li>
<li>Instruction stream coherence (“coherent execution”): Same instruction sequence applies to all elements operated upon simultaneously.<br />
“Divergent” execution: A lack of instruction stream coherence</li>
<li>Coherent execution is necessary for efficient use of SIMD processing resources. However, coherent execution is unnecessary for efficient parallelization across cores since each core can fetch/decode a different instruction stream.</li>
</ol>
<h4 id="simd-on-modern-cpus"><a class="markdownIt-Anchor" href="#simd-on-modern-cpus"></a> SIMD on modern CPUs</h4>
<ol>
<li>SSE instructions: 128-bit operations: 4x32 bits or 2x64 bits (4-wide float vectors)<br />
AVX instructions: 256-bit operations: 8x32 bits or 4x64 bits (8-wide float vectors)<br />
AVX512 instructions: 512 bit operations: 16x32 bits or 8x64 bits (8-wide float vectors)</li>
<li>Instructions are generated by the compiler. Parallelism is explicitly requested by a programmer using intrinsics and conveyed using parallel language semantics. Finally, it is inferred by dependency analysis of loops by “auto-vectorizing” compiler.</li>
<li>Explicit SIMD: SIMD parallelization is performed at compile time. We can inspect the program binary and see SIMD instructions.</li>
</ol>
<h4 id="simd-on-modern-gpus"><a class="markdownIt-Anchor" href="#simd-on-modern-gpus"></a> SIMD on modern GPUs</h4>
<ol>
<li>It is usually SPMD (Single Program Multiple Data) in GPUs, and SIMD is used to implement much of the logic.</li>
<li>Implicit SIMD: Compiler generates a scalar binary (scalar instructions). But N instances of the program are <em>always run</em> together on the processor. In other words, the interface to the hardware itself is data-parallel.</li>
<li>Hardware (not compiler) is responsible for simultaneously executing the same instruction from multiple instances on different data on SIMD ALUs</li>
<li>SIMD width of most modern GPUs ranges from 8 to 32</li>
</ol>
<h1 id="access-memory"><a class="markdownIt-Anchor" href="#access-memory"></a> Access Memory</h1>
<ol>
<li>Memory latency: The time for a memory request (e.g., load, store) from a processor to be serviced by the memory system. Memory “access time” is a measure of latency.</li>
<li>Memory bandwidth: The rate at which the memory system can provide data to a processor</li>
<li>Stall: A processor “stalls” when it cannot run the next instruction in an instruction stream because of a dependency on a previous instruction. Accessing memory is a major source of stalls.</li>
<li>One way to reduce stall is by reducing latency.<br />
One strategy is cache, which provides high bandwidth data transfer to the CPU.</li>
<li>Another way is hiding latency; namely, the latency of the memory operation is not changed; it just no longer causes reduced processor utilization.<br />
One common strategy is the prefetch. All modern CPUs have logic for prefetching data into caches. Prefetching can also reduce performance if the guess is wrong (hogs bandwidth, pollutes caches)<br />
The other is using multi-threading. The idea is to interleave the processing of multiple threads on the same core to hide stalls.</li>
</ol>
<h2 id="multi-threading"><a class="markdownIt-Anchor" href="#multi-threading"></a> Multi-threading</h2>
<ol>
<li>The key idea of throughput-oriented systems is that they potentially increase the time to complete work by any one thread to improve overall system throughput when running multiple threads.<br />
Sometimes, one thread is runnable, but the processor does not execute it since the core is running some other thread.</li>
<li>With more storing execution contexts in the cache, the working set per thread is smaller, and the latency hiding ability is higher.<br />
Since the cache space per thread is smaller, it may go to memory more often. Thus, it relies heavily on memory bandwidth.<br />
When the thread is enough to achieve 100% utilization of the core, additional threads yield no benefit.</li>
<li>The instruction decoder will choose instructions from multiple threads and fire them to the execution units where the threads are shared. The memory interface unit will detect dependencies automatically.</li>
<li>Interleaved multi-threading (a.k.a. temporal multi-threading): For each clock, the core chooses a thread and runs an instruction from the thread on the ALUs</li>
<li>Simultaneous multi-threading (SMT): The core chooses instructions from multiple threads to run on ALUs for each clock. It is an extension of the superscalar CPU design.</li>
<li>The operating system maps your pthreads to the processor’s thread execution contexts.</li>
</ol>
<h2 id="cpus-and-gpus"><a class="markdownIt-Anchor" href="#cpus-and-gpus"></a> CPUs and GPUs</h2>
<ol>
<li>CPU has big caches, few threads, modest memory BW, and relies mainly on caches and prefetching.<br />
GPU has small caches, many threads, huge memory BW, and relies mainly on multi-threading.</li>
<li>The bandwidth of GPUs is a lot faster than CPUs.</li>
<li>In GPUs, ALUs run twice the clock rate of the rest of the chip. So, each decoded instruction runs on 32 pieces of data on the 16 ALUs over two ALU clocks. (but to the programmer, it behaves like a 32-wide SIMD operation)<br />
Warp: An instruction operating on a whole vector of data at a time.<br />
SM: Streaming Multi-processor. Each SM has multiple warp selectors. Each selector has multiple ALUs with different functions. The selectors in the same SM have shared the memory and L1 cache. And all selectors can run parallel.</li>
<li>A core can have multiple scalar ALUs and multiple vector ALUs. Each vector ALU can perform either MUL and ADD or ADD only.<br />
The number of execution contexts determines the maximum number of threads activated in a core.</li>
</ol>
<h2 id="bandwidth"><a class="markdownIt-Anchor" href="#bandwidth"></a> Bandwidth</h2>
<ol>
<li>If processors request data at a too high rate, the memory system cannot keep up. No amount of latency hiding helps this.</li>
<li>Organize computation to fetch data from memory less often.<br />
Reuse data previously loaded by the same thread (traditional intra-thread temporal locality optimizations). <br />
Share data across threads (inter-thread cooperation)</li>
<li>Request data less often (instead, do more arithmetic: it’s “free”).<br />
Arithmetic intensity: Ratio of math operations to data access operations in an instruction stream.<br />
The main point is that programs must have high arithmetic intensity to utilize modern processors efficiently.<br />
If the data needed can be recalculated in ALUs, then don’t access memory; do the calculation.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/about/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/about/">1</a><span class="space">&hellip;</span><a class="page-number" href="/about/page/6/">6</a><span class="page-number current">7</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
