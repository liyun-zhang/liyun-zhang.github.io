<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/about/page/6/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/about/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"about/page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">63</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/16/Courses/CS149/13-Implementing-Synchronization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/16/Courses/CS149/13-Implementing-Synchronization/" class="post-title-link" itemprop="url">13. Implementing Synchronization</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-16 11:05:13" itemprop="dateCreated datePublished" datetime="2022-07-16T11:05:13+08:00">2022-07-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:58:00" itemprop="dateModified" datetime="2024-02-09T12:58:00+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="implementing-locks"><a class="markdownIt-Anchor" href="#implementing-locks"></a> Implementing locks</h1>
<ol>
<li>Three phases of a synchronization event：<br />
Acquire method: How a thread attempts to gain access to protected resource?<br />
Waiting algorithm: How a thread waits for access to be granted to shared resource?<br />
Release method: How thread enables other threads to gain resource when its work in the synchronized region is complete?</li>
<li>Busy waiting (spinning): <code>while (condition X not true) ;</code></li>
<li>Blocking synchronization: <code>if (condition X not true) block until true;</code><br />
If progress cannot be made because a resource cannot be acquired, it is desirable to free up execution resources for another thread and preempt the running thread.</li>
<li>Busy-waiting can be preferable to blocking if scheduling overhead is larger than expected wait time or processor’s resources are not needed for other tasks.<br />
The later situation is often the case in a parallel program since we usually don’t oversubscribe a system when running a performance-critical parallel app</li>
<li>Desirable lock performance characteristics:<br />
<strong>Low latency</strong>: If lock is free and no other processors are trying to acquire it, a processor should be able to acquire the lock quickly<br />
<strong>Low interconnect traffic</strong>: If all processors are trying to acquire lock at once, they should acquire the lock in succession with as little traffic as possible<br />
<strong>Scalability</strong>: Latency / traffic should scale reasonably with number of processors<br />
<strong>Low storage cost</strong><br />
<strong>Fairness</strong>: Avoid starvation or substantial unfairness. One ideal is that processors should acquire lock in the order they request access to it</li>
</ol>
<h2 id="test-and-set-lock"><a class="markdownIt-Anchor" href="#test-and-set-lock"></a> Test-and-set lock</h2>
<h3 id="simple-test-and-set-lock"><a class="markdownIt-Anchor" href="#simple-test-and-set-lock"></a> Simple test-and-set lock</h3>
<ol>
<li>The following spin lock has data race because LOAD-TEST-STORE is not atomic.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// Lock</span><br><span class="line">ld   R0, mem[addr]  // load word into R0</span><br><span class="line">cmp  R0, #0          // compare R0 to 0</span><br><span class="line">bnz  lock            // if nonzero jump to top</span><br><span class="line">st   mem[addr], #1  // set lock to 1</span><br><span class="line"></span><br><span class="line">// Unlock</span><br><span class="line">st   mem[addr], #0  // set lock to 0</span><br></pre></td></tr></table></figure>
</li>
<li>So we need an atomic test-and-set instruction<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ts R0, mem[addr]  // load mem[addr] into R0</span><br><span class="line">                  // if mem[addr] is 0, set mem[addr] to 1</span><br><span class="line">                  </span><br><span class="line">// Lock</span><br><span class="line">ts   R0, mem[addr]  // load word into R0</span><br><span class="line">bnz  R0, lock        // if nonzero jump to top</span><br><span class="line"></span><br><span class="line">// Unlock</span><br><span class="line">st   mem[addr], #0  // store 0 to address</span><br></pre></td></tr></table></figure>
</li>
<li>Everytime a processor execute the <code>ts</code> instruction will send a BusRdX signal and invalidate the lock in all other processor’s cache. When there are many processors trying to execute on the same lock, the coherence traffic may be heavy.<br />
This lock generates one invalidation per waiting processor per test.</li>
<li>Bus contention increases amount of time to transfer lock since lock holder must wait to acquire bus to release. Bus contention also slows down execution of critical section.</li>
<li>In x86, we can do the atomic compare and exchange by <code>lock cmpxchg src, dst</code> instruction. The <code>lock</code> prefix makes the operation atomic. The logic of the instruction is as followed:<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (dst == %eax)  <span class="comment">// eax is the x86 accumulator register</span></span><br><span class="line">  ZF = <span class="number">1</span>          <span class="comment">// ZF is a flag registor</span></span><br><span class="line">dst = src <span class="keyword">else</span></span><br><span class="line">  ZF = <span class="number">0</span></span><br><span class="line">  %eax = dst</span><br></pre></td></tr></table></figure>
</li>
<li>Simple test-and-set lock has low latency (under low contention), high traffic, poor scaling, low storage cost (one int), no provisions for fairness.</li>
</ol>
<h3 id="test-and-set-lock-with-back-off"><a class="markdownIt-Anchor" href="#test-and-set-lock-with-back-off"></a> Test-and-set lock with back off</h3>
<ol>
<li>
<p>Upon failure to acquire lock, delay for awhile before retrying.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Lock</span><span class="params">(<span class="keyword">volatile</span> <span class="type">int</span>* lock)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> amount = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">test_and_set</span>(lock) == <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    <span class="built_in">delay</span>(amount);</span><br><span class="line">    amount *= <span class="number">2</span>;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>It has the same uncontended latency as test-and-set, but potentially higher latency under contention due to that the waiting processor may be still in delaying even when the lock is available.</p>
</li>
<li>
<p>It Generates less traffic than test-and-set since it is not continually attempting to acquire lock. It improves scalability due to less traffic.</p>
</li>
<li>
<p>Storage cost unchanged (still one int for lock)</p>
</li>
<li>
<p>Exponential back-off can cause severe unfairness. Newer requesters back off for shorter intervals</p>
</li>
</ol>
<h3 id="test-and-test-and-set-lock"><a class="markdownIt-Anchor" href="#test-and-test-and-set-lock"></a> Test-and-test-and-set lock</h3>
<ol>
<li>
<p>To prevent the coherence traffic problem, we can test first and do the test-and-set only if the earlier test has passed.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">Lock</span><span class="params">(<span class="keyword">volatile</span> <span class="type">int</span>* lock)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">while</span> (*lock != <span class="number">0</span>) ;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">test_and_set</span>(lock) == <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Unlock</span><span class="params">(<span class="keyword">volatile</span> <span class="type">int</span>* lock)</span> </span>&#123;</span><br><span class="line">  *lock = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>This lock has slightly higher latency than test-and-set in uncontended case, but generates much less interconnect traffic. Only One invalidation is generated per waiting processor per lock release.<br />
Namely, only <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span> invalidation and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(P^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> interconnect traffic.</p>
</li>
<li>
<p>It is more scalable due to less traffic, storage cost unchanged (still one int), and still no provisions for fairness.</p>
</li>
</ol>
<h2 id="ticket-lock"><a class="markdownIt-Anchor" href="#ticket-lock"></a> Ticket lock</h2>
<ol>
<li>The test-and-set style locks cannot provide for fairness because all waiting processors attempt to acquire lock using test-and-set upon release.</li>
<li>We can assign each thread a number when they try to acquire the lock. And everytime, we give the lock to the waiting thread with smallest number.<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">lock</span> &#123;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="type">int</span> next_ticket;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="type">int</span> now_serving;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Lock</span><span class="params">(lock* lock)</span> </span>&#123;</span><br><span class="line">  <span class="type">int</span> my_ticket = <span class="built_in">atomic_increment</span>(&amp;lock-&gt;next_ticket);  <span class="comment">// take a “ticket”</span></span><br><span class="line">  <span class="keyword">while</span> (my_ticket != lock-&gt;now_serving);                <span class="comment">//wait for number</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">unlock</span><span class="params">(lock* lock)</span> </span>&#123;</span><br><span class="line">  lock-&gt;now_serving++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>There is no atomic operation needed to acquire the lock. Only a read is needed when acquiring the lock, and write only happens when the lock is released. So only one invalidation is generated per lock release, namely <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span> interconnect traffic.</li>
</ol>
<h2 id="array-based-lock"><a class="markdownIt-Anchor" href="#array-based-lock"></a> Array-based lock</h2>
<ol>
<li>
<p>Each processor spins on a different memory address. And utilizes atomic operation to assign address on attempt to acquire.<br />
If there are two barriers, after some threads passed the first barrier, they might set the <code>flag</code> to 0 even if some other threads haven’t passed the first barrier yet causing those slower threads waiting.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">lock</span> &#123;</span><br><span class="line">  <span class="keyword">volatile</span> padded_int status[P];  <span class="comment">// padded to keep off same cache line</span></span><br><span class="line">  <span class="keyword">volatile</span> <span class="type">int</span> head;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> my_element;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Lock</span><span class="params">(lock* lock)</span> </span>&#123;</span><br><span class="line">  my_element = <span class="built_in">atomic_circ_increment</span>(&amp;lock-&gt;head);  <span class="comment">// assume modular increment </span></span><br><span class="line">  <span class="keyword">while</span> (lock-&gt;status[my_element] == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">unlock</span><span class="params">(lock* lock)</span> </span>&#123;</span><br><span class="line">  lock-&gt;status[my_element] = <span class="number">1</span>;</span><br><span class="line">  lock-&gt;status[<span class="built_in">circ_next</span>(my_element)] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>The lock only requires <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span> interconnect traffic per release, but requires space linear in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span>.</p>
</li>
<li>
<p>The atomic circular increment is a more complex operation. So the lock has a higher overhead.</p>
</li>
<li>
<p>Queue-based Lock (MCS lock): Create a queue of waiters. Each thread allocates a local space on which to wait.</p>
</li>
</ol>
<h1 id="implementing-barrier"><a class="markdownIt-Anchor" href="#implementing-barrier"></a> Implementing barrier</h1>
<ol>
<li>The following code uses <code>counter</code> to count how many threads have hit the barrier. The last thread hit the barrier wil release all of them.<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Barrier_t</span> &#123;</span><br><span class="line">   LOCK lock;</span><br><span class="line">  <span class="type">int</span> counter;  <span class="comment">// initialize to 0</span></span><br><span class="line">  <span class="type">int</span> flag;      <span class="comment">// the flag field should probably be padded to </span></span><br><span class="line">                <span class="comment">// sit on its own cache line. </span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// barrier for p processors</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Barrier</span><span class="params">(Barrier_t* b, <span class="type">int</span> p)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">lock</span>(b-&gt;lock);</span><br><span class="line">  <span class="keyword">if</span> (b-&gt;counter == <span class="number">0</span>) &#123;</span><br><span class="line">    b-&gt;flag = <span class="number">0</span>; <span class="comment">// first thread arriving at barrier clears flag </span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">int</span> num_arrived = ++(b-&gt;counter);</span><br><span class="line">  <span class="built_in">unlock</span>(b-&gt;lock);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (num_arrived == p) &#123; <span class="comment">// last arriver sets flag b-&gt;counter = 0;</span></span><br><span class="line">    b-&gt;flag = <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (b-&gt;flag == <span class="number">0</span>); <span class="comment">// wait for flag</span></span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>To solve the problem, we should wait for all processes to leave first barrier, before clearing flag for entry into the second<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Centralized barrier</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Barrier_t</span> &#123;</span><br><span class="line">  LOCK lock;</span><br><span class="line">  <span class="type">int</span> arrive_counter;  <span class="comment">// initialize to 0 (number of threads that have arrived)</span></span><br><span class="line">  <span class="type">int</span> leave_counter;  <span class="comment">// initialize to P (number of threads that have left barrier)</span></span><br><span class="line">  <span class="type">int</span> flag;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// barrier for p processors</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Barrier</span><span class="params">(Barrier_t* b, <span class="type">int</span> p)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">lock</span>(b-&gt;lock);</span><br><span class="line">  <span class="keyword">if</span> (b-&gt;arrive_counter == <span class="number">0</span>) &#123;   <span class="comment">// if first to arrive...</span></span><br><span class="line">    <span class="keyword">if</span> (b-&gt;leave_counter == P) &#123;  <span class="comment">// check to make sure no other threads “still in barrier”</span></span><br><span class="line">      b-&gt;flag = <span class="number">0</span>;               <span class="comment">// first arriving thread clears flag</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">unlock</span>(lock);</span><br><span class="line">      <span class="keyword">while</span> (b-&gt;leave_counter != P);  <span class="comment">// wait for all threads to leave before clearing</span></span><br><span class="line">      <span class="built_in">lock</span>(lock);</span><br><span class="line">      b-&gt;flag = <span class="number">0</span>;                <span class="comment">// first arriving thread clears flag</span></span><br><span class="line">    &#125; </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="type">int</span> num_arrived = ++(b-&gt;arrive_counter);</span><br><span class="line">  <span class="built_in">unlock</span>(b-&gt;lock);</span><br><span class="line">  <span class="keyword">if</span> (num_arrived == p) &#123;  <span class="comment">// last arriver sets flag</span></span><br><span class="line">    b-&gt;arrive_counter = <span class="number">0</span>;</span><br><span class="line">    b-&gt;leave_counter = <span class="number">1</span>;</span><br><span class="line">    b-&gt;flag = <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (b-&gt;flag == <span class="number">0</span>);  <span class="comment">// wait for flag</span></span><br><span class="line">    <span class="built_in">lock</span>(b-&gt;lock);</span><br><span class="line">    b-&gt;leave_counter++;</span><br><span class="line">    <span class="built_in">unlock</span>(b-&gt;lock);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>We can save one variable by sense reversal. Processors wait for flag to be equal to local sense.<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">Barrier_t</span> &#123;</span><br><span class="line">  LOCK lock;</span><br><span class="line">  <span class="type">int</span> counter; <span class="comment">// initialize to 0</span></span><br><span class="line">  <span class="type">int</span> flag; <span class="comment">// initialize to 0</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> local_sense = <span class="number">0</span>;  <span class="comment">// private per processor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// barrier for p processors</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">Barrier</span><span class="params">(Barrier_t* b, <span class="type">int</span> p)</span> </span>&#123;</span><br><span class="line">  local_sense = (local_sense == <span class="number">0</span>) ? <span class="number">1</span> : <span class="number">0</span>; <span class="built_in">lock</span>(b-&gt;lock);</span><br><span class="line">  <span class="type">int</span> num_arrived = ++(b-&gt;counter);</span><br><span class="line">  <span class="keyword">if</span> (b-&gt;counter == p) &#123; <span class="comment">// last arriver sets flag</span></span><br><span class="line">    <span class="built_in">unlock</span>(b-&gt;lock);</span><br><span class="line">    b-&gt;counter = <span class="number">0</span>;</span><br><span class="line">    b-&gt;flag = local_sense;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">unlock</span>(b-&gt;lock);</span><br><span class="line">    <span class="keyword">while</span> (b-&gt;flag != local_sense); <span class="comment">// wait for flag</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>There are <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span> traffic on interconnect per barrier.<br />
All threads have <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>P</mi></mrow><annotation encoding="application/x-tex">2P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> write transactions to obtain barrier lock and update counter. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span> traffic assuming lock acquisition is implemented in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span> manner<br />
Last thread has <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span> write transactions to write to the flag and reset the counter. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>P</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(P)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mclose">)</span></span></span></span> traffic since there are many sharers of the flag<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">P-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> transactions to read updated flag</li>
<li>In centralized barrier, all threads share single barrier lock and counter, which causes high contention.</li>
<li>Combining trees make better use of parallelism in interconnect topologies, which has <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi></mrow><annotation encoding="application/x-tex">logP</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> span (latency). This strategy makes less sense on a bus where all traffic still serialized on single shared bus.<br />
Barrier acquire: when processor arrives at barrier, performs increment of parent counter. Process recurses to root.<br />
Barrier release: beginning from root, notify children of release</li>
</ol>
<h1 id="locks-in-cuda-assignment-3"><a class="markdownIt-Anchor" href="#locks-in-cuda-assignment-3"></a> Locks in CUDA (Assignment 3)</h1>
<ol>
<li>
<p>CUDA has provided <code>atomicCAS</code> and <code>atomicExch</code>. But if we use the following code to implement the mutex, threads will end up in a dead loop.<br />
The reason is all warps in a block need to execute the same instructions. But only one thread can have the lock, namely at most one thread in a block can leave the loop, and hence the whole block, including the one with lock, will keep executing the <code>atomicCAS</code>. Obviously the thread with loop cannot execute anything in critical area and the unlock the lock, which causes all threads end up in the dead loop of <code>lock</code>.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">__share__ <span class="type">int</span> mutex;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">lock</span><span class="params">(<span class="type">int</span>* mutex)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(<span class="built_in">atomicCAS</span>(mutex, <span class="number">0</span>, <span class="number">1</span>)) ;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">unlock</span><span class="params">(<span class="type">int</span>* mutex)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">atomicExch</span>(mutex, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="built_in">lock</span>(&amp;mutex);</span><br><span class="line">  <span class="comment">// critical code</span></span><br><span class="line">  <span class="built_in">unlock</span>(&amp;mutex);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>The method to solve the problem also need to consider the special execution mode of condition instruction in CUDA. A loop is necessary, but we want to execute the instruction inside a condition body so that when one thread grabs the lock, it can execute and unlock the lock before the next iteration.<br />
So we can have an <code>if</code> condition inside the loop as the following code.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__share__ <span class="type">int</span> mutex;</span><br><span class="line"></span><br><span class="line"><span class="function">__kernel__ <span class="type">void</span> <span class="title">kernel</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">bool</span> leaveLoop = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">while</span> (!leaveLoop)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">atomicExch</span>(&amp;mutex, <span class="number">1</span>))</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="comment">// critical cade</span></span><br><span class="line">      leaveLoop = <span class="literal">true</span>;      <span class="comment">// this thread can leave the loop, but it need to </span></span><br><span class="line">                             <span class="comment">// wait for other threads in its block</span></span><br><span class="line">      <span class="built_in">atomicExch</span>(&amp;mutex, <span class="number">0</span>); <span class="comment">// unlock the lock, so that in next iteration, </span></span><br><span class="line">                             <span class="comment">// some other threads can have the lock and finish</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/13/Courses/CS149/12-Interconnection-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/13/Courses/CS149/12-Interconnection-Network/" class="post-title-link" itemprop="url">12. Interconnection Network</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-13 10:50:56" itemprop="dateCreated datePublished" datetime="2022-07-13T10:50:56+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 14:01:11" itemprop="dateModified" datetime="2024-02-09T14:01:11+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="interconnection"><a class="markdownIt-Anchor" href="#interconnection"></a> Interconnection</h1>
<ol>
<li>All parallel processors are connected and form an interconnection network.</li>
<li>The interconnection network are used to connect processor cores with other cores, processors and memories, processor cores and caches, caches and caches, I/O devices.</li>
<li>The design of the interconnection network has an important impact on system scalability (How large of a system can be built? How easy is it to add more nodes?), system performance and energy efficiency (How fast can cores, caches, memory communicate? How long is latency to memory? How much energy is spent on communication?)</li>
</ol>
<h2 id="terminology"><a class="markdownIt-Anchor" href="#terminology"></a> Terminology</h2>
<ol>
<li><strong>Network node</strong>: a network endpoint connected to a router/switch, like the processor, the cache controller, or the memory controller</li>
<li><strong>Network interface</strong>: Connects nodes to the network</li>
<li><strong>Switch/router</strong>: Connects a fixed number of input links to a fixed number of output links</li>
<li><strong>Link</strong>: A bundle of wires carrying a signal<br />
<img src="/imgs/CS149/12/1.png" width="40%"></li>
</ol>
<h2 id="design-issues"><a class="markdownIt-Anchor" href="#design-issues"></a> Design issues</h2>
<ol>
<li><strong>Topology</strong>: How switches are connected via links. Affects routing, throughput, latency, complexity/cost of implementation.</li>
<li><strong>Routing</strong>: How a message gets from its source to its destination in the network. Can be static (messages take a predetermined path) or adaptive based on load.</li>
<li><strong>Buffering and flow control</strong>: What data are stored in the network? packets, partial packets? etc. How does the network manage buffer space?</li>
</ol>
<h2 id="properties-of-interconnect-topology"><a class="markdownIt-Anchor" href="#properties-of-interconnect-topology"></a> Properties of interconnect topology</h2>
<ol>
<li><strong>Routing distance</strong>: Number of links (“hops”) along a route between two nodes</li>
<li><strong>Diameter</strong>: the maximum routing distance</li>
<li><strong>Average distance</strong>: average routing distance over all valid routes</li>
<li><strong>Direct network</strong>: The switches and nodes are one in the same. The logic of the switch is built into the node itself.</li>
<li><strong>Indirect network</strong>: The switches are distinct from the nodes that form a chain from one node to the other.</li>
<li><strong>Blocking or non-blocking</strong>: If connecting any pairing of node simultaneously won’t cause conflict (using the same link or the same switch, assuming that one switch can only handle on message at one time), the network is non-blocking. Otherwise, it is blocking.</li>
<li><strong>Bisection bandwidth</strong>: A measure of how much connectivity in the network. If we cut the network in half, it is the connection between those two halves, namely the sum bandwidth of all severed links.<br />
The low bisection bandwidth will be the choke point of the network.</li>
<li>Latency increases with load (throughput).<br />
The topology, routing algorithm and flow control each has their own min latency. The zero load or idle latency is the sum of the three min latencies.<br />
Also, the topology, routing algorithm and flow control each has their own throughput limit. The overall throughput limit is the min of the three limits.</li>
</ol>
<h1 id="interconnect-topologies"><a class="markdownIt-Anchor" href="#interconnect-topologies"></a> Interconnect topologies</h1>
<h2 id="bus"><a class="markdownIt-Anchor" href="#bus"></a> Bus</h2>
<ol>
<li>
<p>Physically, a bus is a wire. But from graph point of view, a bus is a switch.</p>
</li>
<li>
<p>It is simple to design. It costs effective for a small number of nodes. It is easy to implement coherence via snooping</p>
</li>
<li>
<p>Contention: all nodes contend for shared bus</p>
</li>
<li>
<p>Limited bandwidth: all nodes communicate over same wires, and only one communication is allowed at a time.</p>
</li>
<li>
<p>There has a scalibility problem. It is expensive to drive wires across the whole chips. There is quite a lot of power in driving signals on the bus.</p>
<img src="/imgs/CS149/12/2.png" width="40%">
</li>
</ol>
<h2 id="crossbar"><a class="markdownIt-Anchor" href="#crossbar"></a> Crossbar</h2>
<ol>
<li>
<p>Every node is connected to every other node by a direct connection. The network is non-blocking and indirect (the network is indirect, but the nodes are connected directly).</p>
</li>
<li>
<p>It has <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span> latency and high bandwidth.</p>
</li>
<li>
<p>It also has a scalability problem since <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> switches are required. Also it has high cost and is difficult to arbitrate at scale.</p>
</li>
<li>
<p>Crossbars were used in recent multi-core processing from Oracle. In a multi-core chip, the crossbar (CCX) occupies about the same chip area as a core.</p>
<img src="/imgs/CS149/12/3.png" width="40%">
</li>
</ol>
<h2 id="ring"><a class="markdownIt-Anchor" href="#ring"></a> Ring</h2>
<ol>
<li>
<p>It lets the message to circle around.</p>
</li>
<li>
<p>It is simple enough and quite cheap with only <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span> cost.</p>
</li>
<li>
<p>But the latency is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>, which is very high. And the bisection bandwidth remains constant as nodes are added, which will cause the scalibility problem.</p>
</li>
<li>
<p>It is used in recent Intel architectures, Core i7, and in IBM CELL Broadband Engine. This is usually used on the scale of 4 or 8 elements on the ring.</p>
</li>
<li>
<p>The Intel’s ring interconnect has four rings (request, snoop, ack, 32 bytes of data) and six interconnect nodes (four “slices” of L3 cache, system agent, graphics). Each bank of L3 connected to ring bus twice.</p>
<img src="/imgs/CS149/12/4.png" width="40%">
</li>
</ol>
<h2 id="mesh"><a class="markdownIt-Anchor" href="#mesh"></a> Mesh</h2>
<ol>
<li>
<p>This is a direct network. It echoes locality in grid-based applications.</p>
</li>
<li>
<p>The cost is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>, and the average latency is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msqrt><mi>N</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(\sqrt{N})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.176665em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9266650000000001em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-2.886665em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.11333499999999996em;"><span></span></span></span></span></span><span class="mclose">)</span></span></span></span>.</p>
</li>
<li>
<p>It is easy to lay out on chip since all links have a fixed-length.</p>
</li>
<li>
<p>Path diversity: many ways for message to travel from one node to another</p>
<img src="/imgs/CS149/12/5.png" width="40%">
</li>
</ol>
<h2 id="torus"><a class="markdownIt-Anchor" href="#torus"></a> Torus</h2>
<ol>
<li>
<p>Characteristics of node in mesh topology are different based on whether node is near edge or middle of network. Torus topology introduces new links to avoid this problem.</p>
</li>
<li>
<p>In torus topology, each row or each column forms a ring by adding a new link to connect the two nodes on the edge.</p>
</li>
<li>
<p>Its cost is still <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>, but higher than 2D mesh. It also has higher path diversity and bisection bandwidth than mesh.</p>
</li>
<li>
<p>However, it has higher complexity and is difficult to layout on chip since its unequal link lengths.</p>
</li>
<li>
<p>Folded torus interleaving rows and columns to eliminate need for long connections. All connections are doubled in length.</p>
<img src="/imgs/CS149/12/6.png" width="40%">
</li>
</ol>
<h2 id="tree"><a class="markdownIt-Anchor" href="#tree"></a> Tree</h2>
<ol>
<li>
<p>This is a planar, hierarchical topology. Like mesh/torus, it performs good when traffic has locality.</p>
</li>
<li>
<p>The latency is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(logN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span></p>
</li>
<li>
<p>In tree routing, if the signal need to go upward, there is only one option. If the signal need to go downward, we can use <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> to mark go left while <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> to mark go right.<br />
Everytime, the signal will go upward first until have a common ancestor, then it will go downward.<br />
If the tree is a complete tree and we assign numbers to the nodes accroding to their inorder traversal starting from zero, then the way to each node from root node is the binary code of the number.</p>
</li>
<li>
<p>Fat tree increases bandwidth between nodes as move upward. It can alleviate root bandwidth problem with higher bandwidth links near root.</p>
</li>
<li>
<p>The number of wires is the same to the height of the subtree. So the bisection bandwidth of fat tree is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>.</p>
</li>
<li>
<p>The fat tree routing is similar to tree routing, but randomly choose when multiple links possible.</p>
</li>
<li>
<p>Constant-width fat tree (folded clos network): All nodes fixed degree, which makes the hardware design simpler. This is used in Infiniband networks.</p>
<p><img src="/imgs/CS149/12/7.png" width="30%"><img src="/imgs/CS149/12/8.png" width="30%"><img src="/imgs/CS149/12/9.png" width="30%"></p>
</li>
</ol>
<h2 id="hypercube"><a class="markdownIt-Anchor" href="#hypercube"></a> Hypercube</h2>
<ol>
<li>
<p>It has a low latency of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(logN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>. Its number of links is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(NlogN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>.</p>
</li>
<li>
<p>6D hypercube used in 64-core Cosmic Cube computer developed at Caltech in the 80s.</p>
</li>
<li>
<p>If the address of two nodes only differs by one bit, then there is a link connecting them.<br />
So is we want to go from address A to address B, we just do it one bit a time.</p>
</li>
<li>
<p>The problem is that we cannot pack more dimensions to 3D that exists. To implement a higher dimension cube, we need to put enough wires in to make it work. But as we scale up more and more, the wires becomes so much that doesn’t layout well.</p>
<img src="/imgs/CS149/12/10.png" width="40%">
</li>
</ol>
<h2 id="multi-stage-logarithmic"><a class="markdownIt-Anchor" href="#multi-stage-logarithmic"></a> Multi-stage logarithmic</h2>
<ol>
<li>
<p>It is an indirect network with multiple switches between terminals.</p>
</li>
<li>
<p>The cost is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>l</mi><mi>o</mi><mi>g</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(NlogN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span> while the latency is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(logN)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mclose">)</span></span></span></span>.</p>
</li>
<li>
<p>It has many variations: Omega, butterfly, Clos networks, etc.</p>
</li>
<li>
<p>In the topology shown below, each switch has two output wires. In the routing, if 0 stands for up and 1 for down, we can also routing according to the binary code of the number of targeting node.<br />
Here up means that we will take the upper wire, and down means that we will take the lower wire. They don’t always means going up or going down.</p>
<img src="/imgs/CS149/12/11.png" width="40%">
</li>
</ol>
<h1 id="buffering-and-flow-control"><a class="markdownIt-Anchor" href="#buffering-and-flow-control"></a> Buffering and flow control</h1>
<ol>
<li>Circuit switching sets up a full path (acquires all resources) between sender and receiver prior to sending a message. It has higher bandwidth transmission.<br />
It has no per-packet link management overhead, but does incur overhead to set up/tear down path. And reserving links can result in low utilization.</li>
<li>Packet switching makes routing decisions per packet. It has an opportunity to use link for a packet whenever link is idle.<br />
It has overhead due to dynamic switching logic during transmission, but no setup/tear down overhead.</li>
<li>The granularity of communication from larger to miner is message, packet and flit<br />
Message is the unit of transfer between network clients, and can be transmitted using many packets.<br />
Packet is the unit of transfer for network, and can be transmitted using multiple flits.<br />
Flit (flow control digit) is a unit of flow control in the network. Packets broken into flits.</li>
<li>A packet consists of header, payload/body and tail.<br />
Header contains routing and control information, and is at start of packet to router can start forwarding early.<br />
Payload/body contains the data to be sent.<br />
Tail contains control information, like error code, and is generally located at end of packet so it can be generated “on the way out”.</li>
<li>When two packets need to be routed onto the same outbound link at the same time, contention is occurred. There are three options buffer one packet and send it over link later, drop one packet, or reroute one packet (deflection).</li>
</ol>
<h2 id="circuit-switched-routing"><a class="markdownIt-Anchor" href="#circuit-switched-routing"></a> Circuit-switched routing</h2>
<ol>
<li>Main idea: pre-allocate all resources (links across multiple switches) along entire network path for a message (“setup a flow”)</li>
<li>Costs:<br />
Needs setup phase (“probe”) to set up the path (and to tear it down and release<br />
the resources when message complete)<br />
Lower link utilization. Transmission of two messages cannot share same link (even if some resources on a preallocated path are no longer utilized during a transmission)</li>
<li>Benefits:<br />
No contention during transmission due to preallocation, so no need for buffering<br />
Arbitrary message sizes (once path is set up, send data until done)</li>
</ol>
<h2 id="packet-based-flow-control"><a class="markdownIt-Anchor" href="#packet-based-flow-control"></a> Packet-based flow control</h2>
<ol>
<li>Store-and-forward: Packet copied entirely into network switch before moving to next node, which requires buffering for entire packet in each router</li>
<li>Flow control unit is an entire packet. Different packets from the same message can take different routes, but all data in a packet is transmitted over the same route</li>
<li>Store-and-forward has high per-packet latency. Its latency <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo></mrow><annotation encoding="application/x-tex">=</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span></span></span> packet transmission time on link <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">×</span></span></span></span> network distance)</li>
<li>Cut-through flow control: Switch starts forwarding data on next link as soon as packet header is received, since header determines how much link bandwidth packet requires and where to route.</li>
<li>Cut-through flow control reduces transmission latency and reduces to store-and-forward under high contention.</li>
<li>The latency of cut-through flow control is header transmission time <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">×</span></span></span></span> network distance <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo></mrow><annotation encoding="application/x-tex">+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">+</span></span></span></span> rest packet transmission time on one link <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≃</mo></mrow><annotation encoding="application/x-tex">\simeq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.46375em;vertical-align:0em;"></span><span class="mrel">≃</span></span></span></span> network distance <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo></mrow><annotation encoding="application/x-tex">+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">+</span></span></span></span> number of packets.</li>
<li>The difference between store-and-forward and cut-through is whether we parallel the transmission of header and the rest of the packet.</li>
<li>In cut-through flow control, if output link is blocked (cannot transmit head), transmission of tail can continue. So the switch need to store more data before it can transmit and delete them, which requires switches to have buffering for entire packet, just like store-and-forward.<br />
The worst case is that entire message is absorbed into a buffer in a switch, namely cut-through flow control degenerates to store-and-forward in this case.</li>
</ol>
<h2 id="wormhole-flow-contorl"><a class="markdownIt-Anchor" href="#wormhole-flow-contorl"></a> Wormhole flow contorl</h2>
<ol>
<li>Routing information only in head flit, body flits follows head, and tail flit flows body. It looks like that all flits moves to their next switch simultaneously. If head flit blocks, rest of packet stops.</li>
<li>Its latency <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo></mrow><annotation encoding="application/x-tex">=</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span></span></span></span> header transmission time <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>×</mo></mrow><annotation encoding="application/x-tex">\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">×</span></span></span></span></li>
<li>Head-of-line blocking problem: The route of the head flit of one packet is free, but blocked behind flit of another packet in buffer while that packet is blocked waiting for a busy link.</li>
<li>Virtual channel flow control: Multiplex multiple operations over single physical channel, and divide switch’s input buffer into multiple buffers sharing a single physical channel.</li>
<li>Virtual channel reduces head-of-line blocking.<br />
It can break cyclic dependency of resources by ensuring requests and responses use different virtual channels to avoid deadlock.<br />
Also, it provided quality-of-service guarantees. Some virtual channels have higher priority than others</li>
<li>“Escape” virtual channels: retain at least one virtual channel that uses deadlock-free routing</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/10/Courses/CS149/11-Memory-Consistency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/10/Courses/CS149/11-Memory-Consistency/" class="post-title-link" itemprop="url">11. Memory Consistency</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-10 14:07:29" itemprop="dateCreated datePublished" datetime="2022-07-10T14:07:29+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:52" itemprop="dateModified" datetime="2024-02-09T12:57:52+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="consistency"><a class="markdownIt-Anchor" href="#consistency"></a> Consistency</h1>
<h2 id="definition"><a class="markdownIt-Anchor" href="#definition"></a> Definition</h2>
<ol>
<li>In a correct behaviored parallel memory hierachy, reading a location should return the latest value written by any thread.<br />
Side-effects of writes are only observable when reads occur, so we will focus on the values returned by reads.</li>
<li>Within a thread, “latest” can be defined by program order. But when it comes across threads, we don’t want it be physical time, because there is no way that the hardware can pull that off. If it takes &gt;10 cycles to communicate between processors, there is no way that processor 0 can know what processor 1 did 2 clock ticks ago.</li>
<li>Writes from any particular thread must be consistent with program order. And writes across threads must be consistent with a valid interleaving of threads.<br />
We define the memory model that each thread proceeds in program order, and memory accesses interleaved (one at a time) to a single-ported memory while rate of progress of each thread is unpredictable.<br />
“Latest” means consistent with some interleaving that matches this model</li>
</ol>
<h2 id="hide-memory-latency"><a class="markdownIt-Anchor" href="#hide-memory-latency"></a> Hide memory latency</h2>
<ol>
<li>Idea: overlap memory accesses with other accesses and computation</li>
<li>“Out of order” pipelining: When an instruction is stuck, perhaps there are subsequent instructions that can be executed.</li>
<li>We don’t need to wait for a conditional branch to be resolved before proceeding. Just predict the branch outcome and continue executing speculatively. If prediction is wrong, squash any side-effects and restart down correct path.</li>
<li>Modern processors fetch and graduate instructions in-order, but issue out-of-order. So intra-thread dependences are preserved, but memory accesses get reordered.</li>
<li>Hiding write latency is simple in uniprocessors, adding a write buffer. But this affects correctness in multiprocessors.<br />
In multiprocessor, write buffer or write-back cache might cause that later writes write earlier to memory, namely accesses issued in order may be observed out of order by other processors.</li>
</ol>
<h1 id="sequential-consistency-sc-model"><a class="markdownIt-Anchor" href="#sequential-consistency-sc-model"></a> Sequential consistency (SC) model</h1>
<ol>
<li>Accesses of each processor in program order, all accesses appear in sequential order. Any order implicitly assumed by programmer is maintained. Any order implicitly assumed by programmer is maintained.</li>
<li>How to implement sequential consistency:<br />
Implement cache coherence: writes to the same location are observed in same order by all processors<br />
For each processor, delay start of memory access until previous one completes, namely each processor has only one outstanding memory access at a time</li>
<li>A read completes when its return value is bound.<br />
A write completes when the new value is “visible” to other processors. “Visible” does not mean that other processors have necessarily seen the value yet. It means the new value is committed to the hypothetical serializable order (HSO).</li>
<li>The strict requirements of SC model severely restrict common hardware and compiler optimizations.<br />
Processor issues accesses one-at-a-time and stalls for completion, which results the Low processor utilization even with caching.</li>
<li>Total store ordering (TSO) model: Comparing to SC model, a read operation doesn’t need to stall for waiting an earlier write operation. This is similar to the architecture with a FIFO write buffer.</li>
<li>Partial store ordering (PSO) model: Comparing to TSO model, even a write operation doesn’t need to stall for waiting an earlier write operation. This is an architecture with a write buffer that doesn’t have to be FIFO.</li>
</ol>
<h1 id="optimization"><a class="markdownIt-Anchor" href="#optimization"></a> Optimization</h1>
<ol>
<li>Most programs don’t require strict ordering (all of the time) for correctness. Here correctness means same results as sequential consistency.</li>
<li>Two accesses conflict if they access same location, and at least one is a write.</li>
<li>We can order accesses by program order (po) and dependence order (do). Operation2 dependents on operation1 if operation2 reads operation1.</li>
<li>Data Race is that two conflicting accesses on different processors, not ordered by intervening accesses.</li>
<li>Properly Synchronized Programs is that all synchronizations are explicitly identified and all data accesses are ordered through synchronization.</li>
<li>Many parallel programs have mixtures of “private” and “public” parts.  The “private” parts must be protected by synchronization,  like locks and unlocks.<br />
Between synchronization operations, we can allow reordering of memory operations as long as intra-thread dependences are preserved.<br />
Just before and just after synchronization operations, thread must wait for all prior operations to complete</li>
<li>MFENCE does not begin until all prior reads &amp; writes from that thread have completed, and no subsequent read or write from that thread can start until after it finishes. Xchg does this implicitly.<br />
MFENCE operation does not push values out to other threads. It simply stalls the thread that performs the MFENCE until write buffer empty.<br />
MFENCE operations create partial orderings that are observable across threads.</li>
<li>In weak ordering model, we put MFENCEs before lock operation and after unlock operation.</li>
<li>Lock operation: only gains (“acquires”) permission to access data. Unlock operation: only gives away (“releases”) permission to access data.<br />
Release Consistency (RC) model make sure writes before the lock or in the critical section completed before exit critical section, and make sure reads/writes in the critical section or after exit critical section don’t access shared state until lock acquired.</li>
<li>LFENCE serializes only with respect to load operations, and SFENCE serializes only with respect to store operations. In practice MFENCE and xchg are the most likely used ones.</li>
<li>Don’t use only normal memory operations for synchronization, like Peterson’s algorithm. Do use either explicit synchronization operations, like xchg (atomic), or fences.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/07/Courses/CS149/10-Snooping-Implementation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/07/Courses/CS149/10-Snooping-Implementation/" class="post-title-link" itemprop="url">10. Snooping Implementation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-07 10:24:14" itemprop="dateCreated datePublished" datetime="2022-07-07T10:24:14+08:00">2022-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:47" itemprop="dateModified" datetime="2024-02-09T12:57:47+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="building-with-an-atomic-bus"><a class="markdownIt-Anchor" href="#building-with-an-atomic-bus"></a> Building with an atomic bus</h1>
<h2 id="transaction"><a class="markdownIt-Anchor" href="#transaction"></a> Transaction</h2>
<ol>
<li>There is a bus controller to do arbitration. If a processor wants to communicate on the bus, it has to make a request. If there are simultaneous requests from multiple processors, the arbiter will only grant one of them.</li>
<li>A transaction on an atomic bus generally need four steps.<br />
Client is granted bus access (result of arbitration). The client places command on bus (may also place data on bus).<br />
Response to command by another bus client placed on bus. Next client obtains bus access (arbitration)</li>
<li>In a multi-processor with atomic bus scenario, no other bus transactions is allowed between issuing address and receiving data when one processor want to read. Also, when flush is occurred, address and data are sent simultaneously, received by memory before any other transaction is allowed.</li>
<li>Both requests from processor and bus require to look the tag on cache.<br />
If bus receives priority, during bus transaction, processor is locked out from its own cache.<br />
If processor receives priority, during processor cache accesses, cache cannot respond with its snoop result. So it delays other processors even if no sharing of any form is present.</li>
<li>We can alleviate contention to allow simultaneous access by processor-side and snoop controllers through cache duplicate tags or multi-ported tag memory. In either case cost of the additional performance is additional hardware resources.<br />
Tags must stay in sync for correctness, so tag update by one controller will still need to block the other controller, but modifying tags is infrequent compared to checking them.</li>
</ol>
<h2 id="read-miss"><a class="markdownIt-Anchor" href="#read-miss"></a> Read miss</h2>
<ol>
<li>When a cache read miss occurred, memory needs to know what to do. If the line is dirty, memory should not respond. And the loading cache needs to know what to do. If the line is shared, cache should load into S state, not E.</li>
<li>If one cache controller find that the line is shared in its cache, the controller will send a message through the “shared” wire on bus. If that line is dirty, the controller will send a message through the “dirty” wire on bus.<br />
Everytime a processor has responded a snoop, the value in “snoop-pending” wire will be lower and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span> value indicates that all processors have responded.</li>
<li>Memory controller could immediately start accessing DRAM, but not respond (squelch response). If a snoop result from another cache indicates it has copy of most recent data, then cache should provide data, not memory. Memory could assume one of the caches will service request until snoop results are valid. If snoop indicates no cache has data, then memory must respond</li>
</ol>
<h2 id="write-back"><a class="markdownIt-Anchor" href="#write-back"></a> Write back</h2>
<ol>
<li>Write backs involve two bus transactions, incoming line (line requested by processor) and outgoing line (evicted dirty line in cache that must be flushed).<br />
Ideally would like the processor to continue as soon as possible, namely it shouldn’t have to wait for the flush to complete.</li>
<li>The solution is write-back buffer.<br />
Stick line to be flushed in a write-back buffer. Immediately load requested line to allow processor to continue. Flush contents of write-back buffer at a later time.</li>
<li>If a request of another processor for the address of the data in the write-back buffer appears on the bus, snoop controller must check the write-back buffer addresses in addition to cache tags.<br />
If there is a write-back buffer match, the controller will respond with data from write- back buffer rather than cache and cancel outstanding bus access request.</li>
<li>A write commits when a read-exclusive transaction appears on bus and is acknowledged by all other caches. All future reads will reflect the value of this write, even if data from P has not yet been written to P’s dirty cache line, or to memory.<br />
Order of transactions on the bus defines the global order of writes in the parallel program.</li>
<li>“Commit” is not “complete”. A write completes when the updated value is in the cache line</li>
</ol>
<h2 id="race-conditions"><a class="markdownIt-Anchor" href="#race-conditions"></a> Race conditions</h2>
<ol>
<li>Coherence protocol state transition diagrams assumed that transitions between states were atomic. However, in practice state transitions are not atomic.</li>
<li>We’ve assumed the bus transaction itself is atomic, but all the operations the system performs as a result of a memory operation are not.</li>
<li>Processor, cache, and bus all are resources operating in parallel. They often contend for shared resources: processor and bus contend for cache while caches contend for bus access.</li>
<li>Cache must be able to handle requests while waiting to acquire bus AND be able to modify its own outstanding requests.</li>
<li>To avoid deadlock, processor must be able to service incoming transactions while waiting to issue requests.</li>
<li>To avoid livelock, a write that obtains exclusive ownership must be allowed to complete before exclusive ownership is relinquished.</li>
<li>Multiple processors competing for bus access must be careful to avoid (or minimize likelihood of) starvation.</li>
<li>Performance optimization often entails splitting operations into several, smaller transactions. Splitting costs in more hardware needed to exploit additional parallelism, and care needed to ensure abstractions still hold.</li>
</ol>
<h1 id="building-with-non-atomic-bus"><a class="markdownIt-Anchor" href="#building-with-non-atomic-bus"></a> Building with non-atomic bus</h1>
<ol>
<li>Problem with atomic bus: bus is idle while response is pending, which decreases effective bus bandwidth. The interconnect is a limited, shared resource in a multi-processor system. So it is important to use it as efficiently as possible.</li>
<li>Bus transactions are split into two transactions, the request and the response. Other transactions can intervene between a transaction’s request and response.</li>
<li>Basic design:<br />
Up to eight outstanding requests at a time (system wide)<br />
Responses need not occur in the same order as requests. But request order establishes the total order for the system<br />
Flow control via negative acknowledgements (NACKs). When a buffer is full, client can NACK a transaction, causing a retry</li>
<li>We can think of a split-transaction bus as two separate buses, a request bus and a response bus.<br />
The request bus has lines for command and address. The response bus has lines for data and response tag. Response tag has 3 bits to represent 8 requests.</li>
</ol>
<h2 id="read-miss-2"><a class="markdownIt-Anchor" href="#read-miss-2"></a> Read miss</h2>
<h3 id="phase-1"><a class="markdownIt-Anchor" href="#phase-1"></a> Phase 1</h3>
<ol>
<li>Request arbitration: cache controllers present request for address to bus (many caches may be doing so in the same cycle)</li>
<li>Request resolution: address bus arbiter grants access to one of the requestors. Request table entry allocated for request. Special arbitration lines indicate tag assigned to request.</li>
<li>Bus “winner” places command/address on the bus.</li>
<li>Caches perform snoop: look up tags, update cache state, etc. Memory operation commits here. (no bus traffic)</li>
<li>Caches acknowledge this snoop result is ready, or signal they could not complete snoop in time here.</li>
</ol>
<h3 id="phase-2"><a class="markdownIt-Anchor" href="#phase-2"></a> Phase 2</h3>
<ol>
<li>Data response arbitration: responder presents intent to respond to request with tag T. (many caches or memory may be doing so in the same cycle)</li>
<li>Data bus arbiter grants one responder bus access.</li>
<li>Original requestor signals readiness to receive response (or lack thereof: requestor may be busy at this time)</li>
<li>Then in phase 3, Responder places response data on data bus. Caches present snoop result for request with the data. Request table entry is freed. Those 3 actions can happen parallel.</li>
</ol>
<h2 id="pipelined-transactions"><a class="markdownIt-Anchor" href="#pipelined-transactions"></a> Pipelined transactions</h2>
<ol>
<li>Request bus and response bus can run parallel. So the response of the last transaction and the request of the next transaction can happen simultaneously. Pipelining may cause out-of-order completion.</li>
<li>Write backs and BusUpg transactions do not have a response component. Write backs acquire access to both request address bus and data bus as part of “request” phase BusUpg does not need any acknowledgement or data.</li>
<li>Avoid conflicting requests by disallowing them. Each cache has a copy of the request table. Caches do not make requests that conflict with requests in the request table.</li>
<li>Caches/memory have buffers for receiving data off the bus. If the buffer fills, client NACKs relevant requests or responses.</li>
<li>In parallel system, we use queues to accommodate variable (unpredictable) rates of production and consumption. As long as workers, on average, produce and consume at the same rate, all workers can run at full rate. Otherwise, some will stall waiting for others to accept or produce new input.</li>
<li>We have queues to track requests and responses between L1 and L2 caches and between L2 cache and bus. One queue is for requests and responses from closer (to processor) to farther (L1 to L2 or L2 to bus), and the other is from farther to closer (L2 to L1 or bus to L2).</li>
<li>This may rise deadlock due to full queue. Outgoing read request (initiated by processor) and incoming read request (due to another cache) both requests generate responses that require space in the other queue (circular dependency)</li>
<li>Sizing all buffers to accommodate the maximum number of outstanding requests on bus is one solution to avoiding deadlock. But a costly one.</li>
<li>Avoiding buffer deadlock with separate request/response queues. Namely, we distinguish whether it is a request or a response.<br />
Responses can be completed without generating further transactions. Requests increase queue length But responses reduce queue length. While stalled attempting to send a request, cache must be able to service responses. Responses will make progress, eventually freeing up resources for requests.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/06/Courses/CS149/09-Directory-Based-Cache-Cohurence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/06/Courses/CS149/09-Directory-Based-Cache-Cohurence/" class="post-title-link" itemprop="url">09. Directory-Based Cache Cohurence</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-06 17:10:17" itemprop="dateCreated datePublished" datetime="2022-07-06T17:10:17+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:41" itemprop="dateModified" datetime="2024-02-09T12:57:41+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="problems-to-solve"><a class="markdownIt-Anchor" href="#problems-to-solve"></a> Problems to solve</h1>
<ol>
<li>The snooping cache coherence protocols relied on broadcasting coherence information to all processors over the chip interconnect. Every time a cache miss occurred, the triggering cache communicated with all other caches, so the interconnect has a heavy traffic.</li>
<li>The efficiency of NUMA system does little good if the coherence protocol can’t also be scaled. Processor accesses nearby memory, but to ensure coherence still must broadcast to all other processors.</li>
<li>One possible solution is hierarchical snooping which arranges nodes in a tree and uses snooping coherence at each level. The interconnects involved in a communication is as low as possible and kept as local as possible.</li>
<li>The structure of hierarchical snooping is relatively simple to build.<br />
It uses a tree to reduce the conjunction at the center part, but if the workload is not nicely partitioned, then the root of the network can become a bottleneck.<br />
It also has larger latencies than direct communication and does not apply to more general network topologies (meshes, cubes)</li>
</ol>
<h1 id="directory"><a class="markdownIt-Anchor" href="#directory"></a> Directory</h1>
<ol>
<li>Snooping schemes broadcast coherence messages to determine the state of a line in the other caches. An alternative idea is to avoid broadcast by storing information about the status of the line in one place, namely a “directory”.</li>
<li>A line is a region of memory that would be cached as a single block. One directory entry corresponds to one line of memory.<br />
In a directory entry, there are a dirty bit that indicates line is dirty in one of the processors’ caches and P presence bits that indicate whether processor P has line in its cache.</li>
<li>The directory is used in NUMA system, each processor has a “local” memory and a directory.<br />
Home node of a line is the node with memory holding the corresponding data for the line.<br />
Requesting node is the node containing processor requesting line</li>
</ol>
<h2 id="read-and-write"><a class="markdownIt-Anchor" href="#read-and-write"></a> Read and write</h2>
<ol>
<li>When a read miss happens:<br />
The requesting node will send a read miss message to the home node of the requested line. Then home directory checks entry for line.<br />
If dirty bit for cache line is OFF, the home node will respond with contents from memory, set presence bit of the requesting node to true to indicate line is cached by the requesting processor.<br />
If dirty bit for cache line is ON, the home node will respond with message providing identity of line owner. Requesting node requests data from owner and owner changes state in cache to SHARED (read only), responds to requesting node. Owner also responds to home node, home clears dirty, updates presence bits (line is cached by both the requesting node and owner), updates memory.</li>
<li>When a write miss happens:<br />
The requesting node will send a write miss message to the home node of the requested line.<br />
The home node will response the sharer ids and data to the requesting node.<br />
The requesting node will send invalidation signal to all sharers. After receiving invalidation acks from all sharers, the requesting node can perform write.<br />
The home node will update presence bits (line is cached by only the requesting node) and dirty bit.</li>
<li>On reads, directory tells requesting node exactly where to get the line from, either from home node (if the line is clean) or from the owning node (if the line is dirty). Either way, retrieving data involves only point-to-point communication.</li>
<li>On writes, the advantage of directories depends on the number of sharers. In the limit, if all caches are sharing data, all caches must be communicated with, just like broadcast in a snooping protocol.</li>
<li>In general only a few processors share the line, namely only a few processors must be told of writes.  And the expected number of sharers typically increases slowly with P.</li>
</ol>
<h2 id="reduce-storage-overhead"><a class="markdownIt-Anchor" href="#reduce-storage-overhead"></a> Reduce storage overhead</h2>
<ol>
<li>Full bit vector directory storage is proportional to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding="application/x-tex">P\times M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> is the number of nodes and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> is the number of lines in memory. The storage overhead of directory is too much, and we do not want it to be DRAM since we need it to run fast.</li>
<li>One way to reduce storage overhead is to optimize on full-bit vector.<br />
Increase cache line size to reduce <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> term.<br />
Group multiple processors into a single directory node to reduce <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> term. We could use snooping protocol to maintain coherence among processors in a node, directory across nodes.</li>
<li>Another way is to limit sharer pointer. Since data is expected to only be in a few caches at once, storage for a limited number of pointers per directory entry should be sufficient. Only need a list of the nodes holding a valid copy of the line.</li>
<li>When an overflow in limited pointer schemes occurs, we can revert to broadcast if broadcast mechanism exists. If no broadcast mechanism present on machine, newest sharer replaces an existing one (must invalidate line in the old sharer’s cache)</li>
<li>One more way is through sparse directory.<br />
The majority of memory is not resident in cache. And to carry out coherence protocol the system only needs sharing information for lines that are currently in some cache. So most directory entries are empty most of the time.<br />
We can add a tag for each directory line to indicate the memory held in some cache. The overhead is now <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>×</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">P\times C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> is the number of lines in each cache.</li>
</ol>
<h2 id="reduce-number-of-message-sent"><a class="markdownIt-Anchor" href="#reduce-number-of-message-sent"></a> Reduce number of message sent</h2>
<ol>
<li>In a read miss to dirty line, there are five network transactions in total. But only four of the transactions are on the critical path.</li>
<li>In intervention forward, home node requests data from owner node (intervention read). After the owner has responded, home node updates directory, and responds to requesting node with data.<br />
Four network transactions in total (less traffic). But all four of the transactions are on the critical path.</li>
<li>In request forwarding, home node requests the owner to sent data to the requesting node. Then the owner will send data to requesting node and home node at the same time.<br />
Four network transactions in total, with only three of the transactions are on the critical path.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/06/19/Courses/CS149/08-Snooping-based-Cache-Coherence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/19/Courses/CS149/08-Snooping-based-Cache-Coherence/" class="post-title-link" itemprop="url">08. Snooping-based Cache Coherence</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-19 19:30:41" itemprop="dateCreated datePublished" datetime="2022-06-19T19:30:41+08:00">2022-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 13:58:37" itemprop="dateModified" datetime="2024-02-09T13:58:37+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="the-cache-coherence-problem"><a class="markdownIt-Anchor" href="#the-cache-coherence-problem"></a> The cache coherence problem</h1>
<ol>
<li>This problem happens in a shared memory multi-processor system. Reading a value at address X should return the last value written to address X by any processor.</li>
<li>This is a problem created by replicating the data stored at address X in local caches (a hardware implementation detail), and cannot be fixed by adding locks.</li>
<li>Memory coherence problem exists because there is both global storage (main memory) and per-processor local storage (processor caches) implementing the abstraction of a single shared address space.</li>
<li>In a cache hierarchy,<br />
L1 and L2 caches are private per core while L3 cahce is shared by cores in the same chip.<br />
L3 cache is split into sectors or banks. Each bank is physically associated with a core, but managed hardware-wise as a single coherent unit.<br />
L2 cache and L3 cache communicate through a ring interconnect where most inter-processor actions happens.</li>
</ol>
<h2 id="uniprocessor-case"><a class="markdownIt-Anchor" href="#uniprocessor-case"></a> Uniprocessor case</h2>
<ol>
<li>On a uniprocessor, providing coherence is fairly simple, since writes typically come from one client: the processor. Load operation must examine all pending stores in store buffer and select the last sequence.</li>
<li>There is one exception on a uniprocessor, which is device I/O via direct memory access (DMA).</li>
<li>One solution to DMA is that CPU writes to shared buffers using uncached stores.<br />
Another way is supported by OS which will mark virtual memory pages containing shared buffers as not-cachable, and explicitly flush pages from cache when I/O completes.</li>
<li>In practice, DMA transfers are infrequent compared to CPU loads and stores (so these heavyweight software solutions are acceptable)</li>
</ol>
<h2 id="coherence-definition"><a class="markdownIt-Anchor" href="#coherence-definition"></a> Coherence definition</h2>
<ol>
<li>Obeys program order as expected of a uniprocessor system: A read by processor P to address X that follows a write by P to address X, should return the value of the write by P (assuming no other processor wrote to X in between)</li>
<li>Write propagation: A read by processor P1 to address X that follows a write by processor P2 to X returns the written value, if the read and write are “sufficiently separated” in time (assuming no other write to X occurs in between)</li>
<li>Write serialization: Writes to the same address are serialized: two writes to address X by any two processors are observed in the same order by all processors.</li>
<li>Write propagation means that notification of a write must eventually get to the other processors. Note that precisely when information about the write is propagated is not specified in the definition of coherence.</li>
</ol>
<h1 id="implementing-choherence"><a class="markdownIt-Anchor" href="#implementing-choherence"></a> Implementing choherence</h1>
<ol>
<li>Software-based solution: OS uses page-fault mechanism to propagate writes. It can be used to implement memory coherence over clusters of workstations</li>
<li>Hardware-based solutions: “snooping”-based coherence implementations and directory-based coherence implementations</li>
<li>Most modern multi-core CPUs implement cache coherence<br />
Discrete GPUs do not implement cache coherence. Overhead of coherence deemed not worth it for graphics and scientific computing applications (NVIDIA GPUs provide single shared L2 + atomic memory operations)<br />
But the latest Intel Integrated GPUs do implement cache coherence</li>
</ol>
<h2 id="shared-caches"><a class="markdownIt-Anchor" href="#shared-caches"></a> Shared caches</h2>
<ol>
<li>One single cache shared by all processors eliminates problem of replicating state in multiple caches and makes coherence easy.</li>
<li>This has obvious scalability problems since the point of a cache is to be local and fast. It also causes interference and contention due to many clients.</li>
<li>Facilitates fine-grained sharing (overlapping working sets). Loads/stores by one processor might pre-fetch lines for another processor</li>
</ol>
<h2 id="snooping-cache-coherence-schemes"><a class="markdownIt-Anchor" href="#snooping-cache-coherence-schemes"></a> Snooping cache-coherence schemes</h2>
<ol>
<li>Main idea: all coherence-related activity is broadcast to all processors</li>
<li>Cache controllers monitor (“they snoop”) memory operations, and react accordingly to maintain memory coherence</li>
<li>Cache controller must respond to actions from both ends:<br />
It must respond the Load/Store requests from its local processor<br />
It also must respond coherence-related activity broadcast over the chip’s interconnect.</li>
<li>The interconnect is between memory and caches possessed by each processor. There is not only memory-cache information, but also cache-cache information, which will limit the scality of the system.</li>
</ol>
<h3 id="write-through-caches"><a class="markdownIt-Anchor" href="#write-through-caches"></a> Write-through caches</h3>
<ol>
<li>For the invalidation-based protocol, when one processor write into an address, cache controller broadcasts<br />
invalidation message for other caches to mark that line to invalidation.<br />
the next read from other processors will trigger cache miss</li>
<li>For the update-based protocol: other caches will update their local copies as the information is sent.</li>
<li>States: Valid (V) or Invalid (I)<br />
A local processor read (PrRd) always ends at valid. If the operation starts from an invalid state, a message will be sent (BusRd). If it starts from a valid state, no message will be sent.<br />
A local processor write (PrWr) always ends at the same state as before the operation (assumes write no-allocate policy), and always sends a message (BusWr).<br />
When a write message from other processor is received (BusWr), It always ends at invalid state.<br />
<img src="/imgs/CS149/08/1.jpeg" width="20%"></li>
<li>Requirements of the interconnect:<br />
All write transactions visible to all cache controllers.<br />
All write transactions visible to all cache controllers in the same order.</li>
<li>Simplifying assumptions here:<br />
Interconnect and memory transactions are atomic<br />
Processor waits until previous memory operations is complete before issuing next memory operation<br />
Invalidation applied immediately as part of receiving invalidation broadcast</li>
</ol>
<h1 id="write-back-caches-invalidation-based"><a class="markdownIt-Anchor" href="#write-back-caches-invalidation-based"></a> Write-back caches (Invalidation-based)</h1>
<ol>
<li>Dirty state of cache line now indicates exclusive ownership</li>
<li>Exclusive: cache is only cache with a valid copy of line (it can safely be written to)<br />
Owner: cache is responsible for supplying the line to other processors when they attempt to load it from memory (otherwise a load from another processor will get stale data from memory)</li>
<li>A line in the “exclusive” state can be modified without notifying<br />
the other caches<br />
Processor can only write to lines in the exclusive state. So they need a way to tell other caches that they want exclusive access to the line. They will do this by sending all the other caches messages<br />
When cache controller snoops a request for exclusive access to line it contains, it must invalidate the line in its own cache</li>
</ol>
<h2 id="msi-write-back-invalidation-protocol"><a class="markdownIt-Anchor" href="#msi-write-back-invalidation-protocol"></a> MSI write-back invalidation protocol</h2>
<ol>
<li>Three cache line states:<br />
Invalid (I): same as meaning of invalid in uniprocessor cache<br />
Shared (S): line valid in one or more caches<br />
Modified (M): line valid in exactly one cache (a.k.a. “dirty” or “exclusive” state)</li>
<li>The local processors have the same operations as write-through case.<br />
The coherence-related bus transactions from remote caches have three kinds:<br />
BusRd: obtain copy of line with no intent to modify<br />
BusRdX: obtain copy of line with intent to modify<br />
flush: write dirty line out to memory<br />
<img src="/imgs/CS149/08/2.png" width="50%"></li>
<li>When try to write an invalid line without reading it, the content of the current modified state line will be sent to the new writer.</li>
<li>Write propagation is achieved via combination of invalidation on BusRdX, and flush from M-state on subsequent BusRd/BusRdX from another processors</li>
<li>Write serialization<br />
Writes that appear on interconnect are ordered by the order they appear on interconnect (BusRdX)<br />
Reads that appear on interconnect are ordered by order they appear on interconnect (BusRd)<br />
Writes that don’t appear on the interconnect (PrWr to line already in M state):
<ul>
<li>Sequence of writes to line comes between two interconnect transactions for the line</li>
<li>All writes in sequence performed by same processor, P (that processor certainly observes them in correct sequential order)</li>
<li>All other processors observe notification of these writes only after a interconnect transaction for the line.</li>
<li>So all processors see writes in the same order.</li>
</ul>
</li>
</ol>
<h2 id="mesi-invalidation-protocol"><a class="markdownIt-Anchor" href="#mesi-invalidation-protocol"></a> MESI invalidation protocol</h2>
<ol>
<li>MSI requires two interconnect transactions for the common case of reading an address, then writing to it
<ul>
<li>Transaction 1: BusRd to move from I to S state</li>
<li>Transaction 2: BusRdX to move from S to M state</li>
</ul>
</li>
<li>Solution: add additional state E (“exclusive clean”) to mark the line that has not been modified, but only this cache has a copy of the line<br />
This state decouples exclusivity from line ownership (line not dirty, so copy in memory is valid copy of data)<br />
Upgrade from E to M does not require an interconnect transaction</li>
<li>
<img src="/imgs/CS149/08/3.jpeg" width="50%">
</li>
</ol>
<h2 id="5-stage-invalidation-based-protocol"><a class="markdownIt-Anchor" href="#5-stage-invalidation-based-protocol"></a> 5-stage invalidation-based protocol</h2>
<ol>
<li>Who should supply data on a cache miss when line is in the E or S state of another cache? <br />
Can get cache line data from memory or can get data from another cache? If source is another cache, which one should provide it?</li>
<li>Cache-to-cache transfers add complexity, but commonly used to reduce both latency of data access and reduce memory bandwidth required by application</li>
<li>MESIF: Like MESI, but one cache holds shared line in F state rather than S (F=”forward”). Cache with line in F state services miss<br />
Simplifies decision of which cache should service miss (basic MESI: all caches respond)<br />
Used by Intel processors</li>
<li>MOESI: Transition from M to O (O=”owned, but not exclusive”) and do not flush to memory (In MESI protocol, transition from M to S requires flush to memory).<br />
Other processors maintain shared line in S state, one processor maintains line in O state. Data in memory is stale, so cache with line in O state must service cache misses.<br />
Used in AMD Opteron</li>
</ol>
<h1 id="invalidation-based-vs-update-based"><a class="markdownIt-Anchor" href="#invalidation-based-vs-update-based"></a> Invalidation-based vs. Update-based</h1>
<ol>
<li>Invalidation-based protocol: To write to a line, cache must obtain exclusive access to it. All other caches must invalidate their copies</li>
<li>Update-based protocol: Can write to shared copy by broadcasting update to all other copies</li>
<li>Intuitively, update would seem preferable if other processors<br />
sharing data continue to access it after a write occurs<br />
But updates are overhead if data just sits in caches (and is never read by another processor again) or application performs many writes before the next read</li>
<li>Update can reduce cache miss rate since all shared copies remain valid.<br />
Update can suffer from high traffic due to multiple writes before the next read by another processor</li>
</ol>
<h1 id="snoop-for-a-cache-hierarchy"><a class="markdownIt-Anchor" href="#snoop-for-a-cache-hierarchy"></a> Snoop for a cache hierarchy</h1>
<ol>
<li>Challenge: changes made to data at L1 cache may not be visible to L2 cache controller than snoops the interconnect.</li>
<li>Inclusion property:<br />
All lines in closer to processor cache are also in farther from processor cache. Thus, all transactions relevant to L1 are also relevant to L2, so it is sufficient for only the L2 to snoop the interconnect.<br />
If line is in owned state (M in MSI/MESI) in L1, it must also be in owned state in L2. Allows L2 to determine if a bus transaction is requesting a modified cache line in L1 without requiring information from L1.</li>
<li>Even if L2 is larger than L1, the inclusion cannot be maintained automatically. L1 and L2 might choose to evict different lines because the access histories differ.</li>
<li>When line X is invalidated in L2 cache due to BusRdX from another cache. Must also invalidate line X in L1<br />
Solution: Each L2 line contains an additional state bit indicating if line also exists in L1. This bit tells the L2 invalidations of the cache line due to coherence traffic need to be propagated to L1.</li>
<li>When L1 write is hit, the corresponding line in L2 cache is in modified state in the coherence protocol, but L2 data is stale.<br />
When coherence protocol requires X to be flushed from L2, L2 cache must request the data from L1.<br />
Add another bit for “modified-but-stale” (flushing a “modified-but-stale” L2 line requires getting the real data from L1 first.)</li>
</ol>
<h1 id="false-sharing"><a class="markdownIt-Anchor" href="#false-sharing"></a> False sharing</h1>
<ol>
<li>Fasle sharing is that two processors write to different addresses, but those addresses map to the same cache line. The cache line keeps invalidating and request data from another processor, generating significant amounts of communication due to the coherence protocol.</li>
<li>We can split the line into two parts and each processor only write one part.</li>
<li>One way is to insert some paddings to make the data written by one processor take up a whole cache line. This can easily implemented in software level. But it causes memory waste.</li>
<li>Another way is mapping addresses handled by the same processor to the same cache line. This causes no waste, but break the successiveness of memory address within a cache line. Also it needs to know the addresses each processor will write, and is harder to implement.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/06/11/Courses/CS149/07-Workload-driven-Perfromance-Evaluation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/11/Courses/CS149/07-Workload-driven-Perfromance-Evaluation/" class="post-title-link" itemprop="url">07. Workload-driven Perfromance Evaluation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-11 21:39:38" itemprop="dateCreated datePublished" datetime="2022-06-11T21:39:38+08:00">2022-06-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:29" itemprop="dateModified" datetime="2024-02-09T12:57:29+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>We should compare parallel program speedup to the best sequential program, instead of parallel algorithm running on one core.<br />
The reason is that to allow for parallelism, we might change the algorithm, and make it slower when executed sequentially.</p>
<h1 id="scaling"><a class="markdownIt-Anchor" href="#scaling"></a> Scaling</h1>
<h2 id="why-consider-scaling"><a class="markdownIt-Anchor" href="#why-consider-scaling"></a> Why consider scaling?</h2>
<ol>
<li>
<p>Arithmetic intensity is determined by both problem size and the processors number. Small problem size or large processor number both yields low arithmetic intensity.</p>
</li>
<li>
<p>If the problem size is too small, it might already execute fast enough on a single core. Scaling the performance of small problem may not be all that important.<br />
Parallelism overheads dominate parallelism benefits, and may even result in slow downs.</p>
</li>
<li>
<p>If the problem size is too large for a single machine, working set may not fit in memory, and causing thrashing to disk. With enough processors, the key working set fits in per-processor cache.<br />
This may get a super-linear speedup and make speedup on a bigger parallel machine with more memory look amazing.</p>
</li>
<li>
<p>Another situation that we might get a super-linear speedup is when we try to search for a solution. With parallelism, we are trying more different variance of search, and more likely to find the solution earlier.</p>
</li>
<li>
<p>So we shouldn’t only consider a fixed problem size. Instead, it is desirable to scale problem size as machine sizes grow.</p>
</li>
<li>
<p>In architecture, scaling up considers how does performance scale with increasing core count, and will design scale to the high end?<br />
Scaling down considers how does performance scale with decreasing core count, and will desing scale to the low end?</p>
</li>
</ol>
<h2 id="different-scalings"><a class="markdownIt-Anchor" href="#different-scalings"></a> Different scalings</h2>
<ol>
<li>
<p>Strong scaling: scaling processors with a fixed problem size. Consider the ratio between the runtime of problem <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> processors and the runtime <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> processor.</p>
</li>
<li>
<p>The goal ratio is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span>. This kind of scaling tells us does having more processors get job done faster?</p>
</li>
<li>
<p>Weak scaling: scaling problem size and processors proportionally. Consider the ratio of the runtime of problem <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>×</mo><mi>X</mi></mrow><annotation encoding="application/x-tex">P\times X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> processors and the runtime of problem <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span> processor.</p>
</li>
<li>
<p>The goal ratio is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>. This kind of scaling tells us does having more procesors let me do bigger jobs?</p>
</li>
<li>
<p>Problem size is often determined by more than one parameter. So in weak scaling, we need to consider how should the parameter be changed.</p>
</li>
</ol>
<h2 id="scaling-constrains"><a class="markdownIt-Anchor" href="#scaling-constrains"></a> Scaling constrains</h2>
<ol>
<li>
<p>When scaling a probelm, we should first ask that in my situation, under what constraints should the problem be scaled?</p>
</li>
<li>
<p>Problem-constrained scaling focuses on using a parallel computer to solve the same problem faster<br />
Speedup <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mfrac><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">=\frac{time\ 1\ processor}{time\ P\ processors}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.38888em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.907772em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
<li>
<p>Time-constrained scaling focuses on completing more work in a fixed amount of time<br />
Speedup $ = \frac{work\ done\ by\ P\ processors}{work\ done\ by\ 1\ processor}$</p>
</li>
<li>
<p>“Work done” may not be linear function of problem inputs. One approach of defining “work done” is by execution time of same computation on a single processor (but consider effects of thrashing if problem too big)</p>
</li>
<li>
<p>Ideally, a measure of work is simple to understand and scales linearly with sequential run time (So ideal speedup remains linear in <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi></mrow><annotation encoding="application/x-tex">P</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span>)</p>
</li>
<li>
<p>Memory-constrained scaling (weak scaling) focuses on running the largest problem possible without overflowing main memory. Neither work or execution times are held constant.<br />
Speedup <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mfrac><mi mathvariant="normal">/</mi><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>p</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>o</mi><mi>n</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>p</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>o</mi><mi>n</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">= \frac{work\ (P\ processors)}{time\ (P\ processors)}/\frac{work\ (1\ processor)}{time\ (1\ processor)}=\frac{work\ per\ unit\ time\ on\ P\ processors}{work\ per\ unit\ time\ on\ 1\ processor}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">/</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">t</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">t</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>There are two assumptions: memory resources scale with processor count, and spilling to disk is infeasible behavior.</p>
</li>
</ol>
<h2 id="challenges-of-scaling-down-or-up"><a class="markdownIt-Anchor" href="#challenges-of-scaling-down-or-up"></a> Challenges of scaling down or up</h2>
<ol>
<li>
<p>Preserve ratio of time spent in different program phases.</p>
</li>
<li>
<p>Preserve important behavioral characteristics.</p>
</li>
<li>
<p>Preserve contention and communication patterns. Tough to preserve contention since contention is a function of timing and ratios.</p>
</li>
<li>
<p>Preserve scaling relationships between problem parameters.</p>
</li>
</ol>
<h1 id="simulation"><a class="markdownIt-Anchor" href="#simulation"></a> Simulation</h1>
<ol>
<li>
<p>Architects evaluate architectural decisions quantitatively using<br />
hardware performance simulators.</p>
</li>
<li>
<p>Architect runs simulations with new feature, runs simulations without new feature, compare simulated performance. Or simulate against a wide collection of benchmarks.</p>
</li>
<li>
<p>You can design detailed simulator to test new architectural feature. It would be very expensive to simulate a parallel machine in full detail.<br />
Often cannot simulate full machine configurations or realistic problem sizes (must scale down workloads significantly). Architects need to be confident scaled down simulated results predict reality</p>
</li>
<li>
<p>In trace-driven simulator, we instrument real code running on real machine to record a trace of all memory accesses. Then play back trace on simulator.<br />
It may lead to overfit the trace you have instead of having a better generalization.</p>
</li>
<li>
<p>In execution-driven simulator, we execute simulated program in software. Simulated processors generate memory references, which are processed by the simulated memory hierarchy.<br />
Performance of simulator is typically inversely proportional to level of simulated detail.</p>
</li>
<li>
<p>When dealing with large parameter space of machines (number of processors, cache sizes, cache line sizes, memory bandwidths, etc. ), we can use the architectural simulation state space.</p>
</li>
</ol>
<h1 id="understanding-the-performance"><a class="markdownIt-Anchor" href="#understanding-the-performance"></a> Understanding the performance</h1>
<ol>
<li>
<p>Always, always, always try the simplest parallel solution first, then measure performance to see where you stand.</p>
</li>
<li>
<p>Determine if your performance is limited by computation, memory bandwidth (or memory latency), or synchronization?<br />
Try and establish “high watermarks”. What’s the best you can do in practice? How close is your implementation to a best-case scenario?</p>
</li>
<li>
<p>Roofline model: Use microbenchmarks to compute peak performance of a machine as a function of arithmetic intensity of application. Then compare application’s performance to known peak values.</p>
</li>
<li>
<p>The x-axis means operational intensity (like Flops/Byte), and the y-axis means attenable GFlops/s.<br />
In the diagonal region, the y grows with x, which means the memory bendwidth is limited. In the horizontal region, the y stays the same as x grows, which means the compute is limited.</p>
</li>
<li>
<p>When compute is limited, we can make use of ILP or SIMD, or balance floating-point.<br />
When memory bandwidth is limited, we can limit accesses to unit stride accesses only, develope memory affinity, or use software prefetching.</p>
</li>
</ol>
<h2 id="establish-high-watermarks"><a class="markdownIt-Anchor" href="#establish-high-watermarks"></a> Establish high watermarks</h2>
<ol>
<li>
<p>Add “math” (non-memory instructions).<br />
Does execution time increase linearly with operation count as math is added? If so, this is evidence that code is instruction-rate limited</p>
</li>
<li>
<p>Remove almost all math, but load same data.<br />
How much does execution-time decrease? If not much, suspect memory bottleneck</p>
</li>
<li>
<p>The first two way need to avoid compiler optimization.</p>
</li>
<li>
<p>Change all array accesses to A[0].<br />
How much faster does your code get?<br />
This establishes an upper bound on benefit of improving locality of data access</p>
</li>
<li>
<p>Remove all atomic operations or locks.<br />
How much faster does your code get? (provided it still does approximately the same amount of work)<br />
This establishes an upper bound on benefit of reducing sync overhead.</p>
</li>
</ol>
<h2 id="profilersperformance-monitoring-tools"><a class="markdownIt-Anchor" href="#profilersperformance-monitoring-tools"></a> Profilers/performance monitoring tools</h2>
<ol>
<li>
<p>All modern processors have low-level event “performance counters”, which are registers that count important details such as: instructions completed, clock ticks, L2/L3 cache hits/misses, bytes read from memory controller, etc.</p>
</li>
<li>
<p>Intel’s Performance Counter Monitor Tool provides a C++ API for accessing these registers.</p>
</li>
<li>
<p>It can use <code>getIPC(begin, end)</code>, <code>getL3CacheHitRatio(begin, end)</code>, <code>getBytesReadFromMC(begin, end)</code>, etc. to get values of those information.</p>
</li>
<li>
<p>The <code>begin</code> and <code>end</code> is a <code>SystemCountState</code> instance acquired by <code>getSystemCounterState()</code> at the beginning and end of the code to analyze.</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PCM *m = PCM::<span class="built_in">getInstance</span>();</span><br><span class="line">SystemCounterState begin = <span class="built_in">getSystemCounterState</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// code to analyze goes here</span></span><br><span class="line"></span><br><span class="line">SystemCounterState end = <span class="built_in">getSystemCounterState</span>();</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(“Instructions per clock: %f\n”, <span class="built_in">getIPC</span>(begin, end));</span><br><span class="line"><span class="built_in">printf</span>(“L3 cache hit ratio: %f\n”, <span class="built_in">getL3CacheHitRatio</span>(begin, end));</span><br><span class="line"><span class="built_in">printf</span>(“Bytes read: %d\n”, <span class="built_in">getBytesReadFromMC</span>(begin, end));</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/21/Courses/CS149/06-Locality-Communication-and-Contention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/21/Courses/CS149/06-Locality-Communication-and-Contention/" class="post-title-link" itemprop="url">06. Locality, Communication, and Contention</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-21 13:42:23" itemprop="dateCreated datePublished" datetime="2022-05-21T13:42:23+08:00">2022-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:24" itemprop="dateModified" datetime="2024-02-09T12:57:24+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="communication"><a class="markdownIt-Anchor" href="#communication"></a> Communication</h1>
<h2 id="reduce-communication-time"><a class="markdownIt-Anchor" href="#reduce-communication-time"></a> Reduce communication time</h2>
<ol>
<li>
<p>Total communication time = overhead + occupancy + network delay.</p>
</li>
<li>
<p>Overhead is the time spent on the communication by a processor, occupancy is the time for data to pass through slowest component of system, and network delay is everything else.</p>
</li>
<li>
<p>Reduce overhead of communication to sender/receiver:<br />
Reassign tasks in a better way that need to send fewer messages.<br />
Make messages larger to amortize overhead.<br />
Coalesce many small messages into large ones</p>
</li>
<li>
<p>Reduce delay:<br />
Application writer can restructure code to exploit locality.<br />
Hardware implementor can improve communication architecture.</p>
</li>
</ol>
<h2 id="reduce-communication-cost"><a class="markdownIt-Anchor" href="#reduce-communication-cost"></a> Reduce communication cost</h2>
<ol>
<li>
<p>Total communication cost = communication time - overlap</p>
</li>
<li>
<p>Overlap: portion of communication performed concurrently with other work (“other work” can be computation or other communication)</p>
</li>
<li>
<p>Cost is the part that your cannot get over with by changing the protocol. The communication time is obviously necessary, and we can hide the overlap part by pipelining.</p>
</li>
<li>
<p>Increase communication/computation overlap:<br />
Application writer can use asynchronous communication (e.g., async messages)<br />
Hardware implementor can use pipelining, multi-threading, pre-fetching, out-of-order execution<br />
Requires additional concurrency in application (more concurrency than number of execution units)</p>
</li>
<li>
<p>Instruction pipeline: Break execution of each instruction down into several smaller steps.<br />
Enables higher clock frequency, only a simple, short operation is done by each part of pipeline each clock</p>
</li>
<li>
<p>Non-piplined communication: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>T</mi><mn>0</mn></msub><mo>+</mo><mfrac><mi>n</mi><mi>B</mi></mfrac></mrow><annotation encoding="application/x-tex">T(n)=T_0+\frac{n}{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.040392em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">T_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> is the start-up latency, n is bytes transferred in operation, B is transfer rate or bandwidth)<br />
Efficient bandwidth <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mfrac><mi>n</mi><mrow><mi>T</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">=\frac{n}{T(n)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.215392em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">n</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
</ol>
<h2 id="improve-arithmetic-intensity"><a class="markdownIt-Anchor" href="#improve-arithmetic-intensity"></a> Improve arithmetic intensity</h2>
<ol>
<li>
<p>Communication-to-computation ratio = amount of communication  / amount of computation<br />
The units can be different. If the denominator is the execution time of computation, the ratio gives the average bandwidth requirement</p>
</li>
<li>
<p>Arithmetic intensity = 1 / communication-to-computation ratio</p>
</li>
<li>
<p>Change the traversal order to reduce the time between accesses to same data. We want to do all the calculations related to the accessing data now.<br />
This way can improve the utilization of cache by preventing that those data get flushed out when we try to access them again.</p>
</li>
<li>
<p>Fuse loops to reduce the frequency of store operation.</p>
</li>
<li>
<p>Improve arithmetic intensity by sharing data. Co-locate tasks that operate on the same data.  Schedule threads working on the same data structure at the same time on the same processor. Reduces inherent communication</p>
</li>
</ol>
<h2 id="reduce-artifactual-communication"><a class="markdownIt-Anchor" href="#reduce-artifactual-communication"></a> Reduce artifactual communication</h2>
<ol>
<li>
<p>Inherent communication: Communication that must occur in a parallel algorithm. The communication is fundamental to the algorithm.<br />
Artifactual communication: all other communication that happens because we want to efficiently use resource.</p>
</li>
<li>
<p>Granularity of communication can be important because it may introduce artifactual communication.<br />
Assume that communication granularity is a cache line.</p>
</li>
<li>
<p>If we see data as row-major layout, when the data required is in column, each communication actually only provides one needed data.</p>
</li>
<li>
<p>When Threads access their assigned elements (no inherent communication exists), real machine triggers (artifactual) communication due to the cache line being written to by both processors.</p>
</li>
<li>
<p>Reducing artifactual comm by blocked data layout. Each communication only involves  the data in the same block. If communication granularity is larger than a block row, each communication will transfer multiple rows.</p>
</li>
</ol>
<h1 id="contention"><a class="markdownIt-Anchor" href="#contention"></a> Contention</h1>
<ol>
<li>
<p>Contention occurs when many requests to a resource are made within a small window of time. The resource is a hot spot.<br />
Contention for shared resource results in longer overall operation times.</p>
</li>
<li>
<p>Distributed work queues serve to reduce contention (contention in access to single shared work queue)</p>
</li>
<li>
<p>One way to reduce contention is to use finer-granularity locks. Instead of locking the whole data structure, we can only lock a part of that structure.</p>
</li>
<li>
<p>Another way is that each CUDA block computes partial results and merges them afterwards. But this requires extra work for merging, and each CUDA block need to store a partial result instead of all of them using the same result.</p>
</li>
<li>
<p>The best way is to stagger access to contended resources. For example, instead of calculate the final result directly, we can calculate several temporal results which has no contention, and get the final result from them.</p>
</li>
</ol>
<h1 id="understanding-the-performance"><a class="markdownIt-Anchor" href="#understanding-the-performance"></a> Understanding the performance</h1>
<ol>
<li>
<p>Always, always, always try the simplest parallel solution first, then measure performance to see where you stand.</p>
</li>
<li>
<p>We should compare parallel program speedup to the best sequential program, instead of parallel algorithm running on one core.<br />
The reason is that to allow for parallelism, we might change the algorithm, and make it slower when executed sequentially.</p>
</li>
<li>
<p>Determine if your performance is limited by computation, memory bandwidth (or memory latency), or synchronization?<br />
Try and establish “high watermarks”. What’s the best you can do in practice? How close is your implementation to a best-case scenario?</p>
</li>
<li>
<p>Roofline model: Use microbenchmarks to compute peak performance of a machine as a function of arithmetic intensity of application. Then compare application’s performance to known peak values</p>
</li>
</ol>
<h2 id="establish-high-watermarks"><a class="markdownIt-Anchor" href="#establish-high-watermarks"></a> Establish high watermarks</h2>
<ol>
<li>
<p>Add “math” (non-memory instructions).<br />
Does execution time increase linearly with operation count as math is added?<br />
If so, this is evidence that code is instruction-rate limited</p>
</li>
<li>
<p>Remove almost all math, but load same data.<br />
How much does execution-time decrease?<br />
If not much, suspect memory bottleneck</p>
</li>
<li>
<p>Change all array accesses to A[0].<br />
How much faster does your code get?<br />
This establishes an upper bound on benefit of improving locality of data access</p>
</li>
<li>
<p>Remove all atomic operations or locks.<br />
How much faster does your code get? (provided it still does approximately the same amount of work)<br />
This establishes an upper bound on benefit of reducing sync overhead.</p>
</li>
</ol>
<h2 id="scaling"><a class="markdownIt-Anchor" href="#scaling"></a> Scaling</h2>
<ol>
<li>
<p>Arithmetic intensity is determined by both problem size and the processors number. Small problem size or large processor number both yields low arithmetic intensity.</p>
</li>
<li>
<p>If the problem size is too small, it might already execute fast enough on a single core. Scaling the performance of small problem ay not be all that important.</p>
</li>
<li>
<p>If the problem size is too large for a single machine, working set may not fit in memory, and causing thrashing to disk. With enough processors, the key working set fits in per-processor cache.<br />
This may get a super-linear speedup and make speedup on a bigger parallel machine with more memory look amazing.</p>
</li>
<li>
<p>Desire to scale problem size as machine sizes grow (buy a bigger machine to compute more, rather than just compute the same problem faster)</p>
</li>
<li>
<p>Problem-constrained scaling focuses on using a parallel computer to solve the same problem faster<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>=</mo><mfrac><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Speedup=\frac{time\ 1\ processor}{time\ P\ processors}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.38888em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.907772em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
<li>
<p>Time-constrained scaling focuses on completing more work in a fixed amount of time<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>d</mi><mi>o</mi><mi>n</mi><mi>e</mi><mtext> </mtext><mi>b</mi><mi>y</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>d</mi><mi>o</mi><mi>n</mi><mi>e</mi><mtext> </mtext><mi>b</mi><mi>y</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Speedup = \frac{work\ done\ by\ P\ processors}{work\ done\ by\ 1\ processor}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><br />
“Work done” may not be linear function of problem inputs. One approach of defining “work done” is by execution time of same computation on a single processor (but consider effects of thrashing if problem too big)</p>
</li>
<li>
<p>Memory-constrained scaling (wak scaling) focuses on running the largest problem possible without overflowing main memory. Neither work or execution times are held constant.<br />
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mfrac><mi mathvariant="normal">/</mi><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow><mrow><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mo stretchy="false">(</mo><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>p</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>o</mi><mi>n</mi><mtext> </mtext><mi>P</mi><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi><mi>s</mi></mrow><mrow><mi>w</mi><mi>o</mi><mi>r</mi><mi>k</mi><mtext> </mtext><mi>p</mi><mi>e</mi><mi>r</mi><mtext> </mtext><mi>u</mi><mi>n</mi><mi>i</mi><mi>t</mi><mtext> </mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mtext> </mtext><mi>o</mi><mi>n</mi><mtext> </mtext><mn>1</mn><mtext> </mtext><mi>p</mi><mi>r</mi><mi>o</mi><mi>c</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>o</mi><mi>r</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">Speedup = \frac{work\ (P\ processors)}{time\ (P\ processors)}/\frac{work\ (1\ processor)}{time\ (1\ processor)}=\frac{work\ per\ unit\ time\ on\ P\ processors}{work\ per\ unit\ time\ on\ 1\ processor}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">p</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">/</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">t</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight">1</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">t</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">e</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">n</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/" class="post-title-link" itemprop="url">05. Graphic processing units and CUDA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-19 09:59:09" itemprop="dateCreated datePublished" datetime="2022-05-19T09:59:09+08:00">2022-05-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 13:51:23" itemprop="dateModified" datetime="2024-02-09T13:51:23+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="graphics"><a class="markdownIt-Anchor" href="#graphics"></a> Graphics</h1>
<ol>
<li>The first step to draw a graphic in screen is to describe the things (key entities) that are manipulated, whose surface is represented as a 3D triangle mesh.<br />
So the input of the calculating system is a list of vertices in 3D space and their connectivity into primitives.</li>
<li>The operations of system is as following:<br />
Given a scene camera position, compute where the vertices lie on screen<br />
Group vertices into primitives<br />
Generate one fragment for each pixel a primitive overlaps<br />
Compute color of primitive for each fragment based on scene lighting and primitive material properties<br />
Put color of the “closest fragment” to the camera in the output image</li>
<li>We can Abstract process of rendering a picture as a sequence of operations on vertices, primitives, fragments, and pixels.<br />
GPUs are very fast processors for performing the same computation (shader programs) on large collections of data (streams of vertices, fragments, and pixels), which sounds like data-parallelism.</li>
<li>To do GPU-based scientific computation, we need to set OpenGL output image size to be output array size, and render 2 triangles that exactly cover screen.</li>
</ol>
<h1 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h1>
<h2 id="grid-block-and-thread"><a class="markdownIt-Anchor" href="#grid-block-and-thread"></a> Grid, Block and Thread</h2>
<ol>
<li>CUDA thread IDs can be up to 3-dimensional. Multiple threads make up a block, and multiple blocks make up a grid.<br />
Each kernel has a grid. All the the blocks in a grid form a 3D matrix, and all the threads in a block also form a 3D matrix.<br />
We can get information about the shape of threads in a block matrix or block in a grid matrix in a block with <code>block_dim</code> and <code>grid_dim</code></li>
<li>When launching the CUDA threads, we need to specify the size of blocks and threads with <code>kernel_function&lt;&lt;&lt;block_dim, thread_dim&gt;&gt;&gt;(parameters)</code>.<br />
Here <code>block_dim</code> and <code>thread_dim</code> can be either a <code>dim3</code> value or a number of the total number of blocks and threads per-block to make the CUDA compiler figure out how to arrange blocks and threads.</li>
<li>So the coordinate to declare or locate a block in a grid, or a thread in a block is 3D. In the CUDA code, we can get the information about current block or thread with <code>blockIdx</code>, <code>threadIdx</code>, and their properties <code>x</code>, <code>y</code>, <code>z</code>.</li>
<li>The calculation object of CUDA is usually matrices. So we would prefer to cut matrices into blocks. Each CUDA block calculate one  block in matrices, and each thread calculate one element in matrices.</li>
<li>In <code>pthread</code>, there is stack space for thread, and need to allocate control block so OS can schedule thread.<br />
Unlike <code>pthread</code>, CUDA control those instances by thread blocks. If control by threads, there will be too many to control.</li>
<li>Major CUDA assumption: thread block execution can be carried out in any order (no dependencies between blocks)</li>
</ol>
<h2 id="kernel-function"><a class="markdownIt-Anchor" href="#kernel-function"></a> Kernel function</h2>
<ol>
<li>“Host” code: serial execution runs as part of normal C/C++ application on CPU<br />
“CUDA device” code: a kernel function runs on GPU, which is denoted by <code>__global__</code> before the definition of the kernel function.<br />
Device function: SPMD execution on GPU, which is denoted by <code>__device__</code>. These functions are called by kernels, and don’t generate new threads.<br />
Kernels and device functions only reference device memory. Host code can only reference host memory, but can have pointers to device memory.</li>
<li>In kernel function, we need to get the indices of the element a thread calculating. For a 2D matrix, we usually take <code>x</code> as column direction, and <code>y</code> as row direction.<br />
So to access <code>A[i][j]</code>, <code>i = blockIdx.y * blockDim * y + threadIdx.y</code> and <code>j = blockIdx.x * blockDim.x + threadIdx.x</code>.</li>
<li>Number of kernel invocations is not determined by size of data collection. So normally we want to do a test before accessing the vector values (array values). The overhead of the test can be ignored, although it does cause some threads are not used.</li>
<li><code>__syncthreads()</code> is a barrier that wait for all threads in the block to arrive at this point.<br />
Atomic operations on both global memory and shared memory variables is also provided, but they are very expensive, should only be used as the last resort.<br />
There is an implicit barrier across all threads at return of kernel.</li>
<li>A compiled CUDA device binary includes program text (instructions) and information about required resources (how many threads per block, how much space per thread, how much shared space per thread block)</li>
</ol>
<h2 id="memory-model"><a class="markdownIt-Anchor" href="#memory-model"></a> Memory Model</h2>
<ol>
<li>Host and device have distinct address spaces, so before the execution, we need to copy data into CUDA memory.<br />
<code>cudaMalloc(ptr, size)</code> can be used to allocate CUDA memory, and <code>cudaFree(ptr)</code> can free CUDA memory. The pointer in <code>cudaMalloc</code> can just be a host variable, but the pointer in <code>cudaFree</code> must be allocated by <code>cudaMalloc</code>.<br />
<code>cudaMemcpy(dest, src, size, kind)</code> can copy those data into CUDA memory. <code>kind</code> is the direction of copy, which can be <code>cudaMemcpyHostToDevice</code> or <code>curdaMemcpyDeviceToHost</code>.</li>
<li>There are three distinct types of memory visible to kernels, per-thread private memory, per-block shared memory, and device global memory.</li>
<li><code>cudaMalloc</code>, <code>cudaMemcpy</code> and <code>cudaFree</code> all operate on device global memory, and those local variables in kernel function are in perthread private memory.<br />
We can declare variables in per-block shared memory with <code>__share__</code>.</li>
<li>If multiple threads in a block need to access a common memory, they will all read that memory once. And if that memory is in the global memory, it would be really slow.</li>
<li>To avoid such efficient decrease, we can copy the common memory into the per-block shared memory. So we only need to access global memory once, and later accesses only in per-block shared memory.<br />
We just need to declare a <code>__share__</code> array, and assign it with values in global memory.</li>
<li>All threads copy data from global memory to per-block shared memory asynchronous, so we better ass a <code>_syncthread()</code> to  make sure later operations only start when all data have been copied.</li>
</ol>
<h2 id="hardware-implementation"><a class="markdownIt-Anchor" href="#hardware-implementation"></a> Hardware implementation</h2>
<ol>
<li>Those synchronization within a warp and scheduling are done by hardware, and programmers don’t need to worry about these things.</li>
<li>GPU implementation maps thread blocks (“work”) to cores using a dynamic scheduling policy that respects resource requirements.</li>
<li>In GPU implementation (not a CUDA abstraction), each SM has multiple groups of warps. However, the number of warp execution contexts is far more than the number of warps.<br />
And there is a warp selector for each warp to choose the instruction to be executed by that warp. Each warp selector usually has two (or more) Fectch/Decoder unit.</li>
<li>Shared per-block memory and L1-cache are inside each SM, but L2-cache is shared by all SMs.<br />
The devide global memory (GPU memory) only communicates with blocks through L2-cache.</li>
<li>Each clock, an SM will choose several warp contexts to fill all warps to use thread-level pallelism, and a warp will choose several instructions to fill all Fetch/Decoder units to use instruction-level pallelism.</li>
<li>When running a CUDA program, GPU work schedulor will map one block to multiple warps in the same SM core. CUDA threads numbered within block in row-major order.<br />
Inside a single thread block is SPMD shared address space programming.</li>
<li>When single warp accesses consecutive memory locations, do block read or write. When single warp accesses separated memory locations, requires gather (read) or scatter(write)</li>
<li>One SM core may have multiple blocks, so a SMM core is capable of concurrently executing multiple CUDA thread blocks.<br />
When all threads in block complete, block resources (shared memory allocations, warp execution contexts) become available for next block.</li>
<li>The CUDA program is not compiled to SIMD instructions like ISPC gangs. GPU hardware is dynamically checking whether 32 independent CUDA threads share an instruction, and if this is true, it executes all 32 threads in a SIMD manner, or performance can suer due to divergent execution.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/" class="post-title-link" itemprop="url">04. Work Distribution and Scheduling</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-17 15:35:33" itemprop="dateCreated datePublished" datetime="2022-05-17T15:35:33+08:00">2022-05-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 12:57:21" itemprop="dateModified" datetime="2024-02-09T12:57:21+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="balancing-the-workload"><a class="markdownIt-Anchor" href="#balancing-the-workload"></a> Balancing the workload</h1>
<p>Always implement the simplest solution first, then measure performance to determine if you need to do better.</p>
<h2 id="static-assignment"><a class="markdownIt-Anchor" href="#static-assignment"></a> Static assignment</h2>
<ol>
<li>The assignment of work to threads is pre-determined. But not necessarily determined at compile-time, may depend on runtime parameters.</li>
<li>Benefit: simple, essentially zero runtime overhead</li>
<li>Applicable situation: When the cost (execution time) of work and the amount of work is predictable. The followings are some most common situations:<br />
When it is known up front that all work has the same cost<br />
When work is predictable, but not all jobs have same cost<br />
When statistics about execution time are known</li>
<li>Semi-static assignment: When the cost of work is predictable for near-term future, we can periodically profile itself and re-adjust assignment.<br />
Assignment is “static” for the interval between re-adjustments</li>
</ol>
<h2 id="dynamic-assignment"><a class="markdownIt-Anchor" href="#dynamic-assignment"></a> Dynamic assignment</h2>
<ol>
<li>
<p>Program determines assignment dynamically at runtime to ensure a well distributed load. Often used when the execution time of tasks, or the total number of tasks, is unpredictable.</p>
</li>
<li>
<p>The ISPC task is implemented dynamically.</p>
</li>
</ol>
<h3 id="with-one-queue"><a class="markdownIt-Anchor" href="#with-one-queue"></a> With one queue</h3>
<ol>
<li>
<p>The programmers divide the whole problem into sub-problems (or “tasks”, “work”). A queue shared by all threads is a collection of work to do. Whenever a thread finished its task, it will grab another task from the queue.</p>
</li>
<li>
<p>Fine granularity partitioning: each task is small.<br />
This is likely to have a good workload balance, but potential for high synchronization cost.</p>
</li>
<li>
<p>Coarse granularity partitioning: each task is larger.<br />
This will decrease synchronization cost and the overhead, but may have a worse workload balance.</p>
</li>
<li>
<p>Long tasks should be scheduled first. Thread performing long task performs fewer overall tasks, but approximately the same amount of work as the other threads. This requires some knowledge of workload.</p>
</li>
</ol>
<h3 id="with-a-set-of-queues"><a class="markdownIt-Anchor" href="#with-a-set-of-queues"></a> With a set of queues</h3>
<ol>
<li>
<p>When assign with one queue, all threads have to communicate with each other about the queue.</p>
</li>
<li>
<p>Each thread has their own queue, and they only execute tasks in their queue. So there is no need to communication with other threads.</p>
</li>
<li>
<p>At the beginning, the programmer push tasks into queues arbitarily (a bit like static assignment).<br />
The dynamic is that when a queue is empty, that thread can steal from other threads that is  still working.<br />
It will steal from a random thread. Every time, it will steal a proportion of the tasks in the target queue, not all of them, and usually more than one task.<br />
A thread is terminated when there is no thread for it to steal, namely when a steal is failed, it will try to steal from other threads, util it have tried all of them.</p>
</li>
<li>
<p>Stealing involves communication, but in a lower frequency then one queue method. And in this way, the local queue access is fast.</p>
</li>
<li>
<p>Sometimes it is hard to have fully independent task, but work in task queues need not be independent.<br />
A task is not removed from queue and assigned to worker thread until all task dependencies are satisfied. Workers can submit new tasks (with optional explicit dependencies) to task system</p>
</li>
</ol>
<h1 id="scheduling"><a class="markdownIt-Anchor" href="#scheduling"></a> Scheduling</h1>
<p>In a divide-and-conquer algorithm, there is both dependencies and independencies. Like in quick-sort, both the divide is dependent on the partition, and those two divide is independent.<br />
With Cilk Plus, we can express divide-and-conquer easier.</p>
<h2 id="cilk_spawn"><a class="markdownIt-Anchor" href="#cilk_spawn"></a> cilk_spawn</h2>
<ol>
<li>
<p><code>cilk_spawn</code> is labelled before a function call, so that the called function can run with the code after the call concurrently.<br />
The call labelled <code>cilk_spawn</code> is the spawned child, and the rest of the code is the continuation.</p>
</li>
<li>
<p>In divide-and-conquer, there is always a time, when the problem size is small enough that overhead of spawn trumps benefits of<br />
potential parallelization. And then we will solve those problems sequentially.</p>
</li>
<li>
<p>The main idea is to expose independent work (potential parallelism) to the system using <code>cilk_spawn</code>.</p>
</li>
<li>
<p><code>cilk_spawn</code> is a bit like <code>pthread_create</code>, and <code>cilk_sync</code> is similar to <code>pthread_join</code>. But <code>pthread</code> has some problems when too many threads are spawned.<br />
The first is the heavyweight spawn operation. And many more concurrently running threads than cores, which will cause context switching overhead and larger working set than necessary, less cache locality.</p>
</li>
<li>
<p>The Cilk Plus runtime maintains pool of worker threads. All threads created at application launch.  There are exactly as many worker threads as execution contexts in the machine.<br />
If we labelled everything <code>cilk_spawn</code>, the main thread has nothing to do.</p>
</li>
<li>
<p>Each thread in the pool will maintain a work queue to store what word needs to be done.<br />
When a thread goes idle, it will look in busy thread’s queue for work, and moves work from busy thread’s queue to its own queue.</p>
</li>
<li>
<p>If caller thread runs the continuation first, the queue should record the child for later execution, and child is made available for stealing by other threads.<br />
In this method, caller thread will spawn as many child as it could, like BFS. If no stealing, execution order is very different than<br />
that of program withcilk_spawnremoved.</p>
</li>
<li>
<p>If caller thread runs the child first, the queue should record continuation for later execution, and continuation is made available for stealing by other threads.<br />
In this method, caller thread will only create one item to steal.<br />
If no stealing occurs, thread continually pops continuation from work queue, enqueues new continuation (like DFS). The order of execution is the same as for program with spawn removed.<br />
If continuation is stolen, stealing thread spawns next child.</p>
</li>
<li>
<p>If the continuation is run first, there will be more items to steal, and thus makes a better advantage of multi-thread.<br />
But if the continuation is run first, the work queue storage for system with T threads is no more than T times that of stack storage for single threaded execution, and thus saves more space.</p>
</li>
<li>
<p>Work queue is implemented as a dequeue (double ended queue).<br />
Local thread pushes/pops from the “tail” (bottom), while remote threads steal from “head” (top).<br />
Reduces contention with local thread: local thread is not accessing same part of dequeue that stealing threads do.<br />
Do larger work first: in divide-and-conquer, the top of the queue is usually at the beginning of call tree, and is a larger piece of work.<br />
Maximizes locality: in conjunction with run-child-first policy, local thread works on local part of call tree</p>
</li>
</ol>
<h2 id="cilk_sync"><a class="markdownIt-Anchor" href="#cilk_sync"></a> cilk_sync</h2>
<ol>
<li>
<p><code>cilk_sync</code> is used after those <code>cilk_spawn</code> call code. It will return when all calls spawned by current function have completed.</p>
</li>
<li>
<p>There is an implicitcilk_syncat the end of every function that contains a cilk_spawn, so when a Cilk function returns, all work associated with that function is complete.</p>
</li>
<li>
<p>If no work has been stolen by other threads, then there’s nothing to do at the sync point, <code>cilk_sync</code> is a no-op. But this is not a common situation.</p>
</li>
<li>
<p>One way to implement sync is with stalling joint.<br />
Thread that initiates the fork must perform the sync. Therefore it waits for all spawned work to be complete.<br />
descriptor for block A created<br />
When a stealing from the initial thread happens, a descriptor for that stolen work is created to track the number of outstanding spawns for the block, and the number of those spawns that have completed.<br />
When all the child spowned for the block is done, this block is considered done, and the descriptor is free. When all the blocks are done, sync is fullfilled.</p>
</li>
<li>
<p>Another way to implement sync is the greedy policy.<br />
When thread that initiates the fork goes idle, it can look to steal new work.  Last thread to reach the join point continues execution after sync. This will also create a decriptor for those stolen work.<br />
Worker thread that initiated spawn may not be thread that executes logic after <code>cilk_sync</code>.</p>
</li>
<li>
<p>In greedy policy, All threads always attempt to steal if there is nothing to do, and thread only goes idle if no work to steal is present in system. But in stalling policy, the initial thread doesn’t steal, and only waits when its work is done.</p>
</li>
<li>
<p>Overhead of bookkeeping steals and managing sync points only occurs when steals occur. If large pieces of work are stolen, this should occur infrequently. Most of the time, threads are pushing/popping local work from their local dequeue.</p>
</li>
<li>
<p>Cilk uses greedy join scheduling.</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/about/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/about/">1</a><span class="space">&hellip;</span><a class="page-number" href="/about/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/about/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/about/page/7/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
