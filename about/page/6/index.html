<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="LiyunZhang">
<meta property="og:url" content="http://liyun-zhang.github.io/about/page/6/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:locale">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/about/page/6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-Hans","comments":"","permalink":"","path":"about/page/6/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">LiyunZhang</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/13/Courses/CS149/12-Interconnection-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/13/Courses/CS149/12-Interconnection-Network/" class="post-title-link" itemprop="url">12. Interconnection Network</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-13 10:50:56" itemprop="dateCreated datePublished" datetime="2022-07-13T10:50:56+08:00">2022-07-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 21:45:15" itemprop="dateModified" datetime="2024-03-16T21:45:15+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Interconnection"><a href="#Interconnection" class="headerlink" title="Interconnection"></a>Interconnection</h1><ol>
<li>All parallel processors are connected and form an interconnection network. </li>
<li>The interconnection network connects processor cores with other cores, processors and memories, processor cores, and caches, caches, caches, and I/O devices. </li>
<li>The design of the interconnection network has an important impact on system scalability (How large of a system can be built? How easy is it to add more nodes?), system performance, and energy efficiency (How fast can cores, caches, and memory communicate? How long is latency to memory? How much energy is spent on communication?)</li>
</ol>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><ol>
<li><strong>Network node</strong>: a network endpoint connected to a router/switch, like the processor, the cache controller, or the memory controller</li>
<li><strong>Network interface</strong>: Connects nodes to the network</li>
<li><strong>Switch/router</strong>: Connects a fixed number of input links to a fixed number of output links</li>
<li><strong>Link</strong>: A bundle of wires carrying a signal<br><img src="/imgs/CS149/12/1.png" width="40%"></li>
</ol>
<h2 id="Design-issues"><a href="#Design-issues" class="headerlink" title="Design issues"></a>Design issues</h2><ol>
<li><strong>Topology</strong>: How switches are connected via links. Affects routing, throughput, latency, and complexity/cost of implementation. </li>
<li><strong>Routing</strong>: How a message gets from its source to its destination in the network. Can be static (messages take a predetermined path) or adaptive based on load. </li>
<li><strong>Buffering and flow control</strong>: What data are stored in the network? Packets, partial packets? Etc. How does the network manage buffer space?</li>
</ol>
<h2 id="Properties-of-interconnect-topology"><a href="#Properties-of-interconnect-topology" class="headerlink" title="Properties of interconnect topology"></a>Properties of interconnect topology</h2><ol>
<li><strong>Routing distance</strong>: Number of links (“hops”) along a route between two nodes</li>
<li><strong>Diameter</strong>: the maximum routing distance</li>
<li><strong>Average distance</strong>: average routing distance over all valid routes</li>
<li><strong>Direct network</strong>: The switches and nodes are one in the same. The logic of the switch is built into the node itself. </li>
<li><strong>Indirect network</strong>: The switches are distinct from the nodes, forming a chain from one node to another. </li>
<li><strong>Blocking or non-blocking</strong>: The network is non-blocking if connecting any pairing of nodes simultaneously won’t cause conflict (using the same link or switch, assuming that one switch can only handle one message simultaneously). Otherwise, it is blocking. </li>
<li><strong>Bisection bandwidth</strong>: A measure of how much connectivity is in the network. If we cut the network in half, it is the connection between those two halves, namely the sum bandwidth of all severed links.<br>The low bisection bandwidth will be the choke point of the network. </li>
<li>Latency increases with load (throughput).<br>The topology, routing algorithm, and flow control have their minimum latency. The zero load or idle latency is the sum of the three min latencies.<br>Also, the topology, routing algorithm, and flow control have their throughput limit. The overall throughput limit is the min of the three limits. </li>
</ol>
<h1 id="Interconnect-topologies"><a href="#Interconnect-topologies" class="headerlink" title="Interconnect topologies"></a>Interconnect topologies</h1><h2 id="Bus"><a href="#Bus" class="headerlink" title="Bus"></a>Bus</h2><ol>
<li><p>Physically, a bus is a wire. But from a graph point of view, a bus is a switch. </p>
</li>
<li><p>It is simple to design. It is cost-effective for a small number of nodes. It is easy to implement coherence via snooping.</p>
</li>
<li><p>Contention: all nodes contend for shared bus</p>
</li>
<li><p>Limited bandwidth: all nodes communicate over the same wires, and only one communication is allowed simultaneously. </p>
</li>
<li><p>There is a scalability problem. It is expensive to drive wires across the whole chips. There is quite a lot of power in driving signals on the bus. </p>
<p><img src="/imgs/CS149/12/2.png" width="40%"></p>
</li>
</ol>
<h2 id="Crossbar"><a href="#Crossbar" class="headerlink" title="Crossbar"></a>Crossbar</h2><ol>
<li><p>Every node is connected to every other node by a direct connection. The network is non-blocking and indirect (the network is indirect, but the nodes are connected directly). </p>
</li>
<li><p>It has $O(1)$ latency and high bandwidth. </p>
</li>
<li><p>It also has a scalability problem since $O(N^2)$ switches are required. Also, it is expensive and difficult to arbitrate at scale. </p>
</li>
<li><p>Crossbars have been used in Oracle’s recent multi-core processing. The crossbar (CCX) occupies about the same chip area as a core in a multi-core chip. </p>
<p><img src="/imgs/CS149/12/3.png" width="40%"></p>
</li>
</ol>
<h2 id="Ring"><a href="#Ring" class="headerlink" title="Ring"></a>Ring</h2><ol>
<li><p>It lets the message to circle. </p>
</li>
<li><p>It is simple enough and relatively cheap, with only $O(N)$ cost. </p>
</li>
<li><p>But the latency is $O(N)$, which is very high. The bisection bandwidth remains constant as nodes are added, which will cause the scalability problem. </p>
</li>
<li><p>It is used in recent Intel architectures, Core i7, and IBM CELL Broadband Engine. This is usually used on the ring scale of 4 or 8 elements. </p>
</li>
<li><p>Intel’s ring interconnect has four rings (request, snoop, ack, 32 bytes of data) and six interconnect nodes (four “slices” of L3 cache, system agent, and graphics). Each bank of L3 connected to the ring bus twice. </p>
<p><img src="/imgs/CS149/12/4.png" width="40%"></p>
</li>
</ol>
<h2 id="Mesh"><a href="#Mesh" class="headerlink" title="Mesh"></a>Mesh</h2><ol>
<li><p>This is a direct network. It echoes locality in grid-based applications. </p>
</li>
<li><p>The cost is $O(N)$, and the average latency is $O(\sqrt{N})$. </p>
</li>
<li><p>It is easy to lay out on a chip since all links have a fixed length. </p>
</li>
<li><p>Path diversity: many ways for the message to travel from one node to another</p>
<p><img src="/imgs/CS149/12/5.png" width="40%"></p>
</li>
</ol>
<h2 id="Torus"><a href="#Torus" class="headerlink" title="Torus"></a>Torus</h2><ol>
<li><p>Characteristics of nodes in mesh topology are different based on whether the node is near the edge or middle of the network. Torus topology introduces new links to avoid this problem. </p>
</li>
<li><p>In torus topology, each row or column forms a ring by adding a new link to connect the two nodes on the edge. </p>
</li>
<li><p>Its cost is still $O(N)$, but higher than 2D mesh. It also has higher path diversity and bisection bandwidth than mesh. </p>
</li>
<li><p>However, it has higher complexity and is difficult to layout on a chip because of its unequal link lengths. </p>
</li>
<li><p>Folded torus interleaving rows and columns to eliminate the need for long connections. All connections are doubled in length. </p>
<p><img src="/imgs/CS149/12/6.png" width="40%"></p>
</li>
</ol>
<h2 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h2><ol>
<li><p>This is a planar, hierarchical topology. Like mesh/torus, it performs well when traffic has a locality. </p>
</li>
<li><p>The latency is $O(logN)$</p>
</li>
<li><p>If the signal needs to go upward in tree routing, there is only one option. If the signal needs to go downward, we can use $0$ to mark go left while $1$ to mark go right.<br>Every time, the signal will go upward first until having a common ancestor; then, it will go downward.<br>If the tree is complete and we assign numbers to the nodes according to their inorder traversal starting from zero, then the way to each node from the root node is the binary code of the number. </p>
</li>
<li><p>A fat tree increases bandwidth between nodes as it moves upward. It can alleviate root bandwidth problems with higher bandwidth links near the root. </p>
</li>
<li><p>The number of wires is the same as the height of the subtree. So, the bisection bandwidth of the fat tree is $O(N)$. </p>
</li>
<li><p>The fat tree routing is similar to tree routing but randomly chooses when multiple links are possible. </p>
</li>
<li><p>Constant-width fat tree (folded clos network): All nodes have fixed degrees, which makes the hardware design simpler. This is used in Infiniband networks. </p>
<p><img src="/imgs/CS149/12/7.png" width="30%"><img src="/imgs/CS149/12/8.png" width="30%"><img src="/imgs/CS149/12/9.png" width="30%"></p>
</li>
</ol>
<h2 id="Hypercube"><a href="#Hypercube" class="headerlink" title="Hypercube"></a>Hypercube</h2><ol>
<li><p>It has a low latency of $O(logN)$. Its number of links is $O(NlogN)$. </p>
</li>
<li><p>6D hypercube used in 64-core Cosmic Cube computer developed at Caltech in the 80s. </p>
</li>
<li><p>If the address of two nodes only differs by one bit, then a link connects them.<br>So, if we want to go from address A to address B, we do it one bit at a time. </p>
</li>
<li><p>The problem is that we cannot pack more dimensions into 3D that exists. We need to put enough wires in to make it work to implement a higher-dimension cube. But as we scale up more and more, the wires become so much that they don’t layout well. </p>
<p><img src="/imgs/CS149/12/10.png" width="40%"></p>
</li>
</ol>
<h2 id="Multi-stage-logarithmic"><a href="#Multi-stage-logarithmic" class="headerlink" title="Multi-stage logarithmic"></a>Multi-stage logarithmic</h2><ol>
<li><p>It is an indirect network with multiple switches between terminals. </p>
</li>
<li><p>The cost is $O(NlogN)$ while the latency is $O(logN)$. </p>
</li>
<li><p>It has many variations: Omega, butterfly, Clos networks, etc. </p>
</li>
<li><p>In the topology shown below, each switch has two output wires. In the routing, if 0 stands for up and 1 for down, we can also route according to the binary code of the number of targeting nodes.<br>Here, up means we will take the upper wire, and down means we will take the lower wire. They don’t always mean going up or going down. </p>
<p><img src="/imgs/CS149/12/11.png" width="40%"></p>
</li>
</ol>
<h1 id="Buffering-and-flow-control"><a href="#Buffering-and-flow-control" class="headerlink" title="Buffering and flow control"></a>Buffering and flow control</h1><ol>
<li>Circuit switching sets up a full path (acquires all resources) between sender and receiver before sending a message. It has higher bandwidth transmission.<br>It has no per-packet link management overhead but does incur overhead to set up/tear down the path. Reserving links can result in low utilization. </li>
<li>Packet switching makes routing decisions per packet. It can use a link for a packet whenever a link is idle.<br>It has overhead due to dynamic switching logic during transmission but no setup/tear-down overhead. </li>
<li>The communication granularity from larger to miner is message, packet, and flit.<br>A message is the transfer unit between network clients and can be transmitted using many packets.<br>The packet is the unit of transfer for the network and can be transmitted using multiple flits.<br>Flit (flow control digit) is a network flow control unit. Packets broken into flits. </li>
<li>A packet consists of a header, payload/body, and tail.<br>The header contains routing and control information; at the start of the packet, the router can start forwarding early.<br>The payload/body contains the data to be sent.<br>The tail contains control information, like an error code, and is generally located at the end of the packet so it can be generated “on the way out.” </li>
<li>When two packets need to be routed onto the same outbound link simultaneously, contention occurs. There are three options: buffer one packet and send it over the link later, drop one packet, or reroute one packet (deflection). </li>
</ol>
<h2 id="Circuit-switched-routing"><a href="#Circuit-switched-routing" class="headerlink" title="Circuit-switched routing"></a>Circuit-switched routing</h2><ol>
<li>Main idea: pre-allocate all resources (links across multiple switches) along the entire network path for a message (“setup a flow”)</li>
<li>Costs:<br>Needs a setup phase (“probe”) to set up the path (and to tear it down and release the resources when the message is complete)<br>Lower link utilization. Transmission of two messages cannot share the same link (even if some resources on a preallocated path are no longer utilized during a transmission)</li>
<li>Benefits:<br>No contention during transmission due to preallocation, so no need for buffering<br>Arbitrary message sizes (once the path is set up, send data until done)</li>
</ol>
<h2 id="Packet-based-flow-control"><a href="#Packet-based-flow-control" class="headerlink" title="Packet-based flow control"></a>Packet-based flow control</h2><ol>
<li>Store-and-forward: Packet copied entirely into network switch before moving to next node, which requires buffering for the entire packet in each router</li>
<li>The flow control unit is an entire packet. Different packets from the same message can take different routes, but all data in a packet is transmitted over the same route.</li>
<li>Store-and-forward has high per-packet latency. Its latency $=$ packet transmission time on the link $\times$ network distance)</li>
<li>Cut-through flow control: The switch starts forwarding data to the next link as soon as the packet header is received since the header determines how much link bandwidth the packet requires and where to route. </li>
<li>Cut-through flow control reduces transmission latency and reduces store-and-forward under high contention. </li>
<li>The latency of cut-through flow control is header transmission time. $\times$ network distance $+$ rest packet transmission time on one link $\simeq$ network distance $+$ number of packets. </li>
<li>The difference between store-and-forward and cut-through is whether we parallel the header transmission and the rest of the packet. </li>
<li>In cut-through flow control, if the output link is blocked (cannot transmit head), the transmission of the tail can continue. So, the switch needs to store more data before transmitting and deleting them, which requires switches to have buffering for the entire packet, just like store-and-forward.<br>The worst case is that the entire message is absorbed into a buffer in a switch, namely cut-through flow control degenerates to store-and-forward in this case. </li>
</ol>
<h2 id="Wormhole-flow-control"><a href="#Wormhole-flow-control" class="headerlink" title="Wormhole flow control"></a>Wormhole flow control</h2><ol>
<li>Routing information is only in the head flit; body flits follow the head, and tail flit flows through the body. All flits move to their next switch simultaneously. If the head flit blocks, the rest of the packet stops. </li>
<li>Its latency $=$ header transmission time $\times$</li>
<li>Head-of-line blocking problem: The route of the head flit of one packet is free but blocked behind the flit of another packet in the buffer while that packet is blocked, waiting for a busy link. </li>
<li>Virtual channel flow control: Multiplex multiple operations over a single physical channel and divide the switch’s input buffer into multiple buffers sharing a single physical channel. </li>
<li>Virtual channel reduces head-of-line blocking.<br>It can break the cyclic dependency of resources by ensuring requests and responses use different virtual channels to avoid deadlock.<br>Also, it provided quality-of-service guarantees. Some virtual channels have higher priority than others.</li>
<li>“Escape” virtual channels: retain at least one virtual channel that uses deadlock-free routing.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/10/Courses/CS149/11-Memory-Consistency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/10/Courses/CS149/11-Memory-Consistency/" class="post-title-link" itemprop="url">11. Memory Consistency</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-10 14:07:29" itemprop="dateCreated datePublished" datetime="2022-07-10T14:07:29+08:00">2022-07-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 21:22:02" itemprop="dateModified" datetime="2024-03-16T21:22:02+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><ol>
<li>In a correctly behaved parallel memory hierarchy, reading a location should return the latest value written by any thread.<br>Side-effects of writes are only observable when reads occur so that we will focus on the values returned by reads. </li>
<li>Within a thread, “latest” can be defined by program order. But when it comes across threads, we don’t want it to be physical time because there is no way that the hardware can pull that off. If it takes &gt;10 cycles to communicate between processors, there is no way that processor 0 can know what processor 1 did 2 clock ticks ago. </li>
<li>Writes from any particular thread must be consistent with program order. Writes across threads must be consistent with valid interleaving of threads.<br>We define the memory model as one in which each thread proceeds in program order, and memory accesses interleaved (one at a time) to a single-ported memory while the rate of progress of each thread is unpredictable.<br>“Latest” means consistent with some interleaving that matches this model.</li>
</ol>
<h2 id="Hide-memory-latency"><a href="#Hide-memory-latency" class="headerlink" title="Hide memory latency"></a>Hide memory latency</h2><ol>
<li>Idea: overlap memory accesses with other accesses and computation</li>
<li>“Out of order” pipelining: When an instruction is stuck, perhaps subsequent instructions can be executed. </li>
<li>We don’t need to wait for a conditional branch to be resolved before proceeding. Just predict the branch outcome and continue executing speculatively. If the prediction is wrong, squash any side effects and restart down the correct path. </li>
<li>Modern processors fetch and graduate instructions in order but issue out-of-order. So, intra-thread dependencies are preserved, but memory accesses get reordered. </li>
<li>Hiding write latency is simple in uniprocessors, adding a write buffer. But this affects correctness in multiprocessors.<br>In a multiprocessor, a write buffer or write-back cache might cause later writes to write earlier to memory, and accesses issued in order may be observed out of order by other processors. </li>
</ol>
<h1 id="Sequential-consistency-SC-model"><a href="#Sequential-consistency-SC-model" class="headerlink" title="Sequential consistency (SC) model"></a>Sequential consistency (SC) model</h1><ol>
<li>Each processor’s access is in program order, and all accesses appear in sequential order. Any order implicitly assumed by the programmer is maintained. Any order implicitly assumed by the programmer is maintained. </li>
<li>How to implement sequential consistency:<br>Implement cache coherence: writes to the same location are observed in the same order by all processors<br>For each processor, delay the start of memory access until the previous one is complete. Namely, each processor has only one outstanding memory access at a time.</li>
<li>A read completes when its return value is bound.<br>A write completes when the new value is “visible” to other processors. “Visible” does not mean that other processors have necessarily seen the value yet. The new value is committed to the hypothetical serializable order (HSO). </li>
<li>The strict requirements of the SC model severely restrict common hardware and compiler optimizations.<br>Processor issues access one at a time and stalls for completion, which results in Low processor utilization even with caching. </li>
<li>Total store ordering (TSO) model: Compared to the SC model, a read operation doesn’t need to stall to wait for an earlier write operation. This is similar to the architecture with a FIFO write buffer. </li>
<li>Partial store ordering (PSO) model: Compared to the TSO model, even a write operation doesn’t need to stall to wait for an earlier write operation. This architecture has a write buffer that doesn’t have to be FIFO. </li>
</ol>
<h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><ol>
<li>Most programs don’t require strict ordering (all of the time) for correctness. Here, correctness means the same results as sequential consistency. </li>
<li>Two accesses conflict if they access the same location; at least one is a write. </li>
<li>We can order accesses by program order (PO) and dependence order (DO). Operation2 is dependent on operation1 if operation2 reads operation1. </li>
<li>Data Race is two conflicting accesses on different processors, not ordered by intervening accesses. </li>
<li>Properly synchronized programs are where all synchronizations are explicitly identified, and all data accesses are ordered through synchronization. </li>
<li>Many parallel programs have mixtures of “private” and “public” parts.  The “private” parts must be protected by synchronization,  like locks and unlocks.<br>Between synchronization operations, we can allow memory operations to be reordered as long as intra-thread dependencies are preserved.<br>Just before and just after synchronization operations, the thread must wait for all prior operations to complete</li>
<li>MFENCE does not begin until all prior reads &amp; writes from that thread have completed, and no subsequent read or write from that thread can start until after it finishes. Xchg does this implicitly.<br>MFENCE operation does not push values out to other threads. It simply stalls the thread that performs the MFENCE until the write buffer is empty.<br>MFENCE operations create partial orderings that are observable across threads. </li>
<li>In the weak ordering model, we put MFENCEs before the lock operation and after the unlock operation. </li>
<li>Lock operation: only gains (“acquires”) permission to access data. Unlock operation: only gives away (“releases”) permission to access data.<br>The Release Consistency (RC) model ensures that writes before the lock or in the critical section are completed before the exit critical section and that reads/writes in the critical section or after the exit critical section doesn’t access the shared state until the lock is acquired. </li>
<li>LFENCE serializes only with respect to load operations, and SFENCE serializes only with respect to store operations. In practice, MFENCE and xchg are the most likely used ones. </li>
<li>Like Peterson’s algorithm, don’t use only normal memory operations for synchronization. Do use either explicit synchronization operations, like xchg (atomic), or fences. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/07/Courses/CS149/10-Snooping-Implementation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/07/Courses/CS149/10-Snooping-Implementation/" class="post-title-link" itemprop="url">10. Snooping Implementation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-07 10:24:14" itemprop="dateCreated datePublished" datetime="2022-07-07T10:24:14+08:00">2022-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 21:07:27" itemprop="dateModified" datetime="2024-03-16T21:07:27+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Building-with-an-atomic-bus"><a href="#Building-with-an-atomic-bus" class="headerlink" title="Building with an atomic bus"></a>Building with an atomic bus</h1><h2 id="Transaction"><a href="#Transaction" class="headerlink" title="Transaction"></a>Transaction</h2><ol>
<li>There is a bus controller to do arbitration. If a processor wants to communicate on the bus, it has to make a request. If there are simultaneous requests from multiple processors, the arbiter will only grant one of them. </li>
<li>A transaction on an atomic bus generally needs four steps.<br>The client is granted bus access (the result of arbitration). The client places command on the bus (may also place data on the bus).<br>Response to command by another bus client placed on the bus. Next, the client obtains bus access (arbitration)</li>
<li>In a multi-processor with an atomic bus scenario, no other bus transactions are allowed between issuing addresses and receiving data when one processor wants to read. Also, when flush occurs, address and data are sent simultaneously and received by memory before any other transaction is allowed. </li>
<li>Both requests from the processor and bus require to look the tag on the cache.<br>If the bus receives priority during the bus transaction, the processor is locked out of its cache.<br>If the processor receives priority during processor cache accesses, the cache cannot respond with its snoop result. So, it delays other processors even if no sharing of any form is present. </li>
<li>We can alleviate contention to allow simultaneous access by processor-side and snoop controllers through cache duplicate tags or multi-ported tag memory. In either case, the additional performance cost is additional hardware resources.<br>Tags must stay in sync for correctness, so tag updates by one controller will still need to block the other controller, but modifying tags is infrequent compared to checking them. </li>
</ol>
<h2 id="Read-miss"><a href="#Read-miss" class="headerlink" title="Read miss"></a>Read miss</h2><ol>
<li>Memory needs to know what to do when a cache read miss occurs. If the line is dirty, memory should not respond. And the loading cache needs to know what to do. If the line is shared, the cache should load into the S state, not E. </li>
<li>If one cache controller finds that the line is shared in its cache, it will send a message through the “shared” wire on the bus. If that line is dirty, the controller will send a message through the “dirty” wire on the bus.<br>Every time a processor responds to a snoop, the value in the “snoop-pending” wire will be lower and $0$ value indicates that all processors have reacted. </li>
<li>The memory controller could immediately start accessing DRAM but not respond (squelch response). If a snoop result from another cache indicates it has a copy of the most recent data, then the cache should provide data, not memory. The memory could assume one of the caches will service request until the snoop results are valid. If snoop indicates no cache has data, then memory must respond.</li>
</ol>
<h2 id="Write-back"><a href="#Write-back" class="headerlink" title="Write back"></a>Write back</h2><ol>
<li>Write-backs involve two bus transactions: incoming line (line requested by the processor) and outgoing line (evicted dirty line in the cache that must be flushed).<br>Ideally, we would like the processor to continue as soon as possible; it shouldn’t have to wait for the flush to complete. </li>
<li>The solution is a write-back buffer.<br>The stick line is to be flushed in a write-back buffer. Immediately load the requested line to allow the processor to continue. Flush contents of the write-back buffer at a later time. </li>
<li>If a request from another processor for the data address in the write-back buffer appears on the bus, the snoop controller must check the write-back buffer addresses and cache tags.<br>If there is a write-back buffer match, the controller will respond with data from the write-back buffer rather than cache and cancel the outstanding bus access requests. </li>
<li>A write commits when a read-exclusive transaction appears on the bus and is acknowledged by all other caches. All future reads will reflect the value of this write, even if data from P has not yet been written to P’s dirty cache line or memory.<br>The order of transactions on the bus defines the global order of writes in the parallel program. </li>
<li>“Commit” is not “complete”. A write completes when the updated value is in the cache line.</li>
</ol>
<h2 id="Race-conditions"><a href="#Race-conditions" class="headerlink" title="Race conditions"></a>Race conditions</h2><ol>
<li>Coherence protocol state transition diagrams assumed that transitions between states were atomic. However, in practice, state transitions are not atomic. </li>
<li>We’ve assumed the bus transaction itself is atomic, but all the operations the system performs as a result of a memory operation are not. </li>
<li>The processor, cache, and bus are all resources operating in parallel. They often contend for shared resources: processor and bus contend for cache, while caches contend for bus access. </li>
<li>The cache must be able to handle requests while waiting to acquire the bus AND be able to modify its outstanding requests. </li>
<li>To avoid deadlock, the processor must be able to service incoming transactions while waiting to issue requests. </li>
<li>To avoid livelock, a write that obtains exclusive ownership must be allowed to complete before exclusive ownership is relinquished. </li>
<li>Multiple processors competing for bus access must be careful to avoid (or minimize the likelihood of) starvation. </li>
<li>Performance optimization often entails splitting operations into several smaller transactions. Splitting costs in more hardware is needed to exploit additional parallelism, and care is needed to ensure abstractions still hold. </li>
</ol>
<h1 id="Building-with-non-atomic-bus"><a href="#Building-with-non-atomic-bus" class="headerlink" title="Building with non-atomic bus"></a>Building with non-atomic bus</h1><ol>
<li>Problem with atomic bus: the bus is idle while the response is pending, which decreases effective bus bandwidth. The interconnect is a limited, shared resource in a multi-processor system. So it is important to use it as efficiently as possible. </li>
<li>Bus transactions are split into two transactions: the request and the response. Other transactions can intervene between a transaction’s request and response. </li>
<li>Basic design:<br>Up to eight outstanding requests at a time (system-wide)<br>Responses need not occur in the same order as requests. However, the request order establishes the total order for the system.<br>Flow control via negative acknowledgments (NACKs). The client can NACK a transaction when a buffer is full, causing a retry.</li>
<li>We can think of a split-transaction bus as two separate buses, a request bus, and a response bus.<br>The request bus has lines for command and address. The response bus has lines for data and response tags. Response tag has 3 bits to represent 8 requests. </li>
</ol>
<h2 id="Read-miss-1"><a href="#Read-miss-1" class="headerlink" title="Read miss"></a>Read miss</h2><h3 id="Phase-1"><a href="#Phase-1" class="headerlink" title="Phase 1"></a>Phase 1</h3><ol>
<li>Request arbitration: cache controllers present a request for address to bus (many caches may be doing so in the same cycle)</li>
<li>Request resolution: address bus arbiter grants access to one of the requestors. Request table entry allocated for the request. Special arbitration lines indicate the tag assigned to the request. </li>
<li>The bus “winner” places the command/address on the bus. </li>
<li>Caches perform snoop: look up tags, update cache state, etc. Memory operation commits here. (no bus traffic)</li>
<li>Caches acknowledge this snoop result is ready or signal they could not complete it in time here. </li>
</ol>
<h3 id="Phase-2"><a href="#Phase-2" class="headerlink" title="Phase 2"></a>Phase 2</h3><ol>
<li>Data response arbitration: responder presents intent to respond to request with tag T. (many caches or memory may be doing so in the same cycle)</li>
<li>Data bus arbiter grants one responder bus access. </li>
<li>The original requestor signals readiness to receive a response (or lack thereof: the requestor may be busy at this time)</li>
<li>Then, in phase 3, the Responder places response data on the data bus. Caches present snoop results for requests with the data. The request table entry is freed. Those 3 actions can happen in parallel. </li>
</ol>
<h2 id="Pipelined-transactions"><a href="#Pipelined-transactions" class="headerlink" title="Pipelined transactions"></a>Pipelined transactions</h2><ol>
<li>The request bus and response bus can run parallel. So, the response to the last transaction and the request for the next transaction can happen simultaneously. Pipelining may cause out-of-order completion. </li>
<li>Write-backs and BusUpg transactions do not have a response component. Write-backs acquire access to the request address and data bus as part of the “request” phase. BusUpg does not need any acknowledgment or data. </li>
<li>Avoid conflicting requests by disallowing them. Each cache has a copy of the request table. Caches do not make requests that conflict with requests in the request table. </li>
<li>Caches/memory have buffers for receiving data off the bus. If the buffer fills, the client NACKs relevant requests or responses. </li>
<li>In a parallel system, we use queues to accommodate variable (unpredictable) production and consumption rates. As long as workers, on average, produce and consume at the same rate, all workers can run at full rate. Otherwise, some will stall, waiting for others to accept or produce new input. </li>
<li>We have queues to track requests and responses between the L1 and L2 caches and between the L2 cache and bus. One queue is for requests and responses from closer (to processor) to farther (L1 to L2 or L2 to bus), and the other is from farther to closer (L2 to L1 or bus to L2). </li>
<li>This may cause deadlock due to a full queue. Outgoing read requests (initiated by the processor) and incoming read requests (due to another cache) both requests generate responses that require space in the other queue (circular dependency)</li>
<li>Sizing all buffers to accommodate the maximum number of outstanding requests on the bus is one solution to avoiding deadlock. But a costly one. </li>
<li>Avoid buffer deadlock with separate request/response queues. Namely, we distinguish whether it is a request or a response.<br>Responses can be completed without generating further transactions. Requests increase queue length, But responses reduce queue length. While attempting to send a stalled request, the cache must be able to service responses. Responses will make progress, eventually freeing up resources for requests. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/07/06/Courses/CS149/09-Directory-Based-Cache-Cohurence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/06/Courses/CS149/09-Directory-Based-Cache-Cohurence/" class="post-title-link" itemprop="url">09. Directory-Based Cache Coherence</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-07-06 17:10:17" itemprop="dateCreated datePublished" datetime="2022-07-06T17:10:17+08:00">2022-07-06</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 20:49:25" itemprop="dateModified" datetime="2024-03-16T20:49:25+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Problems-to-solve"><a href="#Problems-to-solve" class="headerlink" title="Problems to solve"></a>Problems to solve</h1><ol>
<li>The snooping cache coherence protocols relied on broadcasting coherence information to all processors over the chip interconnect. Whenever a cache miss occurs, the triggering cache communicates with all other caches, so the interconnect has heavy traffic. </li>
<li>The efficiency of the NUMA system does little good if the coherence protocol can’t also be scaled. The processor accesses nearby memory, but it must still broadcast to all other processors to ensure coherence. </li>
<li>One possible solution is hierarchical snooping, which arranges nodes in a tree and uses snooping coherence at each level. The interconnects involved in communication are as low as possible and kept as local as possible. </li>
<li>The structure of hierarchical snooping is relatively simple to build.<br>It uses a tree to reduce the conjunction at the center part, but if the workload is not nicely partitioned, then the root of the network can become a bottleneck.<br>It also has larger latencies than direct communication and does not apply to more general network topologies (meshes, cubes)</li>
</ol>
<h1 id="Directory"><a href="#Directory" class="headerlink" title="Directory"></a>Directory</h1><ol>
<li>Snooping schemes broadcast coherence messages to determine the state of a line in the other caches. The alternative idea is to avoid broadcasting by storing information about the line’s status in one place, namely a “directory.” </li>
<li>A line is a region of memory that would be cached as a single block. One directory entry corresponds to one line of memory.<br>In a directory entry, there is a dirty bit that indicates the line is dirty in one of the processors’ caches and P presence bits that indicate whether processor P has a line in its cache. </li>
<li><p>The NUMA system uses the directory; each processor has a “local” memory and a directory.<br>The home node of a line is the node with memory holding the corresponding data for the line.<br>The requesting node is the node containing the processor requesting line.</p>
<h2 id="Read-and-write"><a href="#Read-and-write" class="headerlink" title="Read and write"></a>Read and write</h2></li>
<li><p>When a read miss happens:<br>The requesting node will send a read miss message to the home node of the requested line. Then, the home directory checks the entry for the line.<br>If the dirty bit for the cache line is OFF, the home node will respond with contents from memory and set the presence bit of the requesting node to true to indicate that the line is cached by the requesting processor.<br>If the dirty bit for the cache line is ON, the home node will respond with a message providing the identity of the line owner. The requesting node requests data from the owner, and the owner changes the state in the cache to SHARED (read-only) and responds to the requesting node. The owner also responds to the home node, which will clear the dirty bit, update presence bits (both the requesting node and owner cache line), and update memory. </p>
</li>
<li>When a write miss happens:<br>The requesting node will send a write miss message to the home node of the requested line.<br>The home node will respond to the sharer IDs and data to the requesting node.<br>The requesting node will send an invalidation signal to all sharers. After receiving invalidation acks from all sharers, the requesting node can perform write.<br>The home node will update the presence bits (the line is cached by only the requesting node) and dirty bits. </li>
<li>On reads, the directory tells the requesting node exactly where to get the line from, either from the node (if the line is clean) or the owning node (if the line is dirty). Either way, retrieving data involves only point-to-point communication. </li>
<li>On writes, the advantage of directories depends on the number of sharers. In the limit, if all caches are sharing data, all caches must be communicated with, just like broadcast in a snooping protocol. </li>
<li>In general, only a few processors share the line; only a few processors must be told of writes.  The expected number of sharers typically increases slowly with P. </li>
</ol>
<h2 id="Reduce-storage-overhead"><a href="#Reduce-storage-overhead" class="headerlink" title="Reduce storage overhead"></a>Reduce storage overhead</h2><ol>
<li>Full bit vector directory storage is proportional to $P\times M$ where $P$ is the number of nodes and $M$ is the number of lines in memory. The storage overhead of the directory is too much, and we do not want it to be DRAM since we need it to run fast.</li>
<li>One way to reduce storage overhead is to optimize the full-bit vector.<br> Increase cache line size to reduce $M$ term.<br> Group multiple processors into a single directory node to reduce $P$ term. We could use a snooping protocol to maintain coherence among processors in a node or directory across nodes. </li>
<li>Another way is to limit the sharer pointer. Since data is expected to only be in a few caches simultaneously, storage for a limited number of pointers per directory entry should be sufficient. Only need a list of the nodes holding a valid copy of the line. </li>
<li>When an overflow in limited pointer schemes occurs, we can revert to broadcast if the broadcast mechanism exists. If no broadcast mechanism is present on the machine, the newest sharer replaces an existing one (must invalidate line in the old sharer’s cache)</li>
<li>One more way is through the sparse directory.<br> The majority of memory is not resident in the cache. To carry out the coherence protocol, the system only needs to share information for lines currently in some cache. So, most directory entries are empty most of the time.<br> We can add a tag for each directory line to indicate the memory in some cache. The overhead is now $P\times C$ where $C$ is the number of lines in each cache. </li>
</ol>
<h2 id="Reduce-the-number-of-messages-sent"><a href="#Reduce-the-number-of-messages-sent" class="headerlink" title="Reduce the number of messages sent"></a>Reduce the number of messages sent</h2><ol>
<li>In a read miss to the dirty line, there are five network transactions in total. However, only four of the transactions are on the critical path. </li>
<li>In intervention forward, the home node requests data from the owner node (intervention read). After the owner has responded, the home node updates the directory and responds to the requesting node with data.<br>Four network transactions in total (less traffic). But all four of the transactions are on the critical path. </li>
<li>In request forwarding, the home node requests the owner to send data to the requesting node. Then, the owner will simultaneously send data to the requesting node and home node.<br>Four network transactions in total, with only three of the transactions, are on the critical path. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/06/19/Courses/CS149/08-Snooping-based-Cache-Coherence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/19/Courses/CS149/08-Snooping-based-Cache-Coherence/" class="post-title-link" itemprop="url">08. Snooping-based Cache Coherence</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-19 19:30:41" itemprop="dateCreated datePublished" datetime="2022-06-19T19:30:41+08:00">2022-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 20:39:51" itemprop="dateModified" datetime="2024-03-16T20:39:51+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="The-cache-coherence-problem"><a href="#The-cache-coherence-problem" class="headerlink" title="The cache coherence problem"></a>The cache coherence problem</h1><ol>
<li>This problem happens in a shared memory multi-processor system. Reading a value at address X should return the last value written to address X by any processor. </li>
<li>This problem is created by replicating the data stored at address X in local caches (a hardware implementation detail) and cannot be fixed by adding locks. </li>
<li>Memory coherence problems exist because global storage (main memory) and per-processor local storage (processor caches) implement the abstraction of a single shared address space.</li>
<li>In a cache hierarchy,<br>L1 and L2 caches are private per core, while cores share the L3 cache in the same chip.<br>L3 cache is split into sectors or banks. Each bank is physically associated with a core but managed hardware-wise as a single coherent unit.<br>L2 and L3 cache communicate through a ring interconnect where most inter-processor actions happen. </li>
</ol>
<h2 id="Uniprocessor-case"><a href="#Uniprocessor-case" class="headerlink" title="Uniprocessor case"></a>Uniprocessor case</h2><ol>
<li>Providing coherence is fairly simple on a uniprocessor since writes typically come from one client: the processor. Load operation must examine all pending stores in the store buffer and select the last sequence. </li>
<li>One exception on a uniprocessor is device I/O via direct memory access (DMA). </li>
<li>One solution to DMA is that the CPU writes to shared buffers using uncached stores.<br>Another way OS supports this is by marking virtual memory pages containing shared buffers as not-cachable and explicitly flushing pages from the cache when I/O is completed. </li>
<li>In practice, DMA transfers are infrequent compared to CPU loads and stores (so these heavyweight software solutions are acceptable)</li>
</ol>
<h2 id="Coherence-definition"><a href="#Coherence-definition" class="headerlink" title="Coherence definition"></a>Coherence definition</h2><ol>
<li>Obeys program order as expected of a uniprocessor system: A read by processor P to address X that follows a write by P to address X should return the value of the write by P (assuming no other processor wrote to X in between)</li>
<li>Write propagation: A read by processor P1 to address X that follows a write by processor P2 to X returns the written value if the read and write are “sufficiently separated” in time (assuming no other write to X occurs in between)</li>
<li>Write serialization: Writes to the same address are serialized: two writes to address X by any two processors are observed in the same order by all processors.</li>
<li>Write propagation means that notification of a write must eventually get to the other processors. Note that precisely when information about the write is propagated is not specified in the definition of coherence.</li>
</ol>
<h1 id="Implementing-coherence"><a href="#Implementing-coherence" class="headerlink" title="Implementing coherence"></a>Implementing coherence</h1><ol>
<li>Software-based solution: OS uses a page-fault mechanism to propagate writes. It can be used to implement memory coherence over clusters of workstations.</li>
<li>Hardware-based solutions: “snooping”-based coherence implementations and directory-based coherence implementations</li>
<li>Most modern multi-core CPUs implement cache coherence<br>Discrete GPUs do not implement cache coherence. Overhead of coherence deemed not worth it for graphics and scientific computing applications (NVIDIA GPUs provide single shared L2 + atomic memory operations)<br>But the latest Intel Integrated GPUs do implement cache coherence</li>
</ol>
<h2 id="Shared-caches"><a href="#Shared-caches" class="headerlink" title="Shared caches"></a>Shared caches</h2><ol>
<li>One single cache shared by all processors eliminates the problem of replicating the state in multiple caches and makes coherence easy. </li>
<li>This has obvious scalability problems since the point of a cache is to be local and fast. It also causes interference and contention due to many clients. </li>
<li>Facilitates fine-grained sharing (overlapping working sets). Loads/stores by one processor might pre-fetch lines for another processor.</li>
</ol>
<h2 id="Snooping-cache-coherence-schemes"><a href="#Snooping-cache-coherence-schemes" class="headerlink" title="Snooping cache-coherence schemes"></a>Snooping cache-coherence schemes</h2><ol>
<li>Main idea: all coherence-related activity is broadcast to all processors</li>
<li>Cache controllers monitor (“they snoop”) memory operations and react accordingly to maintain memory coherence.</li>
<li>The cache controller must respond to actions from both ends:<br>It must respond to the Load/Store requests from its local processor<br>It also must respond to coherence-related activity broadcast over the chip’s interconnect. </li>
<li>The interconnect is between memory and caches possessed by each processor. There is not only memory-cache information but also cache-cache information, limiting the system’s scalability. </li>
</ol>
<h3 id="Write-through-caches"><a href="#Write-through-caches" class="headerlink" title="Write-through caches"></a>Write-through caches</h3><ol>
<li>For the invalidation-based protocol, when one processor writes into an address, the cache controller broadcasts an invalidation message for other caches to mark that line to invalidation.<br>The next read from other processors will trigger a cache miss.</li>
<li>Other caches will update their local copies as the information is sent for the update-based protocol. </li>
<li>States: Valid (V) or Invalid (I)<br>A local processor read (PrRd) always ends at valid. If the operation starts from an invalid state, a message will be sent (BusRd). If it starts from a valid state, no message will be sent.<br>A local processor write (PrWr) always ends in the same state as before the operation (assumes write no-allocate policy) and always sends a message (BusWr).<br>When a write message from another processor is received (BusWr), It always ends in an invalid state.<br><img src="/imgs/CS149/08/1.jpeg" width="20%"></li>
<li>Requirements of the interconnect:<br>All write transactions are visible to all cache controllers.<br>All write transactions are visible to all cache controllers in the same order. </li>
<li>Simplifying assumptions here:<br>Interconnect and memory transactions are atomic<br>The processor waits until previous memory operations are complete before issuing the next memory operation<br>Invalidation applied immediately as part of receiving an invalidation broadcast</li>
</ol>
<h1 id="Write-back-caches-Invalidation-based"><a href="#Write-back-caches-Invalidation-based" class="headerlink" title="Write-back caches (Invalidation-based)"></a>Write-back caches (Invalidation-based)</h1><ol>
<li>The dirty state of the cache line now indicates exclusive ownership</li>
<li>Exclusive: cache is only cache with a valid copy of line (it can safely be written to)<br>Owner: cache is responsible for supplying the line to other processors when they attempt to load it from memory (otherwise, a load from another processor will get stale data from memory)</li>
<li>A line in the “exclusive” state can be modified without notifying<br>the other caches<br>The processor can only write to lines in the exclusive state. So, they need a way to tell other caches they want exclusive access to the line. They will do this by sending all the other cache messages.<br>When the cache controller snoops a request for exclusive access to the line it contains, it must invalidate the line in its cache.</li>
</ol>
<h2 id="MSI-write-back-invalidation-protocol"><a href="#MSI-write-back-invalidation-protocol" class="headerlink" title="MSI write-back invalidation protocol"></a>MSI write-back invalidation protocol</h2><ol>
<li>Three cache line states:<br>Invalid (I): same as meaning of invalid in uniprocessor cache<br>Shared (S): line valid in one or more caches<br>Modified (M): line valid in exactly one cache (a.k.a. “dirty” or “exclusive” state)</li>
<li>The local processors have the same operations as a write-through case.<br>The coherence-related bus transactions from remote caches have three kinds:<br>BusRd: obtain a copy of the line with no intent to modify<br>BusRdX: obtain a copy of the line with the intent to modify<br>flush: write dirty line out to memory<br><img src="/imgs/CS149/08/2.png" width="50%"></li>
<li>When try to write an invalid line without reading it, the content of the current modified state line will be sent to the new writer. </li>
<li>Write propagation is achieved via a combination of invalidation on BusRdX and flush from M-state on subsequent BusRd/BusRdX from another processor.</li>
<li>Write serialization<br>Writes that appear on interconnect are ordered by the order they appear on interconnect (BusRdX)<br>Reads that appear on interconnect are ordered by the order they appear on interconnect (BusRd)<br>Writes that don’t appear on the interconnect (PrWr to line already in M state):<ul>
<li>The sequence of writes to the line comes between two interconnect transactions for the line</li>
<li>All writes in sequence are performed by the same processor, P (that processor certainly observes them in correct sequential order)</li>
<li>All other processors observe notification of these writes only after an interconnect transaction for the line. </li>
<li>So, all processors see writes in the same order. </li>
</ul>
</li>
</ol>
<h2 id="MESI-invalidation-protocol"><a href="#MESI-invalidation-protocol" class="headerlink" title="MESI invalidation protocol"></a>MESI invalidation protocol</h2><ol>
<li>MSI requires two interconnect transactions for the common case of reading an address and then writing to it<ul>
<li>Transaction 1: BusRd to move from I to S state</li>
<li>Transaction 2: BusRdX to move from S to M state</li>
</ul>
</li>
<li>Solution: add additional state E (“exclusive clean”) to mark the line that has not been modified, but only this cache has a copy of the line<br>This state decouples exclusivity from line ownership (the line is not dirty, so the copy in memory is a valid copy of data)<br>Upgrade from E to M does not require an interconnect transaction. </li>
<li><img src="/imgs/CS149/08/3.jpeg" width="50%"></li>
</ol>
<h2 id="5-stage-invalidation-based-protocol"><a href="#5-stage-invalidation-based-protocol" class="headerlink" title="5-stage invalidation-based protocol"></a>5-stage invalidation-based protocol</h2><ol>
<li>Who should supply data on a cache miss when the line is in the E or S state of another cache?<br>Can get cache line data from memory, or can get data from another cache? If the source is another cache, which one should provide it?</li>
<li>Cache-to-cache transfers add complexity but are commonly used to reduce the latency of data access and the memory bandwidth required by the application.</li>
<li>MESIF: Like MESI, but one cache holds a shared line in the F state rather than S (F=”forward”). Cache with a line in F state services miss<br>Simplifies decision of which cache should service miss (basic MESI: all caches respond)<br>Used by Intel processors</li>
<li>MOESI: Transition from M to O (O=”owned, but not exclusive”) and do not flush to memory (In MESI protocol, transition from M to S requires flush to memory).<br>Other processors maintain a shared line in the S state, while one maintains a line in the O state. Data in memory is stale, so cache with a line in O state must service cache misses.<br>Used in AMD Opteron</li>
</ol>
<h1 id="Invalidation-based-vs-Update-based"><a href="#Invalidation-based-vs-Update-based" class="headerlink" title="Invalidation-based vs. Update-based"></a>Invalidation-based vs. Update-based</h1><ol>
<li>Invalidation-based protocol: The cache must obtain exclusive access to write to a line. All other caches must invalidate their copies.</li>
<li>Update-based protocol: Can write to shared copy by broadcasting update to all other copies</li>
<li>Intuitively, the update would seem preferable if other processors<br>sharing data, continue to access it after a write occurs<br>But updates are overhead if data sits in caches (and is never reread by another processor) or the application performs many writes before the next read</li>
<li>The update can reduce the cache miss rate since all shared copies remain valid.<br>The update can suffer from high traffic due to multiple writes before the next read by another processor.</li>
</ol>
<h1 id="Snoop-for-a-cache-hierarchy"><a href="#Snoop-for-a-cache-hierarchy" class="headerlink" title="Snoop for a cache hierarchy"></a>Snoop for a cache hierarchy</h1><ol>
<li>Challenge: changes made to data at the L1 cache may not be visible to the L2 cache controller, then snoops the interconnect. </li>
<li>Inclusion property:<br>All lines closer to the processor cache are also in farther caches. Thus, all transactions relevant to L1 are also relevant to L2, so it is sufficient for only the L2 to snoop the interconnect.<br>If the line is in the owned state (M in MSI/MESI) in L1, it must also be in an owned state in L2. Allows L2 to determine if a bus transaction requests a modified cache line in L1 without requiring information from L1. </li>
<li>Even if L2 is larger than L1, the inclusion cannot be maintained automatically. L1 and L2 might evict different lines because the access histories differ. </li>
<li>When line X is invalidated in the L2 cache due to BusRdX from another cache. Must also invalidate line X in L1<br>Solution: Each L2 line contains an additional state bit indicating if the line also exists in L1. This bit tells the L2 invalidations of the cache line due to coherence traffic need to be propagated to L1. </li>
<li>When the L1 write is hit, the corresponding line in the L2 cache is in the modified state in the coherence protocol, but the L2 data is stale.<br>When the coherence protocol requires X to be flushed from L2, the L2 cache must request the data from L1.<br>Add another bit for “modified-but-stale” (flushing a “modified-but-stale” L2 line requires getting the real data from L1 first.)</li>
</ol>
<h1 id="False-sharing"><a href="#False-sharing" class="headerlink" title="False sharing"></a>False sharing</h1><ol>
<li>False sharing is when two processors write to different addresses, but those addresses map to the same cache line. The cache line keeps invalidating and requesting data from another processor, generating significant communication due to the coherence protocol. </li>
<li>We can split the line into two parts; each processor only writes one part. </li>
<li>One way is to insert some paddings to make the data written by one processor take up a whole cache line. This can easily implemented at the software level. But it causes memory waste. </li>
<li>Another way is mapping addresses handled by the same processor to the same cache line. This causes no waste but breaks the successiveness of memory address within a cache line. Also, it needs to know the addresses each processor will write and is harder to implement. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/06/11/Courses/CS149/07-Workload-driven-Perfromance-Evaluation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/11/Courses/CS149/07-Workload-driven-Perfromance-Evaluation/" class="post-title-link" itemprop="url">07. Workload-driven Performance Evaluation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-06-11 21:39:38" itemprop="dateCreated datePublished" datetime="2022-06-11T21:39:38+08:00">2022-06-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 19:56:38" itemprop="dateModified" datetime="2024-03-16T19:56:38+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>We should compare parallel program speedup to the best sequential program instead of parallel algorithm running on one core.<br>The reason is that to allow for parallelism, we might change the algorithm and make it slower when executed sequentially.</p>
<h1 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h1><h2 id="Why-consider-scaling"><a href="#Why-consider-scaling" class="headerlink" title="Why consider scaling?"></a>Why consider scaling?</h2><ol>
<li><p>Both problem size and the processors’ number determine arithmetic intensity. Small problem sizes or large processor numbers yield low arithmetic intensity.</p>
</li>
<li><p>If the problem is too small, it might execute fast enough on a single core. Scaling the performance of small problems may not be all that important.<br>Parallelism overheads dominate parallelism benefits and may even result in slowdowns. </p>
</li>
<li><p>If the problem size is too large for a single machine, the working set may not fit in memory, causing thrashing to disk. With enough processors, the key working set fits in the per-processor cache.<br>This may get a super-linear speedup and make speedup on a bigger parallel machine with more memory look amazing. </p>
</li>
<li><p>Another situation in which we might get a super-linear speedup is when we try to search for a solution. With parallelism, we are trying more different variances of search and are more likely to find the solution earlier. </p>
</li>
<li><p>So, we shouldn’t only consider a fixed problem size. Instead, it is desirable to scale problem size as machine sizes grow. </p>
</li>
<li><p>In architecture, scaling up considers how performance scales with increasing core count, and will the design scale to the high end?<br>Scaling down considers how performance scales with decreasing core count, and will the design scale to the low end? </p>
</li>
</ol>
<h2 id="Different-scalings"><a href="#Different-scalings" class="headerlink" title="Different scalings"></a>Different scalings</h2><ol>
<li><p>Strong scaling: scaling processors with a fixed problem size. Consider the ratio between the runtime of the problem $X$ on $P$ processors and the runtime $X$ on $1$ processor. </p>
</li>
<li><p>The goal ratio is $P$. This scaling tells us whether having more processors gets the job done faster.</p>
</li>
<li><p>Weak scaling: scaling problem size and processors proportionally. Consider the ratio of the runtime of the problem. $P\times X$ on $P$ processors and the runtime of the problem $X$ on $1$ processor. </p>
</li>
<li><p>The goal ratio is $1$. This scaling tells us whether having more processors allows me to do bigger jobs. </p>
</li>
<li><p>Problem size is often determined by more than one parameter. So, in weak scaling, we need to consider how the parameter should be changed. </p>
</li>
</ol>
<h2 id="Scaling-constraints"><a href="#Scaling-constraints" class="headerlink" title="Scaling constraints"></a>Scaling constraints</h2><ol>
<li><p>When scaling a problem, we should first ask that, in my situation, under what constraints should the problem be scaled? </p>
</li>
<li><p>Problem-constrained scaling uses a parallel computer to solve the same problem faster.<br>Speedup $=\frac{time\ 1\ processor}{time\ P\ processors}$</p>
</li>
<li><p>Time-constrained scaling focuses on completing more work in a fixed amount of time.<br>Speedup $ = \frac{work\ done\ by\ P\ processors}{work\ done\ by\ 1\ processor}$</p>
</li>
<li><p>“Work done” may not be a linear function of problem inputs. One approach to defining “work done” is by execution time of the same computation on a single processor (but consider the effects of thrashing if the problem is too big)</p>
</li>
<li><p>Ideally, a measure of work is simple to understand and scales linearly with sequential run time (So ideal speedup remains linear in $P$)</p>
</li>
<li><p>Memory-constrained scaling (weak scaling) focuses on running the largest problem possible without overflowing main memory. Neither work nor execution times are held constant.<br>Speedup $= \frac{work\ (P\ processors)}{time\ (P\ processors)}/\frac{work\ (1\ processor)}{time\ (1\ processor)}=\frac{work\ per\ unit\ time\ on\ P\ processors}{work\ per\ unit\ time\ on\ 1\ processor}<br>$There are two assumptions: memory resources scale with processor count, and spilling to disk is infeasible behavior. </p>
</li>
</ol>
<h2 id="Challenges-of-scaling-down-or-up"><a href="#Challenges-of-scaling-down-or-up" class="headerlink" title="Challenges of scaling down or up"></a>Challenges of scaling down or up</h2><ol>
<li><p>Preserve the ratio of time spent in different program phases. </p>
</li>
<li><p>Preserve important behavioral characteristics. </p>
</li>
<li><p>Preserve contention and communication patterns. It is tough to preserve contention since contention is a function of timing and ratios. </p>
</li>
<li><p>Preserve scaling relationships between problem parameters. </p>
</li>
</ol>
<h1 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h1><ol>
<li><p>Architects evaluate architectural decisions quantitatively using hardware performance simulators. </p>
</li>
<li><p>Before comparing simulated performance, the architect runs simulations with new features and simulations without new features. Or simulate against a wide collection of benchmarks. </p>
</li>
<li><p>You can design a detailed simulator to test new architectural features. It would be costly to simulate a parallel machine in full detail.<br>Often, it cannot simulate full machine configurations or realistic problem sizes (must scale down workloads significantly). Architects need to be confident in the scaled-down simulated results to predict reality.</p>
</li>
<li><p>In the trace-driven simulator, we instrument real code running on a real machine to record a trace of all memory accesses. Then, play back the trace on the simulator.<br>It may lead to overfitting your trace instead of having a better generalization. </p>
</li>
<li><p>In the execution-driven simulator, we execute the simulated program in software. Simulated processors generate memory references, which the simulated memory hierarchy processes.<br>The simulator’s performance is typically inversely proportional to the level of simulated detail. </p>
</li>
<li><p>When dealing with large parameter space of machines (number of processors, cache sizes, cache line sizes, memory bandwidths, etc. ), we can use the architectural simulation state space. </p>
</li>
</ol>
<h1 id="Understanding-the-performance"><a href="#Understanding-the-performance" class="headerlink" title="Understanding the performance"></a>Understanding the performance</h1><ol>
<li><p>Always, always, always try the simplest parallel solution first, then measure performance to see where you stand.</p>
</li>
<li><p>Determine if your performance is limited by computation, memory bandwidth (or latency), or synchronization.<br>Try to establish “high watermarks”. What’s the best you can do in practice? How close is your implementation to a best-case scenario?</p>
</li>
<li><p>Roofline model: Use microbenchmarks to compute the peak performance of a machine as a function of the arithmetic intensity of the application. Then, compare the application’s performance to known peak values. </p>
</li>
<li><p>The x-axis means operational intensity (like Flops/Byte), and the y-axis means attenable GFlops/s.<br>In the diagonal region, the y grows with x, which limits memory bandwidth. In the horizontal region, the y stays the same as x grows, which means the compute is limited. </p>
</li>
<li><p>We can use ILP, SIMD, or balance floating-point when computing is limited.<br>When memory bandwidth is limited, we can limit accesses to unit stride accesses only, develop memory affinity, or use software prefetching. </p>
</li>
</ol>
<h2 id="Establish-high-watermarks"><a href="#Establish-high-watermarks" class="headerlink" title="Establish high watermarks"></a>Establish high watermarks</h2><ol>
<li><p>Add “math” (non-memory instructions).<br>Does execution time increase linearly with operation count as math is added? If so, this is evidence that the code is instruction-rate limited.</p>
</li>
<li><p>Remove almost all math, but load the same data.<br>How much does execution time decrease? If not much, suspect memory bottleneck</p>
</li>
<li><p>The first two ways need to avoid compiler optimization. </p>
</li>
<li><p>Change all array accesses to A[0].<br>How much faster does your code get?<br>This establishes an upper bound for improving the locality of data access.</p>
</li>
<li><p>Remove all atomic operations or locks.<br>How much faster does your code get? (provided it still does approximately the same amount of work)<br>This establishes an upper bound on the benefit of reducing sync overhead.</p>
</li>
</ol>
<h2 id="Profilers-performance-monitoring-tools"><a href="#Profilers-performance-monitoring-tools" class="headerlink" title="Profilers/performance monitoring tools"></a>Profilers/performance monitoring tools</h2><ol>
<li><p>All modern processors have low-level event “performance counters,” which are registers that count important details such as instructions completed, clock ticks, L2/L3 cache hits/misses, bytes read from the memory controller, etc. </p>
</li>
<li><p>Intel’s Performance Counter Monitor Tool provides a C++ API for accessing these registers. </p>
</li>
<li><p>It can use <code>getIPC(begin, end)</code>, <code>getL3CacheHitRatio(begin, end)</code>, <code>getBytesReadFromMC(begin, end)</code>, etc. to get values of that information. </p>
</li>
<li><p>The <code>begin</code> and <code>end</code> are <code>SystemCountState</code> instances acquired by <code>getSystemCounterState()</code> at the beginning and end of the code to analyze. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PCM *m = PCM::<span class="built_in">getInstance</span>();</span><br><span class="line">SystemCounterState begin = <span class="built_in">getSystemCounterState</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// code to analyze goes here</span></span><br><span class="line"></span><br><span class="line">SystemCounterState end = <span class="built_in">getSystemCounterState</span>();</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(“Instructions per clock: %f\n”, <span class="built_in">getIPC</span>(begin, end));</span><br><span class="line"><span class="built_in">printf</span>(“L3 cache hit ratio: %f\n”, <span class="built_in">getL3CacheHitRatio</span>(begin, end));</span><br><span class="line"><span class="built_in">printf</span>(“Bytes read: %d\n”, <span class="built_in">getBytesReadFromMC</span>(begin, end));</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/21/Courses/CS149/06-Locality-Communication-and-Contention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/21/Courses/CS149/06-Locality-Communication-and-Contention/" class="post-title-link" itemprop="url">06. Locality, Communication, and Contention</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-21 13:42:23" itemprop="dateCreated datePublished" datetime="2022-05-21T13:42:23+08:00">2022-05-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 19:43:13" itemprop="dateModified" datetime="2024-03-16T19:43:13+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Communication"><a href="#Communication" class="headerlink" title="Communication"></a>Communication</h1><h2 id="Reduce-communication-time"><a href="#Reduce-communication-time" class="headerlink" title="Reduce communication time"></a>Reduce communication time</h2><ol>
<li><p>Total communication time = overhead + occupancy + network delay. </p>
</li>
<li><p>Overhead is the time spent on communication by a processor, occupancy is the time for data to pass through the slowest component of the system, and network delay is everything else. </p>
</li>
<li><p>Reduce overhead of communication to sender/receiver:<br>Reassign tasks in a better way that needs to send fewer messages.<br>Make messages larger to amortize overhead.<br>Coalesce many small messages into large ones</p>
</li>
<li><p>Reduce delay:<br>Application writers can restructure code to exploit locality.<br>Hardware implementors can improve communication architecture. </p>
</li>
</ol>
<h2 id="Reduce-communication-cost"><a href="#Reduce-communication-cost" class="headerlink" title="Reduce communication cost"></a>Reduce communication cost</h2><ol>
<li><p>Total communication cost = communication time - overlap</p>
</li>
<li><p>Overlap: portion of communication performed concurrently with other work (“other work” can be computation or other communication)</p>
</li>
<li><p>Cost is the part you cannot overcome by changing the protocol. The communication time is necessary, and we can hide the overlap part by pipelining. </p>
</li>
<li><p>Increase communication/computation overlap:<br>Application writers can use asynchronous communication (e.g., async messages)<br>Hardware implementors can use pipelining, multi-threading, pre-fetching, out-of-order execution<br>Requires additional concurrency in the application (more concurrency than the number of execution units)</p>
</li>
<li><p>Instruction pipeline: Break the execution of each instruction down into several smaller steps.<br>Enables higher clock frequency; only a simple, short operation is done by each part of the pipeline in each clock</p>
</li>
<li><p>Non-pipelined communication: $T(n)=T_0+\frac{n}{B}$ ($T_0$ is the start-up latency, n is bytes transferred in operation, and B is the transfer rate or bandwidth)<br>Efficient bandwidth $=\frac{n}{T(n)}$</p>
</li>
</ol>
<h2 id="Improve-arithmetic-intensity"><a href="#Improve-arithmetic-intensity" class="headerlink" title="Improve arithmetic intensity"></a>Improve arithmetic intensity</h2><ol>
<li><p>Communication-to-computation ratio = amount of communication/amount of computation<br>The units can be different. If the denominator is the execution time of computation, the ratio gives the average bandwidth requirement.</p>
</li>
<li><p>Arithmetic intensity = 1 / communication-to-computation ratio</p>
</li>
<li><p>Change the traversal order to reduce the time between accesses to the same data. We want to do all the calculations related to accessing data now.<br>This way, we can improve the cache utilization by preventing those data from getting flushed out when we try to reaccess them. </p>
</li>
<li><p>Fuse loops to reduce the frequency of store operation. </p>
</li>
<li><p>Improve arithmetic intensity by sharing data. Co-locate tasks that operate on the same data.  Schedule threads to work on the same data structure and on the same processor at the same time. Reduces inherent communication</p>
</li>
</ol>
<h2 id="Reduce-artifactual-communication"><a href="#Reduce-artifactual-communication" class="headerlink" title="Reduce artifactual communication"></a>Reduce artifactual communication</h2><ol>
<li><p>Inherent communication: Communication that must occur in a parallel algorithm. The communication is fundamental to the algorithm.<br>Artifactual communication: all other communication happens because we want to use resources efficiently.</p>
</li>
<li><p>The granularity of communication can be important because it may introduce artifactual communication.<br>Assume that communication granularity is a cache line. </p>
</li>
<li><p>If we see data as a row-major layout, when the data required is in the column, each communication only provides one needed data. </p>
</li>
<li><p>When Threads access their assigned elements (no inherent communication exists), real machine triggers (artifactual) communication due to the cache line being written to by both processors. </p>
</li>
<li><p>Reduce artifactual communications by blocking data layout. Each communication only involves the data in the same block. If communication granularity is larger than a block row, each communication will transfer multiple rows. </p>
</li>
</ol>
<h1 id="Contention"><a href="#Contention" class="headerlink" title="Contention"></a>Contention</h1><ol>
<li><p>Contention occurs when many requests for a resource are made within a small window of time. The resource is a hot spot.<br>Contention of shared resources results in longer overall operation times. </p>
</li>
<li><p>Distributed work queues serve to reduce contention (contention in access to a single shared work queue)</p>
</li>
<li><p>One way to reduce contention is to use finer-granularity locks. Instead of locking the whole data structure, we can only lock a part of that structure. </p>
</li>
<li><p>Another way is that each CUDA block computes partial results and merges them afterward. However, this requires extra work for merging, and each CUDA block needs to store a partial result instead of all using the same result. </p>
</li>
<li><p>The best way is to stagger access to contended resources. For example, instead of calculating the final result directly, we can calculate several temporal results with no contention and get the final result from them. </p>
</li>
</ol>
<h1 id="Understanding-the-performance"><a href="#Understanding-the-performance" class="headerlink" title="Understanding the performance"></a>Understanding the performance</h1><ol>
<li><p>Always, always, always try the simplest parallel solution first, then measure performance to see where you stand. </p>
</li>
<li><p>We should compare parallel program speedup to the best sequential program instead of parallel algorithm running on one core.<br>The reason is that to allow for parallelism, we might change the algorithm and make it slower when executed sequentially. </p>
</li>
<li><p>Determine if your performance is limited by computation, memory bandwidth (or latency), or synchronization.<br>Try to establish “high watermarks”. What’s the best you can do in practice? How close is your implementation to a best-case scenario?</p>
</li>
<li><p>Roofline model: Use microbenchmarks to compute the peak performance of a machine as a function of the arithmetic intensity of the application. Then, compare the application’s performance to known peak values.</p>
</li>
</ol>
<h2 id="Establish-high-watermarks"><a href="#Establish-high-watermarks" class="headerlink" title="Establish high watermarks"></a>Establish high watermarks</h2><ol>
<li><p>Add “math” (non-memory instructions).<br>Does execution time increase linearly with operation count as math is added?<br>If so, this is evidence that the code is instruction-rate limited.</p>
</li>
<li><p>Remove almost all math, but load the same data.<br>How much does execution time decrease?<br>If not much, suspect memory bottleneck</p>
</li>
<li><p>Change all array accesses to A[0].<br>How much faster does your code get?<br>This establishes an upper bound for improving the locality of data access.</p>
</li>
<li><p>Remove all atomic operations or locks.<br>How much faster does your code get? (provided it still does approximately the same amount of work)<br>This establishes an upper bound on the benefit of reducing sync overhead. </p>
</li>
</ol>
<h2 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h2><ol>
<li><p>Both problem size and the processor’s number determine arithmetic intensity. Small problem sizes or large processor numbers yield low arithmetic intensity. </p>
</li>
<li><p>If the problem is too small, it might execute fast enough on a single core. Scaling the performance of small problems may not be all that important. </p>
</li>
<li><p>If the problem size is too large for a single machine, the working set may not fit in memory, causing thrashing to disk. With enough processors, the key working set fits in the per-processor cache.<br>This may get a super-linear speedup and make speedup on a bigger parallel machine with more memory look amazing. </p>
</li>
<li><p>Desire to scale problem size as machine sizes grow (buy a bigger machine to compute more, rather than compute the same problem faster)</p>
</li>
<li><p>Problem-constrained scaling uses a parallel computer to solve the same problem faster.<br>$Speedup=\frac{time\ 1\ processor}{time\ P\ processors}$</p>
</li>
<li><p>Time-constrained scaling focuses on completing more work in a fixed amount of time.<br>$Speedup = \frac{work\ done\ by\ P\ processors}{work\ done\ by\ 1\ processor}$<br>“Work done” may not be a linear function of problem inputs. One approach to defining “work done” is by execution time of exact computation on a single processor (but consider the effects of thrashing if the problem is too big)</p>
</li>
<li><p>Memory-constrained scaling (weak scaling) focuses on running the largest problem possible without overflowing main memory. Neither work nor execution times are held constant.<br>$Speedup = \frac{work\ (P\ processors)}{time\ (P\ processors)}/\frac{work\ (1\ processor)}{time\ (1\ processor)}=\frac{work\ per\ unit\ time\ on\ P\ processors}{work\ per\ unit\ time\ on\ 1\ processor}$</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/" class="post-title-link" itemprop="url">05. Graphic processing units and CUDA</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-19 09:59:09" itemprop="dateCreated datePublished" datetime="2022-05-19T09:59:09+08:00">2022-05-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 19:22:24" itemprop="dateModified" datetime="2024-03-16T19:22:24+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Graphics"><a href="#Graphics" class="headerlink" title="Graphics"></a>Graphics</h1><ol>
<li>The first step to drawing a graphic on the screen is to describe the manipulated things (key entities), whose surface is represented as a 3D triangle mesh.<br>So, the input of the calculating system is a list of vertices in 3D space and their connectivity to primitives. </li>
<li>The operations of the system are as follows:<br>Given a scene camera position, compute where the vertices lie on the screen<br>Group vertices into primitives<br>Generate one fragment for each pixel a primitive overlaps<br>Compute the color of the primitive for each fragment based on scene lighting and primitive material properties<br>Put the color of the “closest fragment” to the camera in the output image</li>
<li>We can Abstract the process of rendering a picture as a sequence of operations on vertices, primitives, fragments, and pixels.<br>GPUs are very fast processors for performing the same computation (shader programs) on large collections of data (streams of vertices, fragments, and pixels), which sounds like data-parallelism. </li>
<li>To do GPU-based scientific computation, we need to set the OpenGL output image size to the output array size and render two triangles that exactly cover the screen. </li>
</ol>
<h1 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h1><h2 id="Grid-Block-and-Thread"><a href="#Grid-Block-and-Thread" class="headerlink" title="Grid, Block, and Thread"></a>Grid, Block, and Thread</h2><ol>
<li>CUDA thread IDs can be up to 3-dimensional. Multiple threads make up a block, and multiple blocks make up a grid.<br>Each kernel has a grid. All the blocks in a grid form a 3D matrix, and all the threads in a block also form a 3D matrix.<br>We can get information about the shape of threads in a block matrix or block in a grid matrix in a block with <code>block_dim</code> and <code>grid_dim</code></li>
<li>When launching the CUDA threads, we need to specify the size of blocks and threads with <code>kernel_function&lt;&lt;&lt;block_dim, thread_dim&gt;&gt;&gt;(parameters)</code>.<br>Here, block_dim and thread_dim can be either a dim3 value or a number of the total number of blocks and threads per block to make the CUDA compiler figure out how to arrange blocks and threads.</li>
<li>So, the coordinates for declaring or locating a block in a grid or a thread in a block are in 3D. In the CUDA code, we can get information about the current block or thread with <code>blockIdx</code> and <code>threadIdx</code> and their properties <code>x</code>, <code>y</code>, and <code>z</code>. </li>
<li>The calculation object of CUDA is usually matrices. So, we prefer to cut matrices into blocks. Each CUDA block calculates one block in matrices, and each thread calculates one element in matrices. </li>
<li>In <code>pthread</code>, there is stack space for the thread, and we need to allocate a control block so OS can schedule the thread.<br>Unlike <code>pthread</code>, CUDA controls those instances by thread blocks. If controlled by threads, there will be too many to control.</li>
<li>Major CUDA assumption: thread block execution can be carried out in any order (no dependencies between blocks)</li>
</ol>
<h2 id="Kernel-function"><a href="#Kernel-function" class="headerlink" title="Kernel function"></a>Kernel function</h2><ol>
<li>“Host” code: serial execution runs as part of normal C/C++ application on CPU<br>“CUDA device” code: a kernel function runs on GPU, denoted by <code>__global__</code> before the definition of the kernel function.<br>Device function: SPMD execution on GPU, denoted by <code>__device__</code>. Kernels call these functions and don’t generate new threads.<br>Kernels and device functions only reference device memory. Host code can only reference host memory but can have pointers to device memory. </li>
<li>In the kernel function, we need to get the indices of the element a thread is calculating. For a 2D matrix, we usually take <code>x</code> as the column direction and y as the row direction.<br>So to access <code>A[i][j]</code>, <code>i = blockIdx.y * blockDim * y + threadIdx.y</code> and <code>j = blockIdx.x * blockDim.x + threadIdx.x</code>.</li>
<li>The data collection size does not determine the number of kernel invocations. So, normally, we want to do a test before accessing the vector values (array values). The overhead of the test can be ignored, although it does cause some threads not to be used. </li>
<li><code>__syncthreads()</code> is a barrier that waits for all threads in the block to arrive at this point.<br>Atomic operations on global memory and shared memory variables are also provided, but they are costly and should only be used as a last resort.<br>There is an implicit barrier across all threads at the return of the kernel. </li>
<li>A compiled CUDA device binary includes program text (instructions) and information about required resources (how many threads per block, how much space per thread, how much shared-space per thread block) </li>
</ol>
<h2 id="Memory-Model"><a href="#Memory-Model" class="headerlink" title="Memory Model"></a>Memory Model</h2><ol>
<li>The host and device have distinct address spaces, so we need to copy data into CUDA memory before executing.<br><code>cudaMalloc(ptr, size)</code> can allocate CUDA memory, and <code>cudaFree(ptr)</code> can free CUDA memory. The pointer in <code>cudaMalloc</code> can be a host variable, but the pointer in <code>cudaFree</code> must be allocated by <code>cudaMalloc</code>.<br><code>cudaMemcpy(dest, src, size, kind)</code> can copy those data into CUDA memory. The kind of direction of copy can be <code>cudaMemcpyHostToDevice</code> or <code>curdaMemcpyDeviceToHost</code>. </li>
<li>Three distinct types of memory visible to kernels are per-thread private, per-block shared, and device global memory. </li>
<li><code>cudaMalloc</code>, <code>cudaMemcpy</code>, and cudaFree operate on the device’s global memory, and those local variables in the kernel function are in per-thread private memory.<br>We can declare variables in per-block shared memory with <code>__share__</code>. </li>
<li>If multiple threads in a block need to access a common memory, they will all read it once. And if that memory is in the global memory, it would be really slow. </li>
<li>We can copy the common memory into the per-block shared memory to avoid such an efficient decrease. So we only need to access global memory once, and later accesses only in per-block shared memory.<br>We need to declare a <code>__share__</code> array and assign it to values in global memory. </li>
<li>All threads copy data from global memory to per-block shared memory asynchronous, so we better ass a <code>_syncthread()</code> to make sure later operations only start when all data have been copied. </li>
</ol>
<h2 id="Hardware-implementation"><a href="#Hardware-implementation" class="headerlink" title="Hardware implementation"></a>Hardware implementation</h2><ol>
<li>Those synchronizations within a warp and scheduling are done by hardware, and programmers don’t need to worry about these things.</li>
<li>GPU implementation maps thread blocks (“work”) to cores using a dynamic scheduling policy that respects resource requirements. </li>
<li>In GPU implementation (not a CUDA abstraction), each SM has multiple groups of warps. However, the number of warp execution contexts is far more than the number of warps.<br>There is a warp selector for each warp to choose the instruction to be executed by that warp. Each warp selector usually has two (or more) Fectch/Decoder units. </li>
<li>Shared per-block memory and L1-cache are inside each SM, but L2-cache is shared by all SMs.<br>The device-global memory (GPU memory) only communicates with blocks through the L2 cache. </li>
<li>For each clock, an SM will choose several warp contexts to fill all warps to use thread-level parallelism, and a warp will choose several instructions to fill all Fetch/Decoder units to use instruction-level parallelism. </li>
<li>When running a CUDA program, the GPU work scheduler will map one block to multiple warps in the same SM core. CUDA threads are numbered within a block in row-major order.<br>Inside a single thread block is SPMD shared address space programming. </li>
<li>When a single warp accesses consecutive memory locations, do block read or write. When single warp accesses separated memory locations, it requires gather (read) or scatter(write)</li>
<li>One SM core may have multiple blocks, so an SMM core can concurrently execute multiple CUDA thread blocks.<br>When all threads in the block are complete, block resources (shared memory allocations, warp execution contexts) become available for the next block. </li>
<li>The CUDA program is not compiled into SIMD instructions like ISPC gangs. GPU hardware dynamically checks whether 32 independent CUDA threads share an instruction. If true, it executes all 32 threads in a SIMD manner, or performance can suffer due to divergent execution. </li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/" class="post-title-link" itemprop="url">04. Work Distribution and Scheduling</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-17 15:35:33" itemprop="dateCreated datePublished" datetime="2022-05-17T15:35:33+08:00">2022-05-17</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 19:12:41" itemprop="dateModified" datetime="2024-03-16T19:12:41+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Balancing-the-workload"><a href="#Balancing-the-workload" class="headerlink" title="Balancing the workload"></a>Balancing the workload</h1><p>Always implement the simplest solution first, then measure performance to determine if you need to do better. </p>
<h2 id="Static-assignment"><a href="#Static-assignment" class="headerlink" title="Static assignment"></a>Static assignment</h2><ol>
<li>The assignment of work to threads is pre-determined. But it is not necessarily determined at compile-time; it may depend on runtime parameters. </li>
<li>Benefit: simple, essentially zero runtime overhead</li>
<li>Applicable situation: When the cost (execution time) of work and the amount of work is predictable. The following are some of the most common situations:<br>When it is known upfront that all work has the same cost<br>When work is predictable, but not all jobs have the same cost<br>When statistics about execution time are known</li>
<li>Semi-static assignment: When the work cost is predictable for the near-term future, we can periodically profile itself and re-adjust the assignment.<br>The assignment is “static” for the interval between re-adjustments</li>
</ol>
<h2 id="Dynamic-assignment"><a href="#Dynamic-assignment" class="headerlink" title="Dynamic assignment"></a>Dynamic assignment</h2><ol>
<li><p>The program determines assignment dynamically at runtime to ensure a well-distributed load. They are often used when the execution time or the total number of tasks is unpredictable. </p>
</li>
<li><p>The ISPC task is implemented dynamically. </p>
</li>
</ol>
<h3 id="With-one-queue"><a href="#With-one-queue" class="headerlink" title="With one queue"></a>With one queue</h3><ol>
<li><p>The programmers divide the whole problem into sub-problems (or “tasks”, “work”). A queue shared by all threads is a collection of work to do. A thread will grab another task from the queue whenever it finishes its task. </p>
</li>
<li><p>Fine granularity partitioning: each task is small.<br>This is likely to have a good workload balance, but there is potential for high synchronization costs. </p>
</li>
<li><p>Coarse granularity partitioning: each task is larger.<br>This will decrease synchronization cost and the overhead but may have a worse workload balance. </p>
</li>
<li><p>Long tasks should be scheduled first. Thread performing long tasks performs fewer overall tasks but approximately the same amount of work as the other threads. This requires some knowledge of the workload. </p>
</li>
</ol>
<h3 id="With-a-set-of-queues"><a href="#With-a-set-of-queues" class="headerlink" title="With a set of queues"></a>With a set of queues</h3><ol>
<li><p>When assigned to one queue, all threads have to communicate with each other about the queue. </p>
</li>
<li><p>Each thread has its queue, and it only executes tasks in its queue. So, there is no need to communicate with other threads. </p>
</li>
<li><p>Initially, the programmer pushes tasks into queues arbitrarily (a bit like a static assignment).<br>The dynamic is that when a queue is empty, that thread can steal from other still-working threads.<br>It will steal from a random thread. Every time, it will steal a proportion of the tasks in the target queue, not all of them, and usually more than one task.<br>A thread is terminated when there is no thread for it to steal; when a steal fails, it will try to steal from other threads until it has tried all of them. </p>
</li>
<li><p>Stealing involves communication but at a lower frequency than one queue method. In this way, the local queue access is fast. </p>
</li>
<li><p>Sometimes, it is hard to have fully independent tasks, but work in task queues need not be independent.<br>A task is not removed from the queue and assigned to the worker thread until all task dependencies are satisfied. Workers can submit new tasks (with optional, explicit dependencies) to the task system.</p>
</li>
</ol>
<h1 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a>Scheduling</h1><p>In a divide-and-conquer algorithm, there are both dependencies and independencies. Like in quick-sort, both divides depend on the partition, and those two divides are independent.<br>With Cilk Plus, we can express divide-and-conquer easier. </p>
<h2 id="cilk-spawn"><a href="#cilk-spawn" class="headerlink" title="cilk_spawn"></a>cilk_spawn</h2><ol>
<li><p><code>cilk_spawn</code> is labeled before a function call so that the called function can run concurrently with the code after the call.<br>The call labeled <code>cilk_spawn</code> is the spawned child, and the rest of the code is the continuation.</p>
</li>
<li><p>In divide-and-conquer, there is always a time when the problem size is small enough that the overhead of spawn trumps the benefits of<br>potential parallelization. Then, we will solve those problems sequentially. </p>
</li>
<li><p>The main idea is to expose independent work (potential parallelism) to the system using <code>cilk_spawn</code>. </p>
</li>
<li><p><code>cilk_spawn</code> is a bit like <code>pthread_create</code>, and <code>cilk_sync</code> is similar to <code>pthread_join</code>. But the <code>pthread</code> has some problems when too many threads are spawned.<br>The first is the heavyweight spawn operation. Many more concurrently running threads than cores will cause context-switching overhead, a larger working set than necessary, and less cache locality. </p>
</li>
<li><p>The Cilk Plus runtime maintains a pool of worker threads. All threads are created at the application launch.  The machine has exactly as many worker threads as in execution contexts.<br>If we labeled everything <code>cilk_spawn</code>, the main thread has nothing to do. </p>
</li>
<li><p>Each thread in the pool will maintain a work queue to store what word needs to be done.<br>When a thread goes idle, it will look in the busy thread’s queue for work and move work from the busy thread’s queue to its queue.</p>
</li>
<li><p>If the caller thread runs the continuation first, the queue should record the child for later execution, and the child is made available for stealing by other threads.<br>The caller thread will spawn as many children as possible using this method, like BFS. If there is no stealing, the execution order is very different than<br>that of program withcilk_spawnremoved. </p>
</li>
<li><p>If the caller thread runs the child first, the queue should record continuation for later execution, and continuation is made available for stealing by other threads.<br>In this method, the caller thread will only create one item to steal.<br>If no stealing occurs, the thread continually pops continuation from the work queue and enqueues new continuation (like DFS). The execution order is the same as for the program with spawn removed.<br>If continuation is stolen, the stealing thread spawns the next child. </p>
</li>
<li><p>If the continuation is run first, there will be more items to steal; thus, it will be a better advantage of multi-thread.<br>But if the continuation is run first, the work queue storage for a system with T threads is no more than T times that of stack storage for single-threaded execution and thus saves more space. </p>
</li>
<li><p>The work queue is implemented as a dequeue (double-ended queue).<br>Local thread pushes/pops from the “tail” (bottom), while remote threads steal from the “head” (top).<br>Reduces contention with local thread: local thread does not access the same part of dequeue as stealing threads.<br>Do larger work first: in divide-and-conquer, the top of the queue is usually at the beginning of the call tree and is a larger piece of work.<br>Maximizes locality: in conjunction with the run-child-first policy, the local thread works on the local part of the call tree</p>
</li>
</ol>
<h2 id="cilk-sync"><a href="#cilk-sync" class="headerlink" title="cilk_sync"></a>cilk_sync</h2><ol>
<li><p><code>cilk_sync</code> is used after those <code>cilk_spawn</code> call code. It will return when all calls spawned by the current function have been completed. </p>
</li>
<li><p>There is an implicit <code>cilk_sync</code> at the end of every function containing a cilk_spawn, so when a Cilk function returns, all work associated with that function is complete. </p>
</li>
<li><p>If no work has been stolen by other threads, then there’s nothing to do at the sync point, <code>cilk_sync</code> is a no-op. But this is not a common situation. </p>
</li>
<li><p>One way to implement sync is with a stalling joint.<br>The thread that initiates the fork must perform the sync. Therefore, it waits for all spawned work to be complete.<br>The descriptor for block A was created.<br>When stealing from the initial thread happens, a descriptor for that stolen work is created to track the number of outstanding spawns for the block and the number of those that have completed.<br>When all the child spawned for the block is done, this block is considered done, and the descriptor is free. When all the blocks are done, sync is fulfilled. </p>
</li>
<li><p>Another way to implement sync is the greedy policy.<br>When the thread that initiates the fork goes idle, it can look to steal new work.  The last thread to reach the join point continues execution after sync. This will also create a descriptor for those stolen works.<br>The worker thread that initiated spawn may not be the thread that executes logic after <code>cilk_sync</code>. </p>
</li>
<li><p>In greed policy, All threads always attempt to steal if there is nothing to do, and the thread only goes idle if no work to steal is present in the system. But in stalling policy, the initial thread doesn’t steal and only waits until its work is done. </p>
</li>
<li><p>The overhead of bookkeeping steals and managing sync points only occurs when steals occur. If large pieces of work are stolen, this should occur infrequently. Most of the time, threads push/pop local work from their local queue. </p>
</li>
<li><p>Cilk uses greedy join scheduling. </p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/04/20/Courses/CS149/03-Parallel-Programming-Basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/04/20/Courses/CS149/03-Parallel-Programming-Basics/" class="post-title-link" itemprop="url">03. Parallel Programming Basics</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-04-20 14:27:45" itemprop="dateCreated datePublished" datetime="2022-04-20T14:27:45+08:00">2022-04-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 18:45:17" itemprop="dateModified" datetime="2024-03-16T18:45:17+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Decomposition"><a href="#Decomposition" class="headerlink" title="Decomposition"></a>Decomposition</h1><ol>
<li><p>Decomposition: The problem to solve is usually a chunk of work. So the first step we need to do is to decomposite them into subproblems (a.k.a tasks, work to do)</p>
</li>
<li><p>We usually want the number of subproblems to be at least as many as processors.</p>
</li>
<li><p>The key aspect of decomposition is to identify dependencies. We want subproblems to be independent so that they can be paralleled. </p>
</li>
<li><p>Amdahl’s Law: dependencies limit maximum speedup due to parallelism<br>Let S = the fraction of sequential execution that is inherently sequential (dependencies prevent parallel execution). The maximum speedup due to parallel execution ≤ 1/S. </p>
<script type="math/tex; mode=display">speedup \le\frac{t}{st+\frac{(1-s)t}{p}}=\frac1{s+\frac{1-s}{p}}</script></li>
<li><p>In most cases, the programmer is responsible for performing decomposition. </p>
</li>
<li><p>When doing the decomposition, it is better to think of partitioning computation instead of data. </p>
</li>
</ol>
<h1 id="Assignment"><a href="#Assignment" class="headerlink" title="Assignment"></a>Assignment</h1><ol>
<li><p>Assignment: When the subproblems are more than processors, we will group them to form a larger chunk of work and assign the grouped tasks to parallel threads. </p>
</li>
<li><p>One goal is to balance the workload so that each processor finishes their work almost simultaneously.<br>Another goal is to reduce communication costs. Getting data from another processor is nontrivial expensive, either cache miss or waiting for a message.<br>These two goals are at odds with each other. </p>
</li>
<li><p>This step can be performed statically or dynamically during the execution<br>Static way: Before the processors begin the work, we have already decided how to divide things.<br>Dynamic way: We work out how to divide on the way as processing. </p>
</li>
<li><p>We can choose the static way with the programCount and programIndex assignment or pthread</p>
</li>
<li><p>We can choose the dynamic way with foreach if the system chooses the dynamic way or the queue.<br>In the queue way, we arrange all tasks in a queue, and when a processor has done its job, it will grab another task from the queue. This is an excellent way to balance the workload, but maintaining the queue and acquiring tasks from it might cost some performance. </p>
</li>
</ol>
<h1 id="Orchestration"><a href="#Orchestration" class="headerlink" title="Orchestration"></a>Orchestration</h1><ol>
<li><p>Orchestration: When parallel threads are running, they may need to cooperate. So, we need to let them communicate correctly. </p>
</li>
<li><p>We will worry about things like structure communication, synchronization, organizing data structure in memory, and scheduling tasks.</p>
</li>
<li><p>The goal is to reduce the costs of communication/sync, preserve the locality of data reference, reduce the overhead of synchronization or communication, etc.</p>
</li>
<li><p>In the shared address space model, lock/unlock is commonly used for preserving atomicity, and barriers can divide computation into phases. When threads execute to barriers, they will stop and wait. Until enough threads hit the barrier, those stalled threads can execute rest codes. </p>
</li>
<li><p>A commonly used optimization strategy is fewer locks/unlocks and barriers.<br>Every time we operate a shared variable, we must use the lock/unlock to keep atomicity. We could operate on partial variables locally and then merge the partial results.<br>When we use a barrier to keep a shared variable valid when it might be changed at the next phase, we can use different shared variables in successive phases. This is to trade off footprint for removing dependencies. </p>
</li>
</ol>
<h1 id="Mapping"><a href="#Mapping" class="headerlink" title="Mapping"></a>Mapping</h1><ol>
<li><p>Mapping: Finally, we need to map each thread to physical hardware. </p>
</li>
<li><p>When we map pthreads to hardware execution context on a CPU core, this is done by the operating system.<br>When we map ISPC program instances to vector instruction lanes, this is done by the compiler.<br>When we map CUDA thread blocks to GPU cores, this is done by the hardware.</p>
</li>
<li><p>Place related threads (cooperating threads) on the same processor to maximize locality data sharing and minimize costs of comm/sync.<br>Place unrelated threads on the same processor (one might be bandwidth-limited, and another might be compute-limited) to use the machine more efficiently.</p>
</li>
<li><p>Normally, we just let the OS do the mapping as it wants. But sometimes we still want to control the way of mapping.</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="Previous page" aria-label="Previous page" href="/about/page/5/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/about/">1</a><span class="space">&hellip;</span><a class="page-number" href="/about/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/about/page/7/">7</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/about/page/7/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
