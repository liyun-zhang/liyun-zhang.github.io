<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Graphics  The first step to draw a graphic in screen is to describe the things (key entities) that are manipulated, whose surface is represented as a 3D triangle mesh. So the input of the calculating">
<meta property="og:type" content="article">
<meta property="og:title" content="05. Graphic processing units and CUDA">
<meta property="og:url" content="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:description" content="Graphics  The first step to draw a graphic in screen is to describe the things (key entities) that are manipulated, whose surface is represented as a 3D triangle mesh. So the input of the calculating">
<meta property="og:locale">
<meta property="article:published_time" content="2022-05-19T01:59:09.000Z">
<meta property="article:modified_time" content="2024-02-09T05:51:23.604Z">
<meta property="article:author" content="LiyunZhang">
<meta property="article:tag" content="Parallelism">
<meta property="article:tag" content="CUDA">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/","path":"2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/","title":"05. Graphic processing units and CUDA"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>05. Graphic processing units and CUDA | LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">LiyunZhang</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#graphics"><span class="nav-number">1.</span> <span class="nav-text"> Graphics</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cuda"><span class="nav-number">2.</span> <span class="nav-text"> CUDA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#grid-block-and-thread"><span class="nav-number">2.1.</span> <span class="nav-text"> Grid, Block and Thread</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kernel-function"><span class="nav-number">2.2.</span> <span class="nav-text"> Kernel function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#memory-model"><span class="nav-number">2.3.</span> <span class="nav-text"> Memory Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hardware-implementation"><span class="nav-number">2.4.</span> <span class="nav-text"> Hardware implementation</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">

  
  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="05. Graphic processing units and CUDA | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          05. Graphic processing units and CUDA
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-19 09:59:09" itemprop="dateCreated datePublished" datetime="2022-05-19T09:59:09+08:00">2022-05-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-09 13:51:23" itemprop="dateModified" datetime="2024-02-09T13:51:23+08:00">2024-02-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="graphics"><a class="markdownIt-Anchor" href="#graphics"></a> Graphics</h1>
<ol>
<li>The first step to draw a graphic in screen is to describe the things (key entities) that are manipulated, whose surface is represented as a 3D triangle mesh.<br />
So the input of the calculating system is a list of vertices in 3D space and their connectivity into primitives.</li>
<li>The operations of system is as following:<br />
Given a scene camera position, compute where the vertices lie on screen<br />
Group vertices into primitives<br />
Generate one fragment for each pixel a primitive overlaps<br />
Compute color of primitive for each fragment based on scene lighting and primitive material properties<br />
Put color of the “closest fragment” to the camera in the output image</li>
<li>We can Abstract process of rendering a picture as a sequence of operations on vertices, primitives, fragments, and pixels.<br />
GPUs are very fast processors for performing the same computation (shader programs) on large collections of data (streams of vertices, fragments, and pixels), which sounds like data-parallelism.</li>
<li>To do GPU-based scientific computation, we need to set OpenGL output image size to be output array size, and render 2 triangles that exactly cover screen.</li>
</ol>
<h1 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h1>
<h2 id="grid-block-and-thread"><a class="markdownIt-Anchor" href="#grid-block-and-thread"></a> Grid, Block and Thread</h2>
<ol>
<li>CUDA thread IDs can be up to 3-dimensional. Multiple threads make up a block, and multiple blocks make up a grid.<br />
Each kernel has a grid. All the the blocks in a grid form a 3D matrix, and all the threads in a block also form a 3D matrix.<br />
We can get information about the shape of threads in a block matrix or block in a grid matrix in a block with <code>block_dim</code> and <code>grid_dim</code></li>
<li>When launching the CUDA threads, we need to specify the size of blocks and threads with <code>kernel_function&lt;&lt;&lt;block_dim, thread_dim&gt;&gt;&gt;(parameters)</code>.<br />
Here <code>block_dim</code> and <code>thread_dim</code> can be either a <code>dim3</code> value or a number of the total number of blocks and threads per-block to make the CUDA compiler figure out how to arrange blocks and threads.</li>
<li>So the coordinate to declare or locate a block in a grid, or a thread in a block is 3D. In the CUDA code, we can get the information about current block or thread with <code>blockIdx</code>, <code>threadIdx</code>, and their properties <code>x</code>, <code>y</code>, <code>z</code>.</li>
<li>The calculation object of CUDA is usually matrices. So we would prefer to cut matrices into blocks. Each CUDA block calculate one  block in matrices, and each thread calculate one element in matrices.</li>
<li>In <code>pthread</code>, there is stack space for thread, and need to allocate control block so OS can schedule thread.<br />
Unlike <code>pthread</code>, CUDA control those instances by thread blocks. If control by threads, there will be too many to control.</li>
<li>Major CUDA assumption: thread block execution can be carried out in any order (no dependencies between blocks)</li>
</ol>
<h2 id="kernel-function"><a class="markdownIt-Anchor" href="#kernel-function"></a> Kernel function</h2>
<ol>
<li>“Host” code: serial execution runs as part of normal C/C++ application on CPU<br />
“CUDA device” code: a kernel function runs on GPU, which is denoted by <code>__global__</code> before the definition of the kernel function.<br />
Device function: SPMD execution on GPU, which is denoted by <code>__device__</code>. These functions are called by kernels, and don’t generate new threads.<br />
Kernels and device functions only reference device memory. Host code can only reference host memory, but can have pointers to device memory.</li>
<li>In kernel function, we need to get the indices of the element a thread calculating. For a 2D matrix, we usually take <code>x</code> as column direction, and <code>y</code> as row direction.<br />
So to access <code>A[i][j]</code>, <code>i = blockIdx.y * blockDim * y + threadIdx.y</code> and <code>j = blockIdx.x * blockDim.x + threadIdx.x</code>.</li>
<li>Number of kernel invocations is not determined by size of data collection. So normally we want to do a test before accessing the vector values (array values). The overhead of the test can be ignored, although it does cause some threads are not used.</li>
<li><code>__syncthreads()</code> is a barrier that wait for all threads in the block to arrive at this point.<br />
Atomic operations on both global memory and shared memory variables is also provided, but they are very expensive, should only be used as the last resort.<br />
There is an implicit barrier across all threads at return of kernel.</li>
<li>A compiled CUDA device binary includes program text (instructions) and information about required resources (how many threads per block, how much space per thread, how much shared space per thread block)</li>
</ol>
<h2 id="memory-model"><a class="markdownIt-Anchor" href="#memory-model"></a> Memory Model</h2>
<ol>
<li>Host and device have distinct address spaces, so before the execution, we need to copy data into CUDA memory.<br />
<code>cudaMalloc(ptr, size)</code> can be used to allocate CUDA memory, and <code>cudaFree(ptr)</code> can free CUDA memory. The pointer in <code>cudaMalloc</code> can just be a host variable, but the pointer in <code>cudaFree</code> must be allocated by <code>cudaMalloc</code>.<br />
<code>cudaMemcpy(dest, src, size, kind)</code> can copy those data into CUDA memory. <code>kind</code> is the direction of copy, which can be <code>cudaMemcpyHostToDevice</code> or <code>curdaMemcpyDeviceToHost</code>.</li>
<li>There are three distinct types of memory visible to kernels, per-thread private memory, per-block shared memory, and device global memory.</li>
<li><code>cudaMalloc</code>, <code>cudaMemcpy</code> and <code>cudaFree</code> all operate on device global memory, and those local variables in kernel function are in perthread private memory.<br />
We can declare variables in per-block shared memory with <code>__share__</code>.</li>
<li>If multiple threads in a block need to access a common memory, they will all read that memory once. And if that memory is in the global memory, it would be really slow.</li>
<li>To avoid such efficient decrease, we can copy the common memory into the per-block shared memory. So we only need to access global memory once, and later accesses only in per-block shared memory.<br />
We just need to declare a <code>__share__</code> array, and assign it with values in global memory.</li>
<li>All threads copy data from global memory to per-block shared memory asynchronous, so we better ass a <code>_syncthread()</code> to  make sure later operations only start when all data have been copied.</li>
</ol>
<h2 id="hardware-implementation"><a class="markdownIt-Anchor" href="#hardware-implementation"></a> Hardware implementation</h2>
<ol>
<li>Those synchronization within a warp and scheduling are done by hardware, and programmers don’t need to worry about these things.</li>
<li>GPU implementation maps thread blocks (“work”) to cores using a dynamic scheduling policy that respects resource requirements.</li>
<li>In GPU implementation (not a CUDA abstraction), each SM has multiple groups of warps. However, the number of warp execution contexts is far more than the number of warps.<br />
And there is a warp selector for each warp to choose the instruction to be executed by that warp. Each warp selector usually has two (or more) Fectch/Decoder unit.</li>
<li>Shared per-block memory and L1-cache are inside each SM, but L2-cache is shared by all SMs.<br />
The devide global memory (GPU memory) only communicates with blocks through L2-cache.</li>
<li>Each clock, an SM will choose several warp contexts to fill all warps to use thread-level pallelism, and a warp will choose several instructions to fill all Fetch/Decoder units to use instruction-level pallelism.</li>
<li>When running a CUDA program, GPU work schedulor will map one block to multiple warps in the same SM core. CUDA threads numbered within block in row-major order.<br />
Inside a single thread block is SPMD shared address space programming.</li>
<li>When single warp accesses consecutive memory locations, do block read or write. When single warp accesses separated memory locations, requires gather (read) or scatter(write)</li>
<li>One SM core may have multiple blocks, so a SMM core is capable of concurrently executing multiple CUDA thread blocks.<br />
When all threads in block complete, block resources (shared memory allocations, warp execution contexts) become available for next block.</li>
<li>The CUDA program is not compiled to SIMD instructions like ISPC gangs. GPU hardware is dynamically checking whether 32 independent CUDA threads share an instruction, and if this is true, it executes all 32 threads in a SIMD manner, or performance can suer due to divergent execution.</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Parallelism/" rel="tag"><i class="fa fa-tag"></i> Parallelism</a>
              <a href="/tags/CUDA/" rel="tag"><i class="fa fa-tag"></i> CUDA</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/" rel="prev" title="04. Work Distribution and Scheduling">
                  <i class="fa fa-chevron-left"></i> 04. Work Distribution and Scheduling
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/05/21/Courses/CS149/06-Locality-Communication-and-Contention/" rel="next" title="06. Locality, Communication, and Contention">
                  06. Locality, Communication, and Contention <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
