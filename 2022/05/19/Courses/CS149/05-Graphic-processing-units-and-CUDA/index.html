<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liyun-zhang.github.io","root":"/","images":"/images","scheme":"Mist","darkmode":true,"version":"8.17.0","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Graphics  The first step to drawing a graphic on the screen is to describe the manipulated things (key entities), whose surface is represented as a 3D triangle mesh. So, the input of the calculating">
<meta property="og:type" content="article">
<meta property="og:title" content="05. Graphic processing units and CUDA">
<meta property="og:url" content="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/index.html">
<meta property="og:site_name" content="LiyunZhang">
<meta property="og:description" content="Graphics  The first step to drawing a graphic on the screen is to describe the manipulated things (key entities), whose surface is represented as a 3D triangle mesh. So, the input of the calculating">
<meta property="og:locale">
<meta property="article:published_time" content="2022-05-19T01:59:09.000Z">
<meta property="article:modified_time" content="2024-03-16T11:22:24.981Z">
<meta property="article:author" content="LiyunZhang">
<meta property="article:tag" content="Parallelism">
<meta property="article:tag" content="CUDA">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/","path":"2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/","title":"05. Graphic processing units and CUDA"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>05. Graphic processing units and CUDA | LiyunZhang</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">LiyunZhang</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#graphics"><span class="nav-number">1.</span> <span class="nav-text"> Graphics</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cuda"><span class="nav-number">2.</span> <span class="nav-text"> CUDA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#grid-block-and-thread"><span class="nav-number">2.1.</span> <span class="nav-text"> Grid, Block, and Thread</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kernel-function"><span class="nav-number">2.2.</span> <span class="nav-text"> Kernel function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#memory-model"><span class="nav-number">2.3.</span> <span class="nav-text"> Memory Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hardware-implementation"><span class="nav-number">2.4.</span> <span class="nav-text"> Hardware implementation</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">64</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">

  
  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://liyun-zhang.github.io/2022/05/19/Courses/CS149/05-Graphic-processing-units-and-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="LiyunZhang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="05. Graphic processing units and CUDA | LiyunZhang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          05. Graphic processing units and CUDA
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-05-19 09:59:09" itemprop="dateCreated datePublished" datetime="2022-05-19T09:59:09+08:00">2022-05-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-03-16 19:22:24" itemprop="dateModified" datetime="2024-03-16T19:22:24+08:00">2024-03-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/" itemprop="url" rel="index"><span itemprop="name">Open Courses</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Open-Courses/CMU-15-418-Stanford-CS149-Parallel-Computing/" itemprop="url" rel="index"><span itemprop="name">CMU 15-418 / Stanford CS149 Parallel Computing</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="graphics"><a class="markdownIt-Anchor" href="#graphics"></a> Graphics</h1>
<ol>
<li>The first step to drawing a graphic on the screen is to describe the manipulated things (key entities), whose surface is represented as a 3D triangle mesh.<br />
So, the input of the calculating system is a list of vertices in 3D space and their connectivity to primitives.</li>
<li>The operations of the system are as follows:<br />
Given a scene camera position, compute where the vertices lie on the screen<br />
Group vertices into primitives<br />
Generate one fragment for each pixel a primitive overlaps<br />
Compute the color of the primitive for each fragment based on scene lighting and primitive material properties<br />
Put the color of the “closest fragment” to the camera in the output image</li>
<li>We can Abstract the process of rendering a picture as a sequence of operations on vertices, primitives, fragments, and pixels.<br />
GPUs are very fast processors for performing the same computation (shader programs) on large collections of data (streams of vertices, fragments, and pixels), which sounds like data-parallelism.</li>
<li>To do GPU-based scientific computation, we need to set the OpenGL output image size to the output array size and render two triangles that exactly cover the screen.</li>
</ol>
<h1 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h1>
<h2 id="grid-block-and-thread"><a class="markdownIt-Anchor" href="#grid-block-and-thread"></a> Grid, Block, and Thread</h2>
<ol>
<li>CUDA thread IDs can be up to 3-dimensional. Multiple threads make up a block, and multiple blocks make up a grid.<br />
Each kernel has a grid. All the blocks in a grid form a 3D matrix, and all the threads in a block also form a 3D matrix.<br />
We can get information about the shape of threads in a block matrix or block in a grid matrix in a block with <code>block_dim</code> and <code>grid_dim</code></li>
<li>When launching the CUDA threads, we need to specify the size of blocks and threads with <code>kernel_function&lt;&lt;&lt;block_dim, thread_dim&gt;&gt;&gt;(parameters)</code>.<br />
Here, block_dim and thread_dim can be either a dim3 value or a number of the total number of blocks and threads per block to make the CUDA compiler figure out how to arrange blocks and threads.</li>
<li>So, the coordinates for declaring or locating a block in a grid or a thread in a block are in 3D. In the CUDA code, we can get information about the current block or thread with <code>blockIdx</code> and <code>threadIdx</code> and their properties <code>x</code>, <code>y</code>, and <code>z</code>.</li>
<li>The calculation object of CUDA is usually matrices. So, we prefer to cut matrices into blocks. Each CUDA block calculates one block in matrices, and each thread calculates one element in matrices.</li>
<li>In <code>pthread</code>, there is stack space for the thread, and we need to allocate a control block so OS can schedule the thread.<br />
Unlike <code>pthread</code>, CUDA controls those instances by thread blocks. If controlled by threads, there will be too many to control.</li>
<li>Major CUDA assumption: thread block execution can be carried out in any order (no dependencies between blocks)</li>
</ol>
<h2 id="kernel-function"><a class="markdownIt-Anchor" href="#kernel-function"></a> Kernel function</h2>
<ol>
<li>“Host” code: serial execution runs as part of normal C/C++ application on CPU<br />
“CUDA device” code: a kernel function runs on GPU, denoted by <code>__global__</code> before the definition of the kernel function.<br />
Device function: SPMD execution on GPU, denoted by <code>__device__</code>. Kernels call these functions and don’t generate new threads.<br />
Kernels and device functions only reference device memory. Host code can only reference host memory but can have pointers to device memory.</li>
<li>In the kernel function, we need to get the indices of the element a thread is calculating. For a 2D matrix, we usually take <code>x</code> as the column direction and y as the row direction.<br />
So to access <code>A[i][j]</code>, <code>i = blockIdx.y * blockDim * y + threadIdx.y</code> and <code>j = blockIdx.x * blockDim.x + threadIdx.x</code>.</li>
<li>The data collection size does not determine the number of kernel invocations. So, normally, we want to do a test before accessing the vector values (array values). The overhead of the test can be ignored, although it does cause some threads not to be used.</li>
<li><code>__syncthreads()</code> is a barrier that waits for all threads in the block to arrive at this point.<br />
Atomic operations on global memory and shared memory variables are also provided, but they are costly and should only be used as a last resort.<br />
There is an implicit barrier across all threads at the return of the kernel.</li>
<li>A compiled CUDA device binary includes program text (instructions) and information about required resources (how many threads per block, how much space per thread, how much shared-space per thread block)</li>
</ol>
<h2 id="memory-model"><a class="markdownIt-Anchor" href="#memory-model"></a> Memory Model</h2>
<ol>
<li>The host and device have distinct address spaces, so we need to copy data into CUDA memory before executing.<br />
<code>cudaMalloc(ptr, size)</code> can allocate CUDA memory, and <code>cudaFree(ptr)</code> can free CUDA memory. The pointer in <code>cudaMalloc</code> can be a host variable, but the pointer in <code>cudaFree</code> must be allocated by <code>cudaMalloc</code>.<br />
<code>cudaMemcpy(dest, src, size, kind)</code> can copy those data into CUDA memory. The kind of direction of copy can be <code>cudaMemcpyHostToDevice</code> or <code>curdaMemcpyDeviceToHost</code>.</li>
<li>Three distinct types of memory visible to kernels are per-thread private, per-block shared, and device global memory.</li>
<li><code>cudaMalloc</code>, <code>cudaMemcpy</code>, and cudaFree operate on the device’s global memory, and those local variables in the kernel function are in per-thread private memory.<br />
We can declare variables in per-block shared memory with <code>__share__</code>.</li>
<li>If multiple threads in a block need to access a common memory, they will all read it once. And if that memory is in the global memory, it would be really slow.</li>
<li>We can copy the common memory into the per-block shared memory to avoid such an efficient decrease. So we only need to access global memory once, and later accesses only in per-block shared memory.<br />
We need to declare a <code>__share__</code> array and assign it to values in global memory.</li>
<li>All threads copy data from global memory to per-block shared memory asynchronous, so we better ass a <code>_syncthread()</code> to make sure later operations only start when all data have been copied.</li>
</ol>
<h2 id="hardware-implementation"><a class="markdownIt-Anchor" href="#hardware-implementation"></a> Hardware implementation</h2>
<ol>
<li>Those synchronizations within a warp and scheduling are done by hardware, and programmers don’t need to worry about these things.</li>
<li>GPU implementation maps thread blocks (“work”) to cores using a dynamic scheduling policy that respects resource requirements.</li>
<li>In GPU implementation (not a CUDA abstraction), each SM has multiple groups of warps. However, the number of warp execution contexts is far more than the number of warps.<br />
There is a warp selector for each warp to choose the instruction to be executed by that warp. Each warp selector usually has two (or more) Fectch/Decoder units.</li>
<li>Shared per-block memory and L1-cache are inside each SM, but L2-cache is shared by all SMs.<br />
The device-global memory (GPU memory) only communicates with blocks through the L2 cache.</li>
<li>For each clock, an SM will choose several warp contexts to fill all warps to use thread-level parallelism, and a warp will choose several instructions to fill all Fetch/Decoder units to use instruction-level parallelism.</li>
<li>When running a CUDA program, the GPU work scheduler will map one block to multiple warps in the same SM core. CUDA threads are numbered within a block in row-major order.<br />
Inside a single thread block is SPMD shared address space programming.</li>
<li>When a single warp accesses consecutive memory locations, do block read or write. When single warp accesses separated memory locations, it requires gather (read) or scatter(write)</li>
<li>One SM core may have multiple blocks, so an SMM core can concurrently execute multiple CUDA thread blocks.<br />
When all threads in the block are complete, block resources (shared memory allocations, warp execution contexts) become available for the next block.</li>
<li>The CUDA program is not compiled into SIMD instructions like ISPC gangs. GPU hardware dynamically checks whether 32 independent CUDA threads share an instruction. If true, it executes all 32 threads in a SIMD manner, or performance can suffer due to divergent execution.</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Parallelism/" rel="tag"><i class="fa fa-tag"></i> Parallelism</a>
              <a href="/tags/CUDA/" rel="tag"><i class="fa fa-tag"></i> CUDA</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/05/17/Courses/CS149/04-Work-Distribution-and-Scheduling/" rel="prev" title="04. Work Distribution and Scheduling">
                  <i class="fa fa-chevron-left"></i> 04. Work Distribution and Scheduling
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/05/21/Courses/CS149/06-Locality-Communication-and-Contention/" rel="next" title="06. Locality, Communication, and Contention">
                  06. Locality, Communication, and Contention <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">LiyunZhang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>







  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css" integrity="sha256-hLTCMFlKxdNgPXyWlSSxYN0ykJmxxq9Yt3MNfdRGWeA=" crossorigin="anonymous">



</body>
</html>
